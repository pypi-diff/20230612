# Comparing `tmp/power_grid_model-1.5.0rc9166510100968-py3-none-win_amd64.whl.zip` & `tmp/power_grid_model-1.5.0rc9171103440397-py3-none-macosx_10_9_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,26 +1,26 @@
-Zip file size: 368455 bytes, number of entries: 24
--rw-rw-rw-  2.0 fat      526 b- defN 23-Apr-26 14:06 power_grid_model/__init__.py
--rw-rw-rw-  2.0 fat     5404 b- defN 23-Apr-26 14:06 power_grid_model/data_types.py
--rw-rw-rw-  2.0 fat     2293 b- defN 23-Apr-26 14:06 power_grid_model/enum.py
--rw-rw-rw-  2.0 fat      533 b- defN 23-Apr-26 14:06 power_grid_model/errors.py
--rw-rw-rw-  2.0 fat    22198 b- defN 23-Apr-26 14:06 power_grid_model/utils.py
--rw-rw-rw-  2.0 fat      157 b- defN 23-Apr-26 14:06 power_grid_model/core/__init__.py
--rw-rw-rw-  2.0 fat   912384 b- defN 23-Apr-26 14:17 power_grid_model/core/_power_grid_core.dll
--rw-rw-rw-  2.0 fat     2168 b- defN 23-Apr-26 14:06 power_grid_model/core/error_handling.py
--rw-rw-rw-  2.0 fat      384 b- defN 23-Apr-26 14:06 power_grid_model/core/index_integer.py
--rw-rw-rw-  2.0 fat     1750 b- defN 23-Apr-26 14:06 power_grid_model/core/options.py
--rw-rw-rw-  2.0 fat    10321 b- defN 23-Apr-26 14:06 power_grid_model/core/power_grid_core.py
--rw-rw-rw-  2.0 fat     9525 b- defN 23-Apr-26 14:06 power_grid_model/core/power_grid_meta.py
--rw-rw-rw-  2.0 fat    18183 b- defN 23-Apr-26 14:06 power_grid_model/core/power_grid_model.py
--rw-rw-rw-  2.0 fat      557 b- defN 23-Apr-26 14:06 power_grid_model/validation/__init__.py
--rw-rw-rw-  2.0 fat     4260 b- defN 23-Apr-26 14:06 power_grid_model/validation/assertions.py
--rw-rw-rw-  2.0 fat    15124 b- defN 23-Apr-26 14:06 power_grid_model/validation/errors.py
--rw-rw-rw-  2.0 fat    31381 b- defN 23-Apr-26 14:06 power_grid_model/validation/rules.py
--rw-rw-rw-  2.0 fat     8815 b- defN 23-Apr-26 14:06 power_grid_model/validation/utils.py
--rw-rw-rw-  2.0 fat    29299 b- defN 23-Apr-26 14:06 power_grid_model/validation/validation.py
--rw-rw-rw-  2.0 fat    15199 b- defN 23-Apr-26 14:17 power_grid_model-1.5.0rc9166510100968.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     6947 b- defN 23-Apr-26 14:17 power_grid_model-1.5.0rc9166510100968.dist-info/METADATA
--rw-rw-rw-  2.0 fat       99 b- defN 23-Apr-26 14:17 power_grid_model-1.5.0rc9166510100968.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       17 b- defN 23-Apr-26 14:14 power_grid_model-1.5.0rc9166510100968.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     2261 b- defN 23-Apr-26 14:17 power_grid_model-1.5.0rc9166510100968.dist-info/RECORD
-24 files, 1099785 bytes uncompressed, 364715 bytes compressed:  66.8%
+Zip file size: 404833 bytes, number of entries: 24
+-rw-r--r--  2.0 unx      509 b- defN 23-May-12 11:49 power_grid_model/__init__.py
+-rw-r--r--  2.0 unx     5239 b- defN 23-May-12 11:49 power_grid_model/data_types.py
+-rw-r--r--  2.0 unx    21645 b- defN 23-May-12 11:49 power_grid_model/utils.py
+-rw-r--r--  2.0 unx      505 b- defN 23-May-12 11:49 power_grid_model/errors.py
+-rw-r--r--  2.0 unx     2183 b- defN 23-May-12 11:49 power_grid_model/enum.py
+-rw-r--r--  2.0 unx     1682 b- defN 23-May-12 11:49 power_grid_model/core/options.py
+-rw-r--r--  2.0 unx     9994 b- defN 23-May-12 11:49 power_grid_model/core/power_grid_core.py
+-rw-r--r--  2.0 unx    17761 b- defN 23-May-12 11:49 power_grid_model/core/power_grid_model.py
+-rw-r--r--  2.0 unx      154 b- defN 23-May-12 11:49 power_grid_model/core/__init__.py
+-rw-r--r--  2.0 unx     2100 b- defN 23-May-12 11:49 power_grid_model/core/error_handling.py
+-rwxr-xr-x  2.0 unx  1529096 b- defN 23-May-12 11:53 power_grid_model/core/_power_grid_core.so
+-rw-r--r--  2.0 unx     9240 b- defN 23-May-12 11:49 power_grid_model/core/power_grid_meta.py
+-rw-r--r--  2.0 unx      367 b- defN 23-May-12 11:49 power_grid_model/core/index_integer.py
+-rw-r--r--  2.0 unx      547 b- defN 23-May-12 11:49 power_grid_model/validation/__init__.py
+-rw-r--r--  2.0 unx    30704 b- defN 23-May-12 11:49 power_grid_model/validation/rules.py
+-rw-r--r--  2.0 unx     4166 b- defN 23-May-12 11:49 power_grid_model/validation/assertions.py
+-rw-r--r--  2.0 unx     8601 b- defN 23-May-12 11:49 power_grid_model/validation/utils.py
+-rw-r--r--  2.0 unx    14686 b- defN 23-May-12 11:49 power_grid_model/validation/errors.py
+-rw-r--r--  2.0 unx    28625 b- defN 23-May-12 11:49 power_grid_model/validation/validation.py
+-rw-rw-r--  2.0 unx     2285 b- defN 23-May-12 11:53 power_grid_model-1.5.0rc9171103440397.dist-info/RECORD
+-rw-r--r--  2.0 unx    14907 b- defN 23-May-12 11:53 power_grid_model-1.5.0rc9171103440397.dist-info/LICENSE
+-rw-r--r--  2.0 unx      108 b- defN 23-May-12 11:53 power_grid_model-1.5.0rc9171103440397.dist-info/WHEEL
+-rw-r--r--  2.0 unx       17 b- defN 23-May-12 11:52 power_grid_model-1.5.0rc9171103440397.dist-info/top_level.txt
+-rw-r--r--  2.0 unx     7048 b- defN 23-May-12 11:53 power_grid_model-1.5.0rc9171103440397.dist-info/METADATA
+24 files, 1712169 bytes uncompressed, 401095 bytes compressed:  76.6%
```

## zipnote {}

```diff
@@ -1,73 +1,73 @@
 Filename: power_grid_model/__init__.py
 Comment: 
 
 Filename: power_grid_model/data_types.py
 Comment: 
 
-Filename: power_grid_model/enum.py
+Filename: power_grid_model/utils.py
 Comment: 
 
 Filename: power_grid_model/errors.py
 Comment: 
 
-Filename: power_grid_model/utils.py
+Filename: power_grid_model/enum.py
 Comment: 
 
-Filename: power_grid_model/core/__init__.py
+Filename: power_grid_model/core/options.py
 Comment: 
 
-Filename: power_grid_model/core/_power_grid_core.dll
+Filename: power_grid_model/core/power_grid_core.py
 Comment: 
 
-Filename: power_grid_model/core/error_handling.py
+Filename: power_grid_model/core/power_grid_model.py
 Comment: 
 
-Filename: power_grid_model/core/index_integer.py
+Filename: power_grid_model/core/__init__.py
 Comment: 
 
-Filename: power_grid_model/core/options.py
+Filename: power_grid_model/core/error_handling.py
 Comment: 
 
-Filename: power_grid_model/core/power_grid_core.py
+Filename: power_grid_model/core/_power_grid_core.so
 Comment: 
 
 Filename: power_grid_model/core/power_grid_meta.py
 Comment: 
 
-Filename: power_grid_model/core/power_grid_model.py
+Filename: power_grid_model/core/index_integer.py
 Comment: 
 
 Filename: power_grid_model/validation/__init__.py
 Comment: 
 
-Filename: power_grid_model/validation/assertions.py
+Filename: power_grid_model/validation/rules.py
 Comment: 
 
-Filename: power_grid_model/validation/errors.py
+Filename: power_grid_model/validation/assertions.py
 Comment: 
 
-Filename: power_grid_model/validation/rules.py
+Filename: power_grid_model/validation/utils.py
 Comment: 
 
-Filename: power_grid_model/validation/utils.py
+Filename: power_grid_model/validation/errors.py
 Comment: 
 
 Filename: power_grid_model/validation/validation.py
 Comment: 
 
-Filename: power_grid_model-1.5.0rc9166510100968.dist-info/LICENSE
+Filename: power_grid_model-1.5.0rc9171103440397.dist-info/RECORD
 Comment: 
 
-Filename: power_grid_model-1.5.0rc9166510100968.dist-info/METADATA
+Filename: power_grid_model-1.5.0rc9171103440397.dist-info/LICENSE
 Comment: 
 
-Filename: power_grid_model-1.5.0rc9166510100968.dist-info/WHEEL
+Filename: power_grid_model-1.5.0rc9171103440397.dist-info/WHEEL
 Comment: 
 
-Filename: power_grid_model-1.5.0rc9166510100968.dist-info/top_level.txt
+Filename: power_grid_model-1.5.0rc9171103440397.dist-info/top_level.txt
 Comment: 
 
-Filename: power_grid_model-1.5.0rc9166510100968.dist-info/RECORD
+Filename: power_grid_model-1.5.0rc9171103440397.dist-info/METADATA
 Comment: 
 
 Zip file comment:
```

## power_grid_model/__init__.py

 * *Ordering differences only*

```diff
@@ -1,17 +1,17 @@
-# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
-#
-# SPDX-License-Identifier: MPL-2.0
-
-"""Power Grid Model"""
-
-from power_grid_model.core.power_grid_meta import initialize_array, power_grid_meta_data
-from power_grid_model.core.power_grid_model import PowerGridModel
-from power_grid_model.enum import (
-    Branch3Side,
-    BranchSide,
-    CalculationMethod,
-    CalculationType,
-    LoadGenType,
-    MeasuredTerminalType,
-    WindingType,
-)
+# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
+#
+# SPDX-License-Identifier: MPL-2.0
+
+"""Power Grid Model"""
+
+from power_grid_model.core.power_grid_meta import initialize_array, power_grid_meta_data
+from power_grid_model.core.power_grid_model import PowerGridModel
+from power_grid_model.enum import (
+    Branch3Side,
+    BranchSide,
+    CalculationMethod,
+    CalculationType,
+    LoadGenType,
+    MeasuredTerminalType,
+    WindingType,
+)
```

## power_grid_model/data_types.py

 * *Ordering differences only*

```diff
@@ -1,165 +1,165 @@
-# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
-#
-# SPDX-License-Identifier: MPL-2.0
-"""
-Many data types are used throughout the power grid model project. In an attempt to clarify type hints, some types
-have been defined and explained in this file
-"""
-
-from typing import Dict, List, Tuple, Union
-
-import numpy as np
-
-# When we're dropping python 3.8, we should introduce proper NumPy type hinting
-
-SparseBatchArray = Dict[str, np.ndarray]
-"""
-A sparse batch array is a dictionary containing the keys "indptr" and "data".
-    indptr: a one-dimensional numpy int32 array
-    data: a one-dimensional structured numpy array. The exact dtype depends on the type component.
-
-Example: {"indptr": <1d-array>, "data": <1d-array>}
-"""
-
-BatchArray = Union[np.ndarray, SparseBatchArray]
-"""
-A batch is a either a dense or a sparse batch array
-
-Examples:
-    dense:  <2d-array>
-    sparse: {"indptr": <1d-array>, "data": <1d-array>}
-"""
-
-SingleDataset = Dict[str, np.ndarray]
-"""
-A single dataset is a dictionary where the keys are the component types and the values are one-dimensional
-structured numpy arrays.
-
-Example: {"node": <1d-array>, "line": <1d-array>}
-"""
-
-BatchDataset = Dict[str, BatchArray]
-"""
-A batch dataset is a dictionary where the keys are the component types and the values are either two-dimensional
-structured numpy arrays (dense batch array) or dictionaries with an indptr and a one-dimensional structured numpy
-array (sparse batch array).
-
-Example: {"node": <2d-array>, "line": {"indptr": <1d-array>, "data": <1d-array>}}
-"""
-
-Dataset = Union[SingleDataset, BatchDataset]
-"""
-A general data set can be a single or a batch dataset.
-
-Examples:
-    single: {"node": <1d-array>, "line": <1d-array>}
-    batch:  {"node": <2d-array>, "line": {"indptr": <1d-array>, "data": <1d-array>}}
-
-"""
-
-BatchList = List[SingleDataset]
-"""
-A batch list is an alternative representation of a batch. It is a list of single datasets, where each single dataset
-is actually a batch. The batch list is intended as an intermediate data type, during conversions.
-
-Example: [{"node": <1d-array>, "line": <1d-array>}, {"node": <1d-array>, "line": <1d-array>}]
-"""
-
-NominalValue = int
-"""
-Nominal values can be IDs, booleans, enums, tap pos
-
-Example: 123
-"""
-
-RealValue = float
-"""
-Symmetrical values can be anything like cable properties, symmetric loads, etc.
-
-Example: 10500.0
-"""
-
-AsymValue = Tuple[RealValue, RealValue, RealValue]
-"""
-Asymmetrical values are three-phase values like p or u_measured.
-
-Example: (10400.0, 10500.0, 10600.0)
-"""
-
-AttributeValue = Union[RealValue, NominalValue, AsymValue]
-"""
-When representing a grid as a native python structure, each attribute (u_rated etc) is either a nominal value,
-a real value, or a tuple of three real values.
-
-Examples:
-    real:    10500.0
-    nominal: 123
-    asym:    (10400.0, 10500.0, 10600.0)
-"""
-
-Component = Dict[str, Union[AttributeValue, str]]
-"""
-A component, when represented in native python format, is a dictionary, where the keys are the attributes and the values
-are the corresponding values. It is allowed to add extra fields, containing either an AttributeValue or a string.
-
-Example: {"id": 1, "u_rated": 10500.0, "original_id": "Busbar #1"}
-"""
-
-ComponentList = List[Component]
-"""
-A component list is a list containing components. In essence it stores the same information as a np.ndarray,
-but in a native python format, without using numpy.
-
-Example: [{"id": 1, "u_rated": 10500.0}, {"id": 2, "u_rated": 10500.0}]
-"""
-
-SinglePythonDataset = Dict[str, ComponentList]
-"""
-A single dataset in native python representation is a dictionary, where the keys are the component names and the
-values are a list of all the instances of such a component. In essence it stores the same information as a
-SingleDataset, but in a native python format, without using numpy.
-
-Example:
-    {
-        "node": [{"id": 1, "u_rated": 10500.0}, {"id": 2, "u_rated": 10500.0}],
-        "line": [{"id": 3, "from_node": 1, "to_node": 2, ...}],
-    }
-"""
-
-BatchPythonDataset = List[SinglePythonDataset]
-"""
-A batch dataset in native python representation is a list of dictionaries, where the keys are the component names and
-the values are a list of all the instances of such a component. In essence it stores the same information as a
-BatchDataset, but in a native python format, without using numpy. Actually it looks more like the BatchList.
-
-Example:
-    [
-        {
-            "line": [{"id": 3, "from_status": 0, "to_status": 0, ...}],
-        },
-        {
-            "line": [{"id": 3, "from_status": 1, "to_status": 1, ...}],
-        }
-    ]
-"""
-
-PythonDataset = Union[SinglePythonDataset, BatchPythonDataset]
-"""
-A general python data set can be a single or a batch python dataset.
-
-Examples:
-    single:
-        {
-            "node": [{"id": 1, "u_rated": 10500.0}, {"id": 2, "u_rated": 10500.0}],
-            "line": [{"id": 3, "from_node": 1, "to_node": 2, ...}],
-        }
-    batch:
-        [
-            {
-                "line": [{"id": 3, "from_status": 0, "to_status": 0, ...}],
-            },
-            {
-                "line": [{"id": 3, "from_status": 1, "to_status": 1, ...}],
-            }
-        ]
-"""
+# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
+#
+# SPDX-License-Identifier: MPL-2.0
+"""
+Many data types are used throughout the power grid model project. In an attempt to clarify type hints, some types
+have been defined and explained in this file
+"""
+
+from typing import Dict, List, Tuple, Union
+
+import numpy as np
+
+# When we're dropping python 3.8, we should introduce proper NumPy type hinting
+
+SparseBatchArray = Dict[str, np.ndarray]
+"""
+A sparse batch array is a dictionary containing the keys "indptr" and "data".
+    indptr: a one-dimensional numpy int32 array
+    data: a one-dimensional structured numpy array. The exact dtype depends on the type component.
+
+Example: {"indptr": <1d-array>, "data": <1d-array>}
+"""
+
+BatchArray = Union[np.ndarray, SparseBatchArray]
+"""
+A batch is a either a dense or a sparse batch array
+
+Examples:
+    dense:  <2d-array>
+    sparse: {"indptr": <1d-array>, "data": <1d-array>}
+"""
+
+SingleDataset = Dict[str, np.ndarray]
+"""
+A single dataset is a dictionary where the keys are the component types and the values are one-dimensional
+structured numpy arrays.
+
+Example: {"node": <1d-array>, "line": <1d-array>}
+"""
+
+BatchDataset = Dict[str, BatchArray]
+"""
+A batch dataset is a dictionary where the keys are the component types and the values are either two-dimensional
+structured numpy arrays (dense batch array) or dictionaries with an indptr and a one-dimensional structured numpy
+array (sparse batch array).
+
+Example: {"node": <2d-array>, "line": {"indptr": <1d-array>, "data": <1d-array>}}
+"""
+
+Dataset = Union[SingleDataset, BatchDataset]
+"""
+A general data set can be a single or a batch dataset.
+
+Examples:
+    single: {"node": <1d-array>, "line": <1d-array>}
+    batch:  {"node": <2d-array>, "line": {"indptr": <1d-array>, "data": <1d-array>}}
+
+"""
+
+BatchList = List[SingleDataset]
+"""
+A batch list is an alternative representation of a batch. It is a list of single datasets, where each single dataset
+is actually a batch. The batch list is intended as an intermediate data type, during conversions.
+
+Example: [{"node": <1d-array>, "line": <1d-array>}, {"node": <1d-array>, "line": <1d-array>}]
+"""
+
+NominalValue = int
+"""
+Nominal values can be IDs, booleans, enums, tap pos
+
+Example: 123
+"""
+
+RealValue = float
+"""
+Symmetrical values can be anything like cable properties, symmetric loads, etc.
+
+Example: 10500.0
+"""
+
+AsymValue = Tuple[RealValue, RealValue, RealValue]
+"""
+Asymmetrical values are three-phase values like p or u_measured.
+
+Example: (10400.0, 10500.0, 10600.0)
+"""
+
+AttributeValue = Union[RealValue, NominalValue, AsymValue]
+"""
+When representing a grid as a native python structure, each attribute (u_rated etc) is either a nominal value,
+a real value, or a tuple of three real values.
+
+Examples:
+    real:    10500.0
+    nominal: 123
+    asym:    (10400.0, 10500.0, 10600.0)
+"""
+
+Component = Dict[str, Union[AttributeValue, str]]
+"""
+A component, when represented in native python format, is a dictionary, where the keys are the attributes and the values
+are the corresponding values. It is allowed to add extra fields, containing either an AttributeValue or a string.
+
+Example: {"id": 1, "u_rated": 10500.0, "original_id": "Busbar #1"}
+"""
+
+ComponentList = List[Component]
+"""
+A component list is a list containing components. In essence it stores the same information as a np.ndarray,
+but in a native python format, without using numpy.
+
+Example: [{"id": 1, "u_rated": 10500.0}, {"id": 2, "u_rated": 10500.0}]
+"""
+
+SinglePythonDataset = Dict[str, ComponentList]
+"""
+A single dataset in native python representation is a dictionary, where the keys are the component names and the
+values are a list of all the instances of such a component. In essence it stores the same information as a
+SingleDataset, but in a native python format, without using numpy.
+
+Example:
+    {
+        "node": [{"id": 1, "u_rated": 10500.0}, {"id": 2, "u_rated": 10500.0}],
+        "line": [{"id": 3, "from_node": 1, "to_node": 2, ...}],
+    }
+"""
+
+BatchPythonDataset = List[SinglePythonDataset]
+"""
+A batch dataset in native python representation is a list of dictionaries, where the keys are the component names and
+the values are a list of all the instances of such a component. In essence it stores the same information as a
+BatchDataset, but in a native python format, without using numpy. Actually it looks more like the BatchList.
+
+Example:
+    [
+        {
+            "line": [{"id": 3, "from_status": 0, "to_status": 0, ...}],
+        },
+        {
+            "line": [{"id": 3, "from_status": 1, "to_status": 1, ...}],
+        }
+    ]
+"""
+
+PythonDataset = Union[SinglePythonDataset, BatchPythonDataset]
+"""
+A general python data set can be a single or a batch python dataset.
+
+Examples:
+    single:
+        {
+            "node": [{"id": 1, "u_rated": 10500.0}, {"id": 2, "u_rated": 10500.0}],
+            "line": [{"id": 3, "from_node": 1, "to_node": 2, ...}],
+        }
+    batch:
+        [
+            {
+                "line": [{"id": 3, "from_status": 0, "to_status": 0, ...}],
+            },
+            {
+                "line": [{"id": 3, "from_status": 1, "to_status": 1, ...}],
+            }
+        ]
+"""
```

## power_grid_model/enum.py

 * *Ordering differences only*

```diff
@@ -1,110 +1,110 @@
-# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
-#
-# SPDX-License-Identifier: MPL-2.0
-
-"""
-Common Enumerations
-
-Note: these enumeration match the C++ arithmetic core, so don't change the values unless you change them in C++ as well
-
-"""
-
-from enum import IntEnum
-
-# Value names are defined in lower case instead of upper case
-# pylint: disable=invalid-name
-
-
-class LoadGenType(IntEnum):
-    """Load and Generator Types"""
-
-    const_power = 0
-    const_impedance = 1
-    const_current = 2
-
-
-class WindingType(IntEnum):
-    """Transformer Winding Types"""
-
-    wye = 0
-    wye_n = 1
-    delta = 2
-    zigzag = 3
-    zigzag_n = 4
-
-
-class BranchSide(IntEnum):
-    """Branch Sides"""
-
-    from_side = 0
-    to_side = 1
-
-
-class Branch3Side(IntEnum):
-    """Branch3 Sides"""
-
-    side_1 = 0
-    side_2 = 1
-    side_3 = 2
-
-
-class CalculationType(IntEnum):
-    """Calculation Types"""
-
-    power_flow = 0
-    state_estimation = 1
-
-
-class CalculationMethod(IntEnum):
-    """Calculation Methods"""
-
-    linear = 0
-    newton_raphson = 1
-    iterative_linear = 2
-    iterative_current = 3
-    linear_current = 4
-
-
-class MeasuredTerminalType(IntEnum):
-    """The type of asset measured by a (power) sensor"""
-
-    branch_from = 0
-    """
-    Measuring the from-terminal between a branch (except link) and a node
-    """
-    branch_to = 1
-    """
-    Measuring the to-terminal between a branch (except link) and a node
-    """
-    source = 2
-    """
-    Measuring the terminal between a source and a node
-    """
-    shunt = 3
-    """
-    Measuring the terminal between a shunt and a node
-    """
-    load = 4
-    """
-    Measuring the terminal between a load and a node
-    """
-    generator = 5
-    """
-    Measuring the terminal between a generator and a node
-    """
-    branch3_1 = 6
-    """
-    Measuring the terminal-1 between a branch3 and a node
-    """
-    branch3_2 = 7
-    """
-    Measuring the terminal-2 between a branch3 and a node
-    """
-    branch3_3 = 8
-    """
-    Measuring the terminal-3 between a branch3 and a node
-    """
-    node = 9
-    """
-    Measuring the total power injection into a node
-    """
+# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
+#
+# SPDX-License-Identifier: MPL-2.0
+
+"""
+Common Enumerations
+
+Note: these enumeration match the C++ arithmetic core, so don't change the values unless you change them in C++ as well
+
+"""
+
+from enum import IntEnum
+
+# Value names are defined in lower case instead of upper case
+# pylint: disable=invalid-name
+
+
+class LoadGenType(IntEnum):
+    """Load and Generator Types"""
+
+    const_power = 0
+    const_impedance = 1
+    const_current = 2
+
+
+class WindingType(IntEnum):
+    """Transformer Winding Types"""
+
+    wye = 0
+    wye_n = 1
+    delta = 2
+    zigzag = 3
+    zigzag_n = 4
+
+
+class BranchSide(IntEnum):
+    """Branch Sides"""
+
+    from_side = 0
+    to_side = 1
+
+
+class Branch3Side(IntEnum):
+    """Branch3 Sides"""
+
+    side_1 = 0
+    side_2 = 1
+    side_3 = 2
+
+
+class CalculationType(IntEnum):
+    """Calculation Types"""
+
+    power_flow = 0
+    state_estimation = 1
+
+
+class CalculationMethod(IntEnum):
+    """Calculation Methods"""
+
+    linear = 0
+    newton_raphson = 1
+    iterative_linear = 2
+    iterative_current = 3
+    linear_current = 4
+
+
+class MeasuredTerminalType(IntEnum):
+    """The type of asset measured by a (power) sensor"""
+
+    branch_from = 0
+    """
+    Measuring the from-terminal between a branch (except link) and a node
+    """
+    branch_to = 1
+    """
+    Measuring the to-terminal between a branch (except link) and a node
+    """
+    source = 2
+    """
+    Measuring the terminal between a source and a node
+    """
+    shunt = 3
+    """
+    Measuring the terminal between a shunt and a node
+    """
+    load = 4
+    """
+    Measuring the terminal between a load and a node
+    """
+    generator = 5
+    """
+    Measuring the terminal between a generator and a node
+    """
+    branch3_1 = 6
+    """
+    Measuring the terminal-1 between a branch3 and a node
+    """
+    branch3_2 = 7
+    """
+    Measuring the terminal-2 between a branch3 and a node
+    """
+    branch3_3 = 8
+    """
+    Measuring the terminal-3 between a branch3 and a node
+    """
+    node = 9
+    """
+    Measuring the total power injection into a node
+    """
```

## power_grid_model/errors.py

 * *Ordering differences only*

```diff
@@ -1,28 +1,28 @@
-# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
-#
-# SPDX-License-Identifier: MPL-2.0
-
-
-"""
-Error classes
-"""
-
-from typing import List
-
-import numpy as np
-
-
-class PowerGridError(RuntimeError):
-    """
-    Generic power grid error
-    """
-
-
-class PowerGridBatchError(PowerGridError):
-    """
-    Error occurs in batch calculation
-    """
-
-    failed_scenarios: np.ndarray
-    succeeded_scenarios: np.ndarray
-    error_messages: List[str]
+# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
+#
+# SPDX-License-Identifier: MPL-2.0
+
+
+"""
+Error classes
+"""
+
+from typing import List
+
+import numpy as np
+
+
+class PowerGridError(RuntimeError):
+    """
+    Generic power grid error
+    """
+
+
+class PowerGridBatchError(PowerGridError):
+    """
+    Error occurs in batch calculation
+    """
+
+    failed_scenarios: np.ndarray
+    succeeded_scenarios: np.ndarray
+    error_messages: List[str]
```

## power_grid_model/utils.py

 * *Ordering differences only*

```diff
@@ -1,553 +1,553 @@
-# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
-#
-# SPDX-License-Identifier: MPL-2.0
-
-"""
-This file contains all the helper functions for testing purpose
-"""
-
-import json
-from pathlib import Path
-from typing import IO, Any, List, Optional, cast
-
-import numpy as np
-
-from power_grid_model import initialize_array
-from power_grid_model.data_types import (
-    BatchArray,
-    BatchDataset,
-    BatchList,
-    ComponentList,
-    Dataset,
-    PythonDataset,
-    SingleDataset,
-    SinglePythonDataset,
-    SparseBatchArray,
-)
-
-
-def is_nan(data) -> bool:
-    """
-    Determine if the data point is valid
-    Args:
-        data: a single scaler or numpy array
-
-    Returns:
-        True if all the data points are invalid
-        False otherwise
-    """
-    nan_func = {
-        np.dtype("f8"): lambda x: np.all(np.isnan(x)),
-        np.dtype("i4"): lambda x: np.all(x == np.iinfo("i4").min),
-        np.dtype("i1"): lambda x: np.all(x == np.iinfo("i1").min),
-    }
-    return bool(nan_func[data.dtype](data))
-
-
-def convert_list_to_batch_data(list_data: BatchList) -> BatchDataset:
-    """
-    Convert a list of datasets to one single batch dataset
-
-    Example data formats:
-        input:  [{"node": <1d-array>, "line": <1d-array>}, {"node": <1d-array>, "line": <1d-array>}]
-        output: {"node": <2d-array>, "line": <2d-array>}
-         -or-:  {"indptr": <1d-array>, "data": <1d-array>}
-    Args:
-        list_data: list of dataset
-
-    Returns:
-        batch dataset
-        For a certain component, if all the length is the same for all the batches, a 2D array is used
-        Otherwise use a dict of indptr/data key
-    """
-
-    # List all *unique* types
-    components = {x for dataset in list_data for x in dataset.keys()}
-
-    batch_data: BatchDataset = {}
-    for component in components:
-        # Create a 2D array if the component exists in all datasets and number of objects is the same in each dataset
-        comp_exists_in_all_datasets = all(component in x for x in list_data)
-        if comp_exists_in_all_datasets:
-            all_sizes_are_the_same = all(x[component].size == list_data[0][component].size for x in list_data)
-            if all_sizes_are_the_same:
-                batch_data[component] = np.stack([x[component] for x in list_data], axis=0)
-                continue
-
-        # otherwise use indptr/data dict
-        indptr = [0]
-        data = []
-        for dataset in list_data:
-            if component in dataset:
-                # If the current dataset contains the component, increase the indptr for this batch and append the data
-                objects = dataset[component]
-                indptr.append(indptr[-1] + len(objects))
-                data.append(objects)
-
-            else:
-                # If the current dataset does not contain the component, add the last indptr again.
-                indptr.append(indptr[-1])
-
-            # Convert the index pointers to a numpy array and combine the list of object numpy arrays into a singe
-            # numpy array. All objects of all batches are now stores in one large array, the index pointers define
-            # which elemets of the array (rows) belong to which batch.
-            batch_data[component] = {"indptr": np.array(indptr, dtype=np.int64), "data": np.concatenate(data, axis=0)}
-
-    return batch_data
-
-
-def convert_python_to_numpy(data: PythonDataset, data_type: str, ignore_extra: bool = False) -> Dataset:
-    """
-    Convert native python data to internal numpy
-    Args:
-        data: data in dict or list
-        data_type: type of data: input, update, sym_output, or asym_output
-        ignore_extra: Allow (and ignore) extra attributes in the data
-
-    Returns:
-        A single or batch dataset for power-grid-model
-
-    """
-
-    # If the input data is a list, we are dealing with batch data. Each element in the list is a batch. We'll
-    # first convert each batch separately, by recursively calling this function for each batch. Then the numpy
-    # data for all batches in converted into a proper and compact numpy structure.
-    if isinstance(data, list):
-        list_data = [
-            convert_python_single_dataset_to_single_dataset(json_dict, data_type=data_type, ignore_extra=ignore_extra)
-            for json_dict in data
-        ]
-        return convert_list_to_batch_data(list_data)
-
-    # Otherwise this should be a normal (non-batch) structure, with a list of objects (dictionaries) per component.
-    if not isinstance(data, dict):
-        raise TypeError("Data should be either a list or a dictionary!")
-
-    return convert_python_single_dataset_to_single_dataset(data=data, data_type=data_type, ignore_extra=ignore_extra)
-
-
-def convert_python_single_dataset_to_single_dataset(
-    data: SinglePythonDataset, data_type: str, ignore_extra: bool = False
-) -> SingleDataset:
-    """
-    Convert native python data to internal numpy
-    Args:
-        data: data in dict
-        data_type: type of data: input, update, sym_output, or asym_output
-        ignore_extra: Allow (and ignore) extra attributes in the data
-
-    Returns:
-        A single dataset for power-grid-model
-
-    """
-
-    dataset: SingleDataset = {}
-    for component, objects in data.items():
-        dataset[component] = convert_component_list_to_numpy(
-            objects=objects, component=component, data_type=data_type, ignore_extra=ignore_extra
-        )
-
-    return dataset
-
-
-def convert_component_list_to_numpy(
-    objects: ComponentList, component: str, data_type: str, ignore_extra: bool = False
-) -> np.ndarray:
-    """
-    Convert native python data to internal numpy
-    Args:
-        objects: data in dict
-        component: the name of the component
-        data_type: type of data: input, update, sym_output, or asym_output
-        ignore_extra: Allow (and ignore) extra attributes in the data
-
-    Returns:
-        A single numpy array
-
-    """
-
-    # We'll initialize an 1d-array with NaN values for all the objects of this component type
-    array = initialize_array(data_type, component, len(objects))
-
-    for i, obj in enumerate(objects):
-        # As each object is a separate dictionary, and the attributes may differ per object, we need to check
-        # all attributes. Non-existing attributes
-        for attribute, value in obj.items():
-            # If an attribute doesn't exist, the user should explicitly state that she/he is ok with extra
-            # information in the data. This is to protect the user from overlooking errors.
-            if attribute not in array.dtype.names:
-                if ignore_extra:
-                    continue
-                raise ValueError(
-                    f"Invalid attribute '{attribute}' for {component} {data_type} data. "
-                    "(Use ignore_extra=True to ignore the extra data and suppress this exception)"
-                )
-
-            # Assign the value and raise an error if the value cannot be stored in the specific numpy array data format
-            # for this attribute.
-            try:
-                array[i][attribute] = value
-            except ValueError as ex:
-                raise ValueError(f"Invalid '{attribute}' value for {component} {data_type} data: {ex}") from ex
-    return array
-
-
-def convert_batch_dataset_to_batch_list(batch_data: BatchDataset) -> BatchList:
-    """
-    Convert batch datasets to a list of individual batches
-    Args:
-        batch_data: a batch dataset for power-grid-model
-    Returns:
-        A list of individual batches
-    """
-
-    # If the batch data is empty, return an empty list
-    if len(batch_data) == 0:
-        return []
-
-    n_batches = get_and_verify_batch_sizes(batch_data=batch_data)
-
-    # Initialize an empty list with dictionaries
-    # Note that [{}] * n_batches would result in n copies of the same dict.
-    list_data: BatchList = [{} for _ in range(n_batches)]
-
-    # While the number of batches must be the same for each component, the structure (2d numpy array or indptr/data)
-    # doesn't have to be. Therefore, we'll check the structure for each component and copy the data accordingly.
-    for component, data in batch_data.items():
-        if isinstance(data, np.ndarray):
-            component_batches = split_numpy_array_in_batches(data, component)
-        elif isinstance(data, dict):
-            component_batches = split_sparse_batches_in_batches(data, component)
-        else:
-            raise TypeError(
-                f"Invalid data type {type(data).__name__} in batch data for '{component}' "
-                "(should be a Numpy structured array or a python dictionary)."
-            )
-        for i, batch in enumerate(component_batches):
-            if batch.size > 0:
-                list_data[i][component] = batch
-    return list_data
-
-
-def get_and_verify_batch_sizes(batch_data: BatchDataset) -> int:
-    """
-    Determine the number of batches for each component and verify that each component has the same number of batches
-    Args:
-        batch_data: a batch dataset for power-grid-model
-
-    Returns:
-        The number of batches
-    """
-
-    n_batch_size = 0
-    checked_components: List[str] = []
-    for component, data in batch_data.items():
-        n_component_batch_size = get_batch_size(data)
-        if checked_components and n_component_batch_size != n_batch_size:
-            if len(checked_components) == 1:
-                checked_components_str = f"'{checked_components.pop()}'"
-            else:
-                checked_components_str = "/".join(sorted(checked_components))
-            raise ValueError(
-                f"Inconsistent number of batches in batch data. "
-                f"Component '{component}' contains {n_component_batch_size} batches, "
-                f"while {checked_components_str} contained {n_batch_size} batches."
-            )
-        n_batch_size = n_component_batch_size
-        checked_components.append(component)
-    return n_batch_size
-
-
-def get_batch_size(batch_data: BatchArray) -> int:
-    """
-    Determine the number of batches and verify the data structure while we're at it.
-    Args:
-        batch_data: a batch array for power-grid-model
-
-    Returns:
-        The number of batches
-    """
-    if isinstance(batch_data, np.ndarray):
-        # We expect the batch data to be a 2d numpy array of n_batches x n_objects. If it is a 1d numpy array instead,
-        # we assume that it is a single batch.
-        if batch_data.ndim == 1:
-            return 1
-        n_batches = batch_data.shape[0]
-    elif isinstance(batch_data, dict):
-        # If the batch data is a dictionary, we assume that it is an indptr/data structure (otherwise it is an
-        # invalid dictionary). There is always one indptr more than there are batches.
-        if "indptr" not in batch_data:
-            raise ValueError("Invalid batch data format, expected 'indptr' and 'data' entries")
-        n_batches = batch_data["indptr"].size - 1
-    else:
-        # If the batch data is not a numpy array and not a dictionary, it is invalid
-        raise ValueError(
-            "Invalid batch data format, expected a 2-d numpy array or a dictionary with an 'indptr' and 'data' entry"
-        )
-    return n_batches
-
-
-def split_numpy_array_in_batches(data: np.ndarray, component: str) -> List[np.ndarray]:
-    """
-    Split a single dense numpy array into one or more batches
-
-    Args:
-        data: A 1D or 2D Numpy structured array. A 1D array is a single table / batch, a 2D array is a batch per table.
-        component: The name of the component to which the data belongs, only used for errors.
-
-    Returns:
-        A list with a single numpy structured array per batch
-
-    """
-    if not isinstance(data, np.ndarray):
-        raise TypeError(
-            f"Invalid data type {type(data).__name__} in batch data for '{component}' "
-            "(should be a 1D/2D Numpy structured array)."
-        )
-    if data.ndim == 1:
-        return [data]
-    if data.ndim == 2:
-        return [data[i, :] for i in range(data.shape[0])]
-    raise TypeError(
-        f"Invalid data dimension {data.ndim} in batch data for '{component}' "
-        "(should be a 1D/2D Numpy structured array)."
-    )
-
-
-def split_sparse_batches_in_batches(batch_data: SparseBatchArray, component: str) -> List[np.ndarray]:
-    """
-    Split a single numpy array representing, a compressed sparse structure, into one or more batches
-
-    Args:
-        batch_data: Sparse batch data
-        component: The name of the component to which the data belongs, only used for errors.
-
-    Returns:
-        A list with a single numpy structured array per batch
-
-    """
-
-    for key in ["indptr", "data"]:
-        if key not in batch_data:
-            raise KeyError(
-                f"Missing '{key}' in sparse batch data for '{component}' "
-                "(expected a python dictionary containing two keys: 'indptr' and 'data')."
-            )
-
-    data = batch_data["data"]
-    indptr = batch_data["indptr"]
-
-    if not isinstance(data, np.ndarray) or data.ndim != 1:
-        raise TypeError(
-            f"Invalid data type {type(data).__name__} in sparse batch data for '{component}' "
-            "(should be a 1D Numpy structured array (i.e. a single 'table'))."
-        )
-
-    if not isinstance(indptr, np.ndarray) or indptr.ndim != 1 or not np.issubdtype(indptr.dtype, np.integer):
-        raise TypeError(
-            f"Invalid indptr data type {type(indptr).__name__} in batch data for '{component}' "
-            "(should be a 1D Numpy array (i.e. a single 'list'), "
-            "containing indices (i.e. integers))."
-        )
-
-    if indptr[0] != 0 or indptr[-1] != len(data) or any(indptr[i] > indptr[i + 1] for i in range(len(indptr) - 1)):
-        raise TypeError(
-            f"Invalid indptr in batch data for '{component}' "
-            f"(should start with 0, end with the number of objects ({len(data)}) "
-            "and be monotonic increasing)."
-        )
-
-    return [data[indptr[i] : indptr[i + 1]] for i in range(len(indptr) - 1)]
-
-
-def convert_dataset_to_python_dataset(data: Dataset) -> PythonDataset:
-    """
-    Convert internal numpy arrays to native python data
-    If an attribute is not available (NaN value), it will not be exported.
-    Args:
-        data: A single or batch dataset for power-grid-model
-    Returns:
-        A python dict for single dataset
-        A python list for batch dataset
-
-    """
-
-    # Check if the dataset is a single dataset or batch dataset
-    # It is batch dataset if it is 2D array or a indptr/data structure
-    is_batch: Optional[bool] = None
-    for component, array in data.items():
-        is_dense_batch = isinstance(array, np.ndarray) and array.ndim == 2
-        is_sparse_batch = isinstance(array, dict) and "indptr" in array and "data" in array
-        if is_batch is not None and is_batch != (is_dense_batch or is_sparse_batch):
-            raise ValueError(
-                f"Mixed {'' if is_batch else 'non-'}batch data "
-                f"with {'non-' if is_batch else ''}batch data ({component})."
-            )
-        is_batch = is_dense_batch or is_sparse_batch
-
-    # If it is a batch, convert the batch data to a list of batches, then convert each batch individually.
-    if is_batch:
-        # We have established that this is batch data, so let's tell the type checker that this is a BatchDataset
-        data = cast(BatchDataset, data)
-        list_data = convert_batch_dataset_to_batch_list(data)
-        return [convert_single_dataset_to_python_single_dataset(data=x) for x in list_data]
-
-    # We have established that this is not batch data, so let's tell the type checker that this is a BatchDataset
-    data = cast(SingleDataset, data)
-    return convert_single_dataset_to_python_single_dataset(data=data)
-
-
-def convert_single_dataset_to_python_single_dataset(data: SingleDataset) -> SinglePythonDataset:
-    """
-    Convert internal numpy arrays to native python data
-    If an attribute is not available (NaN value), it will not be exported.
-    Args:
-        data: A single dataset for power-grid-model
-    Returns:
-        A python dict for single dataset
-    """
-
-    # This should be a single data set
-    for component, array in data.items():
-        if not isinstance(array, np.ndarray) or array.ndim != 1:
-            raise ValueError("Invalid data format")
-
-    # Convert each numpy array to a list of objects, which contains only the non-NaN attributes:
-    # For example: {"node": [{"id": 0, ...}, {"id": 1, ...}], "line": [{"id": 2, ...}]}
-    return {
-        component: [
-            {attribute: obj[attribute].tolist() for attribute in objects.dtype.names if not is_nan(obj[attribute])}
-            for obj in objects
-        ]
-        for component, objects in data.items()
-    }
-
-
-def import_json_data(json_file: Path, data_type: str, ignore_extra: bool = False) -> Dataset:
-    """
-    import json data
-    Args:
-        json_file: path to the json file
-        data_type: type of data: input, update, sym_output, or asym_output
-        ignore_extra: Allow (and ignore) extra attributes in the json file
-
-    Returns:
-         A single or batch dataset for power-grid-model
-    """
-    with open(json_file, mode="r", encoding="utf-8") as file_pointer:
-        data = json.load(file_pointer)
-    return convert_python_to_numpy(data=data, data_type=data_type, ignore_extra=ignore_extra)
-
-
-def import_input_data(json_file: Path) -> SingleDataset:
-    """
-    import input json data
-    Args:
-        json_file: path to the json file
-
-    Returns:
-         A single dataset for power-grid-model
-    """
-    data = import_json_data(json_file=json_file, data_type="input")
-    assert isinstance(data, dict)
-    assert all(isinstance(component, np.ndarray) and component.ndim == 1 for component in data.values())
-    return cast(SingleDataset, data)
-
-
-def import_update_data(json_file: Path) -> BatchDataset:
-    """
-    import update json data
-    Args:
-        json_file: path to the json file
-
-    Returns:
-         A batch dataset for power-grid-model
-    """
-    return cast(BatchDataset, import_json_data(json_file=json_file, data_type="update"))
-
-
-def export_json_data(json_file: Path, data: Dataset, indent: Optional[int] = 2, compact: bool = False):
-    """
-    export json data
-    Args:
-        json_file: path to json file
-        data: a single or batch dataset for power-grid-model
-        indent: indent of the file, default 2
-        compact: write components on a single line
-
-    Returns:
-        Save to file
-    """
-    json_data = convert_dataset_to_python_dataset(data)
-
-    with open(json_file, mode="w", encoding="utf-8") as file_pointer:
-        if compact and indent:
-            is_batch_data = isinstance(json_data, list)
-            max_level = 4 if is_batch_data else 3
-            compact_json_dump(json_data, file_pointer, indent=indent, max_level=max_level)
-        else:
-            json.dump(json_data, file_pointer, indent=indent)
-
-
-def compact_json_dump(data: Any, io_stream: IO[str], indent: int, max_level: int, level: int = 0):
-    """Custom compact JSON writer that is intended to put data belonging to a single object on a single line.
-
-    For example:
-    {
-        "node": [
-            {"id": 0, "u_rated": 10500.0},
-            {"id": 1, "u_rated": 10500.0},
-        ],
-        "line": [
-            {"id": 2, "node_from": 0, "node_to": 1, ...}
-        ]
-    }
-
-    The function is being called recursively, starting at level 0 and recursing until max_level is reached. It is
-    basically a full json writer, but for efficiency reasons, on the last levels the native json.dump method is used.
-    """
-
-    # Let's define a 'tab' indent, depending on the level
-    tab = " " * level * indent
-
-    # If we are at the max_level, or the data simply doesn't contain any more levels, write the indent and serialize
-    # the data on a single line.
-    if level >= max_level or not isinstance(data, (list, dict)):
-        io_stream.write(tab)
-        json.dump(data, io_stream, indent=None)
-        return
-
-    # We'll need the number of objects later on
-    n_obj = len(data)
-
-    # If the data is a list:
-    # 1. start with an opening bracket
-    # 2. dump each element in the list
-    # 3. add a comma and a new line after each element, except for the last element, there we don't need a comma.
-    # 4. finish with a closing bracket
-    if isinstance(data, list):
-        io_stream.write(tab + "[\n")
-        for i, obj in enumerate(data, start=1):
-            compact_json_dump(obj, io_stream, indent, max_level, level + 1)
-            io_stream.write(",\n" if i < n_obj else "\n")
-        io_stream.write(tab + "]")
-        return
-
-    # If the data is a dictionary:
-    # 1. start with an opening curly bracket
-    # 2. for each element: write it's key, plus a colon ':'
-    # 3. if the next level would be the max_level, add a space and dump the element on a single,
-    #    else add a new line before dumping the element recursively.
-    # 4. add a comma and a new line after each element, except for the last element, there we don't need a comma.
-    # 5. finish with a closing curly bracket
-    io_stream.write(tab + "{\n")
-    for i, (key, obj) in enumerate(data.items(), start=1):
-        io_stream.write(tab + " " * indent + f'"{key}":')
-        if level == max_level - 1 or not isinstance(obj, (list, dict)):
-            io_stream.write(" ")
-            json.dump(obj, io_stream, indent=None)
-        else:
-            io_stream.write("\n")
-            compact_json_dump(obj, io_stream, indent, max_level, level + 2)
-        io_stream.write(",\n" if i < n_obj else "\n")
-    io_stream.write(tab + "}\n")
+# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
+#
+# SPDX-License-Identifier: MPL-2.0
+
+"""
+This file contains all the helper functions for testing purpose
+"""
+
+import json
+from pathlib import Path
+from typing import IO, Any, List, Optional, cast
+
+import numpy as np
+
+from power_grid_model import initialize_array
+from power_grid_model.data_types import (
+    BatchArray,
+    BatchDataset,
+    BatchList,
+    ComponentList,
+    Dataset,
+    PythonDataset,
+    SingleDataset,
+    SinglePythonDataset,
+    SparseBatchArray,
+)
+
+
+def is_nan(data) -> bool:
+    """
+    Determine if the data point is valid
+    Args:
+        data: a single scaler or numpy array
+
+    Returns:
+        True if all the data points are invalid
+        False otherwise
+    """
+    nan_func = {
+        np.dtype("f8"): lambda x: np.all(np.isnan(x)),
+        np.dtype("i4"): lambda x: np.all(x == np.iinfo("i4").min),
+        np.dtype("i1"): lambda x: np.all(x == np.iinfo("i1").min),
+    }
+    return bool(nan_func[data.dtype](data))
+
+
+def convert_list_to_batch_data(list_data: BatchList) -> BatchDataset:
+    """
+    Convert a list of datasets to one single batch dataset
+
+    Example data formats:
+        input:  [{"node": <1d-array>, "line": <1d-array>}, {"node": <1d-array>, "line": <1d-array>}]
+        output: {"node": <2d-array>, "line": <2d-array>}
+         -or-:  {"indptr": <1d-array>, "data": <1d-array>}
+    Args:
+        list_data: list of dataset
+
+    Returns:
+        batch dataset
+        For a certain component, if all the length is the same for all the batches, a 2D array is used
+        Otherwise use a dict of indptr/data key
+    """
+
+    # List all *unique* types
+    components = {x for dataset in list_data for x in dataset.keys()}
+
+    batch_data: BatchDataset = {}
+    for component in components:
+        # Create a 2D array if the component exists in all datasets and number of objects is the same in each dataset
+        comp_exists_in_all_datasets = all(component in x for x in list_data)
+        if comp_exists_in_all_datasets:
+            all_sizes_are_the_same = all(x[component].size == list_data[0][component].size for x in list_data)
+            if all_sizes_are_the_same:
+                batch_data[component] = np.stack([x[component] for x in list_data], axis=0)
+                continue
+
+        # otherwise use indptr/data dict
+        indptr = [0]
+        data = []
+        for dataset in list_data:
+            if component in dataset:
+                # If the current dataset contains the component, increase the indptr for this batch and append the data
+                objects = dataset[component]
+                indptr.append(indptr[-1] + len(objects))
+                data.append(objects)
+
+            else:
+                # If the current dataset does not contain the component, add the last indptr again.
+                indptr.append(indptr[-1])
+
+            # Convert the index pointers to a numpy array and combine the list of object numpy arrays into a singe
+            # numpy array. All objects of all batches are now stores in one large array, the index pointers define
+            # which elemets of the array (rows) belong to which batch.
+            batch_data[component] = {"indptr": np.array(indptr, dtype=np.int64), "data": np.concatenate(data, axis=0)}
+
+    return batch_data
+
+
+def convert_python_to_numpy(data: PythonDataset, data_type: str, ignore_extra: bool = False) -> Dataset:
+    """
+    Convert native python data to internal numpy
+    Args:
+        data: data in dict or list
+        data_type: type of data: input, update, sym_output, or asym_output
+        ignore_extra: Allow (and ignore) extra attributes in the data
+
+    Returns:
+        A single or batch dataset for power-grid-model
+
+    """
+
+    # If the input data is a list, we are dealing with batch data. Each element in the list is a batch. We'll
+    # first convert each batch separately, by recursively calling this function for each batch. Then the numpy
+    # data for all batches in converted into a proper and compact numpy structure.
+    if isinstance(data, list):
+        list_data = [
+            convert_python_single_dataset_to_single_dataset(json_dict, data_type=data_type, ignore_extra=ignore_extra)
+            for json_dict in data
+        ]
+        return convert_list_to_batch_data(list_data)
+
+    # Otherwise this should be a normal (non-batch) structure, with a list of objects (dictionaries) per component.
+    if not isinstance(data, dict):
+        raise TypeError("Data should be either a list or a dictionary!")
+
+    return convert_python_single_dataset_to_single_dataset(data=data, data_type=data_type, ignore_extra=ignore_extra)
+
+
+def convert_python_single_dataset_to_single_dataset(
+    data: SinglePythonDataset, data_type: str, ignore_extra: bool = False
+) -> SingleDataset:
+    """
+    Convert native python data to internal numpy
+    Args:
+        data: data in dict
+        data_type: type of data: input, update, sym_output, or asym_output
+        ignore_extra: Allow (and ignore) extra attributes in the data
+
+    Returns:
+        A single dataset for power-grid-model
+
+    """
+
+    dataset: SingleDataset = {}
+    for component, objects in data.items():
+        dataset[component] = convert_component_list_to_numpy(
+            objects=objects, component=component, data_type=data_type, ignore_extra=ignore_extra
+        )
+
+    return dataset
+
+
+def convert_component_list_to_numpy(
+    objects: ComponentList, component: str, data_type: str, ignore_extra: bool = False
+) -> np.ndarray:
+    """
+    Convert native python data to internal numpy
+    Args:
+        objects: data in dict
+        component: the name of the component
+        data_type: type of data: input, update, sym_output, or asym_output
+        ignore_extra: Allow (and ignore) extra attributes in the data
+
+    Returns:
+        A single numpy array
+
+    """
+
+    # We'll initialize an 1d-array with NaN values for all the objects of this component type
+    array = initialize_array(data_type, component, len(objects))
+
+    for i, obj in enumerate(objects):
+        # As each object is a separate dictionary, and the attributes may differ per object, we need to check
+        # all attributes. Non-existing attributes
+        for attribute, value in obj.items():
+            # If an attribute doesn't exist, the user should explicitly state that she/he is ok with extra
+            # information in the data. This is to protect the user from overlooking errors.
+            if attribute not in array.dtype.names:
+                if ignore_extra:
+                    continue
+                raise ValueError(
+                    f"Invalid attribute '{attribute}' for {component} {data_type} data. "
+                    "(Use ignore_extra=True to ignore the extra data and suppress this exception)"
+                )
+
+            # Assign the value and raise an error if the value cannot be stored in the specific numpy array data format
+            # for this attribute.
+            try:
+                array[i][attribute] = value
+            except ValueError as ex:
+                raise ValueError(f"Invalid '{attribute}' value for {component} {data_type} data: {ex}") from ex
+    return array
+
+
+def convert_batch_dataset_to_batch_list(batch_data: BatchDataset) -> BatchList:
+    """
+    Convert batch datasets to a list of individual batches
+    Args:
+        batch_data: a batch dataset for power-grid-model
+    Returns:
+        A list of individual batches
+    """
+
+    # If the batch data is empty, return an empty list
+    if len(batch_data) == 0:
+        return []
+
+    n_batches = get_and_verify_batch_sizes(batch_data=batch_data)
+
+    # Initialize an empty list with dictionaries
+    # Note that [{}] * n_batches would result in n copies of the same dict.
+    list_data: BatchList = [{} for _ in range(n_batches)]
+
+    # While the number of batches must be the same for each component, the structure (2d numpy array or indptr/data)
+    # doesn't have to be. Therefore, we'll check the structure for each component and copy the data accordingly.
+    for component, data in batch_data.items():
+        if isinstance(data, np.ndarray):
+            component_batches = split_numpy_array_in_batches(data, component)
+        elif isinstance(data, dict):
+            component_batches = split_sparse_batches_in_batches(data, component)
+        else:
+            raise TypeError(
+                f"Invalid data type {type(data).__name__} in batch data for '{component}' "
+                "(should be a Numpy structured array or a python dictionary)."
+            )
+        for i, batch in enumerate(component_batches):
+            if batch.size > 0:
+                list_data[i][component] = batch
+    return list_data
+
+
+def get_and_verify_batch_sizes(batch_data: BatchDataset) -> int:
+    """
+    Determine the number of batches for each component and verify that each component has the same number of batches
+    Args:
+        batch_data: a batch dataset for power-grid-model
+
+    Returns:
+        The number of batches
+    """
+
+    n_batch_size = 0
+    checked_components: List[str] = []
+    for component, data in batch_data.items():
+        n_component_batch_size = get_batch_size(data)
+        if checked_components and n_component_batch_size != n_batch_size:
+            if len(checked_components) == 1:
+                checked_components_str = f"'{checked_components.pop()}'"
+            else:
+                checked_components_str = "/".join(sorted(checked_components))
+            raise ValueError(
+                f"Inconsistent number of batches in batch data. "
+                f"Component '{component}' contains {n_component_batch_size} batches, "
+                f"while {checked_components_str} contained {n_batch_size} batches."
+            )
+        n_batch_size = n_component_batch_size
+        checked_components.append(component)
+    return n_batch_size
+
+
+def get_batch_size(batch_data: BatchArray) -> int:
+    """
+    Determine the number of batches and verify the data structure while we're at it.
+    Args:
+        batch_data: a batch array for power-grid-model
+
+    Returns:
+        The number of batches
+    """
+    if isinstance(batch_data, np.ndarray):
+        # We expect the batch data to be a 2d numpy array of n_batches x n_objects. If it is a 1d numpy array instead,
+        # we assume that it is a single batch.
+        if batch_data.ndim == 1:
+            return 1
+        n_batches = batch_data.shape[0]
+    elif isinstance(batch_data, dict):
+        # If the batch data is a dictionary, we assume that it is an indptr/data structure (otherwise it is an
+        # invalid dictionary). There is always one indptr more than there are batches.
+        if "indptr" not in batch_data:
+            raise ValueError("Invalid batch data format, expected 'indptr' and 'data' entries")
+        n_batches = batch_data["indptr"].size - 1
+    else:
+        # If the batch data is not a numpy array and not a dictionary, it is invalid
+        raise ValueError(
+            "Invalid batch data format, expected a 2-d numpy array or a dictionary with an 'indptr' and 'data' entry"
+        )
+    return n_batches
+
+
+def split_numpy_array_in_batches(data: np.ndarray, component: str) -> List[np.ndarray]:
+    """
+    Split a single dense numpy array into one or more batches
+
+    Args:
+        data: A 1D or 2D Numpy structured array. A 1D array is a single table / batch, a 2D array is a batch per table.
+        component: The name of the component to which the data belongs, only used for errors.
+
+    Returns:
+        A list with a single numpy structured array per batch
+
+    """
+    if not isinstance(data, np.ndarray):
+        raise TypeError(
+            f"Invalid data type {type(data).__name__} in batch data for '{component}' "
+            "(should be a 1D/2D Numpy structured array)."
+        )
+    if data.ndim == 1:
+        return [data]
+    if data.ndim == 2:
+        return [data[i, :] for i in range(data.shape[0])]
+    raise TypeError(
+        f"Invalid data dimension {data.ndim} in batch data for '{component}' "
+        "(should be a 1D/2D Numpy structured array)."
+    )
+
+
+def split_sparse_batches_in_batches(batch_data: SparseBatchArray, component: str) -> List[np.ndarray]:
+    """
+    Split a single numpy array representing, a compressed sparse structure, into one or more batches
+
+    Args:
+        batch_data: Sparse batch data
+        component: The name of the component to which the data belongs, only used for errors.
+
+    Returns:
+        A list with a single numpy structured array per batch
+
+    """
+
+    for key in ["indptr", "data"]:
+        if key not in batch_data:
+            raise KeyError(
+                f"Missing '{key}' in sparse batch data for '{component}' "
+                "(expected a python dictionary containing two keys: 'indptr' and 'data')."
+            )
+
+    data = batch_data["data"]
+    indptr = batch_data["indptr"]
+
+    if not isinstance(data, np.ndarray) or data.ndim != 1:
+        raise TypeError(
+            f"Invalid data type {type(data).__name__} in sparse batch data for '{component}' "
+            "(should be a 1D Numpy structured array (i.e. a single 'table'))."
+        )
+
+    if not isinstance(indptr, np.ndarray) or indptr.ndim != 1 or not np.issubdtype(indptr.dtype, np.integer):
+        raise TypeError(
+            f"Invalid indptr data type {type(indptr).__name__} in batch data for '{component}' "
+            "(should be a 1D Numpy array (i.e. a single 'list'), "
+            "containing indices (i.e. integers))."
+        )
+
+    if indptr[0] != 0 or indptr[-1] != len(data) or any(indptr[i] > indptr[i + 1] for i in range(len(indptr) - 1)):
+        raise TypeError(
+            f"Invalid indptr in batch data for '{component}' "
+            f"(should start with 0, end with the number of objects ({len(data)}) "
+            "and be monotonic increasing)."
+        )
+
+    return [data[indptr[i] : indptr[i + 1]] for i in range(len(indptr) - 1)]
+
+
+def convert_dataset_to_python_dataset(data: Dataset) -> PythonDataset:
+    """
+    Convert internal numpy arrays to native python data
+    If an attribute is not available (NaN value), it will not be exported.
+    Args:
+        data: A single or batch dataset for power-grid-model
+    Returns:
+        A python dict for single dataset
+        A python list for batch dataset
+
+    """
+
+    # Check if the dataset is a single dataset or batch dataset
+    # It is batch dataset if it is 2D array or a indptr/data structure
+    is_batch: Optional[bool] = None
+    for component, array in data.items():
+        is_dense_batch = isinstance(array, np.ndarray) and array.ndim == 2
+        is_sparse_batch = isinstance(array, dict) and "indptr" in array and "data" in array
+        if is_batch is not None and is_batch != (is_dense_batch or is_sparse_batch):
+            raise ValueError(
+                f"Mixed {'' if is_batch else 'non-'}batch data "
+                f"with {'non-' if is_batch else ''}batch data ({component})."
+            )
+        is_batch = is_dense_batch or is_sparse_batch
+
+    # If it is a batch, convert the batch data to a list of batches, then convert each batch individually.
+    if is_batch:
+        # We have established that this is batch data, so let's tell the type checker that this is a BatchDataset
+        data = cast(BatchDataset, data)
+        list_data = convert_batch_dataset_to_batch_list(data)
+        return [convert_single_dataset_to_python_single_dataset(data=x) for x in list_data]
+
+    # We have established that this is not batch data, so let's tell the type checker that this is a BatchDataset
+    data = cast(SingleDataset, data)
+    return convert_single_dataset_to_python_single_dataset(data=data)
+
+
+def convert_single_dataset_to_python_single_dataset(data: SingleDataset) -> SinglePythonDataset:
+    """
+    Convert internal numpy arrays to native python data
+    If an attribute is not available (NaN value), it will not be exported.
+    Args:
+        data: A single dataset for power-grid-model
+    Returns:
+        A python dict for single dataset
+    """
+
+    # This should be a single data set
+    for component, array in data.items():
+        if not isinstance(array, np.ndarray) or array.ndim != 1:
+            raise ValueError("Invalid data format")
+
+    # Convert each numpy array to a list of objects, which contains only the non-NaN attributes:
+    # For example: {"node": [{"id": 0, ...}, {"id": 1, ...}], "line": [{"id": 2, ...}]}
+    return {
+        component: [
+            {attribute: obj[attribute].tolist() for attribute in objects.dtype.names if not is_nan(obj[attribute])}
+            for obj in objects
+        ]
+        for component, objects in data.items()
+    }
+
+
+def import_json_data(json_file: Path, data_type: str, ignore_extra: bool = False) -> Dataset:
+    """
+    import json data
+    Args:
+        json_file: path to the json file
+        data_type: type of data: input, update, sym_output, or asym_output
+        ignore_extra: Allow (and ignore) extra attributes in the json file
+
+    Returns:
+         A single or batch dataset for power-grid-model
+    """
+    with open(json_file, mode="r", encoding="utf-8") as file_pointer:
+        data = json.load(file_pointer)
+    return convert_python_to_numpy(data=data, data_type=data_type, ignore_extra=ignore_extra)
+
+
+def import_input_data(json_file: Path) -> SingleDataset:
+    """
+    import input json data
+    Args:
+        json_file: path to the json file
+
+    Returns:
+         A single dataset for power-grid-model
+    """
+    data = import_json_data(json_file=json_file, data_type="input")
+    assert isinstance(data, dict)
+    assert all(isinstance(component, np.ndarray) and component.ndim == 1 for component in data.values())
+    return cast(SingleDataset, data)
+
+
+def import_update_data(json_file: Path) -> BatchDataset:
+    """
+    import update json data
+    Args:
+        json_file: path to the json file
+
+    Returns:
+         A batch dataset for power-grid-model
+    """
+    return cast(BatchDataset, import_json_data(json_file=json_file, data_type="update"))
+
+
+def export_json_data(json_file: Path, data: Dataset, indent: Optional[int] = 2, compact: bool = False):
+    """
+    export json data
+    Args:
+        json_file: path to json file
+        data: a single or batch dataset for power-grid-model
+        indent: indent of the file, default 2
+        compact: write components on a single line
+
+    Returns:
+        Save to file
+    """
+    json_data = convert_dataset_to_python_dataset(data)
+
+    with open(json_file, mode="w", encoding="utf-8") as file_pointer:
+        if compact and indent:
+            is_batch_data = isinstance(json_data, list)
+            max_level = 4 if is_batch_data else 3
+            compact_json_dump(json_data, file_pointer, indent=indent, max_level=max_level)
+        else:
+            json.dump(json_data, file_pointer, indent=indent)
+
+
+def compact_json_dump(data: Any, io_stream: IO[str], indent: int, max_level: int, level: int = 0):
+    """Custom compact JSON writer that is intended to put data belonging to a single object on a single line.
+
+    For example:
+    {
+        "node": [
+            {"id": 0, "u_rated": 10500.0},
+            {"id": 1, "u_rated": 10500.0},
+        ],
+        "line": [
+            {"id": 2, "node_from": 0, "node_to": 1, ...}
+        ]
+    }
+
+    The function is being called recursively, starting at level 0 and recursing until max_level is reached. It is
+    basically a full json writer, but for efficiency reasons, on the last levels the native json.dump method is used.
+    """
+
+    # Let's define a 'tab' indent, depending on the level
+    tab = " " * level * indent
+
+    # If we are at the max_level, or the data simply doesn't contain any more levels, write the indent and serialize
+    # the data on a single line.
+    if level >= max_level or not isinstance(data, (list, dict)):
+        io_stream.write(tab)
+        json.dump(data, io_stream, indent=None)
+        return
+
+    # We'll need the number of objects later on
+    n_obj = len(data)
+
+    # If the data is a list:
+    # 1. start with an opening bracket
+    # 2. dump each element in the list
+    # 3. add a comma and a new line after each element, except for the last element, there we don't need a comma.
+    # 4. finish with a closing bracket
+    if isinstance(data, list):
+        io_stream.write(tab + "[\n")
+        for i, obj in enumerate(data, start=1):
+            compact_json_dump(obj, io_stream, indent, max_level, level + 1)
+            io_stream.write(",\n" if i < n_obj else "\n")
+        io_stream.write(tab + "]")
+        return
+
+    # If the data is a dictionary:
+    # 1. start with an opening curly bracket
+    # 2. for each element: write it's key, plus a colon ':'
+    # 3. if the next level would be the max_level, add a space and dump the element on a single,
+    #    else add a new line before dumping the element recursively.
+    # 4. add a comma and a new line after each element, except for the last element, there we don't need a comma.
+    # 5. finish with a closing curly bracket
+    io_stream.write(tab + "{\n")
+    for i, (key, obj) in enumerate(data.items(), start=1):
+        io_stream.write(tab + " " * indent + f'"{key}":')
+        if level == max_level - 1 or not isinstance(obj, (list, dict)):
+            io_stream.write(" ")
+            json.dump(obj, io_stream, indent=None)
+        else:
+            io_stream.write("\n")
+            compact_json_dump(obj, io_stream, indent, max_level, level + 2)
+        io_stream.write(",\n" if i < n_obj else "\n")
+    io_stream.write(tab + "}\n")
```

## power_grid_model/core/__init__.py

 * *Ordering differences only*

```diff
@@ -1,3 +1,3 @@
-# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
-#
-# SPDX-License-Identifier: MPL-2.0
+# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
+#
+# SPDX-License-Identifier: MPL-2.0
```

## power_grid_model/core/error_handling.py

 * *Ordering differences only*

```diff
@@ -1,68 +1,68 @@
-# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
-#
-# SPDX-License-Identifier: MPL-2.0
-
-
-"""
-Error handling
-"""
-
-from typing import Optional
-
-import numpy as np
-
-from power_grid_model.core.index_integer import IdxNp
-from power_grid_model.core.power_grid_core import power_grid_core as pgc
-from power_grid_model.errors import PowerGridBatchError, PowerGridError
-
-VALIDATOR_MSG = "\nTry validate_input_data() or validate_batch_data() to validate your data.\n"
-# error codes
-PGM_NO_ERROR = 0
-PGM_REGULAR_ERROR = 1
-PGM_BATCH_ERROR = 2
-
-
-def find_error(batch_size: int = 1) -> Optional[RuntimeError]:
-    """
-    Check if there is an error and return it
-
-    Args:
-        batch_size: Size of batch
-
-    Returns: error object, can be none
-
-    """
-    error_code: int = pgc.error_code()
-    if error_code == PGM_NO_ERROR:
-        return None
-    if error_code == PGM_REGULAR_ERROR:
-        error_message = pgc.error_message()
-        error_message += VALIDATOR_MSG
-        return PowerGridError(error_message)
-    if error_code == PGM_BATCH_ERROR:
-        error_message = "There are errors in the batch calculation." + VALIDATOR_MSG
-        error = PowerGridBatchError(error_message)
-        n_fails = pgc.n_failed_scenarios()
-        failed_idxptr = pgc.failed_scenarios()
-        failed_msgptr = pgc.batch_errors()
-        error.failed_scenarios = np.ctypeslib.as_array(failed_idxptr, shape=(n_fails,)).copy()
-        error.error_messages = [failed_msgptr[i].decode() for i in range(n_fails)]  # type: ignore
-        all_scenarios = np.arange(batch_size, dtype=IdxNp)
-        mask = np.ones(batch_size, dtype=np.bool_)
-        mask[error.failed_scenarios] = False
-        error.succeeded_scenarios = all_scenarios[mask]
-        return error
-    return RuntimeError("Unknown error!")
-
-
-def assert_no_error(batch_size: int = 1):
-    """
-    Assert there is no error in the last operation
-    If there is an error, raise it
-
-    Returns:
-
-    """
-    error = find_error(batch_size=batch_size)
-    if error is not None:
-        raise error
+# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
+#
+# SPDX-License-Identifier: MPL-2.0
+
+
+"""
+Error handling
+"""
+
+from typing import Optional
+
+import numpy as np
+
+from power_grid_model.core.index_integer import IdxNp
+from power_grid_model.core.power_grid_core import power_grid_core as pgc
+from power_grid_model.errors import PowerGridBatchError, PowerGridError
+
+VALIDATOR_MSG = "\nTry validate_input_data() or validate_batch_data() to validate your data.\n"
+# error codes
+PGM_NO_ERROR = 0
+PGM_REGULAR_ERROR = 1
+PGM_BATCH_ERROR = 2
+
+
+def find_error(batch_size: int = 1) -> Optional[RuntimeError]:
+    """
+    Check if there is an error and return it
+
+    Args:
+        batch_size: Size of batch
+
+    Returns: error object, can be none
+
+    """
+    error_code: int = pgc.error_code()
+    if error_code == PGM_NO_ERROR:
+        return None
+    if error_code == PGM_REGULAR_ERROR:
+        error_message = pgc.error_message()
+        error_message += VALIDATOR_MSG
+        return PowerGridError(error_message)
+    if error_code == PGM_BATCH_ERROR:
+        error_message = "There are errors in the batch calculation." + VALIDATOR_MSG
+        error = PowerGridBatchError(error_message)
+        n_fails = pgc.n_failed_scenarios()
+        failed_idxptr = pgc.failed_scenarios()
+        failed_msgptr = pgc.batch_errors()
+        error.failed_scenarios = np.ctypeslib.as_array(failed_idxptr, shape=(n_fails,)).copy()
+        error.error_messages = [failed_msgptr[i].decode() for i in range(n_fails)]  # type: ignore
+        all_scenarios = np.arange(batch_size, dtype=IdxNp)
+        mask = np.ones(batch_size, dtype=np.bool_)
+        mask[error.failed_scenarios] = False
+        error.succeeded_scenarios = all_scenarios[mask]
+        return error
+    return RuntimeError("Unknown error!")
+
+
+def assert_no_error(batch_size: int = 1):
+    """
+    Assert there is no error in the last operation
+    If there is an error, raise it
+
+    Returns:
+
+    """
+    error = find_error(batch_size=batch_size)
+    if error is not None:
+        raise error
```

## power_grid_model/core/index_integer.py

 * *Ordering differences only*

```diff
@@ -1,17 +1,17 @@
-# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
-#
-# SPDX-License-Identifier: MPL-2.0
-
-"""
-Definition of integers used by the calculation core
-"""
-
-# define internal index integer
-from ctypes import c_int32, c_int64
-
-import numpy as np
-
-IdxC = c_int64
-IdxNp = np.int64
-IdC = c_int32
-IdNp = np.int32
+# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
+#
+# SPDX-License-Identifier: MPL-2.0
+
+"""
+Definition of integers used by the calculation core
+"""
+
+# define internal index integer
+from ctypes import c_int32, c_int64
+
+import numpy as np
+
+IdxC = c_int64
+IdxNp = np.int64
+IdC = c_int32
+IdNp = np.int32
```

## power_grid_model/core/options.py

 * *Ordering differences only*

```diff
@@ -1,68 +1,68 @@
-# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
-#
-# SPDX-License-Identifier: MPL-2.0
-
-
-"""
-Option class
-"""
-from typing import Any, Callable
-
-from power_grid_model.core.power_grid_core import OptionsPtr
-from power_grid_model.core.power_grid_core import power_grid_core as pgc
-
-
-class OptionSetter:
-    """
-    setter for options
-    """
-
-    _setter: Callable
-
-    def __init__(self, setter):
-        self._setter = setter
-
-    def __set__(self, instance: "Options", value: Any):
-        self._setter(instance.opt, value)
-
-    def __get__(self, instance, owner):
-        raise NotImplementedError("Cannot get option value!")
-
-
-class Options:
-    """
-    Option class
-    """
-
-    _opt: OptionsPtr
-    # option setter
-    calculation_type = OptionSetter(pgc.set_calculation_type)
-    calculation_method = OptionSetter(pgc.set_calculation_method)
-    symmetric = OptionSetter(pgc.set_symmetric)
-    error_tolerance = OptionSetter(pgc.set_err_tol)
-    max_iteration = OptionSetter(pgc.set_max_iter)
-    threading = OptionSetter(pgc.set_threading)
-
-    @property
-    def opt(self) -> OptionsPtr:
-        """
-
-        Returns: Pointer to the option object
-
-        """
-        return self._opt
-
-    def __new__(cls, *args, **kwargs):
-        instance = super().__new__(cls, *args, **kwargs)
-        instance._opt = pgc.create_options()
-        return instance
-
-    def __del__(self):
-        pgc.destroy_options(self._opt)
-
-    # not copyable
-    def __copy__(self):
-        raise NotImplementedError("Class not copyable")
-
-    def __deepcopy__(self, memodict):
-        raise NotImplementedError("class not copyable")
+# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
+#
+# SPDX-License-Identifier: MPL-2.0
+
+
+"""
+Option class
+"""
+from typing import Any, Callable
+
+from power_grid_model.core.power_grid_core import OptionsPtr
+from power_grid_model.core.power_grid_core import power_grid_core as pgc
+
+
+class OptionSetter:
+    """
+    setter for options
+    """
+
+    _setter: Callable
+
+    def __init__(self, setter):
+        self._setter = setter
+
+    def __set__(self, instance: "Options", value: Any):
+        self._setter(instance.opt, value)
+
+    def __get__(self, instance, owner):
+        raise NotImplementedError("Cannot get option value!")
+
+
+class Options:
+    """
+    Option class
+    """
+
+    _opt: OptionsPtr
+    # option setter
+    calculation_type = OptionSetter(pgc.set_calculation_type)
+    calculation_method = OptionSetter(pgc.set_calculation_method)
+    symmetric = OptionSetter(pgc.set_symmetric)
+    error_tolerance = OptionSetter(pgc.set_err_tol)
+    max_iteration = OptionSetter(pgc.set_max_iter)
+    threading = OptionSetter(pgc.set_threading)
+
+    @property
+    def opt(self) -> OptionsPtr:
+        """
+
+        Returns: Pointer to the option object
+
+        """
+        return self._opt
+
+    def __new__(cls, *args, **kwargs):
+        instance = super().__new__(cls, *args, **kwargs)
+        instance._opt = pgc.create_options()
+        return instance
+
+    def __del__(self):
+        pgc.destroy_options(self._opt)
+
+    # not copyable
+    def __copy__(self):
+        raise NotImplementedError("Class not copyable")
+
+    def __deepcopy__(self, memodict):
+        raise NotImplementedError("class not copyable")
```

## power_grid_model/core/power_grid_core.py

 * *Ordering differences only*

```diff
@@ -1,327 +1,327 @@
-# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
-#
-# SPDX-License-Identifier: MPL-2.0
-
-"""
-Loader for the dynamic library
-"""
-
-import platform
-from ctypes import CDLL, POINTER, c_char_p, c_double, c_size_t, c_void_p
-from inspect import signature
-from itertools import chain
-from pathlib import Path
-from typing import Callable, Optional
-
-from power_grid_model.core.index_integer import IdC, IdxC
-
-# integer index
-IdxPtr = POINTER(IdxC)
-IdxDoublePtr = POINTER(IdxPtr)
-IDPtr = POINTER(IdC)
-# double pointer to char
-CharDoublePtr = POINTER(c_char_p)
-# double pointer to void
-VoidDoublePtr = POINTER(c_void_p)
-
-# functions with size_t return
-_FUNC_SIZE_T_RES = {"meta_class_size", "meta_class_alignment", "meta_attribute_offset"}
-_ARGS_TYPE_MAPPING = {str: c_char_p, int: IdxC, float: c_double}
-
-# The c_void_p is extended only for type hinting and type checking; therefore no public methods are required.
-# pylint: disable=too-few-public-methods
-
-
-class HandlePtr(c_void_p):
-    """
-    Pointer to handle
-    """
-
-
-class OptionsPtr(c_void_p):
-    """
-    Pointer to option
-    """
-
-
-class ModelPtr(c_void_p):
-    """
-    Pointer to model
-    """
-
-
-def _load_core() -> CDLL:
-    """
-
-    Returns: DLL/SO object
-
-    """
-    if platform.system() == "Windows":
-        dll_file = "_power_grid_core.dll"
-    else:
-        dll_file = "_power_grid_core.so"
-    cdll = CDLL(str(Path(__file__).parent / dll_file))
-    # assign return types
-    # handle
-    cdll.PGM_create_handle.argtypes = []
-    cdll.PGM_create_handle.restype = HandlePtr
-    cdll.PGM_destroy_handle.argtypes = [HandlePtr]
-    cdll.PGM_destroy_handle.restype = None
-    return cdll
-
-
-# load dll once
-_CDLL: CDLL = _load_core()
-
-
-def make_c_binding(func: Callable):
-    """
-    Descriptor to make the function to bind to C
-
-    Args:
-        func: method object from PowerGridCore
-
-    Returns:
-        Binded function
-
-    """
-    name = func.__name__
-    sig = signature(func)
-
-    # get and convert types, skip first argument, as it is self
-    py_argnames = list(sig.parameters.keys())[1:]
-    py_argtypes = [v.annotation for v in sig.parameters.values()][1:]
-    py_restype = sig.return_annotation
-    c_argtypes = [_ARGS_TYPE_MAPPING.get(x, x) for x in py_argtypes]
-    c_restype = _ARGS_TYPE_MAPPING.get(py_restype, py_restype)
-    if c_restype == IdxC and name in _FUNC_SIZE_T_RES:
-        c_restype = c_size_t
-    # set argument in dll
-    # mostly with handle pointer, except destroy function
-    is_destroy_func = "destroy" in name
-    if is_destroy_func:
-        getattr(_CDLL, f"PGM_{name}").argtypes = c_argtypes
-    else:
-        getattr(_CDLL, f"PGM_{name}").argtypes = [HandlePtr] + c_argtypes
-    getattr(_CDLL, f"PGM_{name}").restype = c_restype
-
-    # binding function
-    def cbind_func(self, *args, **kwargs):
-        if "destroy" in name:
-            c_inputs = []
-        else:
-            c_inputs = [self._handle]  # pylint: disable=protected-access
-        args = chain(args, (kwargs[key] for key in py_argnames[len(args) :]))
-        for arg, arg_type in zip(args, c_argtypes):
-            if arg_type == c_char_p:
-                c_inputs.append(arg.encode())
-            else:
-                c_inputs.append(arg)
-
-        # call
-        res = getattr(_CDLL, f"PGM_{name}")(*c_inputs)
-        # convert to string for c_char_p
-        if c_restype == c_char_p:
-            res = res.decode()
-        return res
-
-    return cbind_func
-
-
-# pylint: disable=too-many-arguments
-# pylint: disable=missing-function-docstring
-# pylint: disable=too-many-public-methods
-class PowerGridCore:
-    """
-    DLL caller
-    """
-
-    _handle: HandlePtr
-    _instance: Optional["PowerGridCore"] = None
-
-    # singleton of power grid core
-    def __new__(cls, *args, **kwargs):
-        if cls._instance is None:
-            cls._instance = super().__new__(cls, *args, **kwargs)
-            cls._instance._handle = _CDLL.PGM_create_handle()
-        return cls._instance
-
-    def __del__(self):
-        _CDLL.PGM_destroy_handle(self._handle)
-
-    # not copyable
-    def __copy__(self):
-        raise NotImplementedError("Class not copyable")
-
-    def __deepcopy__(self, memodict):
-        raise NotImplementedError("class not copyable")
-
-    @make_c_binding
-    def error_code(self) -> int:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def error_message(self) -> str:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def n_failed_scenarios(self) -> int:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def failed_scenarios(self) -> IdxPtr:  # type: ignore[empty-body, valid-type]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def batch_errors(self) -> CharDoublePtr:  # type: ignore[empty-body, valid-type]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def clear_error(self) -> None:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def meta_n_datasets(self) -> int:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def meta_dataset_name(self, idx: int) -> str:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def meta_n_components(self, dataset: str) -> int:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def meta_component_name(self, dataset: str, idx: int) -> str:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def meta_component_alignment(self, dataset: str, component: str) -> int:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def meta_component_size(self, dataset: str, component: str) -> int:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def meta_n_attributes(self, dataset: str, component: str) -> int:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def meta_attribute_name(self, dataset: str, component: str, idx: int) -> str:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def meta_attribute_ctype(self, dataset: str, component: str, attribute: str) -> str:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def meta_attribute_offset(self, dataset: str, component: str, attribute: str) -> int:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def is_little_endian(self) -> int:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def create_options(self) -> OptionsPtr:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def destroy_options(self, opt: OptionsPtr) -> None:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def set_calculation_type(self, opt: OptionsPtr, calculation_type: int) -> None:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def set_calculation_method(self, opt: OptionsPtr, method: int) -> None:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def set_symmetric(self, opt: OptionsPtr, sym: int) -> None:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def set_err_tol(self, opt: OptionsPtr, err_tol: float) -> None:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def set_max_iter(self, opt: OptionsPtr, max_iter: int) -> None:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def set_threading(self, opt: OptionsPtr, threading: int) -> None:  # type: ignore[empty-body]
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def create_model(  # type: ignore[empty-body]
-        self,
-        system_frequency: float,
-        n_components: int,
-        components: CharDoublePtr,  # type: ignore[valid-type]
-        component_sizes: IdxPtr,  # type: ignore[valid-type]
-        input_data: VoidDoublePtr,  # type: ignore[valid-type]
-    ) -> ModelPtr:
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def update_model(  # type: ignore[empty-body]
-        self,
-        model: ModelPtr,
-        n_components: int,
-        components: CharDoublePtr,  # type: ignore[valid-type]
-        component_sizes: IdxPtr,  # type: ignore[valid-type]
-        update_data: VoidDoublePtr,  # type: ignore[valid-type]
-    ) -> None:
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def copy_model(  # type: ignore[empty-body]
-        self,
-        model: ModelPtr,
-    ) -> ModelPtr:
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def get_indexer(  # type: ignore[empty-body]
-        self,
-        model: ModelPtr,
-        component: str,
-        size: int,
-        ids: IDPtr,  # type: ignore[valid-type]
-        indexer: IdxPtr,  # type: ignore[valid-type]
-    ) -> None:
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def destroy_model(  # type: ignore[empty-body]
-        self,
-        model: ModelPtr,
-    ) -> None:
-        pass  # pragma: no cover
-
-    @make_c_binding
-    def calculate(  # type: ignore[empty-body]
-        self,
-        model: ModelPtr,
-        opt: OptionsPtr,
-        # output
-        n_output_components: int,
-        output_components: CharDoublePtr,  # type: ignore[valid-type]
-        output_data: VoidDoublePtr,  # type: ignore[valid-type]
-        # update
-        n_scenarios: int,
-        n_update_components: int,
-        update_components: CharDoublePtr,  # type: ignore[valid-type]
-        n_component_elements_per_scenario: IdxPtr,  # type: ignore[valid-type]
-        indptrs_per_component: IdxDoublePtr,  # type: ignore[valid-type]
-        update_data: VoidDoublePtr,  # type: ignore[valid-type]
-    ) -> None:
-        pass  # pragma: no cover
-
-
-# make one instance
-power_grid_core = PowerGridCore()
+# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
+#
+# SPDX-License-Identifier: MPL-2.0
+
+"""
+Loader for the dynamic library
+"""
+
+import platform
+from ctypes import CDLL, POINTER, c_char_p, c_double, c_size_t, c_void_p
+from inspect import signature
+from itertools import chain
+from pathlib import Path
+from typing import Callable, Optional
+
+from power_grid_model.core.index_integer import IdC, IdxC
+
+# integer index
+IdxPtr = POINTER(IdxC)
+IdxDoublePtr = POINTER(IdxPtr)
+IDPtr = POINTER(IdC)
+# double pointer to char
+CharDoublePtr = POINTER(c_char_p)
+# double pointer to void
+VoidDoublePtr = POINTER(c_void_p)
+
+# functions with size_t return
+_FUNC_SIZE_T_RES = {"meta_class_size", "meta_class_alignment", "meta_attribute_offset"}
+_ARGS_TYPE_MAPPING = {str: c_char_p, int: IdxC, float: c_double}
+
+# The c_void_p is extended only for type hinting and type checking; therefore no public methods are required.
+# pylint: disable=too-few-public-methods
+
+
+class HandlePtr(c_void_p):
+    """
+    Pointer to handle
+    """
+
+
+class OptionsPtr(c_void_p):
+    """
+    Pointer to option
+    """
+
+
+class ModelPtr(c_void_p):
+    """
+    Pointer to model
+    """
+
+
+def _load_core() -> CDLL:
+    """
+
+    Returns: DLL/SO object
+
+    """
+    if platform.system() == "Windows":
+        dll_file = "_power_grid_core.dll"
+    else:
+        dll_file = "_power_grid_core.so"
+    cdll = CDLL(str(Path(__file__).parent / dll_file))
+    # assign return types
+    # handle
+    cdll.PGM_create_handle.argtypes = []
+    cdll.PGM_create_handle.restype = HandlePtr
+    cdll.PGM_destroy_handle.argtypes = [HandlePtr]
+    cdll.PGM_destroy_handle.restype = None
+    return cdll
+
+
+# load dll once
+_CDLL: CDLL = _load_core()
+
+
+def make_c_binding(func: Callable):
+    """
+    Descriptor to make the function to bind to C
+
+    Args:
+        func: method object from PowerGridCore
+
+    Returns:
+        Binded function
+
+    """
+    name = func.__name__
+    sig = signature(func)
+
+    # get and convert types, skip first argument, as it is self
+    py_argnames = list(sig.parameters.keys())[1:]
+    py_argtypes = [v.annotation for v in sig.parameters.values()][1:]
+    py_restype = sig.return_annotation
+    c_argtypes = [_ARGS_TYPE_MAPPING.get(x, x) for x in py_argtypes]
+    c_restype = _ARGS_TYPE_MAPPING.get(py_restype, py_restype)
+    if c_restype == IdxC and name in _FUNC_SIZE_T_RES:
+        c_restype = c_size_t
+    # set argument in dll
+    # mostly with handle pointer, except destroy function
+    is_destroy_func = "destroy" in name
+    if is_destroy_func:
+        getattr(_CDLL, f"PGM_{name}").argtypes = c_argtypes
+    else:
+        getattr(_CDLL, f"PGM_{name}").argtypes = [HandlePtr] + c_argtypes
+    getattr(_CDLL, f"PGM_{name}").restype = c_restype
+
+    # binding function
+    def cbind_func(self, *args, **kwargs):
+        if "destroy" in name:
+            c_inputs = []
+        else:
+            c_inputs = [self._handle]  # pylint: disable=protected-access
+        args = chain(args, (kwargs[key] for key in py_argnames[len(args) :]))
+        for arg, arg_type in zip(args, c_argtypes):
+            if arg_type == c_char_p:
+                c_inputs.append(arg.encode())
+            else:
+                c_inputs.append(arg)
+
+        # call
+        res = getattr(_CDLL, f"PGM_{name}")(*c_inputs)
+        # convert to string for c_char_p
+        if c_restype == c_char_p:
+            res = res.decode()
+        return res
+
+    return cbind_func
+
+
+# pylint: disable=too-many-arguments
+# pylint: disable=missing-function-docstring
+# pylint: disable=too-many-public-methods
+class PowerGridCore:
+    """
+    DLL caller
+    """
+
+    _handle: HandlePtr
+    _instance: Optional["PowerGridCore"] = None
+
+    # singleton of power grid core
+    def __new__(cls, *args, **kwargs):
+        if cls._instance is None:
+            cls._instance = super().__new__(cls, *args, **kwargs)
+            cls._instance._handle = _CDLL.PGM_create_handle()
+        return cls._instance
+
+    def __del__(self):
+        _CDLL.PGM_destroy_handle(self._handle)
+
+    # not copyable
+    def __copy__(self):
+        raise NotImplementedError("Class not copyable")
+
+    def __deepcopy__(self, memodict):
+        raise NotImplementedError("class not copyable")
+
+    @make_c_binding
+    def error_code(self) -> int:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def error_message(self) -> str:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def n_failed_scenarios(self) -> int:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def failed_scenarios(self) -> IdxPtr:  # type: ignore[empty-body, valid-type]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def batch_errors(self) -> CharDoublePtr:  # type: ignore[empty-body, valid-type]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def clear_error(self) -> None:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def meta_n_datasets(self) -> int:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def meta_dataset_name(self, idx: int) -> str:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def meta_n_components(self, dataset: str) -> int:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def meta_component_name(self, dataset: str, idx: int) -> str:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def meta_component_alignment(self, dataset: str, component: str) -> int:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def meta_component_size(self, dataset: str, component: str) -> int:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def meta_n_attributes(self, dataset: str, component: str) -> int:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def meta_attribute_name(self, dataset: str, component: str, idx: int) -> str:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def meta_attribute_ctype(self, dataset: str, component: str, attribute: str) -> str:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def meta_attribute_offset(self, dataset: str, component: str, attribute: str) -> int:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def is_little_endian(self) -> int:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def create_options(self) -> OptionsPtr:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def destroy_options(self, opt: OptionsPtr) -> None:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def set_calculation_type(self, opt: OptionsPtr, calculation_type: int) -> None:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def set_calculation_method(self, opt: OptionsPtr, method: int) -> None:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def set_symmetric(self, opt: OptionsPtr, sym: int) -> None:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def set_err_tol(self, opt: OptionsPtr, err_tol: float) -> None:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def set_max_iter(self, opt: OptionsPtr, max_iter: int) -> None:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def set_threading(self, opt: OptionsPtr, threading: int) -> None:  # type: ignore[empty-body]
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def create_model(  # type: ignore[empty-body]
+        self,
+        system_frequency: float,
+        n_components: int,
+        components: CharDoublePtr,  # type: ignore[valid-type]
+        component_sizes: IdxPtr,  # type: ignore[valid-type]
+        input_data: VoidDoublePtr,  # type: ignore[valid-type]
+    ) -> ModelPtr:
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def update_model(  # type: ignore[empty-body]
+        self,
+        model: ModelPtr,
+        n_components: int,
+        components: CharDoublePtr,  # type: ignore[valid-type]
+        component_sizes: IdxPtr,  # type: ignore[valid-type]
+        update_data: VoidDoublePtr,  # type: ignore[valid-type]
+    ) -> None:
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def copy_model(  # type: ignore[empty-body]
+        self,
+        model: ModelPtr,
+    ) -> ModelPtr:
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def get_indexer(  # type: ignore[empty-body]
+        self,
+        model: ModelPtr,
+        component: str,
+        size: int,
+        ids: IDPtr,  # type: ignore[valid-type]
+        indexer: IdxPtr,  # type: ignore[valid-type]
+    ) -> None:
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def destroy_model(  # type: ignore[empty-body]
+        self,
+        model: ModelPtr,
+    ) -> None:
+        pass  # pragma: no cover
+
+    @make_c_binding
+    def calculate(  # type: ignore[empty-body]
+        self,
+        model: ModelPtr,
+        opt: OptionsPtr,
+        # output
+        n_output_components: int,
+        output_components: CharDoublePtr,  # type: ignore[valid-type]
+        output_data: VoidDoublePtr,  # type: ignore[valid-type]
+        # update
+        n_scenarios: int,
+        n_update_components: int,
+        update_components: CharDoublePtr,  # type: ignore[valid-type]
+        n_component_elements_per_scenario: IdxPtr,  # type: ignore[valid-type]
+        indptrs_per_component: IdxDoublePtr,  # type: ignore[valid-type]
+        update_data: VoidDoublePtr,  # type: ignore[valid-type]
+    ) -> None:
+        pass  # pragma: no cover
+
+
+# make one instance
+power_grid_core = PowerGridCore()
```

## power_grid_model/core/power_grid_meta.py

 * *Ordering differences only*

```diff
@@ -1,285 +1,285 @@
-# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
-#
-# SPDX-License-Identifier: MPL-2.0
-
-"""
-Load meta data from C core and define numpy structured array
-"""
-
-from ctypes import Array, c_char_p, c_void_p
-from dataclasses import dataclass
-from typing import Any, Dict, Mapping, Optional, Union
-
-import numpy as np
-
-from power_grid_model.core.error_handling import VALIDATOR_MSG
-from power_grid_model.core.index_integer import IdxC, IdxNp
-from power_grid_model.core.power_grid_core import IdxPtr
-from power_grid_model.core.power_grid_core import power_grid_core as pgc
-
-_CTYPE_NUMPY_MAP = {"double": "f8", "int32_t": "i4", "int8_t": "i1", "double[3]": "(3,)f8"}
-_ENDIANNESS = "<" if pgc.is_little_endian() == 1 else ">"
-_NAN_VALUE_MAP = {
-    f"{_ENDIANNESS}f8": np.nan,
-    f"{_ENDIANNESS}(3,)f8": np.nan,
-    f"{_ENDIANNESS}i4": np.iinfo(f"{_ENDIANNESS}i4").min,
-    f"{_ENDIANNESS}i1": np.iinfo(f"{_ENDIANNESS}i1").min,
-}
-
-
-@dataclass
-class ComponentMetaData:
-    """
-    Data class for component metadata
-    """
-
-    dtype: np.dtype
-    dtype_dict: Dict[str, Any]
-    nans: Dict[str, Union[float, int]]
-    nan_scalar: np.ndarray
-
-    def __getitem__(self, item):
-        """
-        Get item of dataclass
-
-        Args:
-            item: item name
-
-        Returns:
-
-        """
-        return getattr(self, item)
-
-
-DatasetMetaData = Dict[str, ComponentMetaData]
-PowerGridMetaData = Dict[str, DatasetMetaData]
-
-
-def _generate_meta_data() -> PowerGridMetaData:
-    """
-
-    Returns: meta data for all dataset
-
-    """
-    py_meta_data = {}
-    n_datasets = pgc.meta_n_datasets()
-    for i in range(n_datasets):
-        dataset = pgc.meta_dataset_name(i)
-        py_meta_data[dataset] = _generate_meta_dataset(dataset)
-    return py_meta_data
-
-
-def _generate_meta_dataset(dataset: str) -> DatasetMetaData:
-    """
-
-    Args:
-        dataset: dataset name
-
-    Returns: meta data for one dataset
-
-    """
-    py_meta_dataset = {}
-    n_components = pgc.meta_n_components(dataset)
-    for i in range(n_components):
-        component_name = pgc.meta_component_name(dataset, i)
-        py_meta_dataset[component_name] = _generate_meta_component(dataset, component_name)
-    return py_meta_dataset
-
-
-def _generate_meta_component(dataset: str, component_name: str) -> ComponentMetaData:
-    """
-
-    Args:
-        dataset: dataset name
-        component_name: component name
-
-    Returns: meta data for one component
-
-    """
-
-    dtype_dict = _generate_meta_attributes(dataset, component_name)
-    dtype = np.dtype({k: v for k, v in dtype_dict.items() if k != "nans"})  # type: ignore
-    nans = dict(zip(dtype_dict["names"], dtype_dict["nans"]))
-    if dtype.alignment != pgc.meta_component_alignment(dataset, component_name):
-        raise TypeError(f'Aligment mismatch for component type: "{component_name}" !')
-    # get single nan scalar
-    nan_scalar = np.empty(1, dtype=dtype)
-    for key, value in nans.items():
-        nan_scalar[key] = value
-    # return component
-    return ComponentMetaData(dtype=dtype, dtype_dict=dtype_dict, nans=nans, nan_scalar=nan_scalar)
-
-
-def _generate_meta_attributes(dataset: str, component_name: str) -> dict:
-    """
-
-    Args:
-        dataset: dataset name
-        component_name: component name
-
-    Returns: meta data for all attributes
-
-    """
-    names = []
-    formats = []
-    offsets = []
-    nans = []
-    n_attrs = pgc.meta_n_attributes(dataset, component_name)
-    for i in range(n_attrs):
-        attr_name: str = pgc.meta_attribute_name(dataset, component_name, i)
-        attr_ctype: str = pgc.meta_attribute_ctype(dataset, component_name, attr_name)
-        attr_offset: int = pgc.meta_attribute_offset(dataset, component_name, attr_name)
-        attr_np_type = f"{_ENDIANNESS}{_CTYPE_NUMPY_MAP[attr_ctype]}"
-        attr_nan = _NAN_VALUE_MAP[attr_np_type]
-        names.append(attr_name)
-        formats.append(attr_np_type)
-        offsets.append(attr_offset)
-        nans.append(attr_nan)
-    return {
-        "names": names,
-        "formats": formats,
-        "offsets": offsets,
-        "itemsize": pgc.meta_component_size(dataset, component_name),
-        "aligned": True,
-        "nans": nans,
-    }
-
-
-# store meta data
-power_grid_meta_data = _generate_meta_data()
-
-
-def initialize_array(data_type: str, component_type: str, shape: Union[tuple, int], empty: bool = False):
-    """
-    Initializes an array for use in Power Grid Model calculations
-
-    Args:
-        data_type: input, update, sym_output, or asym_output
-        component_type: one component type, e.g. node
-        shape: shape of initialization
-            integer, it is a 1-dimensional array
-            tuple, it is an N-dimensional (tuple.shape) array
-        empty: if leave the memory block un-initialized
-
-    Returns:
-        np structured array with all entries as null value
-    """
-    if not isinstance(shape, tuple):
-        shape = (shape,)
-    if empty:
-        return np.empty(shape=shape, dtype=power_grid_meta_data[data_type][component_type].dtype, order="C")
-    return np.full(
-        shape=shape,
-        fill_value=power_grid_meta_data[data_type][component_type].nan_scalar,
-        dtype=power_grid_meta_data[data_type][component_type].dtype,
-        order="C",
-    )
-
-
-# prepared data for c api
-@dataclass
-class CBuffer:
-    """
-    Buffer for a single component
-    """
-
-    data: c_void_p
-    indptr: Optional[IdxPtr]  # type: ignore
-    n_elements_per_scenario: int
-    batch_size: int
-
-
-@dataclass
-class CDataset:
-    """
-    Dataset definition
-    """
-
-    dataset: Dict[str, CBuffer]
-    batch_size: int
-    n_components: int
-    components: Array
-    n_component_elements_per_scenario: Array
-    indptrs_per_component: Array
-    data_ptrs_per_component: Array
-
-
-# pylint: disable=R0912
-def prepare_cpp_array(
-    data_type: str, array_dict: Mapping[str, Union[np.ndarray, Mapping[str, np.ndarray]]]
-) -> CDataset:
-    """
-    prepare array for cpp pointers
-    Args:
-        data_type: input, update, or symmetric/asymmetric output
-        array_dict:
-            key: component type
-            value:
-                data array: can be 1D or 2D (in batches)
-                or
-                dict with
-                    key:
-                        data -> data array in flat for batches
-                        indptr -> index pointer for variable length input
-    Returns:
-        instance of CDataset ready to be fed into C API
-    """
-    # process
-    schema = power_grid_meta_data[data_type]
-    dataset_dict = {}
-    for component_name, entry in array_dict.items():
-        if component_name not in schema:
-            continue
-        # homogeneous
-        if isinstance(entry, np.ndarray):
-            data = entry
-            ndim = entry.ndim
-            indptr_c = IdxPtr()
-            if ndim == 1:
-                batch_size = 1
-                n_elements_per_scenario = entry.shape[0]
-            elif ndim == 2:  # (n_batch, n_component)
-                batch_size = entry.shape[0]
-                n_elements_per_scenario = entry.shape[1]
-            else:
-                raise ValueError(f"Array can only be 1D or 2D. {VALIDATOR_MSG}")
-        # with indptr
-        else:
-            data = entry["data"]
-            indptr: np.ndarray = entry["indptr"]
-            batch_size = indptr.size - 1
-            n_elements_per_scenario = -1
-            if data.ndim != 1:
-                raise ValueError(f"Data array can only be 1D. {VALIDATOR_MSG}")
-            if indptr.ndim != 1:
-                raise ValueError(f"indptr can only be 1D. {VALIDATOR_MSG}")
-            if indptr[0] != 0 or indptr[-1] != data.size:
-                raise ValueError(f"indptr should start from zero and end at size of data array. {VALIDATOR_MSG}")
-            if np.any(np.diff(indptr) < 0):
-                raise ValueError(f"indptr should be increasing. {VALIDATOR_MSG}")
-            indptr_c = np.ascontiguousarray(indptr, dtype=IdxNp).ctypes.data_as(IdxPtr)
-        # convert data array
-        data_c = np.ascontiguousarray(data, dtype=schema[component_name].dtype).ctypes.data_as(c_void_p)
-        dataset_dict[component_name] = CBuffer(
-            data=data_c, indptr=indptr_c, n_elements_per_scenario=n_elements_per_scenario, batch_size=batch_size
-        )
-    # total set
-    n_components = len(dataset_dict)
-    if n_components == 0:
-        batch_size = 1
-    else:
-        batch_sizes = np.array([x.batch_size for x in dataset_dict.values()])
-        if np.unique(batch_sizes).size > 1:
-            raise ValueError(f"Batch sizes across all the types should be the same! {VALIDATOR_MSG}")
-        batch_size = batch_sizes[0]
-    return CDataset(
-        dataset=dataset_dict,
-        batch_size=batch_size,
-        n_components=n_components,
-        components=(c_char_p * n_components)(*(x.encode() for x in dataset_dict)),
-        n_component_elements_per_scenario=(IdxC * n_components)(
-            *(x.n_elements_per_scenario for x in dataset_dict.values())
-        ),
-        indptrs_per_component=(IdxPtr * n_components)(*(x.indptr for x in dataset_dict.values())),  # type: ignore
-        data_ptrs_per_component=(c_void_p * n_components)(*(x.data for x in dataset_dict.values())),
-    )
+# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
+#
+# SPDX-License-Identifier: MPL-2.0
+
+"""
+Load meta data from C core and define numpy structured array
+"""
+
+from ctypes import Array, c_char_p, c_void_p
+from dataclasses import dataclass
+from typing import Any, Dict, Mapping, Optional, Union
+
+import numpy as np
+
+from power_grid_model.core.error_handling import VALIDATOR_MSG
+from power_grid_model.core.index_integer import IdxC, IdxNp
+from power_grid_model.core.power_grid_core import IdxPtr
+from power_grid_model.core.power_grid_core import power_grid_core as pgc
+
+_CTYPE_NUMPY_MAP = {"double": "f8", "int32_t": "i4", "int8_t": "i1", "double[3]": "(3,)f8"}
+_ENDIANNESS = "<" if pgc.is_little_endian() == 1 else ">"
+_NAN_VALUE_MAP = {
+    f"{_ENDIANNESS}f8": np.nan,
+    f"{_ENDIANNESS}(3,)f8": np.nan,
+    f"{_ENDIANNESS}i4": np.iinfo(f"{_ENDIANNESS}i4").min,
+    f"{_ENDIANNESS}i1": np.iinfo(f"{_ENDIANNESS}i1").min,
+}
+
+
+@dataclass
+class ComponentMetaData:
+    """
+    Data class for component metadata
+    """
+
+    dtype: np.dtype
+    dtype_dict: Dict[str, Any]
+    nans: Dict[str, Union[float, int]]
+    nan_scalar: np.ndarray
+
+    def __getitem__(self, item):
+        """
+        Get item of dataclass
+
+        Args:
+            item: item name
+
+        Returns:
+
+        """
+        return getattr(self, item)
+
+
+DatasetMetaData = Dict[str, ComponentMetaData]
+PowerGridMetaData = Dict[str, DatasetMetaData]
+
+
+def _generate_meta_data() -> PowerGridMetaData:
+    """
+
+    Returns: meta data for all dataset
+
+    """
+    py_meta_data = {}
+    n_datasets = pgc.meta_n_datasets()
+    for i in range(n_datasets):
+        dataset = pgc.meta_dataset_name(i)
+        py_meta_data[dataset] = _generate_meta_dataset(dataset)
+    return py_meta_data
+
+
+def _generate_meta_dataset(dataset: str) -> DatasetMetaData:
+    """
+
+    Args:
+        dataset: dataset name
+
+    Returns: meta data for one dataset
+
+    """
+    py_meta_dataset = {}
+    n_components = pgc.meta_n_components(dataset)
+    for i in range(n_components):
+        component_name = pgc.meta_component_name(dataset, i)
+        py_meta_dataset[component_name] = _generate_meta_component(dataset, component_name)
+    return py_meta_dataset
+
+
+def _generate_meta_component(dataset: str, component_name: str) -> ComponentMetaData:
+    """
+
+    Args:
+        dataset: dataset name
+        component_name: component name
+
+    Returns: meta data for one component
+
+    """
+
+    dtype_dict = _generate_meta_attributes(dataset, component_name)
+    dtype = np.dtype({k: v for k, v in dtype_dict.items() if k != "nans"})  # type: ignore
+    nans = dict(zip(dtype_dict["names"], dtype_dict["nans"]))
+    if dtype.alignment != pgc.meta_component_alignment(dataset, component_name):
+        raise TypeError(f'Aligment mismatch for component type: "{component_name}" !')
+    # get single nan scalar
+    nan_scalar = np.empty(1, dtype=dtype)
+    for key, value in nans.items():
+        nan_scalar[key] = value
+    # return component
+    return ComponentMetaData(dtype=dtype, dtype_dict=dtype_dict, nans=nans, nan_scalar=nan_scalar)
+
+
+def _generate_meta_attributes(dataset: str, component_name: str) -> dict:
+    """
+
+    Args:
+        dataset: dataset name
+        component_name: component name
+
+    Returns: meta data for all attributes
+
+    """
+    names = []
+    formats = []
+    offsets = []
+    nans = []
+    n_attrs = pgc.meta_n_attributes(dataset, component_name)
+    for i in range(n_attrs):
+        attr_name: str = pgc.meta_attribute_name(dataset, component_name, i)
+        attr_ctype: str = pgc.meta_attribute_ctype(dataset, component_name, attr_name)
+        attr_offset: int = pgc.meta_attribute_offset(dataset, component_name, attr_name)
+        attr_np_type = f"{_ENDIANNESS}{_CTYPE_NUMPY_MAP[attr_ctype]}"
+        attr_nan = _NAN_VALUE_MAP[attr_np_type]
+        names.append(attr_name)
+        formats.append(attr_np_type)
+        offsets.append(attr_offset)
+        nans.append(attr_nan)
+    return {
+        "names": names,
+        "formats": formats,
+        "offsets": offsets,
+        "itemsize": pgc.meta_component_size(dataset, component_name),
+        "aligned": True,
+        "nans": nans,
+    }
+
+
+# store meta data
+power_grid_meta_data = _generate_meta_data()
+
+
+def initialize_array(data_type: str, component_type: str, shape: Union[tuple, int], empty: bool = False):
+    """
+    Initializes an array for use in Power Grid Model calculations
+
+    Args:
+        data_type: input, update, sym_output, or asym_output
+        component_type: one component type, e.g. node
+        shape: shape of initialization
+            integer, it is a 1-dimensional array
+            tuple, it is an N-dimensional (tuple.shape) array
+        empty: if leave the memory block un-initialized
+
+    Returns:
+        np structured array with all entries as null value
+    """
+    if not isinstance(shape, tuple):
+        shape = (shape,)
+    if empty:
+        return np.empty(shape=shape, dtype=power_grid_meta_data[data_type][component_type].dtype, order="C")
+    return np.full(
+        shape=shape,
+        fill_value=power_grid_meta_data[data_type][component_type].nan_scalar,
+        dtype=power_grid_meta_data[data_type][component_type].dtype,
+        order="C",
+    )
+
+
+# prepared data for c api
+@dataclass
+class CBuffer:
+    """
+    Buffer for a single component
+    """
+
+    data: c_void_p
+    indptr: Optional[IdxPtr]  # type: ignore
+    n_elements_per_scenario: int
+    batch_size: int
+
+
+@dataclass
+class CDataset:
+    """
+    Dataset definition
+    """
+
+    dataset: Dict[str, CBuffer]
+    batch_size: int
+    n_components: int
+    components: Array
+    n_component_elements_per_scenario: Array
+    indptrs_per_component: Array
+    data_ptrs_per_component: Array
+
+
+# pylint: disable=R0912
+def prepare_cpp_array(
+    data_type: str, array_dict: Mapping[str, Union[np.ndarray, Mapping[str, np.ndarray]]]
+) -> CDataset:
+    """
+    prepare array for cpp pointers
+    Args:
+        data_type: input, update, or symmetric/asymmetric output
+        array_dict:
+            key: component type
+            value:
+                data array: can be 1D or 2D (in batches)
+                or
+                dict with
+                    key:
+                        data -> data array in flat for batches
+                        indptr -> index pointer for variable length input
+    Returns:
+        instance of CDataset ready to be fed into C API
+    """
+    # process
+    schema = power_grid_meta_data[data_type]
+    dataset_dict = {}
+    for component_name, entry in array_dict.items():
+        if component_name not in schema:
+            continue
+        # homogeneous
+        if isinstance(entry, np.ndarray):
+            data = entry
+            ndim = entry.ndim
+            indptr_c = IdxPtr()
+            if ndim == 1:
+                batch_size = 1
+                n_elements_per_scenario = entry.shape[0]
+            elif ndim == 2:  # (n_batch, n_component)
+                batch_size = entry.shape[0]
+                n_elements_per_scenario = entry.shape[1]
+            else:
+                raise ValueError(f"Array can only be 1D or 2D. {VALIDATOR_MSG}")
+        # with indptr
+        else:
+            data = entry["data"]
+            indptr: np.ndarray = entry["indptr"]
+            batch_size = indptr.size - 1
+            n_elements_per_scenario = -1
+            if data.ndim != 1:
+                raise ValueError(f"Data array can only be 1D. {VALIDATOR_MSG}")
+            if indptr.ndim != 1:
+                raise ValueError(f"indptr can only be 1D. {VALIDATOR_MSG}")
+            if indptr[0] != 0 or indptr[-1] != data.size:
+                raise ValueError(f"indptr should start from zero and end at size of data array. {VALIDATOR_MSG}")
+            if np.any(np.diff(indptr) < 0):
+                raise ValueError(f"indptr should be increasing. {VALIDATOR_MSG}")
+            indptr_c = np.ascontiguousarray(indptr, dtype=IdxNp).ctypes.data_as(IdxPtr)
+        # convert data array
+        data_c = np.ascontiguousarray(data, dtype=schema[component_name].dtype).ctypes.data_as(c_void_p)
+        dataset_dict[component_name] = CBuffer(
+            data=data_c, indptr=indptr_c, n_elements_per_scenario=n_elements_per_scenario, batch_size=batch_size
+        )
+    # total set
+    n_components = len(dataset_dict)
+    if n_components == 0:
+        batch_size = 1
+    else:
+        batch_sizes = np.array([x.batch_size for x in dataset_dict.values()])
+        if np.unique(batch_sizes).size > 1:
+            raise ValueError(f"Batch sizes across all the types should be the same! {VALIDATOR_MSG}")
+        batch_size = batch_sizes[0]
+    return CDataset(
+        dataset=dataset_dict,
+        batch_size=batch_size,
+        n_components=n_components,
+        components=(c_char_p * n_components)(*(x.encode() for x in dataset_dict)),
+        n_component_elements_per_scenario=(IdxC * n_components)(
+            *(x.n_elements_per_scenario for x in dataset_dict.values())
+        ),
+        indptrs_per_component=(IdxPtr * n_components)(*(x.indptr for x in dataset_dict.values())),  # type: ignore
+        data_ptrs_per_component=(c_void_p * n_components)(*(x.data for x in dataset_dict.values())),
+    )
```

## power_grid_model/core/power_grid_model.py

 * *Ordering differences only*

```diff
@@ -1,422 +1,422 @@
-# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
-#
-# SPDX-License-Identifier: MPL-2.0
-
-
-"""
-Main power grid model class
-"""
-from typing import Dict, List, Optional, Set, Union
-
-import numpy as np
-
-from power_grid_model.core.error_handling import PowerGridBatchError, assert_no_error, find_error
-from power_grid_model.core.index_integer import IdNp, IdxNp
-from power_grid_model.core.options import Options
-from power_grid_model.core.power_grid_core import IDPtr, IdxPtr, ModelPtr
-from power_grid_model.core.power_grid_core import power_grid_core as pgc
-from power_grid_model.core.power_grid_meta import CDataset, initialize_array, power_grid_meta_data, prepare_cpp_array
-from power_grid_model.enum import CalculationMethod, CalculationType
-
-
-class PowerGridModel:
-    """
-    Main class for Power Grid Model
-    """
-
-    _model_ptr: ModelPtr
-    _all_component_count: Optional[Dict[str, int]]
-    _batch_error: Optional[PowerGridBatchError]
-
-    @property
-    def batch_error(self) -> Optional[PowerGridBatchError]:
-        """
-        Get the batch error object, if present
-
-        Returns: Batch error object, or None
-
-        """
-        return self._batch_error
-
-    @property
-    def _model(self):
-        if not self._model_ptr:
-            raise TypeError("You have an empty instance of PowerGridModel!")
-        return self._model_ptr
-
-    @property
-    def all_component_count(self) -> Dict[str, int]:
-        """
-        Get count of number of elements per component type.
-        If the count for a component type is zero, it will not be in the returned dictionary.
-        Returns:
-            a dictionary with
-                key: component type name
-                value: integer count of elements of this type
-        """
-        if self._all_component_count is None:
-            raise TypeError("You have an empty instance of PowerGridModel!")
-        return self._all_component_count
-
-    def copy(self) -> "PowerGridModel":
-        """
-
-        Copy the current model
-
-        Returns:
-            a copy of PowerGridModel
-        """
-        new_model = PowerGridModel.__new__(PowerGridModel)
-        new_model._model_ptr = pgc.copy_model(self._model)  # pylint: disable=W0212
-        assert_no_error()
-        new_model._all_component_count = self._all_component_count  # pylint: disable=W0212
-        return new_model
-
-    def __copy__(self):
-        return self.copy()
-
-    def __new__(cls, *_args, **_kwargs):
-        instance = super().__new__(cls)
-        instance._model_ptr = ModelPtr()
-        instance._all_component_count = None
-        return instance
-
-    def __init__(self, input_data: Dict[str, np.ndarray], system_frequency: float = 50.0):
-        """
-        Initialize the model from an input data set.
-
-        Args:
-            input_data: input data dictionary
-                key: component type name
-                value: 1D numpy structured array for this component input
-            system_frequency: frequency of the power system, default 50 Hz
-        """
-        # destroy old instance
-        pgc.destroy_model(self._model_ptr)
-        self._all_component_count = None
-        # create new
-        prepared_input: CDataset = prepare_cpp_array("input", input_data)
-        self._model_ptr = pgc.create_model(
-            system_frequency,
-            components=prepared_input.components,
-            n_components=prepared_input.n_components,
-            component_sizes=prepared_input.n_component_elements_per_scenario,
-            input_data=prepared_input.data_ptrs_per_component,
-        )
-        assert_no_error()
-        self._all_component_count = {
-            k: v.n_elements_per_scenario for k, v in prepared_input.dataset.items() if v.n_elements_per_scenario > 0
-        }
-
-    def update(self, *, update_data: Dict[str, np.ndarray]):
-        """
-        Update the model with changes.
-        Args:
-            update_data: update data dictionary
-                key: component type name
-                value: 1D numpy structured array for this component update
-        Returns:
-            None
-        """
-        prepared_update: CDataset = prepare_cpp_array("update", update_data)
-        pgc.update_model(
-            self._model,
-            prepared_update.n_components,
-            prepared_update.components,
-            prepared_update.n_component_elements_per_scenario,
-            prepared_update.data_ptrs_per_component,
-        )
-        assert_no_error()
-
-    def get_indexer(self, component_type: str, ids: np.ndarray):
-        """
-        Get array of indexers given array of ids for component type
-
-        Args:
-            component_type: type of component
-            ids: array of ids
-
-        Returns:
-            array of inderxers, same shape as input array ids
-
-        """
-        ids_c = np.ascontiguousarray(ids, dtype=IdNp).ctypes.data_as(IDPtr)
-        indexer = np.empty_like(ids, dtype=IdxNp, order="C")
-        indexer_c = indexer.ctypes.data_as(IdxPtr)
-        size = ids.size
-        # call c function
-        pgc.get_indexer(self._model, component_type, size, ids_c, indexer_c)
-        assert_no_error()
-        return indexer
-
-    # pylint: disable=too-many-locals
-    # pylint: disable=too-many-branches
-    # pylint: disable=too-many-arguments
-    def _calculate(
-        self,
-        calculation_type: CalculationType,
-        symmetric: bool,
-        error_tolerance: float,
-        max_iterations: int,
-        calculation_method: Union[CalculationMethod, str],
-        update_data: Optional[Dict[str, Union[np.ndarray, Dict[str, np.ndarray]]]],
-        threading: int,
-        output_component_types: Optional[Union[Set[str], List[str]]],
-        continue_on_batch_error: bool,
-    ):
-        """
-        Core calculation routine
-
-        Args:
-            calculation_type:
-            symmetric:
-            error_tolerance:
-            max_iterations:
-            calculation_method:
-            update_data:
-            threading:
-            output_component_types:
-            continue_on_batch_error:
-
-        Returns:
-
-        """
-        if isinstance(calculation_method, str):
-            calculation_method = CalculationMethod[calculation_method]
-        if symmetric:
-            output_type = "sym_output"
-        else:
-            output_type = "asym_output"
-        self._batch_error = None
-
-        # prepare update dataset
-        # update data exist for batch calculation
-        if update_data is not None:
-            batch_calculation = True
-        # no update dataset, create one batch with empty set
-        else:
-            batch_calculation = False
-            update_data = {}
-        prepared_update: CDataset = prepare_cpp_array(data_type="update", array_dict=update_data)
-        batch_size = prepared_update.batch_size
-
-        # prepare result dataset
-        all_component_count = self.all_component_count
-        # for power flow, there is no need for sensor output
-        if calculation_type == CalculationType.power_flow:
-            all_component_count = {k: v for k, v in all_component_count.items() if "sensor" not in k}
-        # limit all component count to user specified component types in output
-        if output_component_types is None:
-            output_component_types = set(all_component_count.keys())
-        # raise error is some specified components are unknown
-        unknown_components = [x for x in output_component_types if x not in power_grid_meta_data[output_type]]
-        if unknown_components:
-            raise KeyError(f"You have specified some unknown component types: {unknown_components}")
-        all_component_count = {k: v for k, v in all_component_count.items() if k in output_component_types}
-        # create result dataset
-        result_dict = {}
-        for name, count in all_component_count.items():
-            # intialize array
-            arr = initialize_array(output_type, name, (batch_size, count), empty=True)
-            result_dict[name] = arr
-        prepared_result: CDataset = prepare_cpp_array(data_type=output_type, array_dict=result_dict)
-
-        # prepare options
-        opt: Options = Options()
-        opt.calculation_type = calculation_type.value
-        opt.calculation_method = calculation_method.value
-        opt.symmetric = symmetric
-        opt.error_tolerance = error_tolerance
-        opt.max_iteration = max_iterations
-        opt.threading = threading
-
-        # run calculation
-        pgc.calculate(
-            # model and options
-            self._model,
-            opt.opt,
-            # result dataset
-            prepared_result.n_components,
-            prepared_result.components,
-            prepared_result.data_ptrs_per_component,
-            # update dataset
-            batch_size,
-            prepared_update.n_components,
-            prepared_update.components,
-            prepared_update.n_component_elements_per_scenario,
-            prepared_update.indptrs_per_component,
-            prepared_update.data_ptrs_per_component,
-        )
-
-        # error handling
-        if not continue_on_batch_error:
-            assert_no_error(batch_size=batch_size)
-        else:
-            # continue on batch error
-            error: Optional[RuntimeError] = find_error(batch_size=batch_size)
-            if error is not None:
-                if isinstance(error, PowerGridBatchError):
-                    # continue on batch error
-                    self._batch_error = error
-                else:
-                    # raise normal error
-                    raise error
-
-        # flatten array for normal calculation
-        if not batch_calculation:
-            result_dict = {k: v.ravel() for k, v in result_dict.items()}
-
-        return result_dict
-
-    def calculate_power_flow(
-        self,
-        *,
-        symmetric: bool = True,
-        error_tolerance: float = 1e-8,
-        max_iterations: int = 20,
-        calculation_method: Union[CalculationMethod, str] = CalculationMethod.newton_raphson,
-        update_data: Optional[Dict[str, Union[np.ndarray, Dict[str, np.ndarray]]]] = None,
-        threading: int = -1,
-        output_component_types: Optional[Union[Set[str], List[str]]] = None,
-        continue_on_batch_error: bool = False,
-    ) -> Dict[str, np.ndarray]:
-        """
-        Calculate power flow once with the current model attributes.
-        Or calculate in batch with the given update dataset in batch
-
-        Args:
-            symmetric:
-                True: three-phase symmetric calculation, even for asymmetric loads/generations
-                False: three-phase asymmetric calculation
-            error_tolerance:
-                error tolerance for voltage in p.u., only applicable when iterative=True
-            max_iterations:
-                maximum number of iterations, only applicable when iterative=True
-            calculation_method: an enumeration or string
-                newton_raphson: use Newton-Raphson iterative method (default)
-                linear: use linear method
-            update_data:
-                None: calculate power flow once with the current model attributes
-                A dictionary for batch calculation with batch update
-                    key: component type name to be updated in batch
-                    value:
-                        a 2D numpy structured array for homogeneous update batch
-                            Dimension 0: each batch
-                            Dimension 1: each updated element per batch for this component type
-                        **or**
-                        a dictionary containing two keys, for inhomogeneous update batch
-                            indptr: a 1D integer numpy array with length n_batch + 1
-                                given batch number k, the update array for this batch is
-                                data[indptr[k]:indptr[k + 1]]
-                                This is the concept of compressed sparse structure
-                                https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html
-                            data: 1D numpy structured array in flat
-            threading:
-                only applicable for batch calculation
-                < 0 sequential
-                = 0 parallel, use number of hardware threads
-                > 0 specify number of parallel threads
-            output_component_types: list or set of component types you want to be present in the output dict.
-                By default all component types will be in the output
-            continue_on_batch_error: if the program continues (instead of throwing error) if some scenarios fails
-
-        Returns:
-            dictionary of results of all components
-                key: component type name to be updated in batch
-                value:
-                    for single calculation: 1D numpy structured array for the results of this component type
-                    for batch calculation: 2D numpy structured array for the results of this component type
-                        Dimension 0: each batch
-                        Dimension 1: the result of each element for this component type
-            Error handling:
-                in case an error in the core occurs, an exception will be thrown
-        """
-        return self._calculate(
-            CalculationType.power_flow,
-            symmetric=symmetric,
-            error_tolerance=error_tolerance,
-            max_iterations=max_iterations,
-            calculation_method=calculation_method,
-            update_data=update_data,
-            threading=threading,
-            output_component_types=output_component_types,
-            continue_on_batch_error=continue_on_batch_error,
-        )
-
-    def calculate_state_estimation(
-        self,
-        *,
-        symmetric: bool = True,
-        error_tolerance: float = 1e-8,
-        max_iterations: int = 20,
-        calculation_method: Union[CalculationMethod, str] = CalculationMethod.iterative_linear,
-        update_data: Optional[Dict[str, Union[np.ndarray, Dict[str, np.ndarray]]]] = None,
-        threading: int = -1,
-        output_component_types: Optional[Union[Set[str], List[str]]] = None,
-        continue_on_batch_error: bool = False,
-    ) -> Dict[str, np.ndarray]:
-        """
-        Calculate state estimation once with the current model attributes.
-        Or calculate in batch with the given update dataset in batch
-
-        Args:
-            symmetric:
-                True: three-phase symmetric calculation, even for asymmetric loads/generations
-                False: three-phase asymmetric calculation
-            error_tolerance:
-                error tolerance for voltage in p.u., only applicable when iterative=True
-            max_iterations:
-                maximum number of iterations, only applicable when iterative=True
-            calculation_method: an enumeration
-                iterative_linear: use iterative linear method
-            update_data:
-                None: calculate state estimation once with the current model attributes
-                A dictionary for batch calculation with batch update
-                    key: component type name to be updated in batch
-                    value:
-                        a 2D numpy structured array for homogeneous update batch
-                            Dimension 0: each batch
-                            Dimension 1: each updated element per batch for this component type
-                        **or**
-                        a dictionary containing two keys, for inhomogeneous update batch
-                            indptr: a 1D integer numpy array with length n_batch + 1
-                                given batch number k, the update array for this batch is
-                                data[indptr[k]:indptr[k + 1]]
-                                This is the concept of compressed sparse structure
-                                https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html
-                            data: 1D numpy structured array in flat
-            threading:
-                only applicable for batch calculation
-                < 0 sequential
-                = 0 parallel, use number of hardware threads
-                > 0 specify number of parallel threads
-            output_component_types: list or set of component types you want to be present in the output dict.
-                By default all component types will be in the output
-            continue_on_batch_error: if the program continues (instead of throwing error) if some scenarios fails
-
-
-        Returns:
-            dictionary of results of all components
-                key: component type name to be updated in batch
-                value:
-                    for single calculation: 1D numpy structured array for the results of this component type
-                    for batch calculation: 2D numpy structured array for the results of this component type
-                        Dimension 0: each batch
-                        Dimension 1: the result of each element for this component type
-            Error handling:
-                in case an error in the core occurs, an exception will be thrown
-        """
-        return self._calculate(
-            CalculationType.state_estimation,
-            symmetric=symmetric,
-            error_tolerance=error_tolerance,
-            max_iterations=max_iterations,
-            calculation_method=calculation_method,
-            update_data=update_data,
-            threading=threading,
-            output_component_types=output_component_types,
-            continue_on_batch_error=continue_on_batch_error,
-        )
-
-    def __del__(self):
-        pgc.destroy_model(self._model_ptr)
+# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
+#
+# SPDX-License-Identifier: MPL-2.0
+
+
+"""
+Main power grid model class
+"""
+from typing import Dict, List, Optional, Set, Union
+
+import numpy as np
+
+from power_grid_model.core.error_handling import PowerGridBatchError, assert_no_error, find_error
+from power_grid_model.core.index_integer import IdNp, IdxNp
+from power_grid_model.core.options import Options
+from power_grid_model.core.power_grid_core import IDPtr, IdxPtr, ModelPtr
+from power_grid_model.core.power_grid_core import power_grid_core as pgc
+from power_grid_model.core.power_grid_meta import CDataset, initialize_array, power_grid_meta_data, prepare_cpp_array
+from power_grid_model.enum import CalculationMethod, CalculationType
+
+
+class PowerGridModel:
+    """
+    Main class for Power Grid Model
+    """
+
+    _model_ptr: ModelPtr
+    _all_component_count: Optional[Dict[str, int]]
+    _batch_error: Optional[PowerGridBatchError]
+
+    @property
+    def batch_error(self) -> Optional[PowerGridBatchError]:
+        """
+        Get the batch error object, if present
+
+        Returns: Batch error object, or None
+
+        """
+        return self._batch_error
+
+    @property
+    def _model(self):
+        if not self._model_ptr:
+            raise TypeError("You have an empty instance of PowerGridModel!")
+        return self._model_ptr
+
+    @property
+    def all_component_count(self) -> Dict[str, int]:
+        """
+        Get count of number of elements per component type.
+        If the count for a component type is zero, it will not be in the returned dictionary.
+        Returns:
+            a dictionary with
+                key: component type name
+                value: integer count of elements of this type
+        """
+        if self._all_component_count is None:
+            raise TypeError("You have an empty instance of PowerGridModel!")
+        return self._all_component_count
+
+    def copy(self) -> "PowerGridModel":
+        """
+
+        Copy the current model
+
+        Returns:
+            a copy of PowerGridModel
+        """
+        new_model = PowerGridModel.__new__(PowerGridModel)
+        new_model._model_ptr = pgc.copy_model(self._model)  # pylint: disable=W0212
+        assert_no_error()
+        new_model._all_component_count = self._all_component_count  # pylint: disable=W0212
+        return new_model
+
+    def __copy__(self):
+        return self.copy()
+
+    def __new__(cls, *_args, **_kwargs):
+        instance = super().__new__(cls)
+        instance._model_ptr = ModelPtr()
+        instance._all_component_count = None
+        return instance
+
+    def __init__(self, input_data: Dict[str, np.ndarray], system_frequency: float = 50.0):
+        """
+        Initialize the model from an input data set.
+
+        Args:
+            input_data: input data dictionary
+                key: component type name
+                value: 1D numpy structured array for this component input
+            system_frequency: frequency of the power system, default 50 Hz
+        """
+        # destroy old instance
+        pgc.destroy_model(self._model_ptr)
+        self._all_component_count = None
+        # create new
+        prepared_input: CDataset = prepare_cpp_array("input", input_data)
+        self._model_ptr = pgc.create_model(
+            system_frequency,
+            components=prepared_input.components,
+            n_components=prepared_input.n_components,
+            component_sizes=prepared_input.n_component_elements_per_scenario,
+            input_data=prepared_input.data_ptrs_per_component,
+        )
+        assert_no_error()
+        self._all_component_count = {
+            k: v.n_elements_per_scenario for k, v in prepared_input.dataset.items() if v.n_elements_per_scenario > 0
+        }
+
+    def update(self, *, update_data: Dict[str, np.ndarray]):
+        """
+        Update the model with changes.
+        Args:
+            update_data: update data dictionary
+                key: component type name
+                value: 1D numpy structured array for this component update
+        Returns:
+            None
+        """
+        prepared_update: CDataset = prepare_cpp_array("update", update_data)
+        pgc.update_model(
+            self._model,
+            prepared_update.n_components,
+            prepared_update.components,
+            prepared_update.n_component_elements_per_scenario,
+            prepared_update.data_ptrs_per_component,
+        )
+        assert_no_error()
+
+    def get_indexer(self, component_type: str, ids: np.ndarray):
+        """
+        Get array of indexers given array of ids for component type
+
+        Args:
+            component_type: type of component
+            ids: array of ids
+
+        Returns:
+            array of inderxers, same shape as input array ids
+
+        """
+        ids_c = np.ascontiguousarray(ids, dtype=IdNp).ctypes.data_as(IDPtr)
+        indexer = np.empty_like(ids, dtype=IdxNp, order="C")
+        indexer_c = indexer.ctypes.data_as(IdxPtr)
+        size = ids.size
+        # call c function
+        pgc.get_indexer(self._model, component_type, size, ids_c, indexer_c)
+        assert_no_error()
+        return indexer
+
+    # pylint: disable=too-many-locals
+    # pylint: disable=too-many-branches
+    # pylint: disable=too-many-arguments
+    def _calculate(
+        self,
+        calculation_type: CalculationType,
+        symmetric: bool,
+        error_tolerance: float,
+        max_iterations: int,
+        calculation_method: Union[CalculationMethod, str],
+        update_data: Optional[Dict[str, Union[np.ndarray, Dict[str, np.ndarray]]]],
+        threading: int,
+        output_component_types: Optional[Union[Set[str], List[str]]],
+        continue_on_batch_error: bool,
+    ):
+        """
+        Core calculation routine
+
+        Args:
+            calculation_type:
+            symmetric:
+            error_tolerance:
+            max_iterations:
+            calculation_method:
+            update_data:
+            threading:
+            output_component_types:
+            continue_on_batch_error:
+
+        Returns:
+
+        """
+        if isinstance(calculation_method, str):
+            calculation_method = CalculationMethod[calculation_method]
+        if symmetric:
+            output_type = "sym_output"
+        else:
+            output_type = "asym_output"
+        self._batch_error = None
+
+        # prepare update dataset
+        # update data exist for batch calculation
+        if update_data is not None:
+            batch_calculation = True
+        # no update dataset, create one batch with empty set
+        else:
+            batch_calculation = False
+            update_data = {}
+        prepared_update: CDataset = prepare_cpp_array(data_type="update", array_dict=update_data)
+        batch_size = prepared_update.batch_size
+
+        # prepare result dataset
+        all_component_count = self.all_component_count
+        # for power flow, there is no need for sensor output
+        if calculation_type == CalculationType.power_flow:
+            all_component_count = {k: v for k, v in all_component_count.items() if "sensor" not in k}
+        # limit all component count to user specified component types in output
+        if output_component_types is None:
+            output_component_types = set(all_component_count.keys())
+        # raise error is some specified components are unknown
+        unknown_components = [x for x in output_component_types if x not in power_grid_meta_data[output_type]]
+        if unknown_components:
+            raise KeyError(f"You have specified some unknown component types: {unknown_components}")
+        all_component_count = {k: v for k, v in all_component_count.items() if k in output_component_types}
+        # create result dataset
+        result_dict = {}
+        for name, count in all_component_count.items():
+            # intialize array
+            arr = initialize_array(output_type, name, (batch_size, count), empty=True)
+            result_dict[name] = arr
+        prepared_result: CDataset = prepare_cpp_array(data_type=output_type, array_dict=result_dict)
+
+        # prepare options
+        opt: Options = Options()
+        opt.calculation_type = calculation_type.value
+        opt.calculation_method = calculation_method.value
+        opt.symmetric = symmetric
+        opt.error_tolerance = error_tolerance
+        opt.max_iteration = max_iterations
+        opt.threading = threading
+
+        # run calculation
+        pgc.calculate(
+            # model and options
+            self._model,
+            opt.opt,
+            # result dataset
+            prepared_result.n_components,
+            prepared_result.components,
+            prepared_result.data_ptrs_per_component,
+            # update dataset
+            batch_size,
+            prepared_update.n_components,
+            prepared_update.components,
+            prepared_update.n_component_elements_per_scenario,
+            prepared_update.indptrs_per_component,
+            prepared_update.data_ptrs_per_component,
+        )
+
+        # error handling
+        if not continue_on_batch_error:
+            assert_no_error(batch_size=batch_size)
+        else:
+            # continue on batch error
+            error: Optional[RuntimeError] = find_error(batch_size=batch_size)
+            if error is not None:
+                if isinstance(error, PowerGridBatchError):
+                    # continue on batch error
+                    self._batch_error = error
+                else:
+                    # raise normal error
+                    raise error
+
+        # flatten array for normal calculation
+        if not batch_calculation:
+            result_dict = {k: v.ravel() for k, v in result_dict.items()}
+
+        return result_dict
+
+    def calculate_power_flow(
+        self,
+        *,
+        symmetric: bool = True,
+        error_tolerance: float = 1e-8,
+        max_iterations: int = 20,
+        calculation_method: Union[CalculationMethod, str] = CalculationMethod.newton_raphson,
+        update_data: Optional[Dict[str, Union[np.ndarray, Dict[str, np.ndarray]]]] = None,
+        threading: int = -1,
+        output_component_types: Optional[Union[Set[str], List[str]]] = None,
+        continue_on_batch_error: bool = False,
+    ) -> Dict[str, np.ndarray]:
+        """
+        Calculate power flow once with the current model attributes.
+        Or calculate in batch with the given update dataset in batch
+
+        Args:
+            symmetric:
+                True: three-phase symmetric calculation, even for asymmetric loads/generations
+                False: three-phase asymmetric calculation
+            error_tolerance:
+                error tolerance for voltage in p.u., only applicable when iterative=True
+            max_iterations:
+                maximum number of iterations, only applicable when iterative=True
+            calculation_method: an enumeration or string
+                newton_raphson: use Newton-Raphson iterative method (default)
+                linear: use linear method
+            update_data:
+                None: calculate power flow once with the current model attributes
+                A dictionary for batch calculation with batch update
+                    key: component type name to be updated in batch
+                    value:
+                        a 2D numpy structured array for homogeneous update batch
+                            Dimension 0: each batch
+                            Dimension 1: each updated element per batch for this component type
+                        **or**
+                        a dictionary containing two keys, for inhomogeneous update batch
+                            indptr: a 1D integer numpy array with length n_batch + 1
+                                given batch number k, the update array for this batch is
+                                data[indptr[k]:indptr[k + 1]]
+                                This is the concept of compressed sparse structure
+                                https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html
+                            data: 1D numpy structured array in flat
+            threading:
+                only applicable for batch calculation
+                < 0 sequential
+                = 0 parallel, use number of hardware threads
+                > 0 specify number of parallel threads
+            output_component_types: list or set of component types you want to be present in the output dict.
+                By default all component types will be in the output
+            continue_on_batch_error: if the program continues (instead of throwing error) if some scenarios fails
+
+        Returns:
+            dictionary of results of all components
+                key: component type name to be updated in batch
+                value:
+                    for single calculation: 1D numpy structured array for the results of this component type
+                    for batch calculation: 2D numpy structured array for the results of this component type
+                        Dimension 0: each batch
+                        Dimension 1: the result of each element for this component type
+            Error handling:
+                in case an error in the core occurs, an exception will be thrown
+        """
+        return self._calculate(
+            CalculationType.power_flow,
+            symmetric=symmetric,
+            error_tolerance=error_tolerance,
+            max_iterations=max_iterations,
+            calculation_method=calculation_method,
+            update_data=update_data,
+            threading=threading,
+            output_component_types=output_component_types,
+            continue_on_batch_error=continue_on_batch_error,
+        )
+
+    def calculate_state_estimation(
+        self,
+        *,
+        symmetric: bool = True,
+        error_tolerance: float = 1e-8,
+        max_iterations: int = 20,
+        calculation_method: Union[CalculationMethod, str] = CalculationMethod.iterative_linear,
+        update_data: Optional[Dict[str, Union[np.ndarray, Dict[str, np.ndarray]]]] = None,
+        threading: int = -1,
+        output_component_types: Optional[Union[Set[str], List[str]]] = None,
+        continue_on_batch_error: bool = False,
+    ) -> Dict[str, np.ndarray]:
+        """
+        Calculate state estimation once with the current model attributes.
+        Or calculate in batch with the given update dataset in batch
+
+        Args:
+            symmetric:
+                True: three-phase symmetric calculation, even for asymmetric loads/generations
+                False: three-phase asymmetric calculation
+            error_tolerance:
+                error tolerance for voltage in p.u., only applicable when iterative=True
+            max_iterations:
+                maximum number of iterations, only applicable when iterative=True
+            calculation_method: an enumeration
+                iterative_linear: use iterative linear method
+            update_data:
+                None: calculate state estimation once with the current model attributes
+                A dictionary for batch calculation with batch update
+                    key: component type name to be updated in batch
+                    value:
+                        a 2D numpy structured array for homogeneous update batch
+                            Dimension 0: each batch
+                            Dimension 1: each updated element per batch for this component type
+                        **or**
+                        a dictionary containing two keys, for inhomogeneous update batch
+                            indptr: a 1D integer numpy array with length n_batch + 1
+                                given batch number k, the update array for this batch is
+                                data[indptr[k]:indptr[k + 1]]
+                                This is the concept of compressed sparse structure
+                                https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html
+                            data: 1D numpy structured array in flat
+            threading:
+                only applicable for batch calculation
+                < 0 sequential
+                = 0 parallel, use number of hardware threads
+                > 0 specify number of parallel threads
+            output_component_types: list or set of component types you want to be present in the output dict.
+                By default all component types will be in the output
+            continue_on_batch_error: if the program continues (instead of throwing error) if some scenarios fails
+
+
+        Returns:
+            dictionary of results of all components
+                key: component type name to be updated in batch
+                value:
+                    for single calculation: 1D numpy structured array for the results of this component type
+                    for batch calculation: 2D numpy structured array for the results of this component type
+                        Dimension 0: each batch
+                        Dimension 1: the result of each element for this component type
+            Error handling:
+                in case an error in the core occurs, an exception will be thrown
+        """
+        return self._calculate(
+            CalculationType.state_estimation,
+            symmetric=symmetric,
+            error_tolerance=error_tolerance,
+            max_iterations=max_iterations,
+            calculation_method=calculation_method,
+            update_data=update_data,
+            threading=threading,
+            output_component_types=output_component_types,
+            continue_on_batch_error=continue_on_batch_error,
+        )
+
+    def __del__(self):
+        pgc.destroy_model(self._model_ptr)
```

## power_grid_model/validation/__init__.py

 * *Ordering differences only*

```diff
@@ -1,10 +1,10 @@
-# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
-#
-# SPDX-License-Identifier: MPL-2.0
-
-"""Power Grid Model input/update data validation"""
-
-from power_grid_model.validation.assertions import ValidationException, assert_valid_batch_data, assert_valid_input_data
-from power_grid_model.validation.errors import ValidationError
-from power_grid_model.validation.utils import errors_to_string
-from power_grid_model.validation.validation import validate_batch_data, validate_input_data
+# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
+#
+# SPDX-License-Identifier: MPL-2.0
+
+"""Power Grid Model input/update data validation"""
+
+from power_grid_model.validation.assertions import ValidationException, assert_valid_batch_data, assert_valid_input_data
+from power_grid_model.validation.errors import ValidationError
+from power_grid_model.validation.utils import errors_to_string
+from power_grid_model.validation.validation import validate_batch_data, validate_input_data
```

## power_grid_model/validation/assertions.py

 * *Ordering differences only*

```diff
@@ -1,94 +1,94 @@
-# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
-#
-# SPDX-License-Identifier: MPL-2.0
-
-"""
-Helper functions to assert valid data. They basically call validate_input_data or validate_batch_data and raise a
-ValidationException if the validation results in one or more errors.
-"""
-from typing import Dict, List, Optional, Union
-
-from power_grid_model.data_types import BatchDataset, SingleDataset
-from power_grid_model.enum import CalculationType
-from power_grid_model.validation.errors import ValidationError
-from power_grid_model.validation.utils import errors_to_string
-from power_grid_model.validation.validation import validate_batch_data, validate_input_data
-
-
-class ValidationException(ValueError):
-    """
-    An exception storing the name of the validated data, a list/dict of errors and a convenient conversion to string
-    to display a summary of all the errors when printing the exception.
-    """
-
-    def __init__(self, errors: Union[List[ValidationError], Dict[int, List[ValidationError]]], name: str = "data"):
-        super().__init__(f"Invalid {name}")
-        self.errors = errors
-        self.name = name
-
-    def __str__(self):
-        return errors_to_string(errors=self.errors, name=self.name)
-
-
-def assert_valid_input_data(
-    input_data: SingleDataset, calculation_type: Optional[CalculationType] = None, symmetric: bool = True
-):
-    """
-    Validates the entire input dataset:
-
-        1. Is the data structure correct? (checking data types and numpy array shapes)
-        2. Are all required values provided? (checking NaNs)
-        3. Are all ID's unique? (checking object identifiers across all components)
-        4. Are the supplied values valid? (checking limits and other logic as described in "Graph Data Model")
-
-    Args:
-        input_data: A power-grid-model input dataset
-        calculation_type: Supply a calculation method, to allow missing values for unused fields
-        symmetric: A boolean to state whether input data will be used for a symmetric or asymmetric calculation
-
-    Raises:
-        KeyError, TypeError or ValueError if the data structure is invalid.
-        ValidationException if the contents are invalid.
-    """
-    validation_errors = validate_input_data(
-        input_data=input_data, calculation_type=calculation_type, symmetric=symmetric
-    )
-    if validation_errors:
-        raise ValidationException(validation_errors, "input_data")
-
-
-def assert_valid_batch_data(
-    input_data: SingleDataset,
-    update_data: BatchDataset,
-    calculation_type: Optional[CalculationType] = None,
-    symmetric: bool = True,
-):
-    """
-    Ihe input dataset is validated:
-
-        1. Is the data structure correct? (checking data types and numpy array shapes)
-        2. Are all input data ID's unique? (checking object identifiers across all components)
-
-    For each batch the update data is validated:
-        3. Is the update data structure correct? (checking data types and numpy array shapes)
-        4. Are all update ID's valid? (checking object identifiers across update and input data)
-
-    Then (for each batch independently) the input dataset is updated with the batch's update data and validated:
-        5. Are all required values provided? (checking NaNs)
-        6. Are the supplied values valid? (checking limits and other logic as described in "Graph Data Model")
-
-    Args:
-        input_data: a power-grid-model input dataset
-        update_data: a power-grid-model update dataset (one or more batches)
-        calculation_type: Supply a calculation method, to allow missing values for unused fields
-        symmetric: A boolean to state whether input data will be used for a symmetric or asymmetric calculation
-
-    Raises:
-        KeyError, TypeError or ValueError if the data structure is invalid.
-        ValidationException if the contents are invalid.
-    """
-    validation_errors = validate_batch_data(
-        input_data=input_data, update_data=update_data, calculation_type=calculation_type, symmetric=symmetric
-    )
-    if validation_errors:
-        raise ValidationException(validation_errors, "update_data")
+# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
+#
+# SPDX-License-Identifier: MPL-2.0
+
+"""
+Helper functions to assert valid data. They basically call validate_input_data or validate_batch_data and raise a
+ValidationException if the validation results in one or more errors.
+"""
+from typing import Dict, List, Optional, Union
+
+from power_grid_model.data_types import BatchDataset, SingleDataset
+from power_grid_model.enum import CalculationType
+from power_grid_model.validation.errors import ValidationError
+from power_grid_model.validation.utils import errors_to_string
+from power_grid_model.validation.validation import validate_batch_data, validate_input_data
+
+
+class ValidationException(ValueError):
+    """
+    An exception storing the name of the validated data, a list/dict of errors and a convenient conversion to string
+    to display a summary of all the errors when printing the exception.
+    """
+
+    def __init__(self, errors: Union[List[ValidationError], Dict[int, List[ValidationError]]], name: str = "data"):
+        super().__init__(f"Invalid {name}")
+        self.errors = errors
+        self.name = name
+
+    def __str__(self):
+        return errors_to_string(errors=self.errors, name=self.name)
+
+
+def assert_valid_input_data(
+    input_data: SingleDataset, calculation_type: Optional[CalculationType] = None, symmetric: bool = True
+):
+    """
+    Validates the entire input dataset:
+
+        1. Is the data structure correct? (checking data types and numpy array shapes)
+        2. Are all required values provided? (checking NaNs)
+        3. Are all ID's unique? (checking object identifiers across all components)
+        4. Are the supplied values valid? (checking limits and other logic as described in "Graph Data Model")
+
+    Args:
+        input_data: A power-grid-model input dataset
+        calculation_type: Supply a calculation method, to allow missing values for unused fields
+        symmetric: A boolean to state whether input data will be used for a symmetric or asymmetric calculation
+
+    Raises:
+        KeyError, TypeError or ValueError if the data structure is invalid.
+        ValidationException if the contents are invalid.
+    """
+    validation_errors = validate_input_data(
+        input_data=input_data, calculation_type=calculation_type, symmetric=symmetric
+    )
+    if validation_errors:
+        raise ValidationException(validation_errors, "input_data")
+
+
+def assert_valid_batch_data(
+    input_data: SingleDataset,
+    update_data: BatchDataset,
+    calculation_type: Optional[CalculationType] = None,
+    symmetric: bool = True,
+):
+    """
+    Ihe input dataset is validated:
+
+        1. Is the data structure correct? (checking data types and numpy array shapes)
+        2. Are all input data ID's unique? (checking object identifiers across all components)
+
+    For each batch the update data is validated:
+        3. Is the update data structure correct? (checking data types and numpy array shapes)
+        4. Are all update ID's valid? (checking object identifiers across update and input data)
+
+    Then (for each batch independently) the input dataset is updated with the batch's update data and validated:
+        5. Are all required values provided? (checking NaNs)
+        6. Are the supplied values valid? (checking limits and other logic as described in "Graph Data Model")
+
+    Args:
+        input_data: a power-grid-model input dataset
+        update_data: a power-grid-model update dataset (one or more batches)
+        calculation_type: Supply a calculation method, to allow missing values for unused fields
+        symmetric: A boolean to state whether input data will be used for a symmetric or asymmetric calculation
+
+    Raises:
+        KeyError, TypeError or ValueError if the data structure is invalid.
+        ValidationException if the contents are invalid.
+    """
+    validation_errors = validate_batch_data(
+        input_data=input_data, update_data=update_data, calculation_type=calculation_type, symmetric=symmetric
+    )
+    if validation_errors:
+        raise ValidationException(validation_errors, "update_data")
```

## power_grid_model/validation/errors.py

 * *Ordering differences only*

```diff
@@ -1,438 +1,438 @@
-# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
-#
-# SPDX-License-Identifier: MPL-2.0
-
-"""
-Error classes
-"""
-import re
-from abc import ABC
-from enum import Enum
-from typing import Any, Dict, List, Optional, Tuple, Type, Union
-
-
-class ValidationError(ABC):
-    """
-    The Validation Error is an abstract base class which should be extended by all validation errors. It supplies
-    three public member variables: component, field and ids; storing information about the origin of the validation
-    error. Error classes can extend the public members. For example:
-
-        NotBetweenError(ValidationError):
-            component = 'vehicle'
-            field = 'direction'
-            id = [3, 14, 15, 92, 65, 35]
-            ref_value = (-3.1416, 3.1416)
-
-    For convenience, a human readable representation of the error is supplied using the str() function.
-    I.e. print(str(error)) will print a human readable error message like:
-
-        Field `direction` is not between -3.1416 and 3.1416 for 6 vehicles
-
-    """
-
-    component: Optional[Union[str, List[str]]] = None
-    """
-    The component, or components, to which the error applies.
-    """
-
-    field: Optional[Union[str, List[str], List[Tuple[str, str]]]] = None
-    """
-    The field, or fields, to which the error applies. A field can also be a tuple (component, field) when multiple
-    components are being addressed.
-    """
-
-    ids: Optional[Union[List[int], List[Tuple[str, int]]]] = None
-    """
-    The object identifiers to which the error applies. A field object identifier can also be a tuple (component, id)
-    when multiple components are being addressed.
-    """
-
-    _message: str = "An unknown validation error occurred."
-
-    @property
-    def component_str(self) -> str:
-        """
-        A string representation of the component to which this error applies
-        """
-        return str(self.component)
-
-    @property
-    def field_str(self) -> str:
-        """
-        A string representation of the field to which this error applies
-        """
-        return f"'{self.field}'"
-
-    def get_context(self, id_lookup: Optional[Union[List[str], Dict[int, str]]] = None) -> Dict[str, Any]:
-        """
-        Returns a dictionary that supplies (human readable) information about this error. Each member variable is
-        included in the dictionary. If a function {field_name}_str() exists, the value is overwritten by that function.
-
-        Args:
-            id_lookup: A list or dict (int->str) containing textual object ids
-        """
-        context = self.__dict__.copy()
-        if id_lookup:
-            if isinstance(id_lookup, list):
-                id_lookup = dict(enumerate(id_lookup))
-            context["ids"] = (
-                {i: id_lookup.get(i[1] if isinstance(i, tuple) else i) for i in self.ids} if self.ids else set()
-            )
-        for key in context:
-            if hasattr(self, key + "_str"):
-                context[key] = str(getattr(self, key + "_str"))
-        return context
-
-    def __str__(self) -> str:
-        n_objects = len(self.ids) if self.ids else 0
-        context = self.get_context()
-        context["n"] = n_objects
-        context["objects"] = context.get("component", "object")
-        if n_objects != 1:
-            context["objects"] = re.sub(r"([a-z_]+)", r"\1s", context["objects"])
-        return self._message.format(**context).strip()
-
-    def __repr__(self) -> str:
-        context = " ".join(f"{key}={value}" for key, value in self.get_context().items())
-        return f"<{type(self).__name__}: {context}>"
-
-    def __eq__(self, other):
-        return (
-            type(self) is type(other)
-            and self.component == other.component
-            and self.field == other.field
-            and self.ids == other.ids
-        )
-
-
-class SingleFieldValidationError(ValidationError):
-    """
-    Base class for an error that applies to a single field in a single component
-    """
-
-    _message = "Field {field} is not valid for {n} {objects}."
-    component: str
-    field: str
-    ids: List[int]
-
-    def __init__(self, component: str, field: str, ids: List[int]):
-        """
-        Args:
-            component: Component name
-            field: Field name
-            ids: List of component IDs (not row indices)
-        """
-        self.component = component
-        self.field = field
-        self.ids = sorted(ids)
-
-
-class MultiFieldValidationError(ValidationError):
-    """
-    Base class for an error that applies to multiple fields in a single component
-    """
-
-    _message = "Combination of fields {field} is not valid for {n} {objects}."
-    component: str
-    field: List[str]
-    ids: List[int]
-
-    def __init__(self, component: str, fields: List[str], ids: List[int]):
-        """
-        Args:
-            component: Component name
-            fields: List of field names
-            ids: List of component IDs (not row indices)
-        """
-        self.component = component
-        self.field = sorted(fields)
-        self.ids = sorted(ids)
-
-        if len(self.field) < 2:
-            raise ValueError(f"{type(self).__name__} expects at least two fields: {self.field}")
-
-    @property
-    def field_str(self) -> str:
-        return " and ".join(f"'{field}'" for field in self.field)
-
-
-class MultiComponentValidationError(ValidationError):
-    """
-    Base class for an error that applies to multiple component, and as a consequence also to multiple fields.
-    Even if both fields have the same name, they are considered to be different fields and notated as such.
-    E.g. the two fields `id` fields of the `node` and `line` component: [('node', 'id'), ('line', 'id')].
-    """
-
-    component: List[str]
-    field: List[Tuple[str, str]]
-    ids: List[Tuple[str, int]]
-    _message = "Fields {field} are not valid for {n} {objects}."
-
-    def __init__(self, fields: List[Tuple[str, str]], ids: List[Tuple[str, int]]):
-        """
-        Args:
-            fields: List of field names, formatted as tuples (component, field)
-            ids: List of component IDs (not row indices), formatted as tuples (component, id)
-        """
-        self.component = sorted(set(component for component, _ in fields))
-        self.field = sorted(fields)
-        self.ids = sorted(ids)
-
-        if len(self.field) < 2:
-            raise ValueError(f"{type(self).__name__} expects at least two fields: {self.field}")
-        if len(self.component) < 2:
-            raise ValueError(f"{type(self).__name__} expects at least two components: {self.component}")
-
-    @property
-    def component_str(self) -> str:
-        return "/".join(self.component)
-
-    @property
-    def field_str(self) -> str:
-        return " and ".join(f"{component}.{field}" for component, field in self.field)
-
-
-class NotUniqueError(SingleFieldValidationError):
-    """
-    The value is not unique within a single column in a dataset
-    E.g. When two nodes share the same id.
-    """
-
-    _message = "Field {field} is not unique for {n} {objects}."
-
-
-class MultiComponentNotUniqueError(MultiComponentValidationError):
-    """
-    The value is not unique between multiple columns in multiple components
-    E.g. When a node and a line share the same id.
-    """
-
-    _message = "Fields {field} are not unique for {n} {objects}."
-
-
-class InvalidEnumValueError(SingleFieldValidationError):
-    """
-    The value is not a valid value in the supplied enumeration type.
-    E.g. a sym_load has a non existing LoadGenType.
-    """
-
-    _message = "Field {field} contains invalid {enum} values for {n} {objects}."
-    enum: Type[Enum]
-
-    def __init__(self, component: str, field: str, ids: List[int], enum: Type[Enum]):
-        super().__init__(component, field, ids)
-        self.enum = enum
-
-    @property
-    def enum_str(self) -> str:
-        """
-        A string representation of the field to which this error applies.
-        """
-        return self.enum.__name__
-
-    def __eq__(self, other):
-        return super().__eq__(other) and self.enum == other.enum
-
-
-class SameValueError(MultiFieldValidationError):
-    """
-    The value of two fields is equal.
-    E.g. A line has the same from_node as to_node.
-    """
-
-    _message = "Same value for {field} for {n} {objects}."
-
-
-class NotBooleanError(SingleFieldValidationError):
-    """
-    Invalid boolean value. Boolean fields don't really exist in our data structure, they are 1-byte signed integers and
-    should contain either a 0 (=False) or a 1 (=True).
-    """
-
-    _message = "Field {field} is not a boolean (0 or 1) for {n} {objects}."
-
-
-class MissingValueError(SingleFieldValidationError):
-    """
-    A required value was missing, i.e. NaN.
-    """
-
-    _message = "Field {field} is missing for {n} {objects}."
-
-
-class IdNotInDatasetError(SingleFieldValidationError):
-    """
-    An object identifier does not exist in the original data.
-    E.g. An update data set contains a record for a line that doesn't exist in the input data set.
-    """
-
-    _message = "ID does not exist in {ref_dataset} for {n} {objects}."
-    ref_dataset: str
-
-    def __init__(self, component: str, ids: List[int], ref_dataset: str):
-        super().__init__(component=component, field="id", ids=ids)
-        self.ref_dataset = ref_dataset
-
-    def __eq__(self, other):
-        return super().__eq__(other) and self.ref_dataset == other.ref_dataset
-
-
-class InvalidIdError(SingleFieldValidationError):
-    """
-    An object identifier does not refer to the right type of object.
-    E.g. An line's from_node refers to a source.
-
-    Filters can have been applied to check a subset of the records. E.g. This error may apply only to power_sensors
-    that are said to be connected to a source (filter: measured_terminal_type=source). This useful to spot validation
-    mistakes, due to ambiguity.
-
-    E.g. when a sym_power_sensor is connected to a load, but measured_terminal_type is accidentally set to 'source',
-    the error is:
-
-    "Field `measured_object` does not contain a valid source id for 1 sym_power_sensor. (measured_terminal_type=source)"
-
-    """
-
-    _message = "Field {field} does not contain a valid {ref_components} id for {n} {objects}. {filters}"
-    ref_components: List[str]
-
-    def __init__(
-        self,
-        component: str,
-        field: str,
-        ids: List[int],
-        ref_components: Union[str, List[str]],
-        filters: Optional[Dict[str, Any]] = None,
-    ):
-        # pylint: disable=too-many-arguments
-        super().__init__(component=component, field=field, ids=ids)
-        self.ref_components = [ref_components] if isinstance(ref_components, str) else ref_components
-        self.filters = filters if filters else None
-
-    @property
-    def ref_components_str(self):
-        """
-        A string representation of the components to which this error applies
-        """
-        return "/".join(self.ref_components)
-
-    @property
-    def filters_str(self):
-        """
-        A string representation of the filters that have been applied to the data to which this error refers
-        """
-        if not self.filters:
-            return ""
-        filters = ", ".join(f"{k}={v.name if isinstance(v, Enum) else v}" for k, v in self.filters.items())
-        return f"({filters})"
-
-    def __eq__(self, other):
-        return super().__eq__(other) and self.ref_components == other.ref_components and self.filters == other.filters
-
-
-class TwoValuesZeroError(MultiFieldValidationError):
-    """
-    A record has a 0.0 value in two fields at the same time.
-    E.g. A line's `r1`, `x1` are both 0.
-    """
-
-    _message = "Fields {field} are both zero for {n} {objects}."
-
-
-class ComparisonError(SingleFieldValidationError):
-    """
-    Base class for comparison errors.
-    E.g. A transformer's `i0` is not greater or equal to it's `p0` divided by it's `sn`
-    """
-
-    _message = "Invalid {field}, compared to {ref_value} for {n} {objects}."
-
-    RefType = Union[int, float, str, Tuple[Union[int, float, str], ...]]
-
-    def __init__(self, component: str, field: str, ids: List[int], ref_value: "ComparisonError.RefType"):
-        super().__init__(component, field, ids)
-        self.ref_value = ref_value
-
-    @property
-    def ref_value_str(self):
-        """
-        A string representation of the reference value. E.g. 'zero', 'one', 'field_a and field_b' or '123'.
-        """
-        if isinstance(self.ref_value, tuple):
-            return " and ".join(map(str, self.ref_value))
-        if self.ref_value == 0:
-            return "zero"
-        if self.ref_value == 1:
-            return "one"
-        return str(self.ref_value)
-
-    def __eq__(self, other):
-        return super().__eq__(other) and self.ref_value == other.ref_value
-
-
-class NotGreaterThanError(ComparisonError):
-    """
-    The value of a field is not greater than a reference value or expression.
-    """
-
-    _message = "Field {field} is not greater than {ref_value} for {n} {objects}."
-
-
-class NotGreaterOrEqualError(ComparisonError):
-    """
-    The value of a field is not greater or equal to a reference value or expression.
-    """
-
-    _message = "Field {field} is not greater than (or equal to) {ref_value} for {n} {objects}."
-
-
-class NotLessThanError(ComparisonError):
-    """
-    The value of a field is not less than a reference value or expression.
-    """
-
-    _message = "Field {field} is not smaller than {ref_value} for {n} {objects}."
-
-
-class NotLessOrEqualError(ComparisonError):
-    """
-    The value of a field is not smaller or equal to a reference value or expression.
-    """
-
-    _message = "Field {field} is not smaller than (or equal to) {ref_value} for {n} {objects}."
-
-
-class NotBetweenError(ComparisonError):
-    """
-    The value of a field is not between two a reference values or expressions (exclusive).
-    """
-
-    _message = "Field {field} is not between {ref_value} for {n} {objects}."
-
-
-class NotBetweenOrAtError(ComparisonError):
-    """
-    The value of a field is not between two a reference values or expressions (inclusive).
-    """
-
-    _message = "Field {field} is not between (or at) {ref_value} for {n} {objects}."
-
-
-class InfinityError(SingleFieldValidationError):
-    """
-    The value of a field is infinite.
-    """
-
-    _message = "Field {field} is infinite for {n} {objects}."
-
-
-class TransformerClockError(MultiFieldValidationError):
-    """
-    The value of a field is infinite.
-    """
-
-    _message = (
-        "Invalid clock number for {n} {objects}. "
-        "If one side has wye winding and the other side has not, the clock number should be odd. "
-        "If either both or none of the sides have wye winding, the clock number should be even."
-    )
+# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
+#
+# SPDX-License-Identifier: MPL-2.0
+
+"""
+Error classes
+"""
+import re
+from abc import ABC
+from enum import Enum
+from typing import Any, Dict, List, Optional, Tuple, Type, Union
+
+
+class ValidationError(ABC):
+    """
+    The Validation Error is an abstract base class which should be extended by all validation errors. It supplies
+    three public member variables: component, field and ids; storing information about the origin of the validation
+    error. Error classes can extend the public members. For example:
+
+        NotBetweenError(ValidationError):
+            component = 'vehicle'
+            field = 'direction'
+            id = [3, 14, 15, 92, 65, 35]
+            ref_value = (-3.1416, 3.1416)
+
+    For convenience, a human readable representation of the error is supplied using the str() function.
+    I.e. print(str(error)) will print a human readable error message like:
+
+        Field `direction` is not between -3.1416 and 3.1416 for 6 vehicles
+
+    """
+
+    component: Optional[Union[str, List[str]]] = None
+    """
+    The component, or components, to which the error applies.
+    """
+
+    field: Optional[Union[str, List[str], List[Tuple[str, str]]]] = None
+    """
+    The field, or fields, to which the error applies. A field can also be a tuple (component, field) when multiple
+    components are being addressed.
+    """
+
+    ids: Optional[Union[List[int], List[Tuple[str, int]]]] = None
+    """
+    The object identifiers to which the error applies. A field object identifier can also be a tuple (component, id)
+    when multiple components are being addressed.
+    """
+
+    _message: str = "An unknown validation error occurred."
+
+    @property
+    def component_str(self) -> str:
+        """
+        A string representation of the component to which this error applies
+        """
+        return str(self.component)
+
+    @property
+    def field_str(self) -> str:
+        """
+        A string representation of the field to which this error applies
+        """
+        return f"'{self.field}'"
+
+    def get_context(self, id_lookup: Optional[Union[List[str], Dict[int, str]]] = None) -> Dict[str, Any]:
+        """
+        Returns a dictionary that supplies (human readable) information about this error. Each member variable is
+        included in the dictionary. If a function {field_name}_str() exists, the value is overwritten by that function.
+
+        Args:
+            id_lookup: A list or dict (int->str) containing textual object ids
+        """
+        context = self.__dict__.copy()
+        if id_lookup:
+            if isinstance(id_lookup, list):
+                id_lookup = dict(enumerate(id_lookup))
+            context["ids"] = (
+                {i: id_lookup.get(i[1] if isinstance(i, tuple) else i) for i in self.ids} if self.ids else set()
+            )
+        for key in context:
+            if hasattr(self, key + "_str"):
+                context[key] = str(getattr(self, key + "_str"))
+        return context
+
+    def __str__(self) -> str:
+        n_objects = len(self.ids) if self.ids else 0
+        context = self.get_context()
+        context["n"] = n_objects
+        context["objects"] = context.get("component", "object")
+        if n_objects != 1:
+            context["objects"] = re.sub(r"([a-z_]+)", r"\1s", context["objects"])
+        return self._message.format(**context).strip()
+
+    def __repr__(self) -> str:
+        context = " ".join(f"{key}={value}" for key, value in self.get_context().items())
+        return f"<{type(self).__name__}: {context}>"
+
+    def __eq__(self, other):
+        return (
+            type(self) is type(other)
+            and self.component == other.component
+            and self.field == other.field
+            and self.ids == other.ids
+        )
+
+
+class SingleFieldValidationError(ValidationError):
+    """
+    Base class for an error that applies to a single field in a single component
+    """
+
+    _message = "Field {field} is not valid for {n} {objects}."
+    component: str
+    field: str
+    ids: List[int]
+
+    def __init__(self, component: str, field: str, ids: List[int]):
+        """
+        Args:
+            component: Component name
+            field: Field name
+            ids: List of component IDs (not row indices)
+        """
+        self.component = component
+        self.field = field
+        self.ids = sorted(ids)
+
+
+class MultiFieldValidationError(ValidationError):
+    """
+    Base class for an error that applies to multiple fields in a single component
+    """
+
+    _message = "Combination of fields {field} is not valid for {n} {objects}."
+    component: str
+    field: List[str]
+    ids: List[int]
+
+    def __init__(self, component: str, fields: List[str], ids: List[int]):
+        """
+        Args:
+            component: Component name
+            fields: List of field names
+            ids: List of component IDs (not row indices)
+        """
+        self.component = component
+        self.field = sorted(fields)
+        self.ids = sorted(ids)
+
+        if len(self.field) < 2:
+            raise ValueError(f"{type(self).__name__} expects at least two fields: {self.field}")
+
+    @property
+    def field_str(self) -> str:
+        return " and ".join(f"'{field}'" for field in self.field)
+
+
+class MultiComponentValidationError(ValidationError):
+    """
+    Base class for an error that applies to multiple component, and as a consequence also to multiple fields.
+    Even if both fields have the same name, they are considered to be different fields and notated as such.
+    E.g. the two fields `id` fields of the `node` and `line` component: [('node', 'id'), ('line', 'id')].
+    """
+
+    component: List[str]
+    field: List[Tuple[str, str]]
+    ids: List[Tuple[str, int]]
+    _message = "Fields {field} are not valid for {n} {objects}."
+
+    def __init__(self, fields: List[Tuple[str, str]], ids: List[Tuple[str, int]]):
+        """
+        Args:
+            fields: List of field names, formatted as tuples (component, field)
+            ids: List of component IDs (not row indices), formatted as tuples (component, id)
+        """
+        self.component = sorted(set(component for component, _ in fields))
+        self.field = sorted(fields)
+        self.ids = sorted(ids)
+
+        if len(self.field) < 2:
+            raise ValueError(f"{type(self).__name__} expects at least two fields: {self.field}")
+        if len(self.component) < 2:
+            raise ValueError(f"{type(self).__name__} expects at least two components: {self.component}")
+
+    @property
+    def component_str(self) -> str:
+        return "/".join(self.component)
+
+    @property
+    def field_str(self) -> str:
+        return " and ".join(f"{component}.{field}" for component, field in self.field)
+
+
+class NotUniqueError(SingleFieldValidationError):
+    """
+    The value is not unique within a single column in a dataset
+    E.g. When two nodes share the same id.
+    """
+
+    _message = "Field {field} is not unique for {n} {objects}."
+
+
+class MultiComponentNotUniqueError(MultiComponentValidationError):
+    """
+    The value is not unique between multiple columns in multiple components
+    E.g. When a node and a line share the same id.
+    """
+
+    _message = "Fields {field} are not unique for {n} {objects}."
+
+
+class InvalidEnumValueError(SingleFieldValidationError):
+    """
+    The value is not a valid value in the supplied enumeration type.
+    E.g. a sym_load has a non existing LoadGenType.
+    """
+
+    _message = "Field {field} contains invalid {enum} values for {n} {objects}."
+    enum: Type[Enum]
+
+    def __init__(self, component: str, field: str, ids: List[int], enum: Type[Enum]):
+        super().__init__(component, field, ids)
+        self.enum = enum
+
+    @property
+    def enum_str(self) -> str:
+        """
+        A string representation of the field to which this error applies.
+        """
+        return self.enum.__name__
+
+    def __eq__(self, other):
+        return super().__eq__(other) and self.enum == other.enum
+
+
+class SameValueError(MultiFieldValidationError):
+    """
+    The value of two fields is equal.
+    E.g. A line has the same from_node as to_node.
+    """
+
+    _message = "Same value for {field} for {n} {objects}."
+
+
+class NotBooleanError(SingleFieldValidationError):
+    """
+    Invalid boolean value. Boolean fields don't really exist in our data structure, they are 1-byte signed integers and
+    should contain either a 0 (=False) or a 1 (=True).
+    """
+
+    _message = "Field {field} is not a boolean (0 or 1) for {n} {objects}."
+
+
+class MissingValueError(SingleFieldValidationError):
+    """
+    A required value was missing, i.e. NaN.
+    """
+
+    _message = "Field {field} is missing for {n} {objects}."
+
+
+class IdNotInDatasetError(SingleFieldValidationError):
+    """
+    An object identifier does not exist in the original data.
+    E.g. An update data set contains a record for a line that doesn't exist in the input data set.
+    """
+
+    _message = "ID does not exist in {ref_dataset} for {n} {objects}."
+    ref_dataset: str
+
+    def __init__(self, component: str, ids: List[int], ref_dataset: str):
+        super().__init__(component=component, field="id", ids=ids)
+        self.ref_dataset = ref_dataset
+
+    def __eq__(self, other):
+        return super().__eq__(other) and self.ref_dataset == other.ref_dataset
+
+
+class InvalidIdError(SingleFieldValidationError):
+    """
+    An object identifier does not refer to the right type of object.
+    E.g. An line's from_node refers to a source.
+
+    Filters can have been applied to check a subset of the records. E.g. This error may apply only to power_sensors
+    that are said to be connected to a source (filter: measured_terminal_type=source). This useful to spot validation
+    mistakes, due to ambiguity.
+
+    E.g. when a sym_power_sensor is connected to a load, but measured_terminal_type is accidentally set to 'source',
+    the error is:
+
+    "Field `measured_object` does not contain a valid source id for 1 sym_power_sensor. (measured_terminal_type=source)"
+
+    """
+
+    _message = "Field {field} does not contain a valid {ref_components} id for {n} {objects}. {filters}"
+    ref_components: List[str]
+
+    def __init__(
+        self,
+        component: str,
+        field: str,
+        ids: List[int],
+        ref_components: Union[str, List[str]],
+        filters: Optional[Dict[str, Any]] = None,
+    ):
+        # pylint: disable=too-many-arguments
+        super().__init__(component=component, field=field, ids=ids)
+        self.ref_components = [ref_components] if isinstance(ref_components, str) else ref_components
+        self.filters = filters if filters else None
+
+    @property
+    def ref_components_str(self):
+        """
+        A string representation of the components to which this error applies
+        """
+        return "/".join(self.ref_components)
+
+    @property
+    def filters_str(self):
+        """
+        A string representation of the filters that have been applied to the data to which this error refers
+        """
+        if not self.filters:
+            return ""
+        filters = ", ".join(f"{k}={v.name if isinstance(v, Enum) else v}" for k, v in self.filters.items())
+        return f"({filters})"
+
+    def __eq__(self, other):
+        return super().__eq__(other) and self.ref_components == other.ref_components and self.filters == other.filters
+
+
+class TwoValuesZeroError(MultiFieldValidationError):
+    """
+    A record has a 0.0 value in two fields at the same time.
+    E.g. A line's `r1`, `x1` are both 0.
+    """
+
+    _message = "Fields {field} are both zero for {n} {objects}."
+
+
+class ComparisonError(SingleFieldValidationError):
+    """
+    Base class for comparison errors.
+    E.g. A transformer's `i0` is not greater or equal to it's `p0` divided by it's `sn`
+    """
+
+    _message = "Invalid {field}, compared to {ref_value} for {n} {objects}."
+
+    RefType = Union[int, float, str, Tuple[Union[int, float, str], ...]]
+
+    def __init__(self, component: str, field: str, ids: List[int], ref_value: "ComparisonError.RefType"):
+        super().__init__(component, field, ids)
+        self.ref_value = ref_value
+
+    @property
+    def ref_value_str(self):
+        """
+        A string representation of the reference value. E.g. 'zero', 'one', 'field_a and field_b' or '123'.
+        """
+        if isinstance(self.ref_value, tuple):
+            return " and ".join(map(str, self.ref_value))
+        if self.ref_value == 0:
+            return "zero"
+        if self.ref_value == 1:
+            return "one"
+        return str(self.ref_value)
+
+    def __eq__(self, other):
+        return super().__eq__(other) and self.ref_value == other.ref_value
+
+
+class NotGreaterThanError(ComparisonError):
+    """
+    The value of a field is not greater than a reference value or expression.
+    """
+
+    _message = "Field {field} is not greater than {ref_value} for {n} {objects}."
+
+
+class NotGreaterOrEqualError(ComparisonError):
+    """
+    The value of a field is not greater or equal to a reference value or expression.
+    """
+
+    _message = "Field {field} is not greater than (or equal to) {ref_value} for {n} {objects}."
+
+
+class NotLessThanError(ComparisonError):
+    """
+    The value of a field is not less than a reference value or expression.
+    """
+
+    _message = "Field {field} is not smaller than {ref_value} for {n} {objects}."
+
+
+class NotLessOrEqualError(ComparisonError):
+    """
+    The value of a field is not smaller or equal to a reference value or expression.
+    """
+
+    _message = "Field {field} is not smaller than (or equal to) {ref_value} for {n} {objects}."
+
+
+class NotBetweenError(ComparisonError):
+    """
+    The value of a field is not between two a reference values or expressions (exclusive).
+    """
+
+    _message = "Field {field} is not between {ref_value} for {n} {objects}."
+
+
+class NotBetweenOrAtError(ComparisonError):
+    """
+    The value of a field is not between two a reference values or expressions (inclusive).
+    """
+
+    _message = "Field {field} is not between (or at) {ref_value} for {n} {objects}."
+
+
+class InfinityError(SingleFieldValidationError):
+    """
+    The value of a field is infinite.
+    """
+
+    _message = "Field {field} is infinite for {n} {objects}."
+
+
+class TransformerClockError(MultiFieldValidationError):
+    """
+    The value of a field is infinite.
+    """
+
+    _message = (
+        "Invalid clock number for {n} {objects}. "
+        "If one side has wye winding and the other side has not, the clock number should be odd. "
+        "If either both or none of the sides have wye winding, the clock number should be even."
+    )
```

## power_grid_model/validation/rules.py

 * *Ordering differences only*

```diff
@@ -1,677 +1,677 @@
-# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
-#
-# SPDX-License-Identifier: MPL-2.0
-
-"""
-This module contains a set of comparison rules. They all share the same (or similar) logic and interface.
-
-In general each function checks the values in a single 'column' (i.e. field) of a numpy structured array and it
-returns an error object containing the component, the field and the ids of the records that did not match the rule.
-E.g. all_greater_than_zero(data, 'node', 'u_rated') returns a NotGreaterThanError if any of the node's `u_rated`
-values are 0 or less.
-
-In general, the rules are designed to ignore NaN values, except for none_missing() which explicitly checks for NaN
-values in the entire data set. It is important to understand that np.less_equal(x) yields different results than
-np.logical_not(np.greater(x)) as a NaN comparison always results in False. The most extreme example is that even
-np.nan == np.nan yields False.
-
-    np.less_equal(            [0.1, 0.2, 0.3, np.nan], 0.0)  = [False, False, False, False] -> OK
-    np.logical_not(np.greater([0.1, 0.2, 0.3, np.nan], 0.0)) = [False, False, False, True] -> Error (false positive)
-
-Input data:
-
-    data: SingleDataset
-        The entire input/update data set
-
-    component: str
-        The name of the component, which should be an existing key in the data
-
-    field: str
-        The name of the column, which should be an field in the component data (numpy structured array)
-
-Output data:
-    errors: List[ValidationError]
-        A list containing errors; in case of success, `errors` is the empty list: [].
-
-"""
-from enum import Enum
-from typing import Any, Callable, Dict, List, Optional, Tuple, Type, TypeVar, Union
-
-import numpy as np
-
-from power_grid_model.data_types import SingleDataset
-from power_grid_model.enum import WindingType
-from power_grid_model.validation.errors import (
-    ComparisonError,
-    IdNotInDatasetError,
-    InfinityError,
-    InvalidEnumValueError,
-    InvalidIdError,
-    MissingValueError,
-    MultiComponentNotUniqueError,
-    NotBetweenError,
-    NotBetweenOrAtError,
-    NotBooleanError,
-    NotGreaterOrEqualError,
-    NotGreaterThanError,
-    NotLessOrEqualError,
-    NotLessThanError,
-    NotUniqueError,
-    SameValueError,
-    TransformerClockError,
-    TwoValuesZeroError,
-    ValidationError,
-)
-from power_grid_model.validation.utils import eval_expression, nan_type, set_default_value
-
-Error = TypeVar("Error", bound=ValidationError)
-CompError = TypeVar("CompError", bound=ComparisonError)
-
-
-def all_greater_than_zero(data: SingleDataset, component: str, field: str) -> List[NotGreaterThanError]:
-    """
-    Check that for all records of a particular type of component, the values in the 'field' column are greater than
-    zero. Returns an empty list on success, or a list containing a single error object on failure.
-
-    Args:
-        data: The input/update data set for all components
-        component: The component of interest
-        field: The field of interest
-
-    Returns:
-        A list containing zero or one NotGreaterThanErrors, listing all ids where the value in the field of interest
-        was zero or less.
-    """
-    return all_greater_than(data, component, field, 0.0)
-
-
-def all_greater_than_or_equal_to_zero(
-    data: SingleDataset,
-    component: str,
-    field: str,
-    default_value: Optional[Union[np.ndarray, int, float]] = None,
-) -> List[NotGreaterOrEqualError]:
-    """
-    Check that for all records of a particular type of component, the values in the 'field' column are greater than,
-    or equal to zero. Returns an empty list on success, or a list containing a single error object on failure.
-
-    Args:
-        data: The input/update data set for all components
-        component: The component of interest
-        field: The field of interest
-        default_value: Some values are not required, but will receive a default value in the C++ core. To do a proper
-        input validation, these default values should be included in the validation. It can be a fixed value for the
-        entire column (int/float) or be different for each element (np.ndarray).
-
-    Returns:
-        A list containing zero or one NotGreaterOrEqualErrors, listing all ids where the value in the field of
-        interest was less than zero.
-    """
-    return all_greater_or_equal(data, component, field, 0.0, default_value)
-
-
-def all_greater_than(
-    data: SingleDataset, component: str, field: str, ref_value: Union[int, float, str]
-) -> List[NotGreaterThanError]:
-    """
-    Check that for all records of a particular type of component, the values in the 'field' column are greater than
-    the reference value. Returns an empty list on success, or a list containing a single error object on failure.
-
-    Args:
-        data: The input/update data set for all components
-        component: The component of interest
-        field: The field of interest
-        ref_value: The reference value against which all values in the 'field' column are compared. If the reference
-        value is a string, it is assumed to be another field (e.g. 'field_x') of the same component, or a ratio between
-        two fields (e.g. 'field_x / field_y')
-
-    Returns:
-        A list containing zero or one NotGreaterThanErrors, listing all ids where the value in the field of interest
-        was less than, or equal to, the ref_value.
-    """
-
-    def not_greater(val: np.ndarray, *ref: np.ndarray):
-        return np.less_equal(val, *ref)
-
-    return none_match_comparison(data, component, field, not_greater, ref_value, NotGreaterThanError)
-
-
-def all_greater_or_equal(
-    data: SingleDataset,
-    component: str,
-    field: str,
-    ref_value: Union[int, float, str],
-    default_value: Optional[Union[np.ndarray, int, float]] = None,
-) -> List[NotGreaterOrEqualError]:
-    """
-    Check that for all records of a particular type of component, the values in the 'field' column are greater than,
-    or equal to the reference value. Returns an empty list on success, or a list containing a single error object on
-    failure.
-
-    Args:
-        data: The input/update data set for all components
-        component: The component of interest
-        field: The field of interest
-        ref_value: The reference value against which all values in the 'field' column are compared. If the reference
-        value is a string, it is assumed to be another field (e.g. 'field_x') of the same component, or a ratio between
-        two fields (e.g. 'field_x / field_y')
-        default_value: Some values are not required, but will receive a default value in the C++ core. To do a proper
-        input validation, these default values should be included in the validation. It can be a fixed value for the
-        entire column (int/float) or be different for each element (np.ndarray).
-
-    Returns:
-        A list containing zero or one NotGreaterOrEqualErrors, listing all ids where the value in the field of
-        interest was less than the ref_value.
-
-    """
-
-    def not_greater_or_equal(val: np.ndarray, *ref: np.ndarray):
-        return np.less(val, *ref)
-
-    return none_match_comparison(
-        data, component, field, not_greater_or_equal, ref_value, NotGreaterOrEqualError, default_value
-    )
-
-
-def all_less_than(
-    data: SingleDataset, component: str, field: str, ref_value: Union[int, float, str]
-) -> List[NotLessThanError]:
-    """
-    Check that for all records of a particular type of component, the values in the 'field' column are less than the
-    reference value. Returns an empty list on success, or a list containing a single error object on failure.
-
-    Args:
-        data: The input/update data set for all components
-        component: The component of interest
-        field: The field of interest
-        ref_value: The reference value against which all values in the 'field' column are compared. If the reference
-        value is a string, it is assumed to be another field (e.g. 'field_x') of the same component, or a ratio between
-        two fields (e.g. 'field_x / field_y')
-
-    Returns:
-        A list containing zero or one NotLessThanErrors, listing all ids where the value in the field of interest was
-        greater than, or equal to, the ref_value.
-    """
-
-    def not_less(val: np.ndarray, *ref: np.ndarray):
-        return np.greater_equal(val, *ref)
-
-    return none_match_comparison(data, component, field, not_less, ref_value, NotLessThanError)
-
-
-def all_less_or_equal(
-    data: SingleDataset, component: str, field: str, ref_value: Union[int, float, str]
-) -> List[NotLessOrEqualError]:
-    """
-    Check that for all records of a particular type of component, the values in the 'field' column are less than,
-    or equal to the reference value. Returns an empty list on success, or a list containing a single error object on
-    failure.
-
-    Args:
-        data: The input/update data set for all components
-        component: The component of interest
-        field: The field of interest
-        ref_value: The reference value against which all values in the 'field' column are compared. If the reference
-        value is a string, it is assumed to be another field (e.g. 'field_x') of the same component, or a ratio between
-        two fields (e.g. 'field_x / field_y')
-
-    Returns:
-        A list containing zero or one NotLessOrEqualErrors, listing all ids where the value in the field of interest was
-        greater than the ref_value.
-
-    """
-
-    def not_less_or_equal(val: np.ndarray, *ref: np.ndarray):
-        return np.greater(val, *ref)
-
-    return none_match_comparison(data, component, field, not_less_or_equal, ref_value, NotLessOrEqualError)
-
-
-def all_between(  # pylint: disable=too-many-arguments
-    data: SingleDataset,
-    component: str,
-    field: str,
-    ref_value_1: Union[int, float, str],
-    ref_value_2: Union[int, float, str],
-    default_value: Optional[Union[np.ndarray, int, float]] = None,
-) -> List[NotBetweenError]:
-    """
-    Check that for all records of a particular type of component, the values in the 'field' column are (exclusively)
-    between reference value 1 and 2. Value 1 may be smaller, but also larger than value 2. Returns an empty list on
-    success, or a list containing a single error object on failure.
-
-    Args:
-        data: The input/update data set for all components
-        component: The component of interest
-        field: The field of interest
-        ref_value_1: The first reference value against which all values in the 'field' column are compared. If the
-        reference value is a string, it is assumed to be another field (e.g. 'field_x') of the same component, or a
-        ratio between two fields (e.g. 'field_x / field_y')
-        ref_value_2: The second reference value against which all values in the 'field' column are compared. If the
-        reference value is a string, it is assumed to be another field (e.g. 'field_x') of the same component,
-        or a ratio between two fields (e.g. 'field_x / field_y')
-        default_value: Some values are not required, but will receive a default value in the C++ core. To do a proper
-        input validation, these default values should be included in the validation. It can be a fixed value for the
-        entire column (int/float) or be different for each element (np.ndarray).
-
-    Returns:
-        A list containing zero or one NotBetweenErrors, listing all ids where the value in the field of interest was
-        outside the range defined by the reference values.
-    """
-
-    def outside(val: np.ndarray, *ref: np.ndarray) -> np.ndarray:
-        return np.logical_or(np.less_equal(val, np.minimum(*ref)), np.greater_equal(val, np.maximum(*ref)))
-
-    return none_match_comparison(
-        data, component, field, outside, (ref_value_1, ref_value_2), NotBetweenError, default_value
-    )
-
-
-def all_between_or_at(  # pylint: disable=too-many-arguments
-    data: SingleDataset,
-    component: str,
-    field: str,
-    ref_value_1: Union[int, float, str],
-    ref_value_2: Union[int, float, str],
-    default_value: Optional[Union[np.ndarray, int, float]] = None,
-) -> List[NotBetweenOrAtError]:
-    """
-    Check that for all records of a particular type of component, the values in the 'field' column are inclusively
-    between reference value 1 and 2. Value 1 may be smaller, but also larger than value 2. Returns an empty list on
-    success, or a list containing a single error object on failure.
-
-    Args:
-        data: The input/update data set for all components
-        component: The component of interest
-        field: The field of interest
-        ref_value_1: The first reference value against which all values in the 'field' column are compared. If the
-        reference value is a string, it is assumed to be another field (e.g. 'field_x') of the same component, or a
-        ratio between two fields (e.g. 'field_x / field_y')
-        ref_value_2: The second reference value against which all values in the 'field' column are compared. If the
-        reference value is a string, it is assumed to be another field (e.g. 'field_x') of the same component,
-        or a ratio between two fields (e.g. 'field_x / field_y')
-        default_value: Some values are not required, but will receive a default value in the C++ core. To do a proper
-        input validation, these default values should be included in the validation. It can be a fixed value for the
-        entire column (int/float) or be different for each element (np.ndarray).
-
-    Returns:
-        A list containing zero or one NotBetweenOrAtErrors, listing all ids where the value in the field of interest was
-        outside the range defined by the reference values.
-    """
-
-    def outside(val: np.ndarray, *ref: np.ndarray) -> np.ndarray:
-        return np.logical_or(np.less(val, np.minimum(*ref)), np.greater(val, np.maximum(*ref)))
-
-    return none_match_comparison(
-        data, component, field, outside, (ref_value_1, ref_value_2), NotBetweenOrAtError, default_value
-    )
-
-
-def none_match_comparison(
-    data: SingleDataset,
-    component: str,
-    field: str,
-    compare_fn: Callable,
-    ref_value: ComparisonError.RefType,
-    error: Type[CompError] = ComparisonError,  # type: ignore
-    default_value: Optional[Union[np.ndarray, int, float]] = None,
-) -> List[CompError]:
-    # pylint: disable=too-many-arguments
-    """
-    For all records of a particular type of component, check if the value in the 'field' column match the comparison.
-    Returns an empty list if none of the value match the comparison, or a list containing a single error object when at
-    the value in 'field' of at least one record matches the comparison.
-
-    Args:
-        data: The input/update data set for all components
-        component: The component of interest
-        field: The field of interest
-        compare_fn: A function that takes the data in the 'field' column, and any number of reference values
-        ref_value: A reference value, or a tuple of reference values, against which all values in the 'field' column
-        are compared using the compare_fn. If a reference value is a string, it is assumed to be another field
-        (e.g. 'field_x') of the same component, or a ratio between two fields (e.g. 'field_x / field_y')
-        error: The type (class) of error that should be returned in case any of the values match the comparison.
-        default_value: Some values are not required, but will receive a default value in the C++ core. To do a proper
-        input validation, these default values should be included in the validation. It can be a fixed value for the
-        entire column (int/float) or be different for each element (np.ndarray).
-
-    Returns:
-        A list containing zero or one comparison errors (should be a subclass of ComparisonError), listing all ids
-        where the value in the field of interest matched the comparison.
-    """
-    if default_value is not None:
-        set_default_value(data=data, component=component, field=field, default_value=default_value)
-    component_data = data[component]
-    if isinstance(ref_value, tuple):
-        ref = tuple(eval_expression(component_data, v) for v in ref_value)
-    else:
-        ref = (eval_expression(component_data, ref_value),)
-    matches = compare_fn(component_data[field], *ref)
-    if matches.any():
-        if matches.ndim > 1:
-            matches = matches.any(axis=1)
-        ids = component_data["id"][matches].flatten().tolist()
-        return [error(component, field, ids, ref_value)]
-    return []
-
-
-def all_unique(data: SingleDataset, component: str, field: str) -> List[NotUniqueError]:
-    """
-    Check that for all records of a particular type of component, the values in the 'field' column are unique within
-    the 'field' column of that component.
-
-    Args:
-        data: The input/update data set for all components
-        component: The component of interest
-        field: The field of interest
-
-    Returns:
-        A list containing zero or one NotUniqueError, listing all ids where the value in the field of interest was
-        not unique. If the field name was 'id' (a very common check), the id is added as many times as it occurred in
-        the 'id' column, to maintain object counts.
-    """
-    _, index, counts = np.unique(data[component][field], return_index=True, return_counts=True)
-    if any(counts != 1):
-        ids = data[component]["id"][index[counts != 1]].flatten().tolist()
-        if field == "id":  # Add ids multiple times
-            counts = counts[counts != 1]
-            for obj_id, count in zip(ids, counts):
-                ids += [obj_id] * (count - 1)
-            ids = sorted(ids)
-        return [NotUniqueError(component, field, ids)]
-    return []
-
-
-def all_cross_unique(
-    data: SingleDataset, fields: List[Tuple[str, str]], cross_only=True
-) -> List[MultiComponentNotUniqueError]:
-    """
-    Check that for all records of a particular type of component, the values in the 'field' column are unique within
-    the 'field' column of that component.
-
-    Args:
-        data: The input/update data set for all components
-        fields: The fields of interest, formatted as [(component_1, field_1), (component_2, field_2)]
-        cross_only: Do not include duplicates within a single field. It is advised that you use all_unique() to
-        explicitly check uniqueness within a single field.
-
-    Returns:
-        A list containing zero or one MultiComponentNotUniqueError, listing all fields and ids where the value was not
-        unique between the fields.
-    """
-    all_values: Dict[int, List[Tuple[Tuple[str, str], int]]] = {}
-    duplicate_ids = set()
-    for component, field in fields:
-        for obj_id, value in zip(data[component]["id"], data[component][field]):
-            component_id = ((component, field), obj_id)
-            if value not in all_values:
-                all_values[value] = []
-            elif not cross_only or not all(f == (component, field) for f, _ in all_values[value]):
-                duplicate_ids.update(all_values[value])
-                duplicate_ids.add(component_id)
-            all_values[value].append(component_id)
-    if duplicate_ids:
-        fields_with_duplicated_ids = {f for f, _ in duplicate_ids}
-        ids_with_duplicated_ids = {(c, i) for (c, _), i in duplicate_ids}
-        return [MultiComponentNotUniqueError(list(fields_with_duplicated_ids), list(ids_with_duplicated_ids))]
-    return []
-
-
-def all_valid_enum_values(
-    data: SingleDataset, component: str, field: str, enum: Type[Enum]
-) -> List[InvalidEnumValueError]:
-    """
-    Check that for all records of a particular type of component, the values in the 'field' column are valid values for
-    the supplied enum class. Returns an empty list on success, or a list containing a single error object on failure.
-
-    Args:
-        data: The input/update data set for all components
-        component: The component of interest
-        field: The field of interest
-        enum: The enum type to validate against
-
-    Returns:
-        A list containing zero or one InvalidEnumValueError, listing all ids where the value in the field of interest
-        was not a valid value in the supplied enum type.
-    """
-    valid = [nan_type(component, field)] + list(enum)
-    invalid = np.isin(data[component][field], np.array(valid, dtype=np.int8), invert=True)
-    if invalid.any():
-        ids = data[component]["id"][invalid].flatten().tolist()
-        return [InvalidEnumValueError(component, field, ids, enum)]
-    return []
-
-
-def all_valid_ids(
-    data: SingleDataset, component: str, field: str, ref_components: Union[str, List[str]], **filters: Any
-) -> List[InvalidIdError]:
-    """
-    For a column which should contain object identifiers (ids), check if the id exists in the data, for a specific set
-    of reference component types. E.g. is the from_node field of each line referring to an existing node id?
-
-    Args:
-        data: The input/update data set for all components
-        component: The component of interest
-        field: The field of interest
-        ref_components: The component or components in which we want to look for ids
-        **filters: One or more filters on the dataset. E.g. measured_terminal_type=MeasuredTerminalType.source.
-
-    Returns:
-        A list containing zero or one InvalidIdError, listing all ids where the value in the field of interest
-        was not a valid object identifier.
-    """
-    # For convenience, ref_component may be a string and we'll convert it to a 'list' containing that string as it's
-    # single element.
-    if isinstance(ref_components, str):
-        ref_components = [ref_components]
-
-    # Create a set of ids by chaining the ids of all ref_components
-    valid_ids = set()
-    for ref_component in ref_components:
-        if ref_component in data:
-            nan = nan_type(ref_component, "id")
-            if np.isnan(nan):
-                mask = ~np.isnan(data[ref_component]["id"])
-            else:
-                mask = np.not_equal(data[ref_component]["id"], nan)
-            valid_ids.update(data[ref_component]["id"][mask])
-
-    # Apply the filters (e.g. to select only records with a certain MeasuredTerminalType)
-    values = data[component][field]
-    mask = np.ones(shape=values.shape, dtype=bool)
-    for filter_field, filter_value in filters.items():
-        mask = np.logical_and(mask, data[component][filter_field] == filter_value)
-
-    # Find any values that can't be found in the set of ids
-    invalid = np.logical_and(mask, np.isin(values, list(valid_ids), invert=True))
-    if invalid.any():
-        ids = data[component]["id"][invalid].flatten().tolist()
-        return [InvalidIdError(component, field, ids, ref_components, filters)]
-    return []
-
-
-def all_boolean(data: SingleDataset, component: str, field: str) -> List[NotBooleanError]:
-    """
-    Check that for all records of a particular type of component, the values in the 'field' column are valid boolean
-    values, i.e. 0 or 1. Returns an empty list on success, or a list containing a single error object on failure.
-
-    Args:
-        data: The input/update data set for all components
-        component: The component of interest
-        field: The field of interest
-
-    Returns:
-        A list containing zero or one NotBooleanError, listing all ids where the value in the field of interest was not
-        a valid boolean value.
-    """
-    invalid = np.isin(data[component][field], [0, 1], invert=True)
-    if invalid.any():
-        ids = data[component]["id"][invalid].flatten().tolist()
-        return [NotBooleanError(component, field, ids)]
-    return []
-
-
-def all_not_two_values_zero(
-    data: SingleDataset, component: str, field_1: str, field_2: str
-) -> List[TwoValuesZeroError]:
-    """
-    Check that for all records of a particular type of component, the values in the 'field_1' and 'field_2' column are
-    not both zero. Returns an empty list on success, or a list containing a single error object on failure.
-
-    Args:
-        data: The input/update data set for all components
-        component: The component of interest
-        field_1: The first field of interest
-        field_2: The second field of interest
-    Returns:
-        A list containing zero or one TwoValuesZeroError, listing all ids where the value in the two fields of interest
-        were both zero.
-    """
-    invalid = np.logical_and(np.equal(data[component][field_1], 0.0), np.equal(data[component][field_2], 0.0))
-    if invalid.any():
-        if invalid.ndim > 1:
-            invalid = invalid.any(axis=1)
-        ids = data[component]["id"][invalid].flatten().tolist()
-        return [TwoValuesZeroError(component, [field_1, field_2], ids)]
-    return []
-
-
-def all_not_two_values_equal(data: SingleDataset, component: str, field_1: str, field_2: str) -> List[SameValueError]:
-    """
-    Check that for all records of a particular type of component, the values in the 'field_1' and 'field_2' column are
-    not both the same value. E.g. from_node and to_node of a line. Returns an empty list on success, or a list
-    containing a single error object on failure.
-
-    Args:
-        data: The input/update data set for all components
-        component: The component of interest
-        field_1: The first field of interest
-        field_2: The second field of interest
-    Returns:
-        A list containing zero or one SameValueError, listing all ids where the value in the two fields of interest
-        were both the same.
-    """
-    invalid = np.equal(data[component][field_1], data[component][field_2])
-    if invalid.any():
-        if invalid.ndim > 1:
-            invalid = invalid.any(axis=1)
-        ids = data[component]["id"][invalid].flatten().tolist()
-        return [SameValueError(component, [field_1, field_2], ids)]
-    return []
-
-
-def all_ids_exist_in_data_set(
-    data: SingleDataset, ref_data: SingleDataset, component: str, ref_name: str
-) -> List[IdNotInDatasetError]:
-    """
-    Check that for all records of a particular type of component, the ids exist in the reference data set.
-
-    Args:
-        data: The (update) data set for all components
-        ref_data: The reference (input) data set for all components
-        component: The component of interest
-        ref_name: The name of the reference data set, e.g. 'input_data'
-    Returns:
-        A list containing zero or one IdNotInDatasetError, listing all ids of the objects in the data set which do not
-        exist in the reference data set.
-    """
-    invalid = np.isin(data[component]["id"], ref_data[component]["id"], invert=True)
-    if invalid.any():
-        ids = data[component]["id"][invalid].flatten().tolist()
-        return [IdNotInDatasetError(component, ids, ref_name)]
-    return []
-
-
-def all_finite(data: SingleDataset) -> List[InfinityError]:
-    """
-    Check that for all records in all component, the values in all columns are finite value, i.e. float values other
-    than inf, or -inf. Nan values are ignored, as in all other comparison functions. You can use non_missing() to
-    check for missing/nan values. Returns an empty list on success, or a list containing an error object for each
-    component/field combination where.
-
-    Args:
-        data: The input/update data set for all components
-
-    Returns:
-        A list containing zero or one NotBooleanError, listing all ids where the value in the field of interest was not
-        a valid boolean value.
-    """
-    errors = []
-    for component, array in data.items():
-        for field, (dtype, _) in array.dtype.fields.items():
-            if not np.issubdtype(dtype, np.floating):
-                continue
-            invalid = np.isinf(array[field])
-            if invalid.any():
-                ids = data[component]["id"][invalid].flatten().tolist()
-                errors.append(InfinityError(component, field, ids))
-    return errors
-
-
-def none_missing(data: SingleDataset, component: str, fields: Union[str, List[str]]) -> List[MissingValueError]:
-    """
-    Check that for all records of a particular type of component, the values in the 'fields' columns are not NaN.
-    Returns an empty list on success, or a list containing a single error object on failure.
-
-    Args:
-        data: The input/update data set for all components
-        component: The component of interest
-        fields: The fields of interest
-
-    Returns:
-        A list containing zero or more MissingValueError; one for each field, listing all ids where the value in the
-        field was NaN.
-    """
-    errors = []
-    if isinstance(fields, str):
-        fields = [fields]
-    for field in fields:
-        nan = nan_type(component, field)
-        if np.isnan(nan):
-            invalid = np.isnan(data[component][field])
-        else:
-            invalid = np.equal(data[component][field], nan)
-        if invalid.any():
-            if invalid.ndim > 1:
-                invalid = invalid.any(axis=1)
-            ids = data[component]["id"][invalid].flatten().tolist()
-            errors.append(MissingValueError(component, field, ids))
-    return errors
-
-
-def all_valid_clocks(
-    data: SingleDataset, component: str, clock_field: str, winding_from_field: str, winding_to_field: str
-) -> List[TransformerClockError]:
-    """
-    Custom validation rule: Odd clock number is only allowed for Dy(n) or Y(N)d configuration.
-
-    Args:
-        data: The input/update data set for all components
-        component: The component of interest
-        clock_field: The clock field
-        winding_from_field: The winding from field
-        winding_to_field: The winding to field
-
-    Returns:
-        A list containing zero or more TransformerClockErrors; listing all the ids of transformers where the clock was
-        invalid, given the winding type.
-    """
-
-    clk = data[component][clock_field]
-    wfr = data[component][winding_from_field]
-    wto = data[component][winding_to_field]
-    wfr_is_wye = np.isin(wfr, [WindingType.wye, WindingType.wye_n])
-    wto_is_wye = np.isin(wto, [WindingType.wye, WindingType.wye_n])
-    odd = clk % 2 == 1
-    # even number is not possible if one side is wye winding and the other side is not wye winding.
-    # odd number is not possible, if both sides are wye winding or both sides are not wye winding.
-    err = (~odd & (wfr_is_wye != wto_is_wye)) | (odd & (wfr_is_wye == wto_is_wye))
-    if err.any():
-        return [
-            TransformerClockError(
-                component=component,
-                fields=[clock_field, winding_from_field, winding_to_field],
-                ids=data[component]["id"][err].flatten().tolist(),
-            )
-        ]
-    return []
+# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
+#
+# SPDX-License-Identifier: MPL-2.0
+
+"""
+This module contains a set of comparison rules. They all share the same (or similar) logic and interface.
+
+In general each function checks the values in a single 'column' (i.e. field) of a numpy structured array and it
+returns an error object containing the component, the field and the ids of the records that did not match the rule.
+E.g. all_greater_than_zero(data, 'node', 'u_rated') returns a NotGreaterThanError if any of the node's `u_rated`
+values are 0 or less.
+
+In general, the rules are designed to ignore NaN values, except for none_missing() which explicitly checks for NaN
+values in the entire data set. It is important to understand that np.less_equal(x) yields different results than
+np.logical_not(np.greater(x)) as a NaN comparison always results in False. The most extreme example is that even
+np.nan == np.nan yields False.
+
+    np.less_equal(            [0.1, 0.2, 0.3, np.nan], 0.0)  = [False, False, False, False] -> OK
+    np.logical_not(np.greater([0.1, 0.2, 0.3, np.nan], 0.0)) = [False, False, False, True] -> Error (false positive)
+
+Input data:
+
+    data: SingleDataset
+        The entire input/update data set
+
+    component: str
+        The name of the component, which should be an existing key in the data
+
+    field: str
+        The name of the column, which should be an field in the component data (numpy structured array)
+
+Output data:
+    errors: List[ValidationError]
+        A list containing errors; in case of success, `errors` is the empty list: [].
+
+"""
+from enum import Enum
+from typing import Any, Callable, Dict, List, Optional, Tuple, Type, TypeVar, Union
+
+import numpy as np
+
+from power_grid_model.data_types import SingleDataset
+from power_grid_model.enum import WindingType
+from power_grid_model.validation.errors import (
+    ComparisonError,
+    IdNotInDatasetError,
+    InfinityError,
+    InvalidEnumValueError,
+    InvalidIdError,
+    MissingValueError,
+    MultiComponentNotUniqueError,
+    NotBetweenError,
+    NotBetweenOrAtError,
+    NotBooleanError,
+    NotGreaterOrEqualError,
+    NotGreaterThanError,
+    NotLessOrEqualError,
+    NotLessThanError,
+    NotUniqueError,
+    SameValueError,
+    TransformerClockError,
+    TwoValuesZeroError,
+    ValidationError,
+)
+from power_grid_model.validation.utils import eval_expression, nan_type, set_default_value
+
+Error = TypeVar("Error", bound=ValidationError)
+CompError = TypeVar("CompError", bound=ComparisonError)
+
+
+def all_greater_than_zero(data: SingleDataset, component: str, field: str) -> List[NotGreaterThanError]:
+    """
+    Check that for all records of a particular type of component, the values in the 'field' column are greater than
+    zero. Returns an empty list on success, or a list containing a single error object on failure.
+
+    Args:
+        data: The input/update data set for all components
+        component: The component of interest
+        field: The field of interest
+
+    Returns:
+        A list containing zero or one NotGreaterThanErrors, listing all ids where the value in the field of interest
+        was zero or less.
+    """
+    return all_greater_than(data, component, field, 0.0)
+
+
+def all_greater_than_or_equal_to_zero(
+    data: SingleDataset,
+    component: str,
+    field: str,
+    default_value: Optional[Union[np.ndarray, int, float]] = None,
+) -> List[NotGreaterOrEqualError]:
+    """
+    Check that for all records of a particular type of component, the values in the 'field' column are greater than,
+    or equal to zero. Returns an empty list on success, or a list containing a single error object on failure.
+
+    Args:
+        data: The input/update data set for all components
+        component: The component of interest
+        field: The field of interest
+        default_value: Some values are not required, but will receive a default value in the C++ core. To do a proper
+        input validation, these default values should be included in the validation. It can be a fixed value for the
+        entire column (int/float) or be different for each element (np.ndarray).
+
+    Returns:
+        A list containing zero or one NotGreaterOrEqualErrors, listing all ids where the value in the field of
+        interest was less than zero.
+    """
+    return all_greater_or_equal(data, component, field, 0.0, default_value)
+
+
+def all_greater_than(
+    data: SingleDataset, component: str, field: str, ref_value: Union[int, float, str]
+) -> List[NotGreaterThanError]:
+    """
+    Check that for all records of a particular type of component, the values in the 'field' column are greater than
+    the reference value. Returns an empty list on success, or a list containing a single error object on failure.
+
+    Args:
+        data: The input/update data set for all components
+        component: The component of interest
+        field: The field of interest
+        ref_value: The reference value against which all values in the 'field' column are compared. If the reference
+        value is a string, it is assumed to be another field (e.g. 'field_x') of the same component, or a ratio between
+        two fields (e.g. 'field_x / field_y')
+
+    Returns:
+        A list containing zero or one NotGreaterThanErrors, listing all ids where the value in the field of interest
+        was less than, or equal to, the ref_value.
+    """
+
+    def not_greater(val: np.ndarray, *ref: np.ndarray):
+        return np.less_equal(val, *ref)
+
+    return none_match_comparison(data, component, field, not_greater, ref_value, NotGreaterThanError)
+
+
+def all_greater_or_equal(
+    data: SingleDataset,
+    component: str,
+    field: str,
+    ref_value: Union[int, float, str],
+    default_value: Optional[Union[np.ndarray, int, float]] = None,
+) -> List[NotGreaterOrEqualError]:
+    """
+    Check that for all records of a particular type of component, the values in the 'field' column are greater than,
+    or equal to the reference value. Returns an empty list on success, or a list containing a single error object on
+    failure.
+
+    Args:
+        data: The input/update data set for all components
+        component: The component of interest
+        field: The field of interest
+        ref_value: The reference value against which all values in the 'field' column are compared. If the reference
+        value is a string, it is assumed to be another field (e.g. 'field_x') of the same component, or a ratio between
+        two fields (e.g. 'field_x / field_y')
+        default_value: Some values are not required, but will receive a default value in the C++ core. To do a proper
+        input validation, these default values should be included in the validation. It can be a fixed value for the
+        entire column (int/float) or be different for each element (np.ndarray).
+
+    Returns:
+        A list containing zero or one NotGreaterOrEqualErrors, listing all ids where the value in the field of
+        interest was less than the ref_value.
+
+    """
+
+    def not_greater_or_equal(val: np.ndarray, *ref: np.ndarray):
+        return np.less(val, *ref)
+
+    return none_match_comparison(
+        data, component, field, not_greater_or_equal, ref_value, NotGreaterOrEqualError, default_value
+    )
+
+
+def all_less_than(
+    data: SingleDataset, component: str, field: str, ref_value: Union[int, float, str]
+) -> List[NotLessThanError]:
+    """
+    Check that for all records of a particular type of component, the values in the 'field' column are less than the
+    reference value. Returns an empty list on success, or a list containing a single error object on failure.
+
+    Args:
+        data: The input/update data set for all components
+        component: The component of interest
+        field: The field of interest
+        ref_value: The reference value against which all values in the 'field' column are compared. If the reference
+        value is a string, it is assumed to be another field (e.g. 'field_x') of the same component, or a ratio between
+        two fields (e.g. 'field_x / field_y')
+
+    Returns:
+        A list containing zero or one NotLessThanErrors, listing all ids where the value in the field of interest was
+        greater than, or equal to, the ref_value.
+    """
+
+    def not_less(val: np.ndarray, *ref: np.ndarray):
+        return np.greater_equal(val, *ref)
+
+    return none_match_comparison(data, component, field, not_less, ref_value, NotLessThanError)
+
+
+def all_less_or_equal(
+    data: SingleDataset, component: str, field: str, ref_value: Union[int, float, str]
+) -> List[NotLessOrEqualError]:
+    """
+    Check that for all records of a particular type of component, the values in the 'field' column are less than,
+    or equal to the reference value. Returns an empty list on success, or a list containing a single error object on
+    failure.
+
+    Args:
+        data: The input/update data set for all components
+        component: The component of interest
+        field: The field of interest
+        ref_value: The reference value against which all values in the 'field' column are compared. If the reference
+        value is a string, it is assumed to be another field (e.g. 'field_x') of the same component, or a ratio between
+        two fields (e.g. 'field_x / field_y')
+
+    Returns:
+        A list containing zero or one NotLessOrEqualErrors, listing all ids where the value in the field of interest was
+        greater than the ref_value.
+
+    """
+
+    def not_less_or_equal(val: np.ndarray, *ref: np.ndarray):
+        return np.greater(val, *ref)
+
+    return none_match_comparison(data, component, field, not_less_or_equal, ref_value, NotLessOrEqualError)
+
+
+def all_between(  # pylint: disable=too-many-arguments
+    data: SingleDataset,
+    component: str,
+    field: str,
+    ref_value_1: Union[int, float, str],
+    ref_value_2: Union[int, float, str],
+    default_value: Optional[Union[np.ndarray, int, float]] = None,
+) -> List[NotBetweenError]:
+    """
+    Check that for all records of a particular type of component, the values in the 'field' column are (exclusively)
+    between reference value 1 and 2. Value 1 may be smaller, but also larger than value 2. Returns an empty list on
+    success, or a list containing a single error object on failure.
+
+    Args:
+        data: The input/update data set for all components
+        component: The component of interest
+        field: The field of interest
+        ref_value_1: The first reference value against which all values in the 'field' column are compared. If the
+        reference value is a string, it is assumed to be another field (e.g. 'field_x') of the same component, or a
+        ratio between two fields (e.g. 'field_x / field_y')
+        ref_value_2: The second reference value against which all values in the 'field' column are compared. If the
+        reference value is a string, it is assumed to be another field (e.g. 'field_x') of the same component,
+        or a ratio between two fields (e.g. 'field_x / field_y')
+        default_value: Some values are not required, but will receive a default value in the C++ core. To do a proper
+        input validation, these default values should be included in the validation. It can be a fixed value for the
+        entire column (int/float) or be different for each element (np.ndarray).
+
+    Returns:
+        A list containing zero or one NotBetweenErrors, listing all ids where the value in the field of interest was
+        outside the range defined by the reference values.
+    """
+
+    def outside(val: np.ndarray, *ref: np.ndarray) -> np.ndarray:
+        return np.logical_or(np.less_equal(val, np.minimum(*ref)), np.greater_equal(val, np.maximum(*ref)))
+
+    return none_match_comparison(
+        data, component, field, outside, (ref_value_1, ref_value_2), NotBetweenError, default_value
+    )
+
+
+def all_between_or_at(  # pylint: disable=too-many-arguments
+    data: SingleDataset,
+    component: str,
+    field: str,
+    ref_value_1: Union[int, float, str],
+    ref_value_2: Union[int, float, str],
+    default_value: Optional[Union[np.ndarray, int, float]] = None,
+) -> List[NotBetweenOrAtError]:
+    """
+    Check that for all records of a particular type of component, the values in the 'field' column are inclusively
+    between reference value 1 and 2. Value 1 may be smaller, but also larger than value 2. Returns an empty list on
+    success, or a list containing a single error object on failure.
+
+    Args:
+        data: The input/update data set for all components
+        component: The component of interest
+        field: The field of interest
+        ref_value_1: The first reference value against which all values in the 'field' column are compared. If the
+        reference value is a string, it is assumed to be another field (e.g. 'field_x') of the same component, or a
+        ratio between two fields (e.g. 'field_x / field_y')
+        ref_value_2: The second reference value against which all values in the 'field' column are compared. If the
+        reference value is a string, it is assumed to be another field (e.g. 'field_x') of the same component,
+        or a ratio between two fields (e.g. 'field_x / field_y')
+        default_value: Some values are not required, but will receive a default value in the C++ core. To do a proper
+        input validation, these default values should be included in the validation. It can be a fixed value for the
+        entire column (int/float) or be different for each element (np.ndarray).
+
+    Returns:
+        A list containing zero or one NotBetweenOrAtErrors, listing all ids where the value in the field of interest was
+        outside the range defined by the reference values.
+    """
+
+    def outside(val: np.ndarray, *ref: np.ndarray) -> np.ndarray:
+        return np.logical_or(np.less(val, np.minimum(*ref)), np.greater(val, np.maximum(*ref)))
+
+    return none_match_comparison(
+        data, component, field, outside, (ref_value_1, ref_value_2), NotBetweenOrAtError, default_value
+    )
+
+
+def none_match_comparison(
+    data: SingleDataset,
+    component: str,
+    field: str,
+    compare_fn: Callable,
+    ref_value: ComparisonError.RefType,
+    error: Type[CompError] = ComparisonError,  # type: ignore
+    default_value: Optional[Union[np.ndarray, int, float]] = None,
+) -> List[CompError]:
+    # pylint: disable=too-many-arguments
+    """
+    For all records of a particular type of component, check if the value in the 'field' column match the comparison.
+    Returns an empty list if none of the value match the comparison, or a list containing a single error object when at
+    the value in 'field' of at least one record matches the comparison.
+
+    Args:
+        data: The input/update data set for all components
+        component: The component of interest
+        field: The field of interest
+        compare_fn: A function that takes the data in the 'field' column, and any number of reference values
+        ref_value: A reference value, or a tuple of reference values, against which all values in the 'field' column
+        are compared using the compare_fn. If a reference value is a string, it is assumed to be another field
+        (e.g. 'field_x') of the same component, or a ratio between two fields (e.g. 'field_x / field_y')
+        error: The type (class) of error that should be returned in case any of the values match the comparison.
+        default_value: Some values are not required, but will receive a default value in the C++ core. To do a proper
+        input validation, these default values should be included in the validation. It can be a fixed value for the
+        entire column (int/float) or be different for each element (np.ndarray).
+
+    Returns:
+        A list containing zero or one comparison errors (should be a subclass of ComparisonError), listing all ids
+        where the value in the field of interest matched the comparison.
+    """
+    if default_value is not None:
+        set_default_value(data=data, component=component, field=field, default_value=default_value)
+    component_data = data[component]
+    if isinstance(ref_value, tuple):
+        ref = tuple(eval_expression(component_data, v) for v in ref_value)
+    else:
+        ref = (eval_expression(component_data, ref_value),)
+    matches = compare_fn(component_data[field], *ref)
+    if matches.any():
+        if matches.ndim > 1:
+            matches = matches.any(axis=1)
+        ids = component_data["id"][matches].flatten().tolist()
+        return [error(component, field, ids, ref_value)]
+    return []
+
+
+def all_unique(data: SingleDataset, component: str, field: str) -> List[NotUniqueError]:
+    """
+    Check that for all records of a particular type of component, the values in the 'field' column are unique within
+    the 'field' column of that component.
+
+    Args:
+        data: The input/update data set for all components
+        component: The component of interest
+        field: The field of interest
+
+    Returns:
+        A list containing zero or one NotUniqueError, listing all ids where the value in the field of interest was
+        not unique. If the field name was 'id' (a very common check), the id is added as many times as it occurred in
+        the 'id' column, to maintain object counts.
+    """
+    _, index, counts = np.unique(data[component][field], return_index=True, return_counts=True)
+    if any(counts != 1):
+        ids = data[component]["id"][index[counts != 1]].flatten().tolist()
+        if field == "id":  # Add ids multiple times
+            counts = counts[counts != 1]
+            for obj_id, count in zip(ids, counts):
+                ids += [obj_id] * (count - 1)
+            ids = sorted(ids)
+        return [NotUniqueError(component, field, ids)]
+    return []
+
+
+def all_cross_unique(
+    data: SingleDataset, fields: List[Tuple[str, str]], cross_only=True
+) -> List[MultiComponentNotUniqueError]:
+    """
+    Check that for all records of a particular type of component, the values in the 'field' column are unique within
+    the 'field' column of that component.
+
+    Args:
+        data: The input/update data set for all components
+        fields: The fields of interest, formatted as [(component_1, field_1), (component_2, field_2)]
+        cross_only: Do not include duplicates within a single field. It is advised that you use all_unique() to
+        explicitly check uniqueness within a single field.
+
+    Returns:
+        A list containing zero or one MultiComponentNotUniqueError, listing all fields and ids where the value was not
+        unique between the fields.
+    """
+    all_values: Dict[int, List[Tuple[Tuple[str, str], int]]] = {}
+    duplicate_ids = set()
+    for component, field in fields:
+        for obj_id, value in zip(data[component]["id"], data[component][field]):
+            component_id = ((component, field), obj_id)
+            if value not in all_values:
+                all_values[value] = []
+            elif not cross_only or not all(f == (component, field) for f, _ in all_values[value]):
+                duplicate_ids.update(all_values[value])
+                duplicate_ids.add(component_id)
+            all_values[value].append(component_id)
+    if duplicate_ids:
+        fields_with_duplicated_ids = {f for f, _ in duplicate_ids}
+        ids_with_duplicated_ids = {(c, i) for (c, _), i in duplicate_ids}
+        return [MultiComponentNotUniqueError(list(fields_with_duplicated_ids), list(ids_with_duplicated_ids))]
+    return []
+
+
+def all_valid_enum_values(
+    data: SingleDataset, component: str, field: str, enum: Type[Enum]
+) -> List[InvalidEnumValueError]:
+    """
+    Check that for all records of a particular type of component, the values in the 'field' column are valid values for
+    the supplied enum class. Returns an empty list on success, or a list containing a single error object on failure.
+
+    Args:
+        data: The input/update data set for all components
+        component: The component of interest
+        field: The field of interest
+        enum: The enum type to validate against
+
+    Returns:
+        A list containing zero or one InvalidEnumValueError, listing all ids where the value in the field of interest
+        was not a valid value in the supplied enum type.
+    """
+    valid = [nan_type(component, field)] + list(enum)
+    invalid = np.isin(data[component][field], np.array(valid, dtype=np.int8), invert=True)
+    if invalid.any():
+        ids = data[component]["id"][invalid].flatten().tolist()
+        return [InvalidEnumValueError(component, field, ids, enum)]
+    return []
+
+
+def all_valid_ids(
+    data: SingleDataset, component: str, field: str, ref_components: Union[str, List[str]], **filters: Any
+) -> List[InvalidIdError]:
+    """
+    For a column which should contain object identifiers (ids), check if the id exists in the data, for a specific set
+    of reference component types. E.g. is the from_node field of each line referring to an existing node id?
+
+    Args:
+        data: The input/update data set for all components
+        component: The component of interest
+        field: The field of interest
+        ref_components: The component or components in which we want to look for ids
+        **filters: One or more filters on the dataset. E.g. measured_terminal_type=MeasuredTerminalType.source.
+
+    Returns:
+        A list containing zero or one InvalidIdError, listing all ids where the value in the field of interest
+        was not a valid object identifier.
+    """
+    # For convenience, ref_component may be a string and we'll convert it to a 'list' containing that string as it's
+    # single element.
+    if isinstance(ref_components, str):
+        ref_components = [ref_components]
+
+    # Create a set of ids by chaining the ids of all ref_components
+    valid_ids = set()
+    for ref_component in ref_components:
+        if ref_component in data:
+            nan = nan_type(ref_component, "id")
+            if np.isnan(nan):
+                mask = ~np.isnan(data[ref_component]["id"])
+            else:
+                mask = np.not_equal(data[ref_component]["id"], nan)
+            valid_ids.update(data[ref_component]["id"][mask])
+
+    # Apply the filters (e.g. to select only records with a certain MeasuredTerminalType)
+    values = data[component][field]
+    mask = np.ones(shape=values.shape, dtype=bool)
+    for filter_field, filter_value in filters.items():
+        mask = np.logical_and(mask, data[component][filter_field] == filter_value)
+
+    # Find any values that can't be found in the set of ids
+    invalid = np.logical_and(mask, np.isin(values, list(valid_ids), invert=True))
+    if invalid.any():
+        ids = data[component]["id"][invalid].flatten().tolist()
+        return [InvalidIdError(component, field, ids, ref_components, filters)]
+    return []
+
+
+def all_boolean(data: SingleDataset, component: str, field: str) -> List[NotBooleanError]:
+    """
+    Check that for all records of a particular type of component, the values in the 'field' column are valid boolean
+    values, i.e. 0 or 1. Returns an empty list on success, or a list containing a single error object on failure.
+
+    Args:
+        data: The input/update data set for all components
+        component: The component of interest
+        field: The field of interest
+
+    Returns:
+        A list containing zero or one NotBooleanError, listing all ids where the value in the field of interest was not
+        a valid boolean value.
+    """
+    invalid = np.isin(data[component][field], [0, 1], invert=True)
+    if invalid.any():
+        ids = data[component]["id"][invalid].flatten().tolist()
+        return [NotBooleanError(component, field, ids)]
+    return []
+
+
+def all_not_two_values_zero(
+    data: SingleDataset, component: str, field_1: str, field_2: str
+) -> List[TwoValuesZeroError]:
+    """
+    Check that for all records of a particular type of component, the values in the 'field_1' and 'field_2' column are
+    not both zero. Returns an empty list on success, or a list containing a single error object on failure.
+
+    Args:
+        data: The input/update data set for all components
+        component: The component of interest
+        field_1: The first field of interest
+        field_2: The second field of interest
+    Returns:
+        A list containing zero or one TwoValuesZeroError, listing all ids where the value in the two fields of interest
+        were both zero.
+    """
+    invalid = np.logical_and(np.equal(data[component][field_1], 0.0), np.equal(data[component][field_2], 0.0))
+    if invalid.any():
+        if invalid.ndim > 1:
+            invalid = invalid.any(axis=1)
+        ids = data[component]["id"][invalid].flatten().tolist()
+        return [TwoValuesZeroError(component, [field_1, field_2], ids)]
+    return []
+
+
+def all_not_two_values_equal(data: SingleDataset, component: str, field_1: str, field_2: str) -> List[SameValueError]:
+    """
+    Check that for all records of a particular type of component, the values in the 'field_1' and 'field_2' column are
+    not both the same value. E.g. from_node and to_node of a line. Returns an empty list on success, or a list
+    containing a single error object on failure.
+
+    Args:
+        data: The input/update data set for all components
+        component: The component of interest
+        field_1: The first field of interest
+        field_2: The second field of interest
+    Returns:
+        A list containing zero or one SameValueError, listing all ids where the value in the two fields of interest
+        were both the same.
+    """
+    invalid = np.equal(data[component][field_1], data[component][field_2])
+    if invalid.any():
+        if invalid.ndim > 1:
+            invalid = invalid.any(axis=1)
+        ids = data[component]["id"][invalid].flatten().tolist()
+        return [SameValueError(component, [field_1, field_2], ids)]
+    return []
+
+
+def all_ids_exist_in_data_set(
+    data: SingleDataset, ref_data: SingleDataset, component: str, ref_name: str
+) -> List[IdNotInDatasetError]:
+    """
+    Check that for all records of a particular type of component, the ids exist in the reference data set.
+
+    Args:
+        data: The (update) data set for all components
+        ref_data: The reference (input) data set for all components
+        component: The component of interest
+        ref_name: The name of the reference data set, e.g. 'input_data'
+    Returns:
+        A list containing zero or one IdNotInDatasetError, listing all ids of the objects in the data set which do not
+        exist in the reference data set.
+    """
+    invalid = np.isin(data[component]["id"], ref_data[component]["id"], invert=True)
+    if invalid.any():
+        ids = data[component]["id"][invalid].flatten().tolist()
+        return [IdNotInDatasetError(component, ids, ref_name)]
+    return []
+
+
+def all_finite(data: SingleDataset) -> List[InfinityError]:
+    """
+    Check that for all records in all component, the values in all columns are finite value, i.e. float values other
+    than inf, or -inf. Nan values are ignored, as in all other comparison functions. You can use non_missing() to
+    check for missing/nan values. Returns an empty list on success, or a list containing an error object for each
+    component/field combination where.
+
+    Args:
+        data: The input/update data set for all components
+
+    Returns:
+        A list containing zero or one NotBooleanError, listing all ids where the value in the field of interest was not
+        a valid boolean value.
+    """
+    errors = []
+    for component, array in data.items():
+        for field, (dtype, _) in array.dtype.fields.items():
+            if not np.issubdtype(dtype, np.floating):
+                continue
+            invalid = np.isinf(array[field])
+            if invalid.any():
+                ids = data[component]["id"][invalid].flatten().tolist()
+                errors.append(InfinityError(component, field, ids))
+    return errors
+
+
+def none_missing(data: SingleDataset, component: str, fields: Union[str, List[str]]) -> List[MissingValueError]:
+    """
+    Check that for all records of a particular type of component, the values in the 'fields' columns are not NaN.
+    Returns an empty list on success, or a list containing a single error object on failure.
+
+    Args:
+        data: The input/update data set for all components
+        component: The component of interest
+        fields: The fields of interest
+
+    Returns:
+        A list containing zero or more MissingValueError; one for each field, listing all ids where the value in the
+        field was NaN.
+    """
+    errors = []
+    if isinstance(fields, str):
+        fields = [fields]
+    for field in fields:
+        nan = nan_type(component, field)
+        if np.isnan(nan):
+            invalid = np.isnan(data[component][field])
+        else:
+            invalid = np.equal(data[component][field], nan)
+        if invalid.any():
+            if invalid.ndim > 1:
+                invalid = invalid.any(axis=1)
+            ids = data[component]["id"][invalid].flatten().tolist()
+            errors.append(MissingValueError(component, field, ids))
+    return errors
+
+
+def all_valid_clocks(
+    data: SingleDataset, component: str, clock_field: str, winding_from_field: str, winding_to_field: str
+) -> List[TransformerClockError]:
+    """
+    Custom validation rule: Odd clock number is only allowed for Dy(n) or Y(N)d configuration.
+
+    Args:
+        data: The input/update data set for all components
+        component: The component of interest
+        clock_field: The clock field
+        winding_from_field: The winding from field
+        winding_to_field: The winding to field
+
+    Returns:
+        A list containing zero or more TransformerClockErrors; listing all the ids of transformers where the clock was
+        invalid, given the winding type.
+    """
+
+    clk = data[component][clock_field]
+    wfr = data[component][winding_from_field]
+    wto = data[component][winding_to_field]
+    wfr_is_wye = np.isin(wfr, [WindingType.wye, WindingType.wye_n])
+    wto_is_wye = np.isin(wto, [WindingType.wye, WindingType.wye_n])
+    odd = clk % 2 == 1
+    # even number is not possible if one side is wye winding and the other side is not wye winding.
+    # odd number is not possible, if both sides are wye winding or both sides are not wye winding.
+    err = (~odd & (wfr_is_wye != wto_is_wye)) | (odd & (wfr_is_wye == wto_is_wye))
+    if err.any():
+        return [
+            TransformerClockError(
+                component=component,
+                fields=[clock_field, winding_from_field, winding_to_field],
+                ids=data[component]["id"][err].flatten().tolist(),
+            )
+        ]
+    return []
```

## power_grid_model/validation/utils.py

 * *Ordering differences only*

```diff
@@ -1,214 +1,214 @@
-# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
-#
-# SPDX-License-Identifier: MPL-2.0
-
-"""
-Utilities used for validation. Only errors_to_string() is intended for end users.
-"""
-import re
-from typing import Dict, List, Optional, Union
-
-import numpy as np
-
-from power_grid_model import power_grid_meta_data
-from power_grid_model.data_types import SingleDataset
-from power_grid_model.validation.errors import ValidationError
-
-
-def eval_expression(data: np.ndarray, expression: Union[int, float, str]) -> np.ndarray:
-    """
-    Wrapper function that checks the type of the 'expression'. If the expression is a string, it is assumed to be a
-    field expression and the expression is validated. Otherwise it is assumed to be a numerical value and the value
-    is casted to a numpy 'array'.
-
-    Args:
-        data: A numpy structured array
-        expression: A numerical value, or a string, representing a (combination of) field(s)
-
-    Returns: The number or an evaluation of the field name(s) in the data, always represented as a Numpy 'array'.
-
-    Examples:
-        123 -> np.array(123)
-        123.4 -> np.array(123.4)
-        'value' -> data['value']
-        'foo/bar' -> data['foo'] / data[bar]
-
-    """
-    if isinstance(expression, str):
-        return eval_field_expression(data, expression)
-    return np.array(expression)
-
-
-def eval_field_expression(data: np.ndarray, expression: str) -> np.ndarray:
-    """
-    A field expression can either be the name of a field (e.g. 'field_x') in the data, or a ratio between two fields
-    (e.g. 'field_x / field_y'). The expression is checked on validity and then the fields are checked to be present in
-    the data. If the expression is a single field name, the field is returned. If it is a ratio, the ratio is
-    calculated and returned. Values divided by 0 will result in nan values without warning.
-
-    Args:
-        data: A numpy structured array
-        expression: A string, representing a (combination of) field(s)
-
-    Expression should be a combination of:
-      - field names (may contain lower case letters, numbers and underscores)
-      - a single mathematical operator /
-
-    Returns: An evaluation of the field name(s) in the data.
-
-    Examples:
-        'value' -> data['value']
-        'foo/bar' -> data['foo'] / data['bar']
-
-    """
-
-    # Validate the expression
-    match = re.fullmatch(r"[a-z][a-z0-9_]*(\s*/\s*[a-z][a-z0-9_]*)?", expression)
-    if not match:
-        raise ValueError(f"Invalid field expression '{expression}'")
-
-    # Find all field names and check if they exist in the dataset
-    fields = [f.strip() for f in expression.split("/")]
-    for field in fields:
-        if field not in data.dtype.names:
-            raise KeyError(f"Invalid field name {field}")
-
-    if len(fields) == 1:
-        return data[fields[0]]
-
-    assert len(fields) == 2
-    zero_div = np.logical_or(np.equal(data[fields[1]], 0.0), np.logical_not(np.isfinite(data[fields[1]])))
-    if np.any(zero_div):
-        result = np.full_like(data[fields[0]], np.nan)
-        np.true_divide(data[fields[0]], data[fields[1]], out=result, where=~zero_div)
-        return result
-    return np.true_divide(data[fields[0]], data[fields[1]])
-
-
-def update_input_data(input_data: SingleDataset, update_data: SingleDataset):
-    """
-    Update the input data using the available non-nan values in the update data.
-    """
-
-    merged_data = {component: array.copy() for component, array in input_data.items()}
-    for component in update_data.keys():
-        update_component_data(component, merged_data[component], update_data[component])
-    return merged_data
-
-
-def update_component_data(component: str, input_data: np.ndarray, update_data: np.ndarray) -> None:
-    """
-    Update the data in a numpy array, with another numpy array,
-    indexed on the "id" field and only non-NaN values are overwritten.
-    """
-    for field in update_data.dtype.names:
-        if field == "id":
-            continue
-        nan = nan_type(component, field, "update")
-        if np.isnan(nan):
-            mask = ~np.isnan(update_data[field])
-        else:
-            mask = np.not_equal(update_data[field], nan)
-
-        if mask.ndim == 2:
-            for phase in range(mask.shape[1]):
-                # find indexers of to-be-updated object
-                sub_mask = mask[:, phase]
-                idx = get_indexer(input_data["id"], update_data["id"][sub_mask])
-                # update
-                input_data[field][idx, phase] = update_data[field][sub_mask, phase]
-        else:
-            # find indexers of to-be-updated object
-            idx = get_indexer(input_data["id"], update_data["id"][mask])
-            # update
-            input_data[field][idx] = update_data[field][mask]
-
-
-def errors_to_string(
-    errors: Union[List[ValidationError], Dict[int, List[ValidationError]], None],
-    name: str = "the data",
-    details: bool = False,
-    id_lookup: Optional[Union[List[str], Dict[int, str]]] = None,
-) -> str:
-    """
-    Convert a set of errors (list or dict) to a human readable string representation.
-    Args:
-        errors: The error objects. List for input_data only, dict for batch data.
-        name: Human understandable name of the dataset, e.g. input_data, or update_data.
-        details: Display object ids and error specific information.
-        id_lookup: A list or dict (int->str) containing textual object ids
-
-    Returns: A human readable string representation of a set of errors.
-    """
-    if errors is None or len(errors) == 0:
-        return f"{name}: OK"
-    if isinstance(errors, dict):
-        return "\n".join(errors_to_string(err, f"{name}, batch #{i}", details) for i, err in sorted(errors.items()))
-    if len(errors) == 1 and not details:
-        return f"There is a validation error in {name}:\n\t{errors[0]}"
-    if len(errors) == 1:
-        msg = f"There is a validation error in {name}:\n"
-    else:
-        msg = f"There are {len(errors)} validation errors in {name}:\n"
-    if details:
-        for error in errors:
-            msg += "\n\t" + str(error) + "\n"
-            msg += "".join(f"\t\t{k}: {v}\n" for k, v in error.get_context(id_lookup).items())
-    else:
-        msg += "\n".join(f"{i + 1:>4}. {err}" for i, err in enumerate(errors))
-    return msg
-
-
-def nan_type(component: str, field: str, data_type="input"):
-    """
-    Helper function to retrieve the nan value for a certain field as defined in the power_grid_meta_data.
-    """
-    return power_grid_meta_data[data_type][component].nans[field]
-
-
-def get_indexer(input_ids: np.ndarray, update_ids: np.ndarray) -> np.ndarray:
-    """
-    Given array of ids from input and update dataset.
-    Find the posision of each id in the update dataset in the context of input dataset.
-    This is needed to update values in the dataset by id lookup.
-    Internally this is done by sorting the input ids, then using binary search lookup.
-
-    Args:
-        input_ids: array of ids in the input dataset
-        update_ids: array of ids in the update dataset
-
-    Returns:
-        np.ndarray: array of positions of the ids from update dataset in the input dataset
-            the following should hold
-            input_ids[result] == update_ids
-    """
-    permutation_sort = np.argsort(input_ids)  # complexity O(N_input * logN_input)
-    return permutation_sort[
-        np.searchsorted(input_ids, update_ids, sorter=permutation_sort)
-    ]  # complexity O(N_update * logN_input)
-
-
-def set_default_value(data: SingleDataset, component: str, field: str, default_value: Union[int, float, np.ndarray]):
-    """
-    This function sets the default value in the data that is to be validated, so the default values are included in the
-    validation.
-
-    Args:
-        data: The input/update data set for all components
-        component: The component of interest
-        field: The field of interest
-        default_value: Some values are not required, but will receive a default value in the C++ core. To do a proper
-        input validation, these default values should be included in the validation. It can be a fixed value for the
-        entire column (int/float) or be different for each element (np.ndarray).
-
-    Returns:
-
-    """
-    if np.isnan(nan_type(component, field)):
-        mask = np.isnan(data[component][field])
-    else:
-        mask = data[component][field] == nan_type(component, field)
-    if isinstance(default_value, np.ndarray):
-        data[component][field][mask] = default_value[mask]
-    else:
-        data[component][field][mask] = default_value
+# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
+#
+# SPDX-License-Identifier: MPL-2.0
+
+"""
+Utilities used for validation. Only errors_to_string() is intended for end users.
+"""
+import re
+from typing import Dict, List, Optional, Union
+
+import numpy as np
+
+from power_grid_model import power_grid_meta_data
+from power_grid_model.data_types import SingleDataset
+from power_grid_model.validation.errors import ValidationError
+
+
+def eval_expression(data: np.ndarray, expression: Union[int, float, str]) -> np.ndarray:
+    """
+    Wrapper function that checks the type of the 'expression'. If the expression is a string, it is assumed to be a
+    field expression and the expression is validated. Otherwise it is assumed to be a numerical value and the value
+    is casted to a numpy 'array'.
+
+    Args:
+        data: A numpy structured array
+        expression: A numerical value, or a string, representing a (combination of) field(s)
+
+    Returns: The number or an evaluation of the field name(s) in the data, always represented as a Numpy 'array'.
+
+    Examples:
+        123 -> np.array(123)
+        123.4 -> np.array(123.4)
+        'value' -> data['value']
+        'foo/bar' -> data['foo'] / data[bar]
+
+    """
+    if isinstance(expression, str):
+        return eval_field_expression(data, expression)
+    return np.array(expression)
+
+
+def eval_field_expression(data: np.ndarray, expression: str) -> np.ndarray:
+    """
+    A field expression can either be the name of a field (e.g. 'field_x') in the data, or a ratio between two fields
+    (e.g. 'field_x / field_y'). The expression is checked on validity and then the fields are checked to be present in
+    the data. If the expression is a single field name, the field is returned. If it is a ratio, the ratio is
+    calculated and returned. Values divided by 0 will result in nan values without warning.
+
+    Args:
+        data: A numpy structured array
+        expression: A string, representing a (combination of) field(s)
+
+    Expression should be a combination of:
+      - field names (may contain lower case letters, numbers and underscores)
+      - a single mathematical operator /
+
+    Returns: An evaluation of the field name(s) in the data.
+
+    Examples:
+        'value' -> data['value']
+        'foo/bar' -> data['foo'] / data['bar']
+
+    """
+
+    # Validate the expression
+    match = re.fullmatch(r"[a-z][a-z0-9_]*(\s*/\s*[a-z][a-z0-9_]*)?", expression)
+    if not match:
+        raise ValueError(f"Invalid field expression '{expression}'")
+
+    # Find all field names and check if they exist in the dataset
+    fields = [f.strip() for f in expression.split("/")]
+    for field in fields:
+        if field not in data.dtype.names:
+            raise KeyError(f"Invalid field name {field}")
+
+    if len(fields) == 1:
+        return data[fields[0]]
+
+    assert len(fields) == 2
+    zero_div = np.logical_or(np.equal(data[fields[1]], 0.0), np.logical_not(np.isfinite(data[fields[1]])))
+    if np.any(zero_div):
+        result = np.full_like(data[fields[0]], np.nan)
+        np.true_divide(data[fields[0]], data[fields[1]], out=result, where=~zero_div)
+        return result
+    return np.true_divide(data[fields[0]], data[fields[1]])
+
+
+def update_input_data(input_data: SingleDataset, update_data: SingleDataset):
+    """
+    Update the input data using the available non-nan values in the update data.
+    """
+
+    merged_data = {component: array.copy() for component, array in input_data.items()}
+    for component in update_data.keys():
+        update_component_data(component, merged_data[component], update_data[component])
+    return merged_data
+
+
+def update_component_data(component: str, input_data: np.ndarray, update_data: np.ndarray) -> None:
+    """
+    Update the data in a numpy array, with another numpy array,
+    indexed on the "id" field and only non-NaN values are overwritten.
+    """
+    for field in update_data.dtype.names:
+        if field == "id":
+            continue
+        nan = nan_type(component, field, "update")
+        if np.isnan(nan):
+            mask = ~np.isnan(update_data[field])
+        else:
+            mask = np.not_equal(update_data[field], nan)
+
+        if mask.ndim == 2:
+            for phase in range(mask.shape[1]):
+                # find indexers of to-be-updated object
+                sub_mask = mask[:, phase]
+                idx = get_indexer(input_data["id"], update_data["id"][sub_mask])
+                # update
+                input_data[field][idx, phase] = update_data[field][sub_mask, phase]
+        else:
+            # find indexers of to-be-updated object
+            idx = get_indexer(input_data["id"], update_data["id"][mask])
+            # update
+            input_data[field][idx] = update_data[field][mask]
+
+
+def errors_to_string(
+    errors: Union[List[ValidationError], Dict[int, List[ValidationError]], None],
+    name: str = "the data",
+    details: bool = False,
+    id_lookup: Optional[Union[List[str], Dict[int, str]]] = None,
+) -> str:
+    """
+    Convert a set of errors (list or dict) to a human readable string representation.
+    Args:
+        errors: The error objects. List for input_data only, dict for batch data.
+        name: Human understandable name of the dataset, e.g. input_data, or update_data.
+        details: Display object ids and error specific information.
+        id_lookup: A list or dict (int->str) containing textual object ids
+
+    Returns: A human readable string representation of a set of errors.
+    """
+    if errors is None or len(errors) == 0:
+        return f"{name}: OK"
+    if isinstance(errors, dict):
+        return "\n".join(errors_to_string(err, f"{name}, batch #{i}", details) for i, err in sorted(errors.items()))
+    if len(errors) == 1 and not details:
+        return f"There is a validation error in {name}:\n\t{errors[0]}"
+    if len(errors) == 1:
+        msg = f"There is a validation error in {name}:\n"
+    else:
+        msg = f"There are {len(errors)} validation errors in {name}:\n"
+    if details:
+        for error in errors:
+            msg += "\n\t" + str(error) + "\n"
+            msg += "".join(f"\t\t{k}: {v}\n" for k, v in error.get_context(id_lookup).items())
+    else:
+        msg += "\n".join(f"{i + 1:>4}. {err}" for i, err in enumerate(errors))
+    return msg
+
+
+def nan_type(component: str, field: str, data_type="input"):
+    """
+    Helper function to retrieve the nan value for a certain field as defined in the power_grid_meta_data.
+    """
+    return power_grid_meta_data[data_type][component].nans[field]
+
+
+def get_indexer(input_ids: np.ndarray, update_ids: np.ndarray) -> np.ndarray:
+    """
+    Given array of ids from input and update dataset.
+    Find the posision of each id in the update dataset in the context of input dataset.
+    This is needed to update values in the dataset by id lookup.
+    Internally this is done by sorting the input ids, then using binary search lookup.
+
+    Args:
+        input_ids: array of ids in the input dataset
+        update_ids: array of ids in the update dataset
+
+    Returns:
+        np.ndarray: array of positions of the ids from update dataset in the input dataset
+            the following should hold
+            input_ids[result] == update_ids
+    """
+    permutation_sort = np.argsort(input_ids)  # complexity O(N_input * logN_input)
+    return permutation_sort[
+        np.searchsorted(input_ids, update_ids, sorter=permutation_sort)
+    ]  # complexity O(N_update * logN_input)
+
+
+def set_default_value(data: SingleDataset, component: str, field: str, default_value: Union[int, float, np.ndarray]):
+    """
+    This function sets the default value in the data that is to be validated, so the default values are included in the
+    validation.
+
+    Args:
+        data: The input/update data set for all components
+        component: The component of interest
+        field: The field of interest
+        default_value: Some values are not required, but will receive a default value in the C++ core. To do a proper
+        input validation, these default values should be included in the validation. It can be a fixed value for the
+        entire column (int/float) or be different for each element (np.ndarray).
+
+    Returns:
+
+    """
+    if np.isnan(nan_type(component, field)):
+        mask = np.isnan(data[component][field])
+    else:
+        mask = data[component][field] == nan_type(component, field)
+    if isinstance(default_value, np.ndarray):
+        data[component][field][mask] = default_value[mask]
+    else:
+        data[component][field][mask] = default_value
```

## power_grid_model/validation/validation.py

 * *Ordering differences only*

```diff
@@ -1,674 +1,674 @@
-# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
-#
-# SPDX-License-Identifier: MPL-2.0
-
-"""
-Power Grid Model Validation Functions.
-
-Although all functions are 'public', you probably only need validate_input_data() and validate_batch_data().
-
-"""
-import copy
-from itertools import chain
-from typing import Dict, List, Optional
-
-import numpy as np
-
-from power_grid_model import power_grid_meta_data
-from power_grid_model.data_types import BatchDataset, Dataset, SingleDataset
-from power_grid_model.enum import (
-    Branch3Side,
-    BranchSide,
-    CalculationType,
-    LoadGenType,
-    MeasuredTerminalType,
-    WindingType,
-)
-from power_grid_model.utils import convert_batch_dataset_to_batch_list
-from power_grid_model.validation.errors import (
-    IdNotInDatasetError,
-    MissingValueError,
-    MultiComponentNotUniqueError,
-    ValidationError,
-)
-from power_grid_model.validation.rules import (
-    all_between,
-    all_between_or_at,
-    all_boolean,
-    all_cross_unique,
-    all_finite,
-    all_greater_or_equal,
-    all_greater_than_or_equal_to_zero,
-    all_greater_than_zero,
-    all_ids_exist_in_data_set,
-    all_less_than,
-    all_not_two_values_equal,
-    all_not_two_values_zero,
-    all_unique,
-    all_valid_clocks,
-    all_valid_enum_values,
-    all_valid_ids,
-    none_missing,
-)
-from power_grid_model.validation.utils import update_input_data
-
-
-def validate_input_data(
-    input_data: SingleDataset, calculation_type: Optional[CalculationType] = None, symmetric: bool = True
-) -> Optional[List[ValidationError]]:
-    """
-    Validates the entire input dataset:
-
-        1. Is the data structure correct? (checking data types and numpy array shapes)
-        2. Are all required values provided? (checking NaNs)
-        3. Are all ID's unique? (checking object identifiers across all components)
-        4. Are the supplied values valid? (checking limits and other logic as described in "Graph Data Model")
-
-    Args:
-        input_data: A power-grid-model input dataset
-        calculation_type: Supply a calculation method, to allow missing values for unused fields
-        symmetric: A boolean to state whether input data will be used for a symmetric or asymmetric calculation
-
-    Raises:
-        KeyError, TypeError or ValueError if the data structure is invalid.
-
-    Returns:
-        None if the data is valid, or a list containing all validation errors.
-    """
-    # A deep copy is made of the input data, since default values will be added in the validation process
-    input_data_copy = copy.deepcopy(input_data)
-    assert_valid_data_structure(input_data_copy, "input")
-
-    errors: List[ValidationError] = []
-    errors += validate_required_values(input_data_copy, calculation_type, symmetric)
-    errors += validate_unique_ids_across_components(input_data_copy)
-    errors += validate_values(input_data_copy)
-    return errors if errors else None
-
-
-def validate_batch_data(
-    input_data: SingleDataset,
-    update_data: BatchDataset,
-    calculation_type: Optional[CalculationType] = None,
-    symmetric: bool = True,
-) -> Optional[Dict[int, List[ValidationError]]]:
-    """
-    Ihe input dataset is validated:
-
-        1. Is the data structure correct? (checking data types and numpy array shapes)
-        2. Are all input data ID's unique? (checking object identifiers across all components)
-
-    For each batch the update data is validated:
-        3. Is the update data structure correct? (checking data types and numpy array shapes)
-        4. Are all update ID's valid? (checking object identifiers across update and input data)
-
-    Then (for each batch independently) the input dataset is updated with the batch's update data and validated:
-        5. Are all required values provided? (checking NaNs)
-        6. Are the supplied values valid? (checking limits and other logic as described in "Graph Data Model")
-
-    Args:
-        input_data: a power-grid-model input dataset
-        update_data: a power-grid-model update dataset (one or more batches)
-        calculation_type: Supply a calculation method, to allow missing values for unused fields
-        symmetric: A boolean to state whether input data will be used for a symmetric or asymmetric calculation
-
-    Raises:
-        KeyError, TypeError or ValueError if the data structure is invalid.
-
-    Returns:
-        None if the data is valid, or a dictionary containing all validation errors,
-        where the key is the batch number (0-indexed).
-    """
-    assert_valid_data_structure(input_data, "input")
-
-    input_errors: List[ValidationError] = list(validate_unique_ids_across_components(input_data))
-
-    # Splitting update_data_into_batches may raise TypeErrors and ValueErrors
-    batch_data = convert_batch_dataset_to_batch_list(update_data)
-
-    errors = {}
-    for batch, batch_update_data in enumerate(batch_data):
-        assert_valid_data_structure(batch_update_data, "update")
-        id_errors: List[ValidationError] = list(validate_ids_exist(batch_update_data, input_data))
-
-        batch_errors = input_errors + id_errors
-        if not id_errors:
-            merged_data = update_input_data(input_data, batch_update_data)
-            batch_errors += validate_required_values(merged_data, calculation_type, symmetric)
-            batch_errors += validate_values(merged_data)
-
-        if batch_errors:
-            errors[batch] = batch_errors
-
-    return errors if errors else None
-
-
-def assert_valid_data_structure(data: Dataset, data_type: str) -> None:
-    """
-    Checks if all component names are valid and if the data inside the component matches the required Numpy
-    structured array as defined in the Power Grid Model meta data.
-
-    Args:
-        data: a power-grid-model input/update dataset
-        data_type: 'input' or 'update'
-
-    Raises: KeyError, TypeError
-
-    """
-    if data_type not in {"input", "update"}:
-        raise KeyError(f"Unexpected data type '{data_type}' (should be 'input' or 'update')")
-
-    component_dtype = {component: meta.dtype for component, meta in power_grid_meta_data[data_type].items()}
-    for component, array in data.items():
-        # Check if component name is valid
-        if component not in component_dtype:
-            raise KeyError(f"Unknown component '{component}' in {data_type}_data.")
-
-        # Check if component definition is as expected
-        dtype = component_dtype[component]
-        if isinstance(array, np.ndarray):
-            if array.dtype != dtype:
-                if not hasattr(array.dtype, "names") or not array.dtype.names:
-                    raise TypeError(
-                        f"Unexpected Numpy array ({array.dtype}) for '{component}' {data_type}_data "
-                        "(should be a Numpy structured array)."
-                    )
-                raise TypeError(f"Unexpected Numpy structured array; (expected = {dtype}, actual = {array.dtype}).")
-        else:
-            raise TypeError(
-                f"Unexpected data type {type(array).__name__} for '{component}' {data_type}_data "
-                "(should be a Numpy structured array)."
-            )
-
-
-def validate_unique_ids_across_components(data: SingleDataset) -> List[MultiComponentNotUniqueError]:
-    """
-    Checks if all ids in the input dataset are unique
-
-    Args:
-        data: a power-grid-model input dataset
-
-    Returns: an empty list if all ids are unique, or a list of MultiComponentNotUniqueErrors for all components that
-             have non-unique ids
-
-    """
-    return all_cross_unique(data, [(component, "id") for component in data])
-
-
-def validate_ids_exist(update_data: Dict[str, np.ndarray], input_data: SingleDataset) -> List[IdNotInDatasetError]:
-    """
-    Checks if all ids of the components in the update data exist in the input data. This needs to be true, because you
-    can only update existing components.
-
-    This function should be called for every update dataset in a batch set
-
-    Args:
-        update_data: a single update dataset
-        input_data: a power-grid-model input dataset
-
-    Returns: an empty list if all update data ids exist in the input dataset, or a list of IdNotInDatasetErrors for
-             all update components of which the id does not exist in the input dataset
-
-    """
-    errors = (all_ids_exist_in_data_set(update_data, input_data, component, "input_data") for component in update_data)
-    return list(chain(*errors))
-
-
-def validate_required_values(
-    data: SingleDataset, calculation_type: Optional[CalculationType] = None, symmetric: bool = True
-) -> List[MissingValueError]:
-    """
-    Checks if all required data is available.
-
-    Args:
-        data: a power-grid-model input dataset
-        calculation_type: Supply a calculation method, to allow missing values for unused fields
-        symmetric: A boolean to state whether input data will be used for a symmetric or asymmetric calculation
-
-    Returns: an empty list if all required data is available, or a list of MissingValueErrors.
-
-    """
-    # Base
-    required = {"base": ["id"]}
-
-    # Nodes
-    required["node"] = required["base"] + ["u_rated"]
-
-    # Branches
-    required["branch"] = required["base"] + ["from_node", "to_node", "from_status", "to_status"]
-    required["link"] = required["branch"].copy()
-    required["line"] = required["branch"] + ["r1", "x1", "c1", "tan1", "i_n"]
-    required["transformer"] = required["branch"] + [
-        "u1",
-        "u2",
-        "sn",
-        "uk",
-        "pk",
-        "i0",
-        "p0",
-        "winding_from",
-        "winding_to",
-        "clock",
-        "tap_side",
-        "tap_pos",
-        "tap_min",
-        "tap_max",
-        "tap_size",
-    ]
-    # Branch3
-    required["branch3"] = required["base"] + ["node_1", "node_2", "node_3", "status_1", "status_2", "status_3"]
-    required["three_winding_transformer"] = required["branch3"] + [
-        "u1",
-        "u2",
-        "u3",
-        "sn_1",
-        "sn_2",
-        "sn_3",
-        "uk_12",
-        "uk_13",
-        "uk_23",
-        "pk_12",
-        "pk_13",
-        "pk_23",
-        "i0",
-        "p0",
-        "winding_1",
-        "winding_2",
-        "winding_3",
-        "clock_12",
-        "clock_13",
-        "tap_side",
-        "tap_pos",
-        "tap_min",
-        "tap_max",
-        "tap_size",
-    ]
-    # Appliances
-    required["appliance"] = required["base"] + ["node", "status"]
-    required["source"] = required["appliance"].copy()
-    if calculation_type is None or calculation_type == CalculationType.power_flow:
-        required["source"] += ["u_ref"]
-    required["shunt"] = required["appliance"] + ["g1", "b1"]
-    required["generic_load_gen"] = required["appliance"] + ["type"]
-    if calculation_type is None or calculation_type == CalculationType.power_flow:
-        required["generic_load_gen"] += ["p_specified", "q_specified"]
-    required["sym_load"] = required["generic_load_gen"].copy()
-    required["asym_load"] = required["generic_load_gen"].copy()
-    required["sym_gen"] = required["generic_load_gen"].copy()
-    required["asym_gen"] = required["generic_load_gen"].copy()
-
-    # Sensors
-    required["sensor"] = required["base"] + ["measured_object"]
-    required["voltage_sensor"] = required["sensor"].copy()
-    required["power_sensor"] = required["sensor"] + ["measured_terminal_type"]
-    if calculation_type is None or calculation_type == CalculationType.state_estimation:
-        required["voltage_sensor"] += ["u_sigma", "u_measured"]
-        required["power_sensor"] += ["power_sigma", "p_measured", "q_measured"]
-    required["sym_voltage_sensor"] = required["voltage_sensor"].copy()
-    required["asym_voltage_sensor"] = required["voltage_sensor"].copy()
-    required["sym_power_sensor"] = required["power_sensor"].copy()
-    required["asym_power_sensor"] = required["power_sensor"].copy()
-
-    if not symmetric:
-        required["line"] += ["r0", "x0", "c0", "tan0"]
-        required["shunt"] += ["g0", "b0"]
-
-    return list(chain(*(none_missing(data, component, required.get(component, [])) for component in data)))
-
-
-def validate_values(data: SingleDataset) -> List[ValidationError]:  # pylint: disable=too-many-branches
-    """
-    For each component supplied in the data, call the appropriate validation function
-
-    Args:
-        data: a power-grid-model input dataset
-
-    Returns: an empty list if all required data is valid, or a list of ValidationErrors.
-
-    """
-    errors: List[ValidationError] = list(all_finite(data))
-    if "node" in data:
-        errors += validate_node(data)
-    if "line" in data:
-        errors += validate_line(data)
-    if "link" in data:
-        errors += validate_branch(data, "link")
-    if "transformer" in data:
-        errors += validate_transformer(data)
-    if "three_winding_transformer" in data:
-        errors += validate_three_winding_transformer(data)
-    if "source" in data:
-        errors += validate_source(data)
-    if "sym_load" in data:
-        errors += validate_generic_load_gen(data, "sym_load")
-    if "sym_gen" in data:
-        errors += validate_generic_load_gen(data, "sym_gen")
-    if "asym_load" in data:
-        errors += validate_generic_load_gen(data, "asym_load")
-    if "asym_gen" in data:
-        errors += validate_generic_load_gen(data, "asym_gen")
-    if "shunt" in data:
-        errors += validate_shunt(data)
-    if "sym_voltage_sensor" in data:
-        errors += validate_generic_voltage_sensor(data, "sym_voltage_sensor")
-    if "asym_voltage_sensor" in data:
-        errors += validate_generic_voltage_sensor(data, "asym_voltage_sensor")
-    if "sym_power_sensor" in data:
-        errors += validate_generic_power_sensor(data, "sym_power_sensor")
-    if "asym_power_sensor" in data:
-        errors += validate_generic_power_sensor(data, "asym_power_sensor")
-    return errors
-
-
-# pylint: disable=missing-function-docstring
-
-
-def validate_base(data: SingleDataset, component: str) -> List[ValidationError]:
-    errors: List[ValidationError] = list(all_unique(data, component, "id"))
-    return errors
-
-
-def validate_node(data: SingleDataset) -> List[ValidationError]:
-    errors = validate_base(data, "node")
-    errors += all_greater_than_zero(data, "node", "u_rated")
-    return errors
-
-
-def validate_branch(data: SingleDataset, component: str) -> List[ValidationError]:
-    errors = validate_base(data, component)
-    errors += all_valid_ids(data, component, "from_node", "node")
-    errors += all_valid_ids(data, component, "to_node", "node")
-    errors += all_not_two_values_equal(data, component, "to_node", "from_node")
-    errors += all_boolean(data, component, "from_status")
-    errors += all_boolean(data, component, "to_status")
-    return errors
-
-
-def validate_line(data: SingleDataset) -> List[ValidationError]:
-    errors = validate_branch(data, "line")
-    errors += all_not_two_values_zero(data, "line", "r1", "x1")
-    errors += all_not_two_values_zero(data, "line", "r0", "x0")
-    errors += all_greater_than_zero(data, "line", "i_n")
-    return errors
-
-
-def validate_transformer(data: SingleDataset) -> List[ValidationError]:
-    errors = validate_branch(data, "transformer")
-    errors += all_greater_than_zero(data, "transformer", "u1")
-    errors += all_greater_than_zero(data, "transformer", "u2")
-    errors += all_greater_than_zero(data, "transformer", "sn")
-    errors += all_greater_or_equal(data, "transformer", "uk", "pk/sn")
-    errors += all_between(data, "transformer", "uk", 0, 1)
-    errors += all_greater_than_or_equal_to_zero(data, "transformer", "pk")
-    errors += all_greater_or_equal(data, "transformer", "i0", "p0/sn")
-    errors += all_less_than(data, "transformer", "i0", 1)
-    errors += all_greater_than_or_equal_to_zero(data, "transformer", "p0")
-    errors += all_valid_enum_values(data, "transformer", "winding_from", WindingType)
-    errors += all_valid_enum_values(data, "transformer", "winding_to", WindingType)
-    errors += all_between_or_at(data, "transformer", "clock", 0, 12)
-    errors += all_valid_clocks(data, "transformer", "clock", "winding_from", "winding_to")
-    errors += all_valid_enum_values(data, "transformer", "tap_side", BranchSide)
-    errors += all_between_or_at(data, "transformer", "tap_pos", "tap_min", "tap_max")
-    errors += all_between_or_at(data, "transformer", "tap_nom", "tap_min", "tap_max", 0)
-    errors += all_greater_than_or_equal_to_zero(data, "transformer", "tap_size")
-    errors += all_greater_or_equal(data, "transformer", "uk_min", "pk_min/sn", data["transformer"]["uk"])
-    errors += all_between(data, "transformer", "uk_min", 0, 1, data["transformer"]["uk"])
-    errors += all_greater_or_equal(data, "transformer", "uk_max", "pk_max/sn", data["transformer"]["uk"])
-    errors += all_between(data, "transformer", "uk_max", 0, 1, data["transformer"]["uk"])
-    errors += all_greater_than_or_equal_to_zero(data, "transformer", "pk_min", data["transformer"]["pk"])
-    errors += all_greater_than_or_equal_to_zero(data, "transformer", "pk_max", data["transformer"]["pk"])
-    return errors
-
-
-def validate_branch3(data: SingleDataset, component: str) -> List[ValidationError]:
-    errors = validate_base(data, component)
-    errors += all_valid_ids(data, component, "node_1", "node")
-    errors += all_valid_ids(data, component, "node_2", "node")
-    errors += all_valid_ids(data, component, "node_3", "node")
-    errors += all_not_two_values_equal(data, component, "node_1", "node_2")
-    errors += all_not_two_values_equal(data, component, "node_1", "node_3")
-    errors += all_not_two_values_equal(data, component, "node_2", "node_3")
-    errors += all_boolean(data, component, "status_1")
-    errors += all_boolean(data, component, "status_2")
-    errors += all_boolean(data, component, "status_3")
-    return errors
-
-
-# pylint: disable=R0915
-def validate_three_winding_transformer(data: SingleDataset) -> List[ValidationError]:
-    errors = validate_branch3(data, "three_winding_transformer")
-    errors += all_greater_than_zero(data, "three_winding_transformer", "u1")
-    errors += all_greater_than_zero(data, "three_winding_transformer", "u2")
-    errors += all_greater_than_zero(data, "three_winding_transformer", "u3")
-    errors += all_greater_than_zero(data, "three_winding_transformer", "sn_1")
-    errors += all_greater_than_zero(data, "three_winding_transformer", "sn_2")
-    errors += all_greater_than_zero(data, "three_winding_transformer", "sn_3")
-    errors += all_greater_or_equal(data, "three_winding_transformer", "uk_12", "pk_12/sn_1")
-    errors += all_greater_or_equal(data, "three_winding_transformer", "uk_12", "pk_12/sn_2")
-    errors += all_greater_or_equal(data, "three_winding_transformer", "uk_13", "pk_13/sn_1")
-    errors += all_greater_or_equal(data, "three_winding_transformer", "uk_13", "pk_13/sn_3")
-    errors += all_greater_or_equal(data, "three_winding_transformer", "uk_23", "pk_23/sn_2")
-    errors += all_greater_or_equal(data, "three_winding_transformer", "uk_23", "pk_23/sn_3")
-    errors += all_between(data, "three_winding_transformer", "uk_12", 0, 1)
-    errors += all_between(data, "three_winding_transformer", "uk_13", 0, 1)
-    errors += all_between(data, "three_winding_transformer", "uk_23", 0, 1)
-    errors += all_greater_than_or_equal_to_zero(data, "three_winding_transformer", "pk_12")
-    errors += all_greater_than_or_equal_to_zero(data, "three_winding_transformer", "pk_13")
-    errors += all_greater_than_or_equal_to_zero(data, "three_winding_transformer", "pk_23")
-    errors += all_greater_or_equal(data, "three_winding_transformer", "i0", "p0/sn_1")
-    errors += all_less_than(data, "three_winding_transformer", "i0", 1)
-    errors += all_greater_than_or_equal_to_zero(data, "three_winding_transformer", "p0")
-    errors += all_valid_enum_values(data, "three_winding_transformer", "winding_1", WindingType)
-    errors += all_valid_enum_values(data, "three_winding_transformer", "winding_2", WindingType)
-    errors += all_valid_enum_values(data, "three_winding_transformer", "winding_3", WindingType)
-    errors += all_between_or_at(data, "three_winding_transformer", "clock_12", 0, 12)
-    errors += all_between_or_at(data, "three_winding_transformer", "clock_13", 0, 12)
-    errors += all_valid_clocks(data, "three_winding_transformer", "clock_12", "winding_1", "winding_2")
-    errors += all_valid_clocks(data, "three_winding_transformer", "clock_13", "winding_1", "winding_3")
-    errors += all_valid_enum_values(data, "three_winding_transformer", "tap_side", Branch3Side)
-    errors += all_between_or_at(data, "three_winding_transformer", "tap_pos", "tap_min", "tap_max")
-    errors += all_between_or_at(data, "three_winding_transformer", "tap_nom", "tap_min", "tap_max", 0)
-    errors += all_greater_than_or_equal_to_zero(data, "three_winding_transformer", "tap_size")
-    errors += all_greater_or_equal(
-        data, "three_winding_transformer", "uk_12_min", "pk_12_min/sn_1", data["three_winding_transformer"]["uk_12"]
-    )
-    errors += all_greater_or_equal(
-        data, "three_winding_transformer", "uk_12_min", "pk_12_min/sn_2", data["three_winding_transformer"]["uk_12"]
-    )
-    errors += all_greater_or_equal(
-        data, "three_winding_transformer", "uk_13_min", "pk_13_min/sn_1", data["three_winding_transformer"]["uk_13"]
-    )
-    errors += all_greater_or_equal(
-        data, "three_winding_transformer", "uk_13_min", "pk_13_min/sn_3", data["three_winding_transformer"]["uk_13"]
-    )
-    errors += all_greater_or_equal(
-        data, "three_winding_transformer", "uk_23_min", "pk_23_min/sn_2", data["three_winding_transformer"]["uk_23"]
-    )
-    errors += all_greater_or_equal(
-        data, "three_winding_transformer", "uk_23_min", "pk_23_min/sn_3", data["three_winding_transformer"]["uk_23"]
-    )
-    errors += all_between(
-        data, "three_winding_transformer", "uk_12_min", 0, 1, data["three_winding_transformer"]["uk_12"]
-    )
-    errors += all_between(
-        data, "three_winding_transformer", "uk_13_min", 0, 1, data["three_winding_transformer"]["uk_13"]
-    )
-    errors += all_between(
-        data, "three_winding_transformer", "uk_23_min", 0, 1, data["three_winding_transformer"]["uk_23"]
-    )
-    errors += all_greater_or_equal(
-        data, "three_winding_transformer", "uk_12_max", "pk_12_max/sn_1", data["three_winding_transformer"]["uk_12"]
-    )
-    errors += all_greater_or_equal(
-        data, "three_winding_transformer", "uk_12_max", "pk_12_max/sn_2", data["three_winding_transformer"]["uk_12"]
-    )
-    errors += all_greater_or_equal(
-        data, "three_winding_transformer", "uk_13_max", "pk_13_max/sn_1", data["three_winding_transformer"]["uk_13"]
-    )
-    errors += all_greater_or_equal(
-        data, "three_winding_transformer", "uk_13_max", "pk_13_max/sn_3", data["three_winding_transformer"]["uk_13"]
-    )
-    errors += all_greater_or_equal(
-        data, "three_winding_transformer", "uk_23_max", "pk_23_max/sn_2", data["three_winding_transformer"]["uk_23"]
-    )
-    errors += all_greater_or_equal(
-        data, "three_winding_transformer", "uk_23_max", "pk_23_max/sn_3", data["three_winding_transformer"]["uk_23"]
-    )
-    errors += all_between(
-        data, "three_winding_transformer", "uk_12_max", 0, 1, data["three_winding_transformer"]["uk_12"]
-    )
-    errors += all_between(
-        data, "three_winding_transformer", "uk_13_max", 0, 1, data["three_winding_transformer"]["uk_13"]
-    )
-    errors += all_between(
-        data, "three_winding_transformer", "uk_23_max", 0, 1, data["three_winding_transformer"]["uk_23"]
-    )
-    errors += all_greater_than_or_equal_to_zero(
-        data, "three_winding_transformer", "pk_12_min", data["three_winding_transformer"]["pk_12"]
-    )
-    errors += all_greater_than_or_equal_to_zero(
-        data, "three_winding_transformer", "pk_13_min", data["three_winding_transformer"]["pk_13"]
-    )
-    errors += all_greater_than_or_equal_to_zero(
-        data, "three_winding_transformer", "pk_23_min", data["three_winding_transformer"]["pk_23"]
-    )
-    errors += all_greater_than_or_equal_to_zero(
-        data, "three_winding_transformer", "pk_12_max", data["three_winding_transformer"]["pk_12"]
-    )
-    errors += all_greater_than_or_equal_to_zero(
-        data, "three_winding_transformer", "pk_13_max", data["three_winding_transformer"]["pk_13"]
-    )
-    errors += all_greater_than_or_equal_to_zero(
-        data, "three_winding_transformer", "pk_23_max", data["three_winding_transformer"]["pk_23"]
-    )
-    return errors
-
-
-def validate_appliance(data: SingleDataset, component: str) -> List[ValidationError]:
-    errors = validate_base(data, component)
-    errors += all_boolean(data, component, "status")
-    errors += all_valid_ids(data, component, "node", "node")
-    return errors
-
-
-def validate_source(data: SingleDataset) -> List[ValidationError]:
-    errors = validate_appliance(data, "source")
-    errors += all_greater_than_zero(data, "source", "u_ref")
-    errors += all_greater_than_zero(data, "source", "sk")
-    errors += all_greater_than_or_equal_to_zero(data, "source", "rx_ratio")
-    errors += all_greater_than_zero(data, "source", "z01_ratio")
-    return errors
-
-
-def validate_generic_load_gen(data: SingleDataset, component: str) -> List[ValidationError]:
-    errors = validate_appliance(data, component)
-    errors += all_valid_enum_values(data, component, "type", LoadGenType)
-    return errors
-
-
-def validate_shunt(data: SingleDataset) -> List[ValidationError]:
-    errors = validate_appliance(data, "shunt")
-    return errors
-
-
-def validate_generic_voltage_sensor(data: SingleDataset, component: str) -> List[ValidationError]:
-    errors = validate_base(data, component)
-    errors += all_greater_than_zero(data, component, "u_sigma")
-    errors += all_greater_than_zero(data, component, "u_measured")
-    errors += all_valid_ids(data, component, "measured_object", "node")
-    return errors
-
-
-def validate_generic_power_sensor(data: SingleDataset, component: str) -> List[ValidationError]:
-    errors = validate_base(data, component)
-    errors += all_greater_than_zero(data, component, "power_sigma")
-    errors += all_valid_enum_values(data, component, "measured_terminal_type", MeasuredTerminalType)
-    errors += all_valid_ids(
-        data,
-        component,
-        field="measured_object",
-        ref_components=[
-            "node",
-            "line",
-            "transformer",
-            "three_winding_transformer",
-            "source",
-            "shunt",
-            "sym_load",
-            "asym_load",
-            "sym_gen",
-            "asym_gen",
-        ],
-    )
-    errors += all_valid_ids(
-        data,
-        component,
-        field="measured_object",
-        ref_components=["line", "transformer"],
-        measured_terminal_type=MeasuredTerminalType.branch_from,
-    )
-    errors += all_valid_ids(
-        data,
-        component,
-        field="measured_object",
-        ref_components=["line", "transformer"],
-        measured_terminal_type=MeasuredTerminalType.branch_to,
-    )
-    errors += all_valid_ids(
-        data,
-        component,
-        field="measured_object",
-        ref_components="source",
-        measured_terminal_type=MeasuredTerminalType.source,
-    )
-    errors += all_valid_ids(
-        data,
-        component,
-        field="measured_object",
-        ref_components="shunt",
-        measured_terminal_type=MeasuredTerminalType.shunt,
-    )
-    errors += all_valid_ids(
-        data,
-        component,
-        field="measured_object",
-        ref_components=["sym_load", "asym_load"],
-        measured_terminal_type=MeasuredTerminalType.load,
-    )
-    errors += all_valid_ids(
-        data,
-        component,
-        field="measured_object",
-        ref_components=["sym_gen", "asym_gen"],
-        measured_terminal_type=MeasuredTerminalType.generator,
-    )
-    errors += all_valid_ids(
-        data,
-        component,
-        field="measured_object",
-        ref_components="three_winding_transformer",
-        measured_terminal_type=MeasuredTerminalType.branch3_1,
-    )
-    errors += all_valid_ids(
-        data,
-        component,
-        field="measured_object",
-        ref_components="three_winding_transformer",
-        measured_terminal_type=MeasuredTerminalType.branch3_2,
-    )
-    errors += all_valid_ids(
-        data,
-        component,
-        field="measured_object",
-        ref_components="three_winding_transformer",
-        measured_terminal_type=MeasuredTerminalType.branch3_3,
-    )
-    errors += all_valid_ids(
-        data,
-        component,
-        field="measured_object",
-        ref_components="node",
-        measured_terminal_type=MeasuredTerminalType.node,
-    )
-
-    return errors
+# SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
+#
+# SPDX-License-Identifier: MPL-2.0
+
+"""
+Power Grid Model Validation Functions.
+
+Although all functions are 'public', you probably only need validate_input_data() and validate_batch_data().
+
+"""
+import copy
+from itertools import chain
+from typing import Dict, List, Optional
+
+import numpy as np
+
+from power_grid_model import power_grid_meta_data
+from power_grid_model.data_types import BatchDataset, Dataset, SingleDataset
+from power_grid_model.enum import (
+    Branch3Side,
+    BranchSide,
+    CalculationType,
+    LoadGenType,
+    MeasuredTerminalType,
+    WindingType,
+)
+from power_grid_model.utils import convert_batch_dataset_to_batch_list
+from power_grid_model.validation.errors import (
+    IdNotInDatasetError,
+    MissingValueError,
+    MultiComponentNotUniqueError,
+    ValidationError,
+)
+from power_grid_model.validation.rules import (
+    all_between,
+    all_between_or_at,
+    all_boolean,
+    all_cross_unique,
+    all_finite,
+    all_greater_or_equal,
+    all_greater_than_or_equal_to_zero,
+    all_greater_than_zero,
+    all_ids_exist_in_data_set,
+    all_less_than,
+    all_not_two_values_equal,
+    all_not_two_values_zero,
+    all_unique,
+    all_valid_clocks,
+    all_valid_enum_values,
+    all_valid_ids,
+    none_missing,
+)
+from power_grid_model.validation.utils import update_input_data
+
+
+def validate_input_data(
+    input_data: SingleDataset, calculation_type: Optional[CalculationType] = None, symmetric: bool = True
+) -> Optional[List[ValidationError]]:
+    """
+    Validates the entire input dataset:
+
+        1. Is the data structure correct? (checking data types and numpy array shapes)
+        2. Are all required values provided? (checking NaNs)
+        3. Are all ID's unique? (checking object identifiers across all components)
+        4. Are the supplied values valid? (checking limits and other logic as described in "Graph Data Model")
+
+    Args:
+        input_data: A power-grid-model input dataset
+        calculation_type: Supply a calculation method, to allow missing values for unused fields
+        symmetric: A boolean to state whether input data will be used for a symmetric or asymmetric calculation
+
+    Raises:
+        KeyError, TypeError or ValueError if the data structure is invalid.
+
+    Returns:
+        None if the data is valid, or a list containing all validation errors.
+    """
+    # A deep copy is made of the input data, since default values will be added in the validation process
+    input_data_copy = copy.deepcopy(input_data)
+    assert_valid_data_structure(input_data_copy, "input")
+
+    errors: List[ValidationError] = []
+    errors += validate_required_values(input_data_copy, calculation_type, symmetric)
+    errors += validate_unique_ids_across_components(input_data_copy)
+    errors += validate_values(input_data_copy)
+    return errors if errors else None
+
+
+def validate_batch_data(
+    input_data: SingleDataset,
+    update_data: BatchDataset,
+    calculation_type: Optional[CalculationType] = None,
+    symmetric: bool = True,
+) -> Optional[Dict[int, List[ValidationError]]]:
+    """
+    Ihe input dataset is validated:
+
+        1. Is the data structure correct? (checking data types and numpy array shapes)
+        2. Are all input data ID's unique? (checking object identifiers across all components)
+
+    For each batch the update data is validated:
+        3. Is the update data structure correct? (checking data types and numpy array shapes)
+        4. Are all update ID's valid? (checking object identifiers across update and input data)
+
+    Then (for each batch independently) the input dataset is updated with the batch's update data and validated:
+        5. Are all required values provided? (checking NaNs)
+        6. Are the supplied values valid? (checking limits and other logic as described in "Graph Data Model")
+
+    Args:
+        input_data: a power-grid-model input dataset
+        update_data: a power-grid-model update dataset (one or more batches)
+        calculation_type: Supply a calculation method, to allow missing values for unused fields
+        symmetric: A boolean to state whether input data will be used for a symmetric or asymmetric calculation
+
+    Raises:
+        KeyError, TypeError or ValueError if the data structure is invalid.
+
+    Returns:
+        None if the data is valid, or a dictionary containing all validation errors,
+        where the key is the batch number (0-indexed).
+    """
+    assert_valid_data_structure(input_data, "input")
+
+    input_errors: List[ValidationError] = list(validate_unique_ids_across_components(input_data))
+
+    # Splitting update_data_into_batches may raise TypeErrors and ValueErrors
+    batch_data = convert_batch_dataset_to_batch_list(update_data)
+
+    errors = {}
+    for batch, batch_update_data in enumerate(batch_data):
+        assert_valid_data_structure(batch_update_data, "update")
+        id_errors: List[ValidationError] = list(validate_ids_exist(batch_update_data, input_data))
+
+        batch_errors = input_errors + id_errors
+        if not id_errors:
+            merged_data = update_input_data(input_data, batch_update_data)
+            batch_errors += validate_required_values(merged_data, calculation_type, symmetric)
+            batch_errors += validate_values(merged_data)
+
+        if batch_errors:
+            errors[batch] = batch_errors
+
+    return errors if errors else None
+
+
+def assert_valid_data_structure(data: Dataset, data_type: str) -> None:
+    """
+    Checks if all component names are valid and if the data inside the component matches the required Numpy
+    structured array as defined in the Power Grid Model meta data.
+
+    Args:
+        data: a power-grid-model input/update dataset
+        data_type: 'input' or 'update'
+
+    Raises: KeyError, TypeError
+
+    """
+    if data_type not in {"input", "update"}:
+        raise KeyError(f"Unexpected data type '{data_type}' (should be 'input' or 'update')")
+
+    component_dtype = {component: meta.dtype for component, meta in power_grid_meta_data[data_type].items()}
+    for component, array in data.items():
+        # Check if component name is valid
+        if component not in component_dtype:
+            raise KeyError(f"Unknown component '{component}' in {data_type}_data.")
+
+        # Check if component definition is as expected
+        dtype = component_dtype[component]
+        if isinstance(array, np.ndarray):
+            if array.dtype != dtype:
+                if not hasattr(array.dtype, "names") or not array.dtype.names:
+                    raise TypeError(
+                        f"Unexpected Numpy array ({array.dtype}) for '{component}' {data_type}_data "
+                        "(should be a Numpy structured array)."
+                    )
+                raise TypeError(f"Unexpected Numpy structured array; (expected = {dtype}, actual = {array.dtype}).")
+        else:
+            raise TypeError(
+                f"Unexpected data type {type(array).__name__} for '{component}' {data_type}_data "
+                "(should be a Numpy structured array)."
+            )
+
+
+def validate_unique_ids_across_components(data: SingleDataset) -> List[MultiComponentNotUniqueError]:
+    """
+    Checks if all ids in the input dataset are unique
+
+    Args:
+        data: a power-grid-model input dataset
+
+    Returns: an empty list if all ids are unique, or a list of MultiComponentNotUniqueErrors for all components that
+             have non-unique ids
+
+    """
+    return all_cross_unique(data, [(component, "id") for component in data])
+
+
+def validate_ids_exist(update_data: Dict[str, np.ndarray], input_data: SingleDataset) -> List[IdNotInDatasetError]:
+    """
+    Checks if all ids of the components in the update data exist in the input data. This needs to be true, because you
+    can only update existing components.
+
+    This function should be called for every update dataset in a batch set
+
+    Args:
+        update_data: a single update dataset
+        input_data: a power-grid-model input dataset
+
+    Returns: an empty list if all update data ids exist in the input dataset, or a list of IdNotInDatasetErrors for
+             all update components of which the id does not exist in the input dataset
+
+    """
+    errors = (all_ids_exist_in_data_set(update_data, input_data, component, "input_data") for component in update_data)
+    return list(chain(*errors))
+
+
+def validate_required_values(
+    data: SingleDataset, calculation_type: Optional[CalculationType] = None, symmetric: bool = True
+) -> List[MissingValueError]:
+    """
+    Checks if all required data is available.
+
+    Args:
+        data: a power-grid-model input dataset
+        calculation_type: Supply a calculation method, to allow missing values for unused fields
+        symmetric: A boolean to state whether input data will be used for a symmetric or asymmetric calculation
+
+    Returns: an empty list if all required data is available, or a list of MissingValueErrors.
+
+    """
+    # Base
+    required = {"base": ["id"]}
+
+    # Nodes
+    required["node"] = required["base"] + ["u_rated"]
+
+    # Branches
+    required["branch"] = required["base"] + ["from_node", "to_node", "from_status", "to_status"]
+    required["link"] = required["branch"].copy()
+    required["line"] = required["branch"] + ["r1", "x1", "c1", "tan1", "i_n"]
+    required["transformer"] = required["branch"] + [
+        "u1",
+        "u2",
+        "sn",
+        "uk",
+        "pk",
+        "i0",
+        "p0",
+        "winding_from",
+        "winding_to",
+        "clock",
+        "tap_side",
+        "tap_pos",
+        "tap_min",
+        "tap_max",
+        "tap_size",
+    ]
+    # Branch3
+    required["branch3"] = required["base"] + ["node_1", "node_2", "node_3", "status_1", "status_2", "status_3"]
+    required["three_winding_transformer"] = required["branch3"] + [
+        "u1",
+        "u2",
+        "u3",
+        "sn_1",
+        "sn_2",
+        "sn_3",
+        "uk_12",
+        "uk_13",
+        "uk_23",
+        "pk_12",
+        "pk_13",
+        "pk_23",
+        "i0",
+        "p0",
+        "winding_1",
+        "winding_2",
+        "winding_3",
+        "clock_12",
+        "clock_13",
+        "tap_side",
+        "tap_pos",
+        "tap_min",
+        "tap_max",
+        "tap_size",
+    ]
+    # Appliances
+    required["appliance"] = required["base"] + ["node", "status"]
+    required["source"] = required["appliance"].copy()
+    if calculation_type is None or calculation_type == CalculationType.power_flow:
+        required["source"] += ["u_ref"]
+    required["shunt"] = required["appliance"] + ["g1", "b1"]
+    required["generic_load_gen"] = required["appliance"] + ["type"]
+    if calculation_type is None or calculation_type == CalculationType.power_flow:
+        required["generic_load_gen"] += ["p_specified", "q_specified"]
+    required["sym_load"] = required["generic_load_gen"].copy()
+    required["asym_load"] = required["generic_load_gen"].copy()
+    required["sym_gen"] = required["generic_load_gen"].copy()
+    required["asym_gen"] = required["generic_load_gen"].copy()
+
+    # Sensors
+    required["sensor"] = required["base"] + ["measured_object"]
+    required["voltage_sensor"] = required["sensor"].copy()
+    required["power_sensor"] = required["sensor"] + ["measured_terminal_type"]
+    if calculation_type is None or calculation_type == CalculationType.state_estimation:
+        required["voltage_sensor"] += ["u_sigma", "u_measured"]
+        required["power_sensor"] += ["power_sigma", "p_measured", "q_measured"]
+    required["sym_voltage_sensor"] = required["voltage_sensor"].copy()
+    required["asym_voltage_sensor"] = required["voltage_sensor"].copy()
+    required["sym_power_sensor"] = required["power_sensor"].copy()
+    required["asym_power_sensor"] = required["power_sensor"].copy()
+
+    if not symmetric:
+        required["line"] += ["r0", "x0", "c0", "tan0"]
+        required["shunt"] += ["g0", "b0"]
+
+    return list(chain(*(none_missing(data, component, required.get(component, [])) for component in data)))
+
+
+def validate_values(data: SingleDataset) -> List[ValidationError]:  # pylint: disable=too-many-branches
+    """
+    For each component supplied in the data, call the appropriate validation function
+
+    Args:
+        data: a power-grid-model input dataset
+
+    Returns: an empty list if all required data is valid, or a list of ValidationErrors.
+
+    """
+    errors: List[ValidationError] = list(all_finite(data))
+    if "node" in data:
+        errors += validate_node(data)
+    if "line" in data:
+        errors += validate_line(data)
+    if "link" in data:
+        errors += validate_branch(data, "link")
+    if "transformer" in data:
+        errors += validate_transformer(data)
+    if "three_winding_transformer" in data:
+        errors += validate_three_winding_transformer(data)
+    if "source" in data:
+        errors += validate_source(data)
+    if "sym_load" in data:
+        errors += validate_generic_load_gen(data, "sym_load")
+    if "sym_gen" in data:
+        errors += validate_generic_load_gen(data, "sym_gen")
+    if "asym_load" in data:
+        errors += validate_generic_load_gen(data, "asym_load")
+    if "asym_gen" in data:
+        errors += validate_generic_load_gen(data, "asym_gen")
+    if "shunt" in data:
+        errors += validate_shunt(data)
+    if "sym_voltage_sensor" in data:
+        errors += validate_generic_voltage_sensor(data, "sym_voltage_sensor")
+    if "asym_voltage_sensor" in data:
+        errors += validate_generic_voltage_sensor(data, "asym_voltage_sensor")
+    if "sym_power_sensor" in data:
+        errors += validate_generic_power_sensor(data, "sym_power_sensor")
+    if "asym_power_sensor" in data:
+        errors += validate_generic_power_sensor(data, "asym_power_sensor")
+    return errors
+
+
+# pylint: disable=missing-function-docstring
+
+
+def validate_base(data: SingleDataset, component: str) -> List[ValidationError]:
+    errors: List[ValidationError] = list(all_unique(data, component, "id"))
+    return errors
+
+
+def validate_node(data: SingleDataset) -> List[ValidationError]:
+    errors = validate_base(data, "node")
+    errors += all_greater_than_zero(data, "node", "u_rated")
+    return errors
+
+
+def validate_branch(data: SingleDataset, component: str) -> List[ValidationError]:
+    errors = validate_base(data, component)
+    errors += all_valid_ids(data, component, "from_node", "node")
+    errors += all_valid_ids(data, component, "to_node", "node")
+    errors += all_not_two_values_equal(data, component, "to_node", "from_node")
+    errors += all_boolean(data, component, "from_status")
+    errors += all_boolean(data, component, "to_status")
+    return errors
+
+
+def validate_line(data: SingleDataset) -> List[ValidationError]:
+    errors = validate_branch(data, "line")
+    errors += all_not_two_values_zero(data, "line", "r1", "x1")
+    errors += all_not_two_values_zero(data, "line", "r0", "x0")
+    errors += all_greater_than_zero(data, "line", "i_n")
+    return errors
+
+
+def validate_transformer(data: SingleDataset) -> List[ValidationError]:
+    errors = validate_branch(data, "transformer")
+    errors += all_greater_than_zero(data, "transformer", "u1")
+    errors += all_greater_than_zero(data, "transformer", "u2")
+    errors += all_greater_than_zero(data, "transformer", "sn")
+    errors += all_greater_or_equal(data, "transformer", "uk", "pk/sn")
+    errors += all_between(data, "transformer", "uk", 0, 1)
+    errors += all_greater_than_or_equal_to_zero(data, "transformer", "pk")
+    errors += all_greater_or_equal(data, "transformer", "i0", "p0/sn")
+    errors += all_less_than(data, "transformer", "i0", 1)
+    errors += all_greater_than_or_equal_to_zero(data, "transformer", "p0")
+    errors += all_valid_enum_values(data, "transformer", "winding_from", WindingType)
+    errors += all_valid_enum_values(data, "transformer", "winding_to", WindingType)
+    errors += all_between_or_at(data, "transformer", "clock", 0, 12)
+    errors += all_valid_clocks(data, "transformer", "clock", "winding_from", "winding_to")
+    errors += all_valid_enum_values(data, "transformer", "tap_side", BranchSide)
+    errors += all_between_or_at(data, "transformer", "tap_pos", "tap_min", "tap_max")
+    errors += all_between_or_at(data, "transformer", "tap_nom", "tap_min", "tap_max", 0)
+    errors += all_greater_than_or_equal_to_zero(data, "transformer", "tap_size")
+    errors += all_greater_or_equal(data, "transformer", "uk_min", "pk_min/sn", data["transformer"]["uk"])
+    errors += all_between(data, "transformer", "uk_min", 0, 1, data["transformer"]["uk"])
+    errors += all_greater_or_equal(data, "transformer", "uk_max", "pk_max/sn", data["transformer"]["uk"])
+    errors += all_between(data, "transformer", "uk_max", 0, 1, data["transformer"]["uk"])
+    errors += all_greater_than_or_equal_to_zero(data, "transformer", "pk_min", data["transformer"]["pk"])
+    errors += all_greater_than_or_equal_to_zero(data, "transformer", "pk_max", data["transformer"]["pk"])
+    return errors
+
+
+def validate_branch3(data: SingleDataset, component: str) -> List[ValidationError]:
+    errors = validate_base(data, component)
+    errors += all_valid_ids(data, component, "node_1", "node")
+    errors += all_valid_ids(data, component, "node_2", "node")
+    errors += all_valid_ids(data, component, "node_3", "node")
+    errors += all_not_two_values_equal(data, component, "node_1", "node_2")
+    errors += all_not_two_values_equal(data, component, "node_1", "node_3")
+    errors += all_not_two_values_equal(data, component, "node_2", "node_3")
+    errors += all_boolean(data, component, "status_1")
+    errors += all_boolean(data, component, "status_2")
+    errors += all_boolean(data, component, "status_3")
+    return errors
+
+
+# pylint: disable=R0915
+def validate_three_winding_transformer(data: SingleDataset) -> List[ValidationError]:
+    errors = validate_branch3(data, "three_winding_transformer")
+    errors += all_greater_than_zero(data, "three_winding_transformer", "u1")
+    errors += all_greater_than_zero(data, "three_winding_transformer", "u2")
+    errors += all_greater_than_zero(data, "three_winding_transformer", "u3")
+    errors += all_greater_than_zero(data, "three_winding_transformer", "sn_1")
+    errors += all_greater_than_zero(data, "three_winding_transformer", "sn_2")
+    errors += all_greater_than_zero(data, "three_winding_transformer", "sn_3")
+    errors += all_greater_or_equal(data, "three_winding_transformer", "uk_12", "pk_12/sn_1")
+    errors += all_greater_or_equal(data, "three_winding_transformer", "uk_12", "pk_12/sn_2")
+    errors += all_greater_or_equal(data, "three_winding_transformer", "uk_13", "pk_13/sn_1")
+    errors += all_greater_or_equal(data, "three_winding_transformer", "uk_13", "pk_13/sn_3")
+    errors += all_greater_or_equal(data, "three_winding_transformer", "uk_23", "pk_23/sn_2")
+    errors += all_greater_or_equal(data, "three_winding_transformer", "uk_23", "pk_23/sn_3")
+    errors += all_between(data, "three_winding_transformer", "uk_12", 0, 1)
+    errors += all_between(data, "three_winding_transformer", "uk_13", 0, 1)
+    errors += all_between(data, "three_winding_transformer", "uk_23", 0, 1)
+    errors += all_greater_than_or_equal_to_zero(data, "three_winding_transformer", "pk_12")
+    errors += all_greater_than_or_equal_to_zero(data, "three_winding_transformer", "pk_13")
+    errors += all_greater_than_or_equal_to_zero(data, "three_winding_transformer", "pk_23")
+    errors += all_greater_or_equal(data, "three_winding_transformer", "i0", "p0/sn_1")
+    errors += all_less_than(data, "three_winding_transformer", "i0", 1)
+    errors += all_greater_than_or_equal_to_zero(data, "three_winding_transformer", "p0")
+    errors += all_valid_enum_values(data, "three_winding_transformer", "winding_1", WindingType)
+    errors += all_valid_enum_values(data, "three_winding_transformer", "winding_2", WindingType)
+    errors += all_valid_enum_values(data, "three_winding_transformer", "winding_3", WindingType)
+    errors += all_between_or_at(data, "three_winding_transformer", "clock_12", 0, 12)
+    errors += all_between_or_at(data, "three_winding_transformer", "clock_13", 0, 12)
+    errors += all_valid_clocks(data, "three_winding_transformer", "clock_12", "winding_1", "winding_2")
+    errors += all_valid_clocks(data, "three_winding_transformer", "clock_13", "winding_1", "winding_3")
+    errors += all_valid_enum_values(data, "three_winding_transformer", "tap_side", Branch3Side)
+    errors += all_between_or_at(data, "three_winding_transformer", "tap_pos", "tap_min", "tap_max")
+    errors += all_between_or_at(data, "three_winding_transformer", "tap_nom", "tap_min", "tap_max", 0)
+    errors += all_greater_than_or_equal_to_zero(data, "three_winding_transformer", "tap_size")
+    errors += all_greater_or_equal(
+        data, "three_winding_transformer", "uk_12_min", "pk_12_min/sn_1", data["three_winding_transformer"]["uk_12"]
+    )
+    errors += all_greater_or_equal(
+        data, "three_winding_transformer", "uk_12_min", "pk_12_min/sn_2", data["three_winding_transformer"]["uk_12"]
+    )
+    errors += all_greater_or_equal(
+        data, "three_winding_transformer", "uk_13_min", "pk_13_min/sn_1", data["three_winding_transformer"]["uk_13"]
+    )
+    errors += all_greater_or_equal(
+        data, "three_winding_transformer", "uk_13_min", "pk_13_min/sn_3", data["three_winding_transformer"]["uk_13"]
+    )
+    errors += all_greater_or_equal(
+        data, "three_winding_transformer", "uk_23_min", "pk_23_min/sn_2", data["three_winding_transformer"]["uk_23"]
+    )
+    errors += all_greater_or_equal(
+        data, "three_winding_transformer", "uk_23_min", "pk_23_min/sn_3", data["three_winding_transformer"]["uk_23"]
+    )
+    errors += all_between(
+        data, "three_winding_transformer", "uk_12_min", 0, 1, data["three_winding_transformer"]["uk_12"]
+    )
+    errors += all_between(
+        data, "three_winding_transformer", "uk_13_min", 0, 1, data["three_winding_transformer"]["uk_13"]
+    )
+    errors += all_between(
+        data, "three_winding_transformer", "uk_23_min", 0, 1, data["three_winding_transformer"]["uk_23"]
+    )
+    errors += all_greater_or_equal(
+        data, "three_winding_transformer", "uk_12_max", "pk_12_max/sn_1", data["three_winding_transformer"]["uk_12"]
+    )
+    errors += all_greater_or_equal(
+        data, "three_winding_transformer", "uk_12_max", "pk_12_max/sn_2", data["three_winding_transformer"]["uk_12"]
+    )
+    errors += all_greater_or_equal(
+        data, "three_winding_transformer", "uk_13_max", "pk_13_max/sn_1", data["three_winding_transformer"]["uk_13"]
+    )
+    errors += all_greater_or_equal(
+        data, "three_winding_transformer", "uk_13_max", "pk_13_max/sn_3", data["three_winding_transformer"]["uk_13"]
+    )
+    errors += all_greater_or_equal(
+        data, "three_winding_transformer", "uk_23_max", "pk_23_max/sn_2", data["three_winding_transformer"]["uk_23"]
+    )
+    errors += all_greater_or_equal(
+        data, "three_winding_transformer", "uk_23_max", "pk_23_max/sn_3", data["three_winding_transformer"]["uk_23"]
+    )
+    errors += all_between(
+        data, "three_winding_transformer", "uk_12_max", 0, 1, data["three_winding_transformer"]["uk_12"]
+    )
+    errors += all_between(
+        data, "three_winding_transformer", "uk_13_max", 0, 1, data["three_winding_transformer"]["uk_13"]
+    )
+    errors += all_between(
+        data, "three_winding_transformer", "uk_23_max", 0, 1, data["three_winding_transformer"]["uk_23"]
+    )
+    errors += all_greater_than_or_equal_to_zero(
+        data, "three_winding_transformer", "pk_12_min", data["three_winding_transformer"]["pk_12"]
+    )
+    errors += all_greater_than_or_equal_to_zero(
+        data, "three_winding_transformer", "pk_13_min", data["three_winding_transformer"]["pk_13"]
+    )
+    errors += all_greater_than_or_equal_to_zero(
+        data, "three_winding_transformer", "pk_23_min", data["three_winding_transformer"]["pk_23"]
+    )
+    errors += all_greater_than_or_equal_to_zero(
+        data, "three_winding_transformer", "pk_12_max", data["three_winding_transformer"]["pk_12"]
+    )
+    errors += all_greater_than_or_equal_to_zero(
+        data, "three_winding_transformer", "pk_13_max", data["three_winding_transformer"]["pk_13"]
+    )
+    errors += all_greater_than_or_equal_to_zero(
+        data, "three_winding_transformer", "pk_23_max", data["three_winding_transformer"]["pk_23"]
+    )
+    return errors
+
+
+def validate_appliance(data: SingleDataset, component: str) -> List[ValidationError]:
+    errors = validate_base(data, component)
+    errors += all_boolean(data, component, "status")
+    errors += all_valid_ids(data, component, "node", "node")
+    return errors
+
+
+def validate_source(data: SingleDataset) -> List[ValidationError]:
+    errors = validate_appliance(data, "source")
+    errors += all_greater_than_zero(data, "source", "u_ref")
+    errors += all_greater_than_zero(data, "source", "sk")
+    errors += all_greater_than_or_equal_to_zero(data, "source", "rx_ratio")
+    errors += all_greater_than_zero(data, "source", "z01_ratio")
+    return errors
+
+
+def validate_generic_load_gen(data: SingleDataset, component: str) -> List[ValidationError]:
+    errors = validate_appliance(data, component)
+    errors += all_valid_enum_values(data, component, "type", LoadGenType)
+    return errors
+
+
+def validate_shunt(data: SingleDataset) -> List[ValidationError]:
+    errors = validate_appliance(data, "shunt")
+    return errors
+
+
+def validate_generic_voltage_sensor(data: SingleDataset, component: str) -> List[ValidationError]:
+    errors = validate_base(data, component)
+    errors += all_greater_than_zero(data, component, "u_sigma")
+    errors += all_greater_than_zero(data, component, "u_measured")
+    errors += all_valid_ids(data, component, "measured_object", "node")
+    return errors
+
+
+def validate_generic_power_sensor(data: SingleDataset, component: str) -> List[ValidationError]:
+    errors = validate_base(data, component)
+    errors += all_greater_than_zero(data, component, "power_sigma")
+    errors += all_valid_enum_values(data, component, "measured_terminal_type", MeasuredTerminalType)
+    errors += all_valid_ids(
+        data,
+        component,
+        field="measured_object",
+        ref_components=[
+            "node",
+            "line",
+            "transformer",
+            "three_winding_transformer",
+            "source",
+            "shunt",
+            "sym_load",
+            "asym_load",
+            "sym_gen",
+            "asym_gen",
+        ],
+    )
+    errors += all_valid_ids(
+        data,
+        component,
+        field="measured_object",
+        ref_components=["line", "transformer"],
+        measured_terminal_type=MeasuredTerminalType.branch_from,
+    )
+    errors += all_valid_ids(
+        data,
+        component,
+        field="measured_object",
+        ref_components=["line", "transformer"],
+        measured_terminal_type=MeasuredTerminalType.branch_to,
+    )
+    errors += all_valid_ids(
+        data,
+        component,
+        field="measured_object",
+        ref_components="source",
+        measured_terminal_type=MeasuredTerminalType.source,
+    )
+    errors += all_valid_ids(
+        data,
+        component,
+        field="measured_object",
+        ref_components="shunt",
+        measured_terminal_type=MeasuredTerminalType.shunt,
+    )
+    errors += all_valid_ids(
+        data,
+        component,
+        field="measured_object",
+        ref_components=["sym_load", "asym_load"],
+        measured_terminal_type=MeasuredTerminalType.load,
+    )
+    errors += all_valid_ids(
+        data,
+        component,
+        field="measured_object",
+        ref_components=["sym_gen", "asym_gen"],
+        measured_terminal_type=MeasuredTerminalType.generator,
+    )
+    errors += all_valid_ids(
+        data,
+        component,
+        field="measured_object",
+        ref_components="three_winding_transformer",
+        measured_terminal_type=MeasuredTerminalType.branch3_1,
+    )
+    errors += all_valid_ids(
+        data,
+        component,
+        field="measured_object",
+        ref_components="three_winding_transformer",
+        measured_terminal_type=MeasuredTerminalType.branch3_2,
+    )
+    errors += all_valid_ids(
+        data,
+        component,
+        field="measured_object",
+        ref_components="three_winding_transformer",
+        measured_terminal_type=MeasuredTerminalType.branch3_3,
+    )
+    errors += all_valid_ids(
+        data,
+        component,
+        field="measured_object",
+        ref_components="node",
+        measured_terminal_type=MeasuredTerminalType.node,
+    )
+
+    return errors
```

## Comparing `power_grid_model-1.5.0rc9166510100968.dist-info/LICENSE` & `power_grid_model-1.5.0rc9171103440397.dist-info/LICENSE`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,292 +1,292 @@
-Mozilla Public License Version 2.0
-
-1. Definitions
-
-1.1. "Contributor" means each individual or legal entity that creates, contributes
-to the creation of, or owns Covered Software.
-
-1.2. "Contributor Version" means the combination of the Contributions of others
-(if any) used by a Contributor and that particular Contributor's Contribution.
-
-     1.3. "Contribution" means Covered Software of a particular Contributor.
-
-1.4. "Covered Software" means Source Code Form to which the initial Contributor
-has attached the notice in Exhibit A, the Executable Form of such Source Code
-Form, and Modifications of such Source Code Form, in each case including portions
-thereof.
-
-     1.5. "Incompatible With Secondary Licenses" means
-
-(a) that the initial Contributor has attached the notice described in Exhibit
-B to the Covered Software; or
-
-(b) that the Covered Software was made available under the terms of version
-1.1 or earlier of the License, but not also under the terms of a Secondary
-License.
-
-1.6. "Executable Form" means any form of the work other than Source Code Form.
-
-1.7. "Larger Work" means a work that combines Covered Software with other
-material, in a separate file or files, that is not Covered Software.
-
-     1.8. "License" means this document.
-
-1.9. "Licensable" means having the right to grant, to the maximum extent possible,
-whether at the time of the initial grant or subsequently, any and all of the
-rights conveyed by this License.
-
-     1.10. "Modifications" means any of the following:
-
-(a) any file in Source Code Form that results from an addition to, deletion
-from, or modification of the contents of Covered Software; or
-
-(b) any new file in Source Code Form that contains any Covered Software.
-
-1.11. "Patent Claims" of a Contributor means any patent claim(s), including
-without limitation, method, process, and apparatus claims, in any patent Licensable
-by such Contributor that would be infringed, but for the grant of the License,
-by the making, using, selling, offering for sale, having made, import, or
-transfer of either its Contributions or its Contributor Version.
-
-1.12. "Secondary License" means either the GNU General Public License, Version
-2.0, the GNU Lesser General Public License, Version 2.1, the GNU Affero General
-Public License, Version 3.0, or any later versions of those licenses.
-
-1.13. "Source Code Form" means the form of the work preferred for making modifications.
-
-1.14. "You" (or "Your") means an individual or a legal entity exercising rights
-under this License. For legal entities, "You" includes any entity that controls,
-is controlled by, or is under common control with You. For purposes of this
-definition, "control" means (a) the power, direct or indirect, to cause the
-direction or management of such entity, whether by contract or otherwise,
-or (b) ownership of more than fifty percent (50%) of the outstanding shares
-or beneficial ownership of such entity.
-
-2. License Grants and Conditions
-
-     2.1. Grants
-Each Contributor hereby grants You a world-wide, royalty-free, non-exclusive
-license:
-
-(a) under intellectual property rights (other than patent or trademark) Licensable
-by such Contributor to use, reproduce, make available, modify, display, perform,
-distribute, and otherwise exploit its Contributions, either on an unmodified
-basis, with Modifications, or as part of a Larger Work; and
-
-(b) under Patent Claims of such Contributor to make, use, sell, offer for
-sale, have made, import, and otherwise transfer either its Contributions or
-its Contributor Version.
-
-     2.2. Effective Date
-The licenses granted in Section 2.1 with respect to any Contribution become
-effective for each Contribution on the date the Contributor first distributes
-such Contribution.
-
-     2.3. Limitations on Grant Scope
-The licenses granted in this Section 2 are the only rights granted under this
-License. No additional rights or licenses will be implied from the distribution
-or licensing of Covered Software under this License. Notwithstanding Section
-2.1(b) above, no patent license is granted by a Contributor:
-
-(a) for any code that a Contributor has removed from Covered Software; or
-
-(b) for infringements caused by: (i) Your and any other third party's modifications
-of Covered Software, or (ii) the combination of its Contributions with other
-software (except as part of its Contributor Version); or
-
-(c) under Patent Claims infringed by Covered Software in the absence of its
-Contributions.
-
-This License does not grant any rights in the trademarks, service marks, or
-logos of any Contributor (except as may be necessary to comply with the notice
-requirements in Section 3.4).
-
-     2.4. Subsequent Licenses
-No Contributor makes additional grants as a result of Your choice to distribute
-the Covered Software under a subsequent version of this License (see Section
-10.2) or under the terms of a Secondary License (if permitted under the terms
-of Section 3.3).
-
-     2.5. Representation
-Each Contributor represents that the Contributor believes its Contributions
-are its original creation(s) or it has sufficient rights to grant the rights
-to its Contributions conveyed by this License.
-
-     2.6. Fair Use
-This License is not intended to limit any rights You have under applicable
-copyright doctrines of fair use, fair dealing, or other equivalents.
-
-     2.7. Conditions
-Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted in
-Section 2.1.
-
-3. Responsibilities
-
-     3.1. Distribution of Source Form
-All distribution of Covered Software in Source Code Form, including any Modifications
-that You create or to which You contribute, must be under the terms of this
-License. You must inform recipients that the Source Code Form of the Covered
-Software is governed by the terms of this License, and how they can obtain
-a copy of this License. You may not attempt to alter or restrict the recipients'
-rights in the Source Code Form.
-
-     3.2. Distribution of Executable Form
-     If You distribute Covered Software in Executable Form then:
-
-(a) such Covered Software must also be made available in Source Code Form,
-as described in Section 3.1, and You must inform recipients of the Executable
-Form how they can obtain a copy of such Source Code Form by reasonable means
-in a timely manner, at a charge no more than the cost of distribution to the
-recipient; and
-
-(b) You may distribute such Executable Form under the terms of this License,
-or sublicense it under different terms, provided that the license for the
-Executable Form does not attempt to limit or alter the recipients' rights
-in the Source Code Form under this License.
-
-     3.3. Distribution of a Larger Work
-You may create and distribute a Larger Work under terms of Your choice, provided
-that You also comply with the requirements of this License for the Covered
-Software. If the Larger Work is a combination of Covered Software with a work
-governed by one or more Secondary Licenses, and the Covered Software is not
-Incompatible With Secondary Licenses, this License permits You to additionally
-distribute such Covered Software under the terms of such Secondary License(s),
-so that the recipient of the Larger Work may, at their option, further distribute
-the Covered Software under the terms of either this License or such Secondary
-License(s).
-
-     3.4. Notices
-You may not remove or alter the substance of any license notices (including
-copyright notices, patent notices, disclaimers of warranty, or limitations
-of liability) contained within the Source Code Form of the Covered Software,
-except that You may alter any license notices to the extent required to remedy
-known factual inaccuracies.
-
-     3.5. Application of Additional Terms
-You may choose to offer, and to charge a fee for, warranty, support, indemnity
-or liability obligations to one or more recipients of Covered Software. However,
-You may do so only on Your own behalf, and not on behalf of any Contributor.
-You must make it absolutely clear that any such warranty, support, indemnity,
-or liability obligation is offered by You alone, and You hereby agree to indemnify
-every Contributor for any liability incurred by such Contributor as a result
-of warranty, support, indemnity or liability terms You offer. You may include
-additional disclaimers of warranty and limitations of liability specific to
-any jurisdiction.
-
-4. Inability to Comply Due to Statute or Regulation
-If it is impossible for You to comply with any of the terms of this License
-with respect to some or all of the Covered Software due to statute, judicial
-order, or regulation then You must: (a) comply with the terms of this License
-to the maximum extent possible; and (b) describe the limitations and the code
-they affect. Such description must be placed in a text file included with
-all distributions of the Covered Software under this License. Except to the
-extent prohibited by statute or regulation, such description must be sufficiently
-detailed for a recipient of ordinary skill to be able to understand it.
-
-5. Termination
-
-5.1. The rights granted under this License will terminate automatically if
-You fail to comply with any of its terms. However, if You become compliant,
-then the rights granted under this License from a particular Contributor are
-reinstated (a) provisionally, unless and until such Contributor explicitly
-and finally terminates Your grants, and (b) on an ongoing basis, if such Contributor
-fails to notify You of the non-compliance by some reasonable means prior to
-60 days after You have come back into compliance. Moreover, Your grants from
-a particular Contributor are reinstated on an ongoing basis if such Contributor
-notifies You of the non-compliance by some reasonable means, this is the first
-time You have received notice of non-compliance with this License from such
-Contributor, and You become compliant prior to 30 days after Your receipt
-of the notice.
-
-5.2. If You initiate litigation against any entity by asserting a patent infringement
-claim (excluding declaratory judgment actions, counter-claims, and cross-claims)
-alleging that a Contributor Version directly or indirectly infringes any patent,
-then the rights granted to You by any and all Contributors for the Covered
-Software under Section 2.1 of this License shall terminate.
-
-5.3. In the event of termination under Sections 5.1 or 5.2 above, all end
-user license agreements (excluding distributors and resellers) which have
-been validly granted by You or Your distributors under this License prior
-to termination shall survive termination.
-
-6. Disclaimer of Warranty
-Covered Software is provided under this License on an "as is" basis, without
-warranty of any kind, either expressed, implied, or statutory, including,
-without limitation, warranties that the Covered Software is free of defects,
-merchantable, fit for a particular purpose or non-infringing. The entire risk
-as to the quality and performance of the Covered Software is with You. Should
-any Covered Software prove defective in any respect, You (not any Contributor)
-assume the cost of any necessary servicing, repair, or correction. This disclaimer
-of warranty constitutes an essential part of this License. No use of any Covered
-Software is authorized under this License except under this disclaimer.
-
-7. Limitation of Liability
-Under no circumstances and under no legal theory, whether tort (including
-negligence), contract, or otherwise, shall any Contributor, or anyone who
-distributes Covered Software as permitted above, be liable to You for any
-direct, indirect, special, incidental, or consequential damages of any character
-including, without limitation, damages for lost profits, loss of goodwill,
-work stoppage, computer failure or malfunction, or any and all other commercial
-damages or losses, even if such party shall have been informed of the possibility
-of such damages. This limitation of liability shall not apply to liability
-for death or personal injury resulting from such party's negligence to the
-extent applicable law prohibits such limitation. Some jurisdictions do not
-allow the exclusion or limitation of incidental or consequential damages,
-so this exclusion and limitation may not apply to You.
-
-8. Litigation
-Any litigation relating to this License may be brought only in the courts
-of a jurisdiction where the defendant maintains its principal place of business
-and such litigation shall be governed by laws of that jurisdiction, without
-reference to its conflict-of-law provisions. Nothing in this Section shall
-prevent a party's ability to bring cross-claims or counter-claims.
-
-9. Miscellaneous
-This License represents the complete agreement concerning the subject matter
-hereof. If any provision of this License is held to be unenforceable, such
-provision shall be reformed only to the extent necessary to make it enforceable.
-Any law or regulation which provides that the language of a contract shall
-be construed against the drafter shall not be used to construe this License
-against a Contributor.
-
-10. Versions of the License
-
-     10.1. New Versions
-Mozilla Foundation is the license steward. Except as provided in Section 10.3,
-no one other than the license steward has the right to modify or publish new
-versions of this License. Each version will be given a distinguishing version
-number.
-
-     10.2. Effect of New Versions
-You may distribute the Covered Software under the terms of the version of
-the License under which You originally received the Covered Software, or under
-the terms of any subsequent version published by the license steward.
-
-     10.3. Modified Versions
-If you create software not governed by this License, and you want to create
-a new license for such software, you may create and use a modified version
-of this License if you rename the license and remove any references to the
-name of the license steward (except to note that such modified license differs
-from this License).
-
-10.4. Distributing Source Code Form that is Incompatible With Secondary Licenses
-If You choose to distribute Source Code Form that is Incompatible With Secondary
-Licenses under the terms of this version of the License, the notice described
-in Exhibit B of this License must be attached.
-
-Exhibit A - Source Code Form License Notice
-
-This Source Code Form is subject to the terms of the Mozilla Public License,
-v. 2.0. If a copy of the MPL was not distributed with this file, You can obtain
-one at http://mozilla.org/MPL/2.0/.
-
-If it is not possible or desirable to put the notice in a particular file,
-then You may include the notice in a location (such as a LICENSE file in a
-relevant directory) where a recipient would be likely to look for such a notice.
-
-You may add additional accurate notices of copyright ownership.
-
-Exhibit B - "Incompatible With Secondary Licenses" Notice
-
-This Source Code Form is "Incompatible With Secondary Licenses", as defined
-by the Mozilla Public License, v. 2.0.
+Mozilla Public License Version 2.0
+
+1. Definitions
+
+1.1. "Contributor" means each individual or legal entity that creates, contributes
+to the creation of, or owns Covered Software.
+
+1.2. "Contributor Version" means the combination of the Contributions of others
+(if any) used by a Contributor and that particular Contributor's Contribution.
+
+     1.3. "Contribution" means Covered Software of a particular Contributor.
+
+1.4. "Covered Software" means Source Code Form to which the initial Contributor
+has attached the notice in Exhibit A, the Executable Form of such Source Code
+Form, and Modifications of such Source Code Form, in each case including portions
+thereof.
+
+     1.5. "Incompatible With Secondary Licenses" means
+
+(a) that the initial Contributor has attached the notice described in Exhibit
+B to the Covered Software; or
+
+(b) that the Covered Software was made available under the terms of version
+1.1 or earlier of the License, but not also under the terms of a Secondary
+License.
+
+1.6. "Executable Form" means any form of the work other than Source Code Form.
+
+1.7. "Larger Work" means a work that combines Covered Software with other
+material, in a separate file or files, that is not Covered Software.
+
+     1.8. "License" means this document.
+
+1.9. "Licensable" means having the right to grant, to the maximum extent possible,
+whether at the time of the initial grant or subsequently, any and all of the
+rights conveyed by this License.
+
+     1.10. "Modifications" means any of the following:
+
+(a) any file in Source Code Form that results from an addition to, deletion
+from, or modification of the contents of Covered Software; or
+
+(b) any new file in Source Code Form that contains any Covered Software.
+
+1.11. "Patent Claims" of a Contributor means any patent claim(s), including
+without limitation, method, process, and apparatus claims, in any patent Licensable
+by such Contributor that would be infringed, but for the grant of the License,
+by the making, using, selling, offering for sale, having made, import, or
+transfer of either its Contributions or its Contributor Version.
+
+1.12. "Secondary License" means either the GNU General Public License, Version
+2.0, the GNU Lesser General Public License, Version 2.1, the GNU Affero General
+Public License, Version 3.0, or any later versions of those licenses.
+
+1.13. "Source Code Form" means the form of the work preferred for making modifications.
+
+1.14. "You" (or "Your") means an individual or a legal entity exercising rights
+under this License. For legal entities, "You" includes any entity that controls,
+is controlled by, or is under common control with You. For purposes of this
+definition, "control" means (a) the power, direct or indirect, to cause the
+direction or management of such entity, whether by contract or otherwise,
+or (b) ownership of more than fifty percent (50%) of the outstanding shares
+or beneficial ownership of such entity.
+
+2. License Grants and Conditions
+
+     2.1. Grants
+Each Contributor hereby grants You a world-wide, royalty-free, non-exclusive
+license:
+
+(a) under intellectual property rights (other than patent or trademark) Licensable
+by such Contributor to use, reproduce, make available, modify, display, perform,
+distribute, and otherwise exploit its Contributions, either on an unmodified
+basis, with Modifications, or as part of a Larger Work; and
+
+(b) under Patent Claims of such Contributor to make, use, sell, offer for
+sale, have made, import, and otherwise transfer either its Contributions or
+its Contributor Version.
+
+     2.2. Effective Date
+The licenses granted in Section 2.1 with respect to any Contribution become
+effective for each Contribution on the date the Contributor first distributes
+such Contribution.
+
+     2.3. Limitations on Grant Scope
+The licenses granted in this Section 2 are the only rights granted under this
+License. No additional rights or licenses will be implied from the distribution
+or licensing of Covered Software under this License. Notwithstanding Section
+2.1(b) above, no patent license is granted by a Contributor:
+
+(a) for any code that a Contributor has removed from Covered Software; or
+
+(b) for infringements caused by: (i) Your and any other third party's modifications
+of Covered Software, or (ii) the combination of its Contributions with other
+software (except as part of its Contributor Version); or
+
+(c) under Patent Claims infringed by Covered Software in the absence of its
+Contributions.
+
+This License does not grant any rights in the trademarks, service marks, or
+logos of any Contributor (except as may be necessary to comply with the notice
+requirements in Section 3.4).
+
+     2.4. Subsequent Licenses
+No Contributor makes additional grants as a result of Your choice to distribute
+the Covered Software under a subsequent version of this License (see Section
+10.2) or under the terms of a Secondary License (if permitted under the terms
+of Section 3.3).
+
+     2.5. Representation
+Each Contributor represents that the Contributor believes its Contributions
+are its original creation(s) or it has sufficient rights to grant the rights
+to its Contributions conveyed by this License.
+
+     2.6. Fair Use
+This License is not intended to limit any rights You have under applicable
+copyright doctrines of fair use, fair dealing, or other equivalents.
+
+     2.7. Conditions
+Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted in
+Section 2.1.
+
+3. Responsibilities
+
+     3.1. Distribution of Source Form
+All distribution of Covered Software in Source Code Form, including any Modifications
+that You create or to which You contribute, must be under the terms of this
+License. You must inform recipients that the Source Code Form of the Covered
+Software is governed by the terms of this License, and how they can obtain
+a copy of this License. You may not attempt to alter or restrict the recipients'
+rights in the Source Code Form.
+
+     3.2. Distribution of Executable Form
+     If You distribute Covered Software in Executable Form then:
+
+(a) such Covered Software must also be made available in Source Code Form,
+as described in Section 3.1, and You must inform recipients of the Executable
+Form how they can obtain a copy of such Source Code Form by reasonable means
+in a timely manner, at a charge no more than the cost of distribution to the
+recipient; and
+
+(b) You may distribute such Executable Form under the terms of this License,
+or sublicense it under different terms, provided that the license for the
+Executable Form does not attempt to limit or alter the recipients' rights
+in the Source Code Form under this License.
+
+     3.3. Distribution of a Larger Work
+You may create and distribute a Larger Work under terms of Your choice, provided
+that You also comply with the requirements of this License for the Covered
+Software. If the Larger Work is a combination of Covered Software with a work
+governed by one or more Secondary Licenses, and the Covered Software is not
+Incompatible With Secondary Licenses, this License permits You to additionally
+distribute such Covered Software under the terms of such Secondary License(s),
+so that the recipient of the Larger Work may, at their option, further distribute
+the Covered Software under the terms of either this License or such Secondary
+License(s).
+
+     3.4. Notices
+You may not remove or alter the substance of any license notices (including
+copyright notices, patent notices, disclaimers of warranty, or limitations
+of liability) contained within the Source Code Form of the Covered Software,
+except that You may alter any license notices to the extent required to remedy
+known factual inaccuracies.
+
+     3.5. Application of Additional Terms
+You may choose to offer, and to charge a fee for, warranty, support, indemnity
+or liability obligations to one or more recipients of Covered Software. However,
+You may do so only on Your own behalf, and not on behalf of any Contributor.
+You must make it absolutely clear that any such warranty, support, indemnity,
+or liability obligation is offered by You alone, and You hereby agree to indemnify
+every Contributor for any liability incurred by such Contributor as a result
+of warranty, support, indemnity or liability terms You offer. You may include
+additional disclaimers of warranty and limitations of liability specific to
+any jurisdiction.
+
+4. Inability to Comply Due to Statute or Regulation
+If it is impossible for You to comply with any of the terms of this License
+with respect to some or all of the Covered Software due to statute, judicial
+order, or regulation then You must: (a) comply with the terms of this License
+to the maximum extent possible; and (b) describe the limitations and the code
+they affect. Such description must be placed in a text file included with
+all distributions of the Covered Software under this License. Except to the
+extent prohibited by statute or regulation, such description must be sufficiently
+detailed for a recipient of ordinary skill to be able to understand it.
+
+5. Termination
+
+5.1. The rights granted under this License will terminate automatically if
+You fail to comply with any of its terms. However, if You become compliant,
+then the rights granted under this License from a particular Contributor are
+reinstated (a) provisionally, unless and until such Contributor explicitly
+and finally terminates Your grants, and (b) on an ongoing basis, if such Contributor
+fails to notify You of the non-compliance by some reasonable means prior to
+60 days after You have come back into compliance. Moreover, Your grants from
+a particular Contributor are reinstated on an ongoing basis if such Contributor
+notifies You of the non-compliance by some reasonable means, this is the first
+time You have received notice of non-compliance with this License from such
+Contributor, and You become compliant prior to 30 days after Your receipt
+of the notice.
+
+5.2. If You initiate litigation against any entity by asserting a patent infringement
+claim (excluding declaratory judgment actions, counter-claims, and cross-claims)
+alleging that a Contributor Version directly or indirectly infringes any patent,
+then the rights granted to You by any and all Contributors for the Covered
+Software under Section 2.1 of this License shall terminate.
+
+5.3. In the event of termination under Sections 5.1 or 5.2 above, all end
+user license agreements (excluding distributors and resellers) which have
+been validly granted by You or Your distributors under this License prior
+to termination shall survive termination.
+
+6. Disclaimer of Warranty
+Covered Software is provided under this License on an "as is" basis, without
+warranty of any kind, either expressed, implied, or statutory, including,
+without limitation, warranties that the Covered Software is free of defects,
+merchantable, fit for a particular purpose or non-infringing. The entire risk
+as to the quality and performance of the Covered Software is with You. Should
+any Covered Software prove defective in any respect, You (not any Contributor)
+assume the cost of any necessary servicing, repair, or correction. This disclaimer
+of warranty constitutes an essential part of this License. No use of any Covered
+Software is authorized under this License except under this disclaimer.
+
+7. Limitation of Liability
+Under no circumstances and under no legal theory, whether tort (including
+negligence), contract, or otherwise, shall any Contributor, or anyone who
+distributes Covered Software as permitted above, be liable to You for any
+direct, indirect, special, incidental, or consequential damages of any character
+including, without limitation, damages for lost profits, loss of goodwill,
+work stoppage, computer failure or malfunction, or any and all other commercial
+damages or losses, even if such party shall have been informed of the possibility
+of such damages. This limitation of liability shall not apply to liability
+for death or personal injury resulting from such party's negligence to the
+extent applicable law prohibits such limitation. Some jurisdictions do not
+allow the exclusion or limitation of incidental or consequential damages,
+so this exclusion and limitation may not apply to You.
+
+8. Litigation
+Any litigation relating to this License may be brought only in the courts
+of a jurisdiction where the defendant maintains its principal place of business
+and such litigation shall be governed by laws of that jurisdiction, without
+reference to its conflict-of-law provisions. Nothing in this Section shall
+prevent a party's ability to bring cross-claims or counter-claims.
+
+9. Miscellaneous
+This License represents the complete agreement concerning the subject matter
+hereof. If any provision of this License is held to be unenforceable, such
+provision shall be reformed only to the extent necessary to make it enforceable.
+Any law or regulation which provides that the language of a contract shall
+be construed against the drafter shall not be used to construe this License
+against a Contributor.
+
+10. Versions of the License
+
+     10.1. New Versions
+Mozilla Foundation is the license steward. Except as provided in Section 10.3,
+no one other than the license steward has the right to modify or publish new
+versions of this License. Each version will be given a distinguishing version
+number.
+
+     10.2. Effect of New Versions
+You may distribute the Covered Software under the terms of the version of
+the License under which You originally received the Covered Software, or under
+the terms of any subsequent version published by the license steward.
+
+     10.3. Modified Versions
+If you create software not governed by this License, and you want to create
+a new license for such software, you may create and use a modified version
+of this License if you rename the license and remove any references to the
+name of the license steward (except to note that such modified license differs
+from this License).
+
+10.4. Distributing Source Code Form that is Incompatible With Secondary Licenses
+If You choose to distribute Source Code Form that is Incompatible With Secondary
+Licenses under the terms of this version of the License, the notice described
+in Exhibit B of this License must be attached.
+
+Exhibit A - Source Code Form License Notice
+
+This Source Code Form is subject to the terms of the Mozilla Public License,
+v. 2.0. If a copy of the MPL was not distributed with this file, You can obtain
+one at http://mozilla.org/MPL/2.0/.
+
+If it is not possible or desirable to put the notice in a particular file,
+then You may include the notice in a location (such as a LICENSE file in a
+relevant directory) where a recipient would be likely to look for such a notice.
+
+You may add additional accurate notices of copyright ownership.
+
+Exhibit B - "Incompatible With Secondary Licenses" Notice
+
+This Source Code Form is "Incompatible With Secondary Licenses", as defined
+by the Mozilla Public License, v. 2.0.
```

## Comparing `power_grid_model-1.5.0rc9166510100968.dist-info/METADATA` & `power_grid_model-1.5.0rc9171103440397.dist-info/METADATA`

 * *Files 11% similar despite different names*

```diff
@@ -1,107 +1,110 @@
-Metadata-Version: 2.1
-Name: power-grid-model
-Version: 1.5.0rc9166510100968
-Summary: Python/C++ library for distribution power system analysis
-Author-email: Alliander Dynamic Grid Calculation <dynamic.grid.calculation@alliander.com>
-License: MPL-2.0
-Project-URL: Home-page, https://github.com/PowerGridModel/power-grid-model
-Project-URL: Documentation, https://power-grid-model.readthedocs.io/en/stable/
-Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: Implementation :: CPython
-Classifier: Programming Language :: C++
-Classifier: Development Status :: 5 - Production/Stable
-Classifier: Intended Audience :: Developers
-Classifier: Intended Audience :: Science/Research
-Classifier: License :: OSI Approved :: Mozilla Public License 2.0 (MPL 2.0)
-Classifier: Operating System :: Microsoft :: Windows
-Classifier: Operating System :: POSIX :: Linux
-Classifier: Operating System :: MacOS
-Classifier: Topic :: Scientific/Engineering :: Physics
-Requires-Python: >=3.7
-Description-Content-Type: text/markdown
-License-File: LICENSE
-Requires-Dist: numpy (>=1.21.0)
-Provides-Extra: dev
-Requires-Dist: pre-commit ; extra == 'dev'
-Requires-Dist: pylint ; extra == 'dev'
-Requires-Dist: pytest ; extra == 'dev'
-Requires-Dist: pytest-cov ; extra == 'dev'
-Provides-Extra: doc
-Requires-Dist: sphinx ; extra == 'doc'
-Requires-Dist: breathe ; extra == 'doc'
-Requires-Dist: myst-nb ; extra == 'doc'
-Requires-Dist: sphinx-rtd-theme ; extra == 'doc'
-Requires-Dist: readthedocs-sphinx-search ; extra == 'doc'
-Requires-Dist: sphinx-hoverxref ; extra == 'doc'
-Requires-Dist: sphinxcontrib-mermaid ; extra == 'doc'
-Requires-Dist: numpydoc ; extra == 'doc'
-Requires-Dist: pandas ; extra == 'doc'
-Requires-Dist: gitpython ; extra == 'doc'
-
-<!--
-SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
-
-SPDX-License-Identifier: MPL-2.0
--->
-[![PyPI version](https://badge.fury.io/py/power-grid-model.svg)](https://badge.fury.io/py/power-grid-model)
-[![License: MIT](https://img.shields.io/badge/License-MPL2.0-informational.svg)](https://github.com/PowerGridModel/power-grid-model/blob/main/LICENSE)
-[![Build and Test C++ and Python](https://github.com/PowerGridModel/power-grid-model/actions/workflows/main.yml/badge.svg)](https://github.com/PowerGridModel/power-grid-model/actions/workflows/main.yml)
-[![Check Code Quality](https://github.com/PowerGridModel/power-grid-model/actions/workflows/check-code-quality.yml/badge.svg)](https://github.com/PowerGridModel/power-grid-model/actions/workflows/check-code-quality.yml)
-[![REUSE Compliance Check](https://github.com/PowerGridModel/power-grid-model/actions/workflows/reuse-compliance.yml/badge.svg)](https://github.com/PowerGridModel/power-grid-model/actions/workflows/reuse-compliance.yml)
-[![docs](https://readthedocs.org/projects/power-grid-model/badge/)](https://power-grid-model.readthedocs.io/en/stable/)
-
-[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=PowerGridModel_power-grid-model&metric=alert_status)](https://sonarcloud.io/summary/new_code?id=PowerGridModel_power-grid-model)
-[![Coverage](https://sonarcloud.io/api/project_badges/measure?project=PowerGridModel_power-grid-model&metric=coverage)](https://sonarcloud.io/summary/new_code?id=PowerGridModel_power-grid-model)
-[![Maintainability Rating](https://sonarcloud.io/api/project_badges/measure?project=PowerGridModel_power-grid-model&metric=sqale_rating)](https://sonarcloud.io/summary/new_code?id=PowerGridModel_power-grid-model)
-[![Reliability Rating](https://sonarcloud.io/api/project_badges/measure?project=PowerGridModel_power-grid-model&metric=reliability_rating)](https://sonarcloud.io/summary/new_code?id=PowerGridModel_power-grid-model)
-[![Security Rating](https://sonarcloud.io/api/project_badges/measure?project=PowerGridModel_power-grid-model&metric=security_rating)](https://sonarcloud.io/summary/new_code?id=PowerGridModel_power-grid-model)
-[![Vulnerabilities](https://sonarcloud.io/api/project_badges/measure?project=PowerGridModel_power-grid-model&metric=vulnerabilities)](https://sonarcloud.io/summary/new_code?id=PowerGridModel_power-grid-model)
-
-[![](https://github.com/PowerGridModel/.github/blob/main/artwork/svg/color.svg)](#)
-
-# Power Grid Model
-
-`power-grid-model` is a library for steady-state distribution power system analysis distributed for Python and C.
-The core of the library is written in C++.
-Currently, it supports the following calculations:
-
-* Symmetric and asymmetric power flow calculation with Newton-Raphson method, iterative current method and linear method
-* Symmetric and asymmetric state estimation with iterative linear method
-
-See the [power-grid-model documentation](https://power-grid-model.readthedocs.io/en/stable/) for more information.
-For various conversions to the power-grid-model, refer to the [power-grid-model-io](https://github.com/PowerGridModel/power-grid-model-io) repository.
-
-# Installation
-
-## Install from PyPI
-
-You can directly install the package from PyPI.
-
-```
-pip install power-grid-model
-```
-
-## Build and install from Source
-
-To install the library from source, refer to the [Build Guide](https://github.com/PowerGridModel/power-grid-model/blob/9a20e8a60aaad9220f504ffe10366c4d105c3aa9/docs/advanced_documentation/build-guide.md).
-
-# Examples
-
-Please refer to [Examples](https://github.com/PowerGridModel/power-grid-model-workshop/tree/main/examples) for more detailed examples for power flow and state estimation. 
-Notebooks for validating the input data and exporting input/output data are also included.
-
-# License
-This project is licensed under the Mozilla Public License, version 2.0 - see [LICENSE](https://github.com/PowerGridModel/power-grid-model/blob/9a20e8a60aaad9220f504ffe10366c4d105c3aa9/LICENSE) for details.
-
-# Licenses third-party libraries
-This project includes third-party libraries, 
-which are licensed under their own respective Open-Source licenses.
-SPDX-License-Identifier headers are used to show which license is applicable. 
-The concerning license files can be found in the [LICENSES](https://github.com/PowerGridModel/power-grid-model/blob/9a20e8a60aaad9220f504ffe10366c4d105c3aa9/LICENSES) directory.
-
-# Contributing
-Please read [CODE_OF_CONDUCT](https://github.com/PowerGridModel/.github/blob/main/CODE_OF_CONDUCT.md), [CONTRIBUTING](https://github.com/PowerGridModel/.github/blob/main/CONTRIBUTING.md), [PROJECT GOVERNANCE](https://github.com/PowerGridModel/.github/blob/main/PROJECT_GOVERNANCE.md) and [RELEASE](https://github.com/PowerGridModel/.github/blob/main/RELEASE.md) for details on the process 
-for submitting pull requests to us.
-
-# Contact
-Please read [SUPPORT](https://github.com/PowerGridModel/.github/blob/main/SUPPORT.md) for how to connect and get into contact with the Power Gird Model project.
+Metadata-Version: 2.1
+Name: power-grid-model
+Version: 1.5.0rc9171103440397
+Summary: Python/C++ library for distribution power system analysis
+Author-email: Alliander Dynamic Grid Calculation <dynamic.grid.calculation@alliander.com>
+License: MPL-2.0
+Project-URL: Home-page, https://lfenergy.org/projects/power-grid-model/
+Project-URL: GitHub, https://github.com/PowerGridModel/power-grid-model
+Project-URL: Documentation, https://power-grid-model.readthedocs.io/en/stable/
+Project-URL: Mailing-list, https://lists.lfenergy.org/g/powergridmodel
+Project-URL: Discussion, https://github.com/orgs/PowerGridModel/discussions
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: Implementation :: CPython
+Classifier: Programming Language :: C++
+Classifier: Development Status :: 5 - Production/Stable
+Classifier: Intended Audience :: Developers
+Classifier: Intended Audience :: Science/Research
+Classifier: License :: OSI Approved :: Mozilla Public License 2.0 (MPL 2.0)
+Classifier: Operating System :: Microsoft :: Windows
+Classifier: Operating System :: POSIX :: Linux
+Classifier: Operating System :: MacOS
+Classifier: Topic :: Scientific/Engineering :: Physics
+Requires-Python: >=3.7
+Description-Content-Type: text/markdown
+License-File: LICENSE
+Requires-Dist: numpy (>=1.21.0)
+Provides-Extra: dev
+Requires-Dist: pre-commit ; extra == 'dev'
+Requires-Dist: pylint ; extra == 'dev'
+Requires-Dist: pytest ; extra == 'dev'
+Requires-Dist: pytest-cov ; extra == 'dev'
+Provides-Extra: doc
+Requires-Dist: sphinx ; extra == 'doc'
+Requires-Dist: breathe ; extra == 'doc'
+Requires-Dist: myst-nb ; extra == 'doc'
+Requires-Dist: sphinx-rtd-theme ; extra == 'doc'
+Requires-Dist: readthedocs-sphinx-search ; extra == 'doc'
+Requires-Dist: sphinx-hoverxref ; extra == 'doc'
+Requires-Dist: sphinxcontrib-mermaid ; extra == 'doc'
+Requires-Dist: numpydoc ; extra == 'doc'
+Requires-Dist: pandas ; extra == 'doc'
+Requires-Dist: gitpython ; extra == 'doc'
+
+<!--
+SPDX-FileCopyrightText: 2022 Contributors to the Power Grid Model project <dynamic.grid.calculation@alliander.com>
+
+SPDX-License-Identifier: MPL-2.0
+-->
+[![PyPI version](https://badge.fury.io/py/power-grid-model.svg)](https://badge.fury.io/py/power-grid-model)
+[![License: MIT](https://img.shields.io/badge/License-MPL2.0-informational.svg)](https://github.com/PowerGridModel/power-grid-model/blob/main/LICENSE)
+[![Build and Test C++ and Python](https://github.com/PowerGridModel/power-grid-model/actions/workflows/main.yml/badge.svg)](https://github.com/PowerGridModel/power-grid-model/actions/workflows/main.yml)
+[![Check Code Quality](https://github.com/PowerGridModel/power-grid-model/actions/workflows/check-code-quality.yml/badge.svg)](https://github.com/PowerGridModel/power-grid-model/actions/workflows/check-code-quality.yml)
+[![REUSE Compliance Check](https://github.com/PowerGridModel/power-grid-model/actions/workflows/reuse-compliance.yml/badge.svg)](https://github.com/PowerGridModel/power-grid-model/actions/workflows/reuse-compliance.yml)
+[![docs](https://readthedocs.org/projects/power-grid-model/badge/)](https://power-grid-model.readthedocs.io/en/stable/)
+
+[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=PowerGridModel_power-grid-model&metric=alert_status)](https://sonarcloud.io/summary/new_code?id=PowerGridModel_power-grid-model)
+[![Coverage](https://sonarcloud.io/api/project_badges/measure?project=PowerGridModel_power-grid-model&metric=coverage)](https://sonarcloud.io/summary/new_code?id=PowerGridModel_power-grid-model)
+[![Maintainability Rating](https://sonarcloud.io/api/project_badges/measure?project=PowerGridModel_power-grid-model&metric=sqale_rating)](https://sonarcloud.io/summary/new_code?id=PowerGridModel_power-grid-model)
+[![Reliability Rating](https://sonarcloud.io/api/project_badges/measure?project=PowerGridModel_power-grid-model&metric=reliability_rating)](https://sonarcloud.io/summary/new_code?id=PowerGridModel_power-grid-model)
+[![Security Rating](https://sonarcloud.io/api/project_badges/measure?project=PowerGridModel_power-grid-model&metric=security_rating)](https://sonarcloud.io/summary/new_code?id=PowerGridModel_power-grid-model)
+[![Vulnerabilities](https://sonarcloud.io/api/project_badges/measure?project=PowerGridModel_power-grid-model&metric=vulnerabilities)](https://sonarcloud.io/summary/new_code?id=PowerGridModel_power-grid-model)
+
+[![](https://github.com/PowerGridModel/.github/blob/main/artwork/svg/color.svg)](#)
+
+# Power Grid Model
+
+`power-grid-model` is a library for steady-state distribution power system analysis distributed for Python and C.
+The core of the library is written in C++.
+Currently, it supports the following calculations:
+
+* Symmetric and asymmetric power flow calculation with Newton-Raphson method, iterative current method and linear method
+* Symmetric and asymmetric state estimation with iterative linear method
+
+See the [power-grid-model documentation](https://power-grid-model.readthedocs.io/en/stable/) for more information.
+For various conversions to the power-grid-model, refer to the [power-grid-model-io](https://github.com/PowerGridModel/power-grid-model-io) repository.
+
+# Installation
+
+## Install from PyPI
+
+You can directly install the package from PyPI.
+
+```
+pip install power-grid-model
+```
+
+## Build and install from Source
+
+To install the library from source, refer to the [Build Guide](https://github.com/PowerGridModel/power-grid-model/blob/347f0d5ac055e93fb11839182b71983c92195956/docs/advanced_documentation/build-guide.md).
+
+# Examples
+
+Please refer to [Examples](https://github.com/PowerGridModel/power-grid-model-workshop/tree/main/examples) for more detailed examples for power flow and state estimation. 
+Notebooks for validating the input data and exporting input/output data are also included.
+
+# License
+This project is licensed under the Mozilla Public License, version 2.0 - see [LICENSE](https://github.com/PowerGridModel/power-grid-model/blob/347f0d5ac055e93fb11839182b71983c92195956/LICENSE) for details.
+
+# Licenses third-party libraries
+This project includes third-party libraries, 
+which are licensed under their own respective Open-Source licenses.
+SPDX-License-Identifier headers are used to show which license is applicable. 
+The concerning license files can be found in the [LICENSES](https://github.com/PowerGridModel/power-grid-model/blob/347f0d5ac055e93fb11839182b71983c92195956/LICENSES) directory.
+
+# Contributing
+Please read [CODE_OF_CONDUCT](https://github.com/PowerGridModel/.github/blob/main/CODE_OF_CONDUCT.md), [CONTRIBUTING](https://github.com/PowerGridModel/.github/blob/main/CONTRIBUTING.md), [PROJECT GOVERNANCE](https://github.com/PowerGridModel/.github/blob/main/GOVERNANCE.md) and [RELEASE](https://github.com/PowerGridModel/.github/blob/main/RELEASE.md) for details on the process 
+for submitting pull requests to us.
+
+# Contact
+Please read [SUPPORT](https://github.com/PowerGridModel/.github/blob/main/SUPPORT.md) for how to connect and get into contact with the Power Gird Model project.
```

## Comparing `power_grid_model-1.5.0rc9166510100968.dist-info/RECORD` & `power_grid_model-1.5.0rc9171103440397.dist-info/RECORD`

 * *Files 13% similar despite different names*

```diff
@@ -1,24 +1,24 @@
-power_grid_model/__init__.py,sha256=wdEGj0cvPerqHrsqdQWbFkTfqvv41KgAsQPm3fO9QBI,526
-power_grid_model/data_types.py,sha256=fs3yPTc034vMkMdBrdG2mZ4vZqStVNQQvt1j9LK2P5E,5404
-power_grid_model/enum.py,sha256=MMgerFvP2i8wsRz-cf7Oe5ffQgk1DrbunrjaNPutZHE,2293
-power_grid_model/errors.py,sha256=IYV-rC4bbyWxkVYRx6mdXb8lFCP4oBWuFDaWqVaJb9s,533
-power_grid_model/utils.py,sha256=B5i80vALze8lYdQsRSqNfKIIokTVy-QHUL_IdH7kAmg,22198
-power_grid_model/core/__init__.py,sha256=aZ9YdVtwQ6JckIugyk854k3Pf07FiM-WrYCZob-4vxY,157
-power_grid_model/core/_power_grid_core.dll,sha256=QvfKVrLWnyWrLRg9a8sVTk3uajdIRlbVdUMFxlwCxsY,912384
-power_grid_model/core/error_handling.py,sha256=lNQ6H3SUQiXHOSC8Fwhiujy__uzI_TbfTSec9hjFh8I,2168
-power_grid_model/core/index_integer.py,sha256=3LXjUDwA0pJWaSYCK8ex7wxL4xLmS_Oq6zyL3-Gt7-4,384
-power_grid_model/core/options.py,sha256=xIHi_CFVjI3YttEgOGl8XxzKZjoUd2AOp7-hLxFslho,1750
-power_grid_model/core/power_grid_core.py,sha256=zbg5aTxumV4NMZsMAl4fpbhdHIOPDTMyMiQApFYZXIg,10321
-power_grid_model/core/power_grid_meta.py,sha256=Ri4uKukv2hnL60PwrIxah74cOFejcva46PmBNVnz8Z0,9525
-power_grid_model/core/power_grid_model.py,sha256=9ChyOJ5ia3zGkUjOjLnkx_Wx5U9JQkLFjXbB3x2C5Pw,18183
-power_grid_model/validation/__init__.py,sha256=zCV3UEv7q9B29wBlBtC5KsofPy2iQb4b5ZbDusEKBAM,557
-power_grid_model/validation/assertions.py,sha256=OJCwxXHTjB4j9NnNzhJW2vRALU_H5Gz_Ifs-yLIxncs,4260
-power_grid_model/validation/errors.py,sha256=HTW6C0wq41b_4PTV_BIBbOvFhL_Grkpc0eTH4DQu2Y8,15124
-power_grid_model/validation/rules.py,sha256=M6g2A0BPe5qDzYuM93jzVlIG5ujo45Jpct302VDiKa0,31381
-power_grid_model/validation/utils.py,sha256=myy_-kzds3fuAtwmrPy2vNtxuP29wE8-a0cmAsiMuAo,8815
-power_grid_model/validation/validation.py,sha256=JCXrCv-hU75FLDrT-3Av8BMTgYHC3ekBMFZGWkatV7w,29299
-power_grid_model-1.5.0rc9166510100968.dist-info/LICENSE,sha256=3KDI9LRd9JZGZcgIO1j9y8Q7lsxlVAbav3UNdr5yp_A,15199
-power_grid_model-1.5.0rc9166510100968.dist-info/METADATA,sha256=lm2yvO-QHNEYD7BKOPB4K9rFeRjVcXW90d9ywrJtMr0,6947
-power_grid_model-1.5.0rc9166510100968.dist-info/WHEEL,sha256=oOatfxF9lem9vgsF-QTj3e-P_rDgLCmqbWx3sLKdyrE,99
-power_grid_model-1.5.0rc9166510100968.dist-info/top_level.txt,sha256=1mz4Aq4A5oedde8uMGpUiWI7jdB4-BwxjRfke5jkBkM,17
-power_grid_model-1.5.0rc9166510100968.dist-info/RECORD,,
+power_grid_model/__init__.py,sha256=-nSJRegU-yLy5T3Mv6eokfbuGwNgsqr6RVPIV0rqZro,509
+power_grid_model/data_types.py,sha256=GIJaixRjIGlLTePCWQaSQTimtZP5lzKH4bLFo9hCVCI,5239
+power_grid_model/utils.py,sha256=0n44bq2_xEJ7X98xYTBW-REoYdhGcZ1PuU7yVfzr32c,21645
+power_grid_model/errors.py,sha256=2cGKBaN-X9nyX2xphE90O4dNfEbPxYi4YnVMIR1dpzM,505
+power_grid_model/enum.py,sha256=WaNq4hvYPSFs9ULzGk_Z155-lNyTTZJUNllfSVPL52g,2183
+power_grid_model/core/options.py,sha256=Zv0VS6slMcdceb4He6EfM9SveSn54AzLx_LG8Eq9Dtc,1682
+power_grid_model/core/power_grid_core.py,sha256=gdHn3Gn6r_z0q1G9aka00uze8Ykn7UA_jRbmCa_IC1s,9994
+power_grid_model/core/power_grid_model.py,sha256=mu8nBzeNib8F_dycFy5r4zGNtUh7Gv2syiXFXtoF-_A,17761
+power_grid_model/core/__init__.py,sha256=4RCCaYmi3VSdnAYw3RGxH73K3Jez5VUymiWg-zXqj_c,154
+power_grid_model/core/error_handling.py,sha256=k7ebC6FbY-o45-8mdeXfH33ylvhhs0Yw1GxQP6HOv0Y,2100
+power_grid_model/core/_power_grid_core.so,sha256=rjTivixadeL__kXzXwNaFNr9gg_xYxcqqV3hu3REbbA,1529096
+power_grid_model/core/power_grid_meta.py,sha256=U-33uREhGn1veF19nSqyXwX-OPNK_X8JIDAzaZz21Uk,9240
+power_grid_model/core/index_integer.py,sha256=s5hnBtf0_IYZUb8I17CpeybLyZLp6lmkJH3VVGoUR1M,367
+power_grid_model/validation/__init__.py,sha256=2j7WmsnDy5Tj2PG5y7qf38yI8T9zK4aUGeH_VKeE_Pk,547
+power_grid_model/validation/rules.py,sha256=Ru8ciPAzcR7wlsxi0Io-JOC3yMPY-H0joIinJS6rAWM,30704
+power_grid_model/validation/assertions.py,sha256=yVwdAj_OHTvpf_zChiGElIr6JMzcV5e8JFvXoGrykL4,4166
+power_grid_model/validation/utils.py,sha256=6L96RUYQLIunlYU0U3vPpvaFrAD2oqJfDTqRzOlzij8,8601
+power_grid_model/validation/errors.py,sha256=wwh4zxMogwkG6Fdo82XldjuFj2djWKSkkS7dO7oPcYU,14686
+power_grid_model/validation/validation.py,sha256=-J-_AmhV7g5RhCmZ4ABm02D1GSv4EB_mMrL9YQ-YtH0,28625
+power_grid_model-1.5.0rc9171103440397.dist-info/RECORD,,
+power_grid_model-1.5.0rc9171103440397.dist-info/LICENSE,sha256=7Pm2fWFFHHUG5lDHed1vl5CjzxObIXQglnYsEdtjo_k,14907
+power_grid_model-1.5.0rc9171103440397.dist-info/WHEEL,sha256=MjE19RBReg4BcAO1BzZVUafNrqHClVQLla7FU9RN5vE,108
+power_grid_model-1.5.0rc9171103440397.dist-info/top_level.txt,sha256=1mz4Aq4A5oedde8uMGpUiWI7jdB4-BwxjRfke5jkBkM,17
+power_grid_model-1.5.0rc9171103440397.dist-info/METADATA,sha256=5l3KLB8TKIow7t7OAAfPSOV6uEhb6Eth3Q83_yAsp50,7048
```

