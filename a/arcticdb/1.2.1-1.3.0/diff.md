# Comparing `tmp/arcticdb-1.2.1-cp39-cp39-win_amd64.whl.zip` & `tmp/arcticdb-1.3.0-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,73 +1,73 @@
-Zip file size: 6256907 bytes, number of entries: 71
--rw-rw-rw-  2.0 fat 22185472 b- defN 23-May-25 09:40 arcticdb_ext.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat       64 b- defN 23-May-25 09:12 arcticc/__init__.py
--rw-rw-rw-  2.0 fat      583 b- defN 23-May-25 09:12 arcticc/pb2/__init__.py
--rw-rw-rw-  2.0 fat      426 b- defN 23-May-25 09:12 arcticdb/__init__.py
--rw-rw-rw-  2.0 fat      447 b- defN 23-May-25 09:12 arcticdb/_msgpack_compat.py
--rw-rw-rw-  2.0 fat    10405 b- defN 23-May-25 09:12 arcticdb/arctic.py
--rw-rw-rw-  2.0 fat     7970 b- defN 23-May-25 09:12 arcticdb/config.py
--rw-rw-rw-  2.0 fat      765 b- defN 23-May-25 09:12 arcticdb/exceptions.py
--rw-rw-rw-  2.0 fat     9363 b- defN 23-May-25 09:12 arcticdb/flattener.py
--rw-rw-rw-  2.0 fat     2071 b- defN 23-May-25 09:12 arcticdb/log.py
--rw-rw-rw-  2.0 fat     6436 b- defN 23-May-25 09:12 arcticdb/options.py
--rw-rw-rw-  2.0 fat      519 b- defN 23-May-25 09:12 arcticdb/preconditions.py
--rw-rw-rw-  2.0 fat     1612 b- defN 23-May-25 09:12 arcticdb/supported_types.py
--rw-rw-rw-  2.0 fat     3142 b- defN 23-May-25 09:12 arcticdb/tools.py
--rw-rw-rw-  2.0 fat      138 b- defN 23-May-25 09:12 arcticdb/adapters/__init__.py
--rw-rw-rw-  2.0 fat     2211 b- defN 23-May-25 09:12 arcticdb/adapters/arctic_library_adapter.py
--rw-rw-rw-  2.0 fat     2394 b- defN 23-May-25 09:12 arcticdb/adapters/lmdb_library_adapter.py
--rw-rw-rw-  2.0 fat     6434 b- defN 23-May-25 09:12 arcticdb/adapters/s3_library_adapter.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-May-25 09:12 arcticdb/authorization/__init__.py
--rw-rw-rw-  2.0 fat      969 b- defN 23-May-25 09:12 arcticdb/authorization/permissions.py
--rw-rw-rw-  2.0 fat     8935 b- defN 23-May-25 09:20 arcticdb/proto/3/arcticc/pb2/config_pb2.py
--rw-rw-rw-  2.0 fat    96954 b- defN 23-May-25 09:20 arcticdb/proto/3/arcticc/pb2/descriptors_pb2.py
--rw-rw-rw-  2.0 fat    28646 b- defN 23-May-25 09:20 arcticdb/proto/3/arcticc/pb2/encoding_pb2.py
--rw-rw-rw-  2.0 fat    13225 b- defN 23-May-25 09:20 arcticdb/proto/3/arcticc/pb2/generation_pb2.py
--rw-rw-rw-  2.0 fat     1990 b- defN 23-May-25 09:20 arcticdb/proto/3/arcticc/pb2/in_memory_storage_pb2.py
--rw-rw-rw-  2.0 fat     4634 b- defN 23-May-25 09:20 arcticdb/proto/3/arcticc/pb2/lmdb_storage_pb2.py
--rw-rw-rw-  2.0 fat    24224 b- defN 23-May-25 09:20 arcticdb/proto/3/arcticc/pb2/logger_pb2.py
--rw-rw-rw-  2.0 fat     2931 b- defN 23-May-25 09:20 arcticdb/proto/3/arcticc/pb2/mongo_storage_pb2.py
--rw-rw-rw-  2.0 fat     6878 b- defN 23-May-25 09:20 arcticdb/proto/3/arcticc/pb2/nfs_backed_storage_pb2.py
--rw-rw-rw-  2.0 fat     3821 b- defN 23-May-25 09:20 arcticdb/proto/3/arcticc/pb2/processors_pb2.py
--rw-rw-rw-  2.0 fat    49227 b- defN 23-May-25 09:20 arcticdb/proto/3/arcticc/pb2/request_pb2.py
--rw-rw-rw-  2.0 fat     6729 b- defN 23-May-25 09:20 arcticdb/proto/3/arcticc/pb2/s3_storage_pb2.py
--rw-rw-rw-  2.0 fat    67292 b- defN 23-May-25 09:20 arcticdb/proto/3/arcticc/pb2/storage_pb2.py
--rw-rw-rw-  2.0 fat    12621 b- defN 23-May-25 09:20 arcticdb/proto/3/arcticc/pb2/utils_pb2.py
--rw-rw-rw-  2.0 fat     2298 b- defN 23-May-25 09:20 arcticdb/proto/4/arcticc/pb2/config_pb2.py
--rw-rw-rw-  2.0 fat    14870 b- defN 23-May-25 09:20 arcticdb/proto/4/arcticc/pb2/descriptors_pb2.py
--rw-rw-rw-  2.0 fat     4895 b- defN 23-May-25 09:20 arcticdb/proto/4/arcticc/pb2/encoding_pb2.py
--rw-rw-rw-  2.0 fat     2998 b- defN 23-May-25 09:20 arcticdb/proto/4/arcticc/pb2/generation_pb2.py
--rw-rw-rw-  2.0 fat     1038 b- defN 23-May-25 09:20 arcticdb/proto/4/arcticc/pb2/in_memory_storage_pb2.py
--rw-rw-rw-  2.0 fat     1367 b- defN 23-May-25 09:20 arcticdb/proto/4/arcticc/pb2/lmdb_storage_pb2.py
--rw-rw-rw-  2.0 fat     4114 b- defN 23-May-25 09:20 arcticdb/proto/4/arcticc/pb2/logger_pb2.py
--rw-rw-rw-  2.0 fat     1195 b- defN 23-May-25 09:20 arcticdb/proto/4/arcticc/pb2/mongo_storage_pb2.py
--rw-rw-rw-  2.0 fat     1517 b- defN 23-May-25 09:20 arcticdb/proto/4/arcticc/pb2/nfs_backed_storage_pb2.py
--rw-rw-rw-  2.0 fat     1259 b- defN 23-May-25 09:20 arcticdb/proto/4/arcticc/pb2/processors_pb2.py
--rw-rw-rw-  2.0 fat     6801 b- defN 23-May-25 09:20 arcticdb/proto/4/arcticc/pb2/request_pb2.py
--rw-rw-rw-  2.0 fat     1496 b- defN 23-May-25 09:20 arcticdb/proto/4/arcticc/pb2/s3_storage_pb2.py
--rw-rw-rw-  2.0 fat     9907 b- defN 23-May-25 09:20 arcticdb/proto/4/arcticc/pb2/storage_pb2.py
--rw-rw-rw-  2.0 fat     2310 b- defN 23-May-25 09:20 arcticdb/proto/4/arcticc/pb2/utils_pb2.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-May-25 09:12 arcticdb/toolbox/__init__.py
--rw-rw-rw-  2.0 fat     5669 b- defN 23-May-25 09:12 arcticdb/toolbox/library_tool.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-May-25 09:12 arcticdb/util/__init__.py
--rw-rw-rw-  2.0 fat     2705 b- defN 23-May-25 09:12 arcticdb/util/errors.py
--rw-rw-rw-  2.0 fat     8769 b- defN 23-May-25 09:12 arcticdb/util/hypothesis.py
--rw-rw-rw-  2.0 fat      719 b- defN 23-May-25 09:12 arcticdb/util/memory.py
--rw-rw-rw-  2.0 fat     6042 b- defN 23-May-25 09:12 arcticdb/util/tasks.py
--rw-rw-rw-  2.0 fat    18195 b- defN 23-May-25 09:12 arcticdb/util/test.py
--rw-rw-rw-  2.0 fat      131 b- defN 23-May-25 09:12 arcticdb/version_store/__init__.py
--rw-rw-rw-  2.0 fat     7142 b- defN 23-May-25 09:12 arcticdb/version_store/_common.py
--rw-rw-rw-  2.0 fat     4193 b- defN 23-May-25 09:12 arcticdb/version_store/_custom_normalizers.py
--rw-rw-rw-  2.0 fat    52604 b- defN 23-May-25 09:12 arcticdb/version_store/_normalization.py
--rw-rw-rw-  2.0 fat   113852 b- defN 23-May-25 09:12 arcticdb/version_store/_store.py
--rw-rw-rw-  2.0 fat    10879 b- defN 23-May-25 09:12 arcticdb/version_store/helper.py
--rw-rw-rw-  2.0 fat    59225 b- defN 23-May-25 09:12 arcticdb/version_store/library.py
--rw-rw-rw-  2.0 fat    24592 b- defN 23-May-25 09:12 arcticdb/version_store/processing.py
--rw-rw-rw-  2.0 fat      603 b- defN 23-May-25 09:12 arcticdb/version_store/read_result.py
--rw-rw-rw-  2.0 fat     4851 b- defN 23-May-25 09:40 arcticdb-1.2.1.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat     8393 b- defN 23-May-25 09:40 arcticdb-1.2.1.dist-info/METADATA
--rw-rw-rw-  2.0 fat    18186 b- defN 23-May-25 09:40 arcticdb-1.2.1.dist-info/NOTICE.txt
--rw-rw-rw-  2.0 fat      100 b- defN 23-May-25 09:40 arcticdb-1.2.1.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       30 b- defN 23-May-25 09:40 arcticdb-1.2.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     6529 b- defN 23-May-25 09:40 arcticdb-1.2.1.dist-info/RECORD
-71 files, 22979002 bytes uncompressed, 6246411 bytes compressed:  72.8%
+Zip file size: 6197531 bytes, number of entries: 71
+-rw-rw-rw-  2.0 fat 22063616 b- defN 23-Jun-09 17:26 arcticdb_ext.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat       66 b- defN 23-Jun-09 17:05 arcticc/__init__.py
+-rw-rw-rw-  2.0 fat      583 b- defN 23-Jun-09 17:05 arcticc/pb2/__init__.py
+-rw-rw-rw-  2.0 fat      424 b- defN 23-Jun-09 17:05 arcticdb/__init__.py
+-rw-rw-rw-  2.0 fat      447 b- defN 23-Jun-09 17:05 arcticdb/_msgpack_compat.py
+-rw-rw-rw-  2.0 fat    10784 b- defN 23-Jun-09 17:05 arcticdb/arctic.py
+-rw-rw-rw-  2.0 fat     7524 b- defN 23-Jun-09 17:05 arcticdb/config.py
+-rw-rw-rw-  2.0 fat      765 b- defN 23-Jun-09 17:05 arcticdb/exceptions.py
+-rw-rw-rw-  2.0 fat     9361 b- defN 23-Jun-09 17:05 arcticdb/flattener.py
+-rw-rw-rw-  2.0 fat     1809 b- defN 23-Jun-09 17:05 arcticdb/log.py
+-rw-rw-rw-  2.0 fat     6476 b- defN 23-Jun-09 17:05 arcticdb/options.py
+-rw-rw-rw-  2.0 fat      519 b- defN 23-Jun-09 17:05 arcticdb/preconditions.py
+-rw-rw-rw-  2.0 fat     1612 b- defN 23-Jun-09 17:05 arcticdb/supported_types.py
+-rw-rw-rw-  2.0 fat     3142 b- defN 23-Jun-09 17:05 arcticdb/tools.py
+-rw-rw-rw-  2.0 fat      138 b- defN 23-Jun-09 17:05 arcticdb/adapters/__init__.py
+-rw-rw-rw-  2.0 fat     2211 b- defN 23-Jun-09 17:05 arcticdb/adapters/arctic_library_adapter.py
+-rw-rw-rw-  2.0 fat     2394 b- defN 23-Jun-09 17:05 arcticdb/adapters/lmdb_library_adapter.py
+-rw-rw-rw-  2.0 fat     6433 b- defN 23-Jun-09 17:05 arcticdb/adapters/s3_library_adapter.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-09 17:05 arcticdb/authorization/__init__.py
+-rw-rw-rw-  2.0 fat      969 b- defN 23-Jun-09 17:05 arcticdb/authorization/permissions.py
+-rw-rw-rw-  2.0 fat     8935 b- defN 23-Jun-09 17:17 arcticdb/proto/3/arcticc/pb2/config_pb2.py
+-rw-rw-rw-  2.0 fat    96954 b- defN 23-Jun-09 17:17 arcticdb/proto/3/arcticc/pb2/descriptors_pb2.py
+-rw-rw-rw-  2.0 fat    28646 b- defN 23-Jun-09 17:17 arcticdb/proto/3/arcticc/pb2/encoding_pb2.py
+-rw-rw-rw-  2.0 fat    13225 b- defN 23-Jun-09 17:17 arcticdb/proto/3/arcticc/pb2/generation_pb2.py
+-rw-rw-rw-  2.0 fat     1990 b- defN 23-Jun-09 17:17 arcticdb/proto/3/arcticc/pb2/in_memory_storage_pb2.py
+-rw-rw-rw-  2.0 fat     4634 b- defN 23-Jun-09 17:17 arcticdb/proto/3/arcticc/pb2/lmdb_storage_pb2.py
+-rw-rw-rw-  2.0 fat    24224 b- defN 23-Jun-09 17:17 arcticdb/proto/3/arcticc/pb2/logger_pb2.py
+-rw-rw-rw-  2.0 fat     2931 b- defN 23-Jun-09 17:17 arcticdb/proto/3/arcticc/pb2/mongo_storage_pb2.py
+-rw-rw-rw-  2.0 fat     6878 b- defN 23-Jun-09 17:17 arcticdb/proto/3/arcticc/pb2/nfs_backed_storage_pb2.py
+-rw-rw-rw-  2.0 fat     3821 b- defN 23-Jun-09 17:17 arcticdb/proto/3/arcticc/pb2/processors_pb2.py
+-rw-rw-rw-  2.0 fat    49227 b- defN 23-Jun-09 17:17 arcticdb/proto/3/arcticc/pb2/request_pb2.py
+-rw-rw-rw-  2.0 fat     6729 b- defN 23-Jun-09 17:17 arcticdb/proto/3/arcticc/pb2/s3_storage_pb2.py
+-rw-rw-rw-  2.0 fat    67292 b- defN 23-Jun-09 17:17 arcticdb/proto/3/arcticc/pb2/storage_pb2.py
+-rw-rw-rw-  2.0 fat    12621 b- defN 23-Jun-09 17:17 arcticdb/proto/3/arcticc/pb2/utils_pb2.py
+-rw-rw-rw-  2.0 fat     2298 b- defN 23-Jun-09 17:17 arcticdb/proto/4/arcticc/pb2/config_pb2.py
+-rw-rw-rw-  2.0 fat    14870 b- defN 23-Jun-09 17:17 arcticdb/proto/4/arcticc/pb2/descriptors_pb2.py
+-rw-rw-rw-  2.0 fat     4895 b- defN 23-Jun-09 17:17 arcticdb/proto/4/arcticc/pb2/encoding_pb2.py
+-rw-rw-rw-  2.0 fat     2998 b- defN 23-Jun-09 17:17 arcticdb/proto/4/arcticc/pb2/generation_pb2.py
+-rw-rw-rw-  2.0 fat     1038 b- defN 23-Jun-09 17:17 arcticdb/proto/4/arcticc/pb2/in_memory_storage_pb2.py
+-rw-rw-rw-  2.0 fat     1367 b- defN 23-Jun-09 17:17 arcticdb/proto/4/arcticc/pb2/lmdb_storage_pb2.py
+-rw-rw-rw-  2.0 fat     4114 b- defN 23-Jun-09 17:17 arcticdb/proto/4/arcticc/pb2/logger_pb2.py
+-rw-rw-rw-  2.0 fat     1195 b- defN 23-Jun-09 17:17 arcticdb/proto/4/arcticc/pb2/mongo_storage_pb2.py
+-rw-rw-rw-  2.0 fat     1517 b- defN 23-Jun-09 17:17 arcticdb/proto/4/arcticc/pb2/nfs_backed_storage_pb2.py
+-rw-rw-rw-  2.0 fat     1259 b- defN 23-Jun-09 17:17 arcticdb/proto/4/arcticc/pb2/processors_pb2.py
+-rw-rw-rw-  2.0 fat     6801 b- defN 23-Jun-09 17:17 arcticdb/proto/4/arcticc/pb2/request_pb2.py
+-rw-rw-rw-  2.0 fat     1496 b- defN 23-Jun-09 17:17 arcticdb/proto/4/arcticc/pb2/s3_storage_pb2.py
+-rw-rw-rw-  2.0 fat     9907 b- defN 23-Jun-09 17:17 arcticdb/proto/4/arcticc/pb2/storage_pb2.py
+-rw-rw-rw-  2.0 fat     2310 b- defN 23-Jun-09 17:17 arcticdb/proto/4/arcticc/pb2/utils_pb2.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-09 17:05 arcticdb/toolbox/__init__.py
+-rw-rw-rw-  2.0 fat     6102 b- defN 23-Jun-09 17:05 arcticdb/toolbox/library_tool.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-09 17:05 arcticdb/util/__init__.py
+-rw-rw-rw-  2.0 fat     2705 b- defN 23-Jun-09 17:05 arcticdb/util/errors.py
+-rw-rw-rw-  2.0 fat     8775 b- defN 23-Jun-09 17:05 arcticdb/util/hypothesis.py
+-rw-rw-rw-  2.0 fat      719 b- defN 23-Jun-09 17:05 arcticdb/util/memory.py
+-rw-rw-rw-  2.0 fat     6013 b- defN 23-Jun-09 17:05 arcticdb/util/tasks.py
+-rw-rw-rw-  2.0 fat    18251 b- defN 23-Jun-09 17:05 arcticdb/util/test.py
+-rw-rw-rw-  2.0 fat      131 b- defN 23-Jun-09 17:05 arcticdb/version_store/__init__.py
+-rw-rw-rw-  2.0 fat     7189 b- defN 23-Jun-09 17:05 arcticdb/version_store/_common.py
+-rw-rw-rw-  2.0 fat     4193 b- defN 23-Jun-09 17:05 arcticdb/version_store/_custom_normalizers.py
+-rw-rw-rw-  2.0 fat    52659 b- defN 23-Jun-09 17:05 arcticdb/version_store/_normalization.py
+-rw-rw-rw-  2.0 fat   113749 b- defN 23-Jun-09 17:05 arcticdb/version_store/_store.py
+-rw-rw-rw-  2.0 fat    11040 b- defN 23-Jun-09 17:05 arcticdb/version_store/helper.py
+-rw-rw-rw-  2.0 fat    59441 b- defN 23-Jun-09 17:05 arcticdb/version_store/library.py
+-rw-rw-rw-  2.0 fat    24581 b- defN 23-Jun-09 17:05 arcticdb/version_store/processing.py
+-rw-rw-rw-  2.0 fat      603 b- defN 23-Jun-09 17:05 arcticdb/version_store/read_result.py
+-rw-rw-rw-  2.0 fat     4851 b- defN 23-Jun-09 17:26 arcticdb-1.3.0.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat     8449 b- defN 23-Jun-09 17:26 arcticdb-1.3.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat    18186 b- defN 23-Jun-09 17:26 arcticdb-1.3.0.dist-info/NOTICE.txt
+-rw-rw-rw-  2.0 fat      100 b- defN 23-Jun-09 17:26 arcticdb-1.3.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       30 b- defN 23-Jun-09 17:26 arcticdb-1.3.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     6529 b- defN 23-Jun-09 17:26 arcticdb-1.3.0.dist-info/RECORD
+71 files, 22857741 bytes uncompressed, 6187035 bytes compressed:  72.9%
```

## zipnote {}

```diff
@@ -189,26 +189,26 @@
 
 Filename: arcticdb/version_store/processing.py
 Comment: 
 
 Filename: arcticdb/version_store/read_result.py
 Comment: 
 
-Filename: arcticdb-1.2.1.dist-info/LICENSE.txt
+Filename: arcticdb-1.3.0.dist-info/LICENSE.txt
 Comment: 
 
-Filename: arcticdb-1.2.1.dist-info/METADATA
+Filename: arcticdb-1.3.0.dist-info/METADATA
 Comment: 
 
-Filename: arcticdb-1.2.1.dist-info/NOTICE.txt
+Filename: arcticdb-1.3.0.dist-info/NOTICE.txt
 Comment: 
 
-Filename: arcticdb-1.2.1.dist-info/WHEEL
+Filename: arcticdb-1.3.0.dist-info/WHEEL
 Comment: 
 
-Filename: arcticdb-1.2.1.dist-info/top_level.txt
+Filename: arcticdb-1.3.0.dist-info/top_level.txt
 Comment: 
 
-Filename: arcticdb-1.2.1.dist-info/RECORD
+Filename: arcticdb-1.3.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## arcticc/__init__.py

```diff
@@ -1 +1 @@
-__path__ = __import__('pkgutil').extend_path(__path__, __name__)
+__path__ = __import__("pkgutil").extend_path(__path__, __name__)
```

## arcticdb/__init__.py

```diff
@@ -7,9 +7,8 @@
 from arcticdb.version_store.processing import QueryBuilder
 import arcticdb.version_store.library as library
 from arcticdb.options import LibraryOptions
 from arcticdb.tools import set_config_from_env_vars
 
 set_config_from_env_vars(_os.environ)
 
-__version__ = "1.2.1"
-
+__version__ = "1.3.0"
```

## arcticdb/arctic.py

```diff
@@ -86,14 +86,15 @@
         else:
             raise ValueError(
                 f"Invalid URI specified. Please see URI format specification for available formats. uri={uri}"
             )
 
         self._library_adapter = _cls(uri)
         self._library_manager = LibraryManager(self._library_adapter.config_library)
+        self._uri = uri
 
     def __getitem__(self, name: str) -> Library:
         lib = NativeVersionStore(
             self._library_manager.get_library(name),
             repr(self._library_adapter),
             lib_cfg=self._library_manager.get_library_config(name),
         )
@@ -186,7 +187,22 @@
         ['test.library']
 
         Returns
         -------
         A list of all library names that exist in this Arctic instance.
         """
         return self._library_manager.list_libraries()
+
+    def get_uri(self) -> str:
+        """
+        Returns the URI that was used to create the Arctic instance.
+
+        Examples
+        --------
+        >>> arctic = Arctic('s3://MY_ENDPOINT:MY_BUCKET')
+        >>> arctic.get_uri()
+
+        Returns
+        -------
+        s3://MY_ENDPOINT:MY_BUCKET
+        """
+        return self._uri
```

## arcticdb/config.py

```diff
@@ -18,15 +18,15 @@
     from yaml import Loader
 
 from arcticc.pb2.logger_pb2 import LoggersConfig, LoggerConfig
 from arcticc.pb2.config_pb2 import RuntimeConfig
 from arcticc.pb2.storage_pb2 import EnvironmentConfigsMap, EnvironmentConfig, LibraryConfig, LibraryDescriptor
 from google.protobuf.json_format import MessageToJson, Parse as JsonToMessage
 from google.protobuf.message import Message
-from typing import AnyStr, Optional
+from typing import AnyStr, Optional, Dict
 
 from arcticdb.exceptions import ArcticNativeException
 from arcticdb.log import logger_by_name, configure
 
 _HOME = osp.expanduser("~/.arctic/native")
 
 EnvName = AnyStr
@@ -167,64 +167,54 @@
 
 def save_runtime_config(config=None, path=Defaults.RUNTIME_CONF_FILE_PATH):
     # type: (Optional[RuntimeConfig], Optional[FilePath])->None
     config = config if config is not None else default_runtime_config()
     _save_config(config, path, RuntimeConfig)
 
 
-def set_log_level(default_level=Defaults.DEFAULT_LOG_LEVEL, specific_log_levels=None):
+def make_loggers_config(default_level=Defaults.DEFAULT_LOG_LEVEL, specific_log_levels: Optional[Dict[str, str]] = None):
     """
-    The possible lognames are:
-    codec, inmem, root, storage, version, memory, timings
+    Generate a ``LoggersConfig`` object with sink set to stderr and the given log levels.
 
-    :param default_level: Default loglevel for all the lognames.
-    :param specific_log_levels: can be used to override the default loglevel for a specific logname.
-    eg. set_log_level("INFO", {'version': "DEBUG", 'storage': "DEBUG"})
-    :return:
+    Parameters
+    ----------
+    default_level
+        Default log level for all the loggers unless overriden with specific_log_levels.
+        Valid values are "DEBUG", "INFO", "WARN", "ERROR".
+    specific_log_levels
+        Optional overrides for specific logger(s). The possible logger names can be found in log.py.
+
+    Examples
+    --------
+    >>> make_loggers_config("INFO", {'version': "DEBUG", 'storage': "DEBUG"})
     """
     log_cfgs = LoggersConfig()
     specific_log_levels = {} if not specific_log_levels else specific_log_levels
 
-    sink = log_cfgs.sink_by_id["file"]
-    sink.daily_file.path = osp.join(Defaults.LOG_DIR, "arcticc.daily.log")
     sink = log_cfgs.sink_by_id["console"]
     sink.console.std_err = True
 
-    for logger_name in logger_by_name.keys():
+    for logger_name in logger_by_name:
         level_to_set = specific_log_levels.get(logger_name, default_level)
         logger = log_cfgs.logger_by_id[logger_name]
-        logger.sink_ids.append("file")
-        logger.sink_ids.append("console")
         logger.sink_ids.append("console")
         logger.level = getattr(LoggerConfig, level_to_set)
-        logger.pattern = "%Y%m%d_%H%M%S.%f %t %L %n %P | %v".format(
-            "arcticc.{}".format(logger_name) if logger_name != "root" else "arcticc"
-        )
 
-    return configure(log_cfgs, force=True)
+    return log_cfgs
 
 
-def default_loggers_config():
-    # type: ()->LoggersConfig
-    log_cfgs = LoggersConfig()
+def set_log_level(default_level=Defaults.DEFAULT_LOG_LEVEL, specific_log_levels=None):
+    """
+    Passes the arguments to ``make_loggers_config`` and then configures the loggers, overwriting any existing config.
+    """
+    return configure(make_loggers_config(default_level, specific_log_levels), force=True)
 
-    sink = log_cfgs.sink_by_id["file"]
-    sink.daily_file.path = osp.join(Defaults.LOG_DIR, "arcticc.daily.log")
-    sink = log_cfgs.sink_by_id["console"]
-    sink.console.std_err = True
 
-    for n in logger_by_name.keys():
-        logger = log_cfgs.logger_by_id[n]
-        logger.sink_ids.append("file")
-        logger.sink_ids.append("console")
-        logger.level = LoggerConfig.INFO
-        logger.pattern = "%Y%m%d_%H%M%S.%f %t %L %n %P | %v".format(
-            "arcticc.{}".format(n) if n != "root" else "arcticc"
-        )
-    return log_cfgs
+def default_loggers_config():
+    return make_loggers_config("INFO")
 
 
 def default_runtime_config():
     # type: ()->RuntimeConfig
     runtime_cfg = RuntimeConfig()
     runtime_cfg.int_values["Store.NumThreads"] = 16
     return runtime_cfg
```

## arcticdb/flattener.py

```diff
@@ -45,15 +45,15 @@
             convert = symbol.encode("utf-8")
         except AttributeError:
             convert = symbol
 
         tokens = symbol.split(self.SEPARATOR)
         vaguely_readable_name = "_".join([token[-3:] for token in tokens])[: (self.MAX_KEY_LENGTH - hash_length)]
 
-        shortened_hash = str(int(hashlib.sha256(convert).hexdigest(), 16) % 10 ** hash_length)
+        shortened_hash = str(int(hashlib.sha256(convert).hexdigest(), 16) % 10**hash_length)
         return "{}_{}".format(vaguely_readable_name, shortened_hash)
 
     def can_flatten(self, item):
         if self.is_named_tuple(item):
             # Tricky to unwrap namedtuples with our current method. Just pickle this.
             return False
         return self.is_sequence_like(item) or self.is_dict_like(item) or self.is_normalizable_to_nested_structure(item)
```

## arcticdb/log.py

```diff
@@ -3,25 +3,18 @@
 
 Use of this software is governed by the Business Source License 1.1 included in the file licenses/BSL.txt.
 
 As of the Change Date specified in that file, in accordance with the Business Source License, use of this software will be governed by the Apache License, version 2.0.
 """
 import traceback
 
-from arcticc.pb2.logger_pb2 import LoggersConfig
-from arcticdb_ext.log import configure as _configure
+from arcticdb_ext.log import configure
 from arcticdb_ext.log import log as _log, LogLevel as _Lvl, LoggerId as _LoggerId, is_active as _is_active
 
 
-def configure(pb_conf, force=False):
-    # type: (LoggersConfig, Optional[bool])->None
-    # necessary since default param are not supported by binding
-    return _configure(pb_conf, force)
-
-
 class _Logger(object):
     def __init__(self, id):
         self._id = id
 
     def log(self, lvl, msg, *args, **kwargs):
         if not _is_active(self._id, lvl):
             return
```

## arcticdb/options.py

```diff
@@ -40,17 +40,17 @@
             modified by successive updates or appends. Each successive update or append must contain the same column set
             in the same order with the same types as the initial write.
 
             When disabled, ArcticDB will tile stored data across both the rows and columns. This enables highly efficient
             retrieval of specific columns regardless of the total number of columns stored in the symbol.
 
             If True, then updates and appends can contain columns not originally seen in the
-            most recent write call. The data will be dynamically backfilled on read when required for the new columns. 
-            Furthermore, Arctic will support numeric type promotions should the type of a column change - for example, 
-            should column A be of type int32 on write, and of type float on the next append, the column will be returned as 
+            most recent write call. The data will be dynamically backfilled on read when required for the new columns.
+            Furthermore, Arctic will support numeric type promotions should the type of a column change - for example,
+            should column A be of type int32 on write, and of type float on the next append, the column will be returned as
             a float to Pandas on read. Supported promotions include (narrow) integer to (wider) integer, and integer to float.
 
             When enabled, ArcticDB will only tile across the rows of the data. This will result in slower column
             subsetting when storing a large number of columns (>1,000).
 
         dedup: bool, default False
             Controls whether calls to write and write_batch will attempt to deduplicate data segments against the
@@ -106,8 +106,11 @@
         """
         self.dynamic_schema = dynamic_schema
         self.dedup = dedup
         self.rows_per_segment = rows_per_segment
         self.columns_per_segment = columns_per_segment
 
     def __repr__(self):
-        return f"LibraryOptions(dynamic_schema={self.dynamic_schema}, dedup={self.dedup}, rows_per_segment={self.rows_per_segment}, columns_per_segment={self.columns_per_segment})"
+        return (
+            f"LibraryOptions(dynamic_schema={self.dynamic_schema}, dedup={self.dedup},"
+            f" rows_per_segment={self.rows_per_segment}, columns_per_segment={self.columns_per_segment})"
+        )
```

## arcticdb/adapters/s3_library_adapter.py

```diff
@@ -105,15 +105,15 @@
 
         for key in parsed_query.keys():
             field_dict = {field.name: field for field in fields(ParsedQuery)}
             if key not in field_dict.keys():
                 raise ValueError(
                     "Invalid S3 URI. "
                     f"Invalid query parameter '{key}' passed in. "
-                    f"Value query parameters: "
+                    "Value query parameters: "
                     f"{list(field_dict.keys())}"
                 )
 
             if field_dict[key].type == bool:
                 parsed_query[key] = bool(strtobool(parsed_query[key][0]))
 
         if parsed_query.get("path_prefix"):
```

## arcticdb/toolbox/library_tool.py

```diff
@@ -1,31 +1,47 @@
 """
 Copyright 2023 Man Group Operations Limited
 Use of this software is governed by the Business Source License 1.1 included in the file licenses/BSL.txt.
 As of the Change Date specified in that file, in accordance with the Business Source License, use of this software will be governed by the Apache License, version 2.0.
 """
-from typing import Optional, Union, List
-
+from typing import Optional, Union, List, Dict, Any
 import pandas as pd
 
 from arcticdb.version_store._normalization import FrameData
-from arcticdb_ext.codec import Segment, decode_segment
-from arcticdb_ext.storage import KeyType, Library
+from arcticdb_ext.codec import decode_segment
+from arcticdb_ext.storage import KeyType
 from arcticdb_ext.stream import SegmentInMemory
 from arcticdb_ext.tools import LibraryTool as LibraryToolImpl
 from arcticdb_ext.version_store import AtomKey, PythonOutputFrame, RefKey
 
+VariantKey = Union[AtomKey, RefKey]
+
+_KEY_PROPERTIES = {
+    key_type: {k: v for k, v in vars(key_type).items() if isinstance(v, property)} for key_type in (AtomKey, RefKey)
+}
+
+
+def key_to_props_dict(key: VariantKey) -> Dict[str, Any]:
+    return {k: v.fget(key) for k, v in _KEY_PROPERTIES[type(key)].items()}
+
+
+def props_dict_to_atom_key(d: Dict[str, Any]) -> AtomKey:
+    args = tuple(d[k] for k in _KEY_PROPERTIES[AtomKey])
+    return AtomKey(*args)
+
 
 class LibraryTool(LibraryToolImpl):
     @staticmethod
     def key_types() -> List[KeyType]:
         return list(KeyType.__members__.values())
 
     @staticmethod
-    def dataframe_to_keys(df: pd.DataFrame, id: Union[str, int], filter_key_type: Optional[KeyType] = None) -> List[AtomKey]:
+    def dataframe_to_keys(
+        df: pd.DataFrame, id: Union[str, int], filter_key_type: Optional[KeyType] = None
+    ) -> List[AtomKey]:
         keys = []
         for index, row in df.iterrows():
             key_type = KeyType(row["key_type"])
             if filter_key_type is None or key_type == filter_key_type:
                 keys.append(
                     AtomKey(
                         id,
@@ -39,25 +55,25 @@
                 )
 
         return keys
 
     def find_keys_for_symbol(self, key_type: KeyType, id: Union[str, int]) -> Union[List[AtomKey], List[RefKey]]:
         return self.find_keys_for_id(key_type, id)
 
-    def read_to_segment_in_memory(self, key: Union[AtomKey, RefKey]) -> SegmentInMemory:
+    def read_to_segment_in_memory(self, key: VariantKey) -> SegmentInMemory:
         return decode_segment(self.read_to_segment(key))
 
-    def read_to_dataframe(self, key: Union[AtomKey, RefKey]) -> pd.DataFrame:
+    def read_to_dataframe(self, key: VariantKey) -> pd.DataFrame:
         """
         Reads the segment associated with the provided key into a Pandas DataFrame format. Any strings in the segment
         are replaced with Nones.
 
         Parameters
         ----------
-        key : Union[AtomKey, RefKey]
+        key : VariantKey
            The key in storage to read.
 
         Returns
         -------
         pandas.DataFrame
             Pandas DataFrame representing the information contained in the segment associated with the given key.
 
@@ -73,22 +89,24 @@
         field_names = [f.name for f in segment.header.stream_descriptor.fields]
         frame_data = FrameData.from_cpp(PythonOutputFrame(decode_segment(segment)))
         cols = {}
         for idx, field_name in enumerate(field_names):
             cols[field_name] = frame_data.data[idx]
         return pd.DataFrame(cols, columns=field_names)
 
-    def read_to_keys(self, key: Union[AtomKey, RefKey], id: Optional[Union[str, int]] = None, filter_key_type: Optional[KeyType] = None) -> List[AtomKey]:
+    def read_to_keys(
+        self, key: VariantKey, id: Optional[Union[str, int]] = None, filter_key_type: Optional[KeyType] = None
+    ) -> List[AtomKey]:
         """
         Reads the segment associated with the provided key into a Pandas DataFrame format, and then converts each row in
         this DataFrame into an AtomKey if all the necessary columns are present.
 
         Parameters
         ----------
-        key : Union[AtomKey, RefKey]
+        key : VariantKey
            The key in storage to read.
 
         id: Optional[Union[str, int]], default=None
             As string symbol names are not read into the DataFrame format, they cannot be processed automaticallyv from
             the stream_id column, and must be provided. If omitted, the id from the key argument will be used.
 
         filter_key_type: Optional[KeyType], default=None
```

## arcticdb/util/hypothesis.py

```diff
@@ -60,22 +60,25 @@
     return non_infinite(x) and x != 0
 
 
 @st.composite
 def integral_type_strategies(draw):
     return draw(from_dtype(draw(st.one_of([unsigned_integer_dtypes(), integer_dtypes()]))).filter(non_infinite))
 
+
 @st.composite
 def signed_integral_type_strategies(draw):
     return draw(from_dtype(draw(st.one_of([integer_dtypes()]))).filter(non_infinite))
 
+
 @st.composite
 def unsigned_integral_type_strategies(draw):
     return draw(from_dtype(draw(st.one_of([unsigned_integer_dtypes()]))).filter(non_infinite))
 
+
 @st.composite
 def dataframes_with_names_and_dtypes(draw, names, dtype_strategy):
     cols = [hs_pd.column(name, dtype=draw(dtype_strategy)) for name in names]
     return draw(hs_pd.data_frames(cols, index=hs_pd.range_indexes()))
 
 
 @st.composite
```

## arcticdb/util/tasks.py

```diff
@@ -1,21 +1,15 @@
 """
 Copyright 2023 Man Group Operations Limited
 
 Use of this software is governed by the Business Source License 1.1 included in the file licenses/BSL.txt.
 
 As of the Change Date specified in that file, in accordance with the Business Source License, use of this software will be governed by the Apache License, version 2.0.
 """
-from arcticdb.util.test import (
-    assert_frame_equal,
-    sample_dataframe,
-    get_wide_dataframe,
-    get_pickle,
-    random_integers,
-)
+from arcticdb.util.test import assert_frame_equal, sample_dataframe, get_wide_dataframe, get_pickle, random_integers
 
 import pandas as pd
 import random
 import numpy as np
 from datetime import datetime
 
 LARGE_DF_SIZE = 100000
@@ -83,14 +77,15 @@
         lib.delete_snapshot(snapshot_name)
 
 
 def read_write_sample(lib, symbol):
     lib.write(symbol, "blah")
     assert lib.read(symbol).data == "blah"
 
+
 def get_int_col_dataframe(size=10000, seed=0):
     np.random.seed(seed)
     return pd.DataFrame({"uint32": random_integers(size, np.uint32)})
 
 
 def write_and_prune_simple_df(lib, symbol):
     df = get_int_col_dataframe(10000)
```

## arcticdb/util/test.py

```diff
@@ -27,14 +27,15 @@
 from arcticdb.version_store.helper import ArcticcFileConfig
 from arcticdb.config import _DEFAULT_ENVS_PATH
 from arcticdb_ext import set_config_int, get_config_int, unset_config_int
 
 
 PANDAS_VERSION = version.parse(pd.__version__)
 CHECK_FREQ_VERSION = version.Version("1.1")
+IS_PANDAS_ZERO = PANDAS_VERSION < version.Version("1.0")
 
 
 def maybe_not_check_freq(f):
     """Ignore frequency when pandas is newer as starts to check frequency which it did not previously do."""
 
     @wraps(f)
     def wrapper(*args, **kwargs):
@@ -153,14 +154,15 @@
             set_config_int(name, initial)
         else:
             unset_config_int(name)
 
 
 def get_artifact_path(frag, *fragments):
     import tests
+
     return os.path.join(tests.__path__, "artifacts", frag, *fragments)
 
 
 def param_dict(fields, cases=None, xfail=None, py2only=None, py3only=None):
     # type: (List[AnyStr], Mapping[AnyStr, Any], Mapping[AnyStr, Any], Mapping[AnyStr, Any], Mapping[AnyStr, Any])->Any
     _cases = deepcopy(cases) if cases else dict()
     if PY3:
@@ -468,15 +470,15 @@
         df_slice = df.iloc[step : step + rows_per_slice]
         col_to_drop_i = (column_index + 1) % num_cols
         if col_to_drop_i != 0:
             col_to_drop = cols[col_to_drop_i]
             df_slice = df_slice.drop(columns=[col_to_drop])
         column_index += 1
         slices.append(df_slice)
-    
+
     expected = pd.concat(slices)
     return expected, slices
 
 
 def regularize_dataframe(df):
     output = df.copy(deep=True)
     for col in output.select_dtypes(include=["object"]).columns:
```

## arcticdb/version_store/_common.py

```diff
@@ -16,15 +16,14 @@
 from typing import NamedTuple, List, AnyStr
 
 if PY3:
 
     def _stringify(v):
         return str(v, "utf-8")
 
-
 else:
 
     def _stringify(v):
         return str(v)
 
 
 _NS_DTYPE = np.dtype("datetime64[ns]")
@@ -43,19 +42,19 @@
             raise ValueError(
                 "Columns names and columns values must have the same size, actual {} != {}".format(
                     len(columns_names), len(columns_values)
                 )
             )
         if not all(times.shape[0] == cv.shape[0] for cv in columns_values):
             s = np.array([cv.shape[0] for cv in columns_values])
-            raise ValueError(
-                "Inconsistent size of column values. times.shape[0]={} must match cv.shape[0] for all column values. actual={}".format(
-                    times.shape[0], s
-                )
-            )
+            error_message = (
+                "Inconsistent size of column values. times.shape[0]={} must match cv.shape[0] for all column values."
+                " actual={}"
+            ).format(times.shape[0], s)
+            raise ValueError(error_message)
         return tuple.__new__(cls, (times, columns_names, columns_values))
 
     class _IlocProxy(object):
         def __init__(self, tf):
             self._tf = tf
 
         def __getitem__(self, item):
@@ -65,15 +64,16 @@
     def iloc(self):
         return TimeFrame._IlocProxy(self)
 
     def _iloc(self, item, resolve_slice=None):
         if isinstance(item, tuple):
             if len(item) != 2:
                 raise ValueError(
-                    "Only support 2 dimensional indexing where dimension 0 is the row indexing and dimension 1 is column one"
+                    "Only support 2 dimensional indexing where dimension 0 is the row indexing and dimension 1 is"
+                    " column one"
                 )
             col_filter = item[1]
             item = item[0]
             if isinstance(col_filter, slice):
                 dst = TimeFrame(self.times, self.columns_names[col_filter], self.columns_values[col_filter])
             elif isinstance(col_filter, int):
                 dst = TimeFrame(self.times, [self.columns_names[col_filter]], [self.columns_values[col_filter]])
```

## arcticdb/version_store/_normalization.py

```diff
@@ -50,14 +50,15 @@
     assert pd.__version__.startswith(
         "0"
     ), "is_utc not present in this Pandas - has it been changed in latest Pandas release?"
 
     def check_is_utc_if_newer_pandas(*args, **kwargs):
         return False  # the UTC specific issue is not present in old Pandas so no need to go down special case
 
+
 log = version
 
 from msgpack import packb, unpackb, pack, ExtType
 
 
 IS_WINDOWS = sys.platform == "win32"
 
@@ -174,17 +175,16 @@
         # not an object type numpy column and not going to later be
         # coerced to an object type column - does not require conversion to a primitive type.
         return arr
 
     if len(arr) == 0:
         if coerce_column_type is None:
             raise ArcticNativeNotYetImplemented(
-                "coercing column type is required when empty column of object type, Column type={} for column={}".format(
-                    arr.dtype, arr_name
-                )
+                "coercing column type is required when empty column of object type, Column type={} for column={}"
+                .format(arr.dtype, arr_name)
             )
         else:
             return arr.astype(coerce_column_type)
 
     # Coerce column allows us to force a column to the given type, which means we can skip expensive iterations in
     # Python with the caveat that if the user gave an invalid type it's going to blow up in the core.
     if coerce_column_type and (coerce_column_type == float or isinstance(coerce_column_type(), np.floating)):
@@ -836,15 +836,17 @@
 
 class MsgPackNormalizer(Normalizer):
     """
     Fall back plan for the time being to store arbitrary data
     """
 
     MSG_PACK_MAX_SIZE = (1 << 32) + 1024
-    MMAP_DEFAULT_SIZE = MSG_PACK_MAX_SIZE  # Allow up to 4 gib pickles in msgpack by default, most of these compress fairly well.
+    MMAP_DEFAULT_SIZE = (
+        MSG_PACK_MAX_SIZE  # Allow up to 4 gib pickles in msgpack by default, most of these compress fairly well.
+    )
     # msgpack checks whether the size of pickled data within 1 << 32 - 1 byte only
     # Extra memory is needed in mmap for msgpack's overhead
 
     def __init__(self, cfg=None):
         self._size = MsgPackNormalizer.MMAP_DEFAULT_SIZE if cfg is None else cfg.max_blob_size
         self.MSG_PACK_MAX_SIZE = self._size  # Override with the max_pickle size if set in config.
         self.strict_mode = cfg.strict_mode if cfg is not None else False
@@ -1147,18 +1149,21 @@
             )
         except Exception as ex:
             log.debug("Could not normalize item of type: {} with the default normalizer due to {}", type(item), ex)
             if pickle_on_failure:
                 log.debug("pickle_on_failure flag set, normalizing the item with MsgPackNormalizer", type(item), ex)
                 return self.fallback_normalizer.normalize(item)
             # Could not normalize with the default handler, pickle_on_failure
-            log.error(
+            error_message = (
                 "Could not normalize item of type: {} with any normalizer."
                 "You can set pickle_on_failure param to force pickling of this object instead."
-                "(Note: Pickling has worse performance and stricter memory limitations)",
+                "(Note: Pickling has worse performance and stricter memory limitations)"
+            )
+            log.error(
+                error_message,
                 type(item),
                 ex,
             )
             raise
 
     def denormalize(self, item, norm_meta):
         # type: (_FrameData, NormalizationMetadata)->_SUPPORTED_TYPES
@@ -1272,16 +1277,15 @@
                 )
             )
 
         v = Timestamp(v)
 
         if v.tzinfo is None:
             log.debug(
-                "DateRange bounds do not have timestamps, will default to UTC for the query,"
-                f"DateRange.{bound_name}={v}"
+                f"DateRange bounds do not have timestamps, will default to UTC for the query,DateRange.{bound_name}={v}"
             )
             v = v.tz_localize("UTC")
 
         return v
 
     if getattr(dtr, "startopen", False) or getattr(dtr, "endopen", False):
         raise ValueError("Only supports closed/closed date range. Actual:{}".format(dtr))
```

## arcticdb/version_store/_store.py

```diff
@@ -76,15 +76,15 @@
 
         _ExtDateRangeTypes = Union[_ExtDateRangeTypes, arctic.date.DateRange]
     except ModuleNotFoundError:
         pass
 
 # These chars are encoded by S3 and on doing a list_symbols they will show up as the encoded form eg. &amp
 UNSUPPORTED_S3_CHARS = {"*", "&", "<", ">"}
-MAX_SYMBOL_SIZE = (2 ** 8) - 1
+MAX_SYMBOL_SIZE = (2**8) - 1
 
 
 TimeSeriesType = Union[pd.DataFrame, pd.Series]
 
 
 IS_WINDOWS = sys.platform == "win32"
 
@@ -153,16 +153,17 @@
                 if dtype.name == "category":
                     categorical_columns.append(column_name)
         else:
             # Series
             if data.dtype.name == "category":
                 categorical_columns.append(data.name)
         if len(categorical_columns) > 0:
-            message = "Symbol: {}\nDataFrame/Series contains categorical data, cannot append or update\nCategorical columns: {}".format(
-                symbol, categorical_columns
+            message = (
+                "Symbol: {}\nDataFrame/Series contains categorical data, cannot append or update\nCategorical"
+                " columns: {}".format(symbol, categorical_columns)
             )
             if throw:
                 raise ArcticNativeNotYetImplemented(message)
             else:
                 log.warn(message)
 
 
@@ -482,16 +483,16 @@
         metadata : `Optional[Any]`, default=None
             Optional metadata to persist along with the symbol.
         prune_previous_version : `bool`, default=True
             Removes previous (non-snapshotted) versions from the database.
         pickle_on_failure: `bool`, default=False
             Pickle `data` if it can't be normalized.
         validate_index: bool, default=False
-            If True, will verify that the index of `data` supports date range searches and update operations. This in effect tests that the data is sorted in ascending order. 
-            ArcticDB relies on Pandas to detect if data is sorted - you can call DataFrame.index.is_monotonic_increasing on your input DataFrame to see if Pandas believes the 
+            If True, will verify that the index of `data` supports date range searches and update operations. This in effect tests that the data is sorted in ascending order.
+            ArcticDB relies on Pandas to detect if data is sorted - you can call DataFrame.index.is_monotonic_increasing on your input DataFrame to see if Pandas believes the
             data to be sorted
         kwargs :
             passed through to the write handler
 
         Returns
         -------
         Optional[VersionedItem]
@@ -623,16 +624,16 @@
             Data to be appended.
         metadata : `Optional[Any]`, default=None
             Optional metadata to persist along with the new symbol version. Note that the metadata is
             not combined in any way with the metadata stored in the previous version.
         prune_previous_version
             Removes previous (non-snapshotted) versions from the database.
         validate_index: bool, default=False
-            If True, will verify that resulting symbol will support date range searches and update operations. This in effect tests that the previous version of the 
-            data and `data` are both sorted in ascending order. ArcticDB relies on Pandas to detect if data is sorted - you can call DataFrame.index.is_monotonic_increasing 
+            If True, will verify that resulting symbol will support date range searches and update operations. This in effect tests that the previous version of the
+            data and `data` are both sorted in ascending order. ArcticDB relies on Pandas to detect if data is sorted - you can call DataFrame.index.is_monotonic_increasing
             on your input DataFrame to see if Pandas believes the data to be sorted
         kwargs :
             passed through to the write handler
 
         Returns
         -------
         Optional[VersionedItem]
@@ -796,18 +797,15 @@
                 version=vit.version,
                 metadata=metadata,
                 data=None,
                 host=self.env,
             )
 
     def create_column_stats(
-            self,
-            symbol: str,
-            column_stats: Dict[str, Set[str]],
-            as_of: VersionQueryInput = None,
+        self, symbol: str, column_stats: Dict[str, Set[str]], as_of: VersionQueryInput = None
     ) -> None:
         """
         Calculates the specified column statistics for each row-slice for the given symbol. In the future, these
         statistics will be used by `QueryBuilder` filtering operations to reduce the number of data segments read out
         of storage.
 
         Parameters
@@ -830,18 +828,15 @@
         None
         """
         column_stats = self._get_column_stats(column_stats)
         version_query = self._get_version_query(as_of)
         self.version_store.create_column_stats_version(symbol, column_stats, version_query)
 
     def drop_column_stats(
-            self,
-            symbol: str,
-            column_stats: Optional[Dict[str, Set[str]]] = None,
-            as_of: VersionQueryInput = None,
+        self, symbol: str, column_stats: Optional[Dict[str, Set[str]]] = None, as_of: VersionQueryInput = None
     ) -> None:
         """
         Deletes the specified column statistics for the given symbol.
 
         Parameters
         ----------
         symbol: `str`
@@ -1134,16 +1129,16 @@
             Metadata to be written.
             i-th entry corresponds to i-th element of `symbols`.
         prune_previous_version : `Optional[bool]`, default=None
             Remove previous versions from version list. Uses library default if left as None.
         pickle_on_failure : `Optional[bool]`, default=None
             Pickle results if normalization fails. Uses library default if left as None.
         validate_index: bool, default=False
-            If True, will verify for each entry in the batch hat the index of `data` supports date range searches and update operations. 
-            This in effect tests that the data is sorted in ascending order. ArcticDB relies on Pandas to detect if data is sorted - 
+            If True, will verify for each entry in the batch hat the index of `data` supports date range searches and update operations.
+            This in effect tests that the data is sorted in ascending order. ArcticDB relies on Pandas to detect if data is sorted -
             you can call DataFrame.index.is_monotonic_increasing on your input DataFrame to see if Pandas believes the data to be sorted
         kwargs :
             passed through to the write handler
 
         Examples
         --------
 
@@ -1153,15 +1148,15 @@
             )
 
         Returns
         -------
         List
             List of versioned items. The data attribute will be None for each versioned item.
             i-th entry corresponds to i-th element of `symbols`.
-            
+
         Raises
         ------
         UnsortedDataException
             If data is unsorted, when validate_index is set to True.
         """
         _check_batch_kwargs(NativeVersionStore.batch_write, NativeVersionStore.write, kwargs)
 
@@ -1266,16 +1261,16 @@
         metadata_vector : `Optional[List[Any]]`, default=None
             Metadata to be written. Note that the metadata is not combined in any way with the metadata stored in the
             previous version.
             i-th entry corresponds to i-th element of `symbols`.
         prune_previous_version : `Optional[bool]`, default=None
             Remove previous versions from version list. Uses library default if left as None.
         validate_index: bool, default=False
-            If True, will verify for each entry in the batch hat the index of `data` supports date range searches and update operations. 
-            This in effect tests that the data is sorted in ascending order. ArcticDB relies on Pandas to detect if data is sorted - 
+            If True, will verify for each entry in the batch hat the index of `data` supports date range searches and update operations.
+            This in effect tests that the data is sorted in ascending order. ArcticDB relies on Pandas to detect if data is sorted -
             you can call DataFrame.index.is_monotonic_increasing on your input DataFrame to see if Pandas believes the data to be sorted
         kwargs :
             passed through to the write handler
 
         Returns
         -------
         List
@@ -1744,15 +1739,14 @@
         index_columns = []
         for field_idx in range(last_index_column_idx):
             index_columns.append(stream_descriptor.fields[field_idx].name)
 
         return index_columns
 
     def _adapt_read_res(self, read_result: ReadResult) -> VersionedItem:
-
         frame_data = FrameData.from_cpp(read_result.frame_data)
 
         meta = denormalize_user_metadata(read_result.udm, self._normalizer)
         data = self._normalizer.denormalize(frame_data, read_result.norm)
         if read_result.norm.HasField("custom"):
             data = self._custom_normalizer.denormalize(data, read_result.norm.custom)
 
@@ -2521,31 +2515,31 @@
             Target for maximum no. of rows per segment, after compaction.
             If parameter is not provided, library option for segments's maximum row size will be used
 
         Notes
         ----------
         Config map setting - SymbolDataCompact.SegmentCount will be replaced by a library setting
         in the future. This API will allow overriding the setting as well.
-        
+
         Returns
         -------
         bool
         """
         return self.version_store.is_symbol_fragmented(symbol, segment_size)
 
     def defragment_symbol_data(self, symbol: str, segment_size: Optional[int] = None) -> VersionedItem:
         """
         Compacts fragmented segments by merging row-sliced segments (https://docs.arcticdb.io/technical/on_disk_storage/#data-layer).
-        This method calls `is_symbol_fragmented` to determine whether to proceed with the defragmentation operation. 
+        This method calls `is_symbol_fragmented` to determine whether to proceed with the defragmentation operation.
 
         CAUTION - Please note that a major restriction of this method at present is that any column slicing present on the data will be
-        removed in the new version created as a result of this method. 
-        As a result, if the impacted symbol has more than 127 columns (default value), the performance of selecting individual columns of 
-        the symbol (by using the `columns` parameter) may be negatively impacted in the defragmented version. 
-        If your symbol has less than 127 columns this caveat does not apply. 
+        removed in the new version created as a result of this method.
+        As a result, if the impacted symbol has more than 127 columns (default value), the performance of selecting individual columns of
+        the symbol (by using the `columns` parameter) may be negatively impacted in the defragmented version.
+        If your symbol has less than 127 columns this caveat does not apply.
         For more information, please see `columns_per_segment` here:
 
         https://docs.arcticdb.io/api/arcticdb/arcticdb.LibraryOptions
 
         Parameters
         ----------
         symbol: `str`
```

## arcticdb/version_store/helper.py

```diff
@@ -3,15 +3,15 @@
 
 Use of this software is governed by the Business Source License 1.1 included in the file licenses/BSL.txt.
 
 As of the Change Date specified in that file, in accordance with the Business Source License, use of this software will be governed by the Apache License, version 2.0.
 """
 import re
 import time
-
+from typing import Iterable, Dict, Any, Union
 
 from arcticc.pb2.lmdb_storage_pb2 import Config as LmdbConfig
 from arcticc.pb2.s3_storage_pb2 import Config as S3Config
 from arcticc.pb2.in_memory_storage_pb2 import Config as MemoryConfig
 from arcticc.pb2.mongo_storage_pb2 import Config as MongoConfig
 from arcticc.pb2.storage_pb2 import (
     EnvironmentConfigsMap,
@@ -23,16 +23,14 @@
     NoCredentialsStore,
 )
 
 from arcticdb.config import *  # for backward compat after moving to config
 from arcticdb.config import _expand_path
 from arcticdb.exceptions import ArcticNativeException, LibraryNotFound
 from arcticdb.version_store._store import NativeVersionStore
-
-from typing import Iterable, Dict, Any
 from arcticdb.authorization.permissions import OpenMode
 
 
 def create_lib_from_config(cfg, env=Defaults.ENV, lib_name=Defaults.LIB):
     return NativeVersionStore.create_lib_from_config(cfg, env, lib_name)
 
 
@@ -185,17 +183,15 @@
     in_mem = MemoryConfig()
 
     sid, storage = get_storage_for_lib_name(lib_name, env)
     storage.config.Pack(in_mem, type_url_prefix="cxx.arctic.org")
     _add_lib_desc_to_env(env, lib_name, sid, description)
 
 
-def add_mongo_library_to_env(
-        cfg, lib_name, env_name, uri=None, description=None
-):
+def add_mongo_library_to_env(cfg, lib_name, env_name, uri=None, description=None):
     env = cfg.env_by_id[env_name]
     mongo = MongoConfig()
     if uri is not None:
         mongo.uri = uri
 
     sid, storage = get_storage_for_lib_name(lib_name, env)
     storage.config.Pack(mongo, type_url_prefix="cxx.arctic.org")
@@ -275,24 +271,33 @@
 
 def create_test_lmdb_cfg(lib_name: str, db_dir: str, lmdb_config: Dict[str, Any] = {}):
     cfg = EnvironmentConfigsMap()
     add_lmdb_library_to_env(cfg, lib_name=lib_name, env_name=Defaults.ENV, db_dir=db_dir, lmdb_config=lmdb_config)
     return cfg
 
 
-def create_test_s3_cfg(lib_name, credential_name, credential_key, bucket_name, endpoint):
+def create_test_s3_cfg(
+    lib_name: str,
+    credential_name: str,
+    credential_key: str,
+    bucket_name: str,
+    endpoint: str,
+    *,
+    with_prefix: Union[str, bool, None] = True,
+) -> EnvironmentConfigsMap:
     cfg = EnvironmentConfigsMap()
     add_s3_library_to_env(
         cfg,
         lib_name=lib_name,
         env_name=Defaults.ENV,
         credential_name=credential_name,
         credential_key=credential_key,
         bucket_name=bucket_name,
         endpoint=endpoint,
+        with_prefix=with_prefix,
     )
     return cfg
 
 
 def create_test_memory_cfg(lib_name=Defaults.LIB, description=None):
     cfg = EnvironmentConfigsMap()
     add_memory_library_to_env(cfg, lib_name=lib_name, env_name=Defaults.ENV, description=description)
```

## arcticdb/version_store/library.py

```diff
@@ -198,15 +198,15 @@
     date_range: Optional[Tuple[Optional[Timestamp], Optional[Timestamp]]] = None
     columns: Optional[List[str]] = None
     query_builder: Optional[QueryBuilder] = None
 
 
 class ReadInfoRequest(NamedTuple):
     """ReadInfoRequest is useful for batch methods like read_metadata_batch and get_description_batch, where we
-    only need to specify the symbol and the version information. Therefore, construction of this object is 
+    only need to specify the symbol and the version information. Therefore, construction of this object is
     only required for these batch operations.
 
     Attributes
     ----------
     symbol : str
         See `read_metadata` method.
     as_of: Optional[AsOf], default=none
@@ -216,14 +216,15 @@
     --------
     Library.read: For documentation on the parameters.
     """
 
     symbol: str
     as_of: Optional[AsOf] = None
 
+
 class StagedDataFinalizeMethod(Enum):
     WRITE = auto()
     APPEND = auto()
 
 
 class Library:
     """
@@ -297,16 +298,16 @@
         keyword argument is set to True*. If ``staged`` is True, written segments will be staged and left in an
         "incomplete" stage, unable to be read until they are finalized. This enables multiple
         writers to a single symbol - all writing staged data at the same time - with one process able to later finalise
         all staged data rendering the data readable by clients. To finalise staged data, see `finalize_staged_data`.
 
         Note: ArcticDB will use the 0-th level index of the Pandas DataFrame for its on-disk index.
 
-        Any non-`DatetimeIndex` will converted into an internal `RowCount` index. That is, ArcticDB will assign each 
-        row a monotonically increasing integer identifier and that will be used for the index. 
+        Any non-`DatetimeIndex` will converted into an internal `RowCount` index. That is, ArcticDB will assign each
+        row a monotonically increasing integer identifier and that will be used for the index.
 
         Parameters
         ----------
         symbol : str
             Symbol name. Limited to 255 characters. The following characters are not supported in symbols:
             ``"*", "&", "<", ">"``
         data : NormalizableType
@@ -314,16 +315,16 @@
         metadata : Any, default=None
             Optional metadata to persist along with the symbol.
         prune_previous_versions : bool, default=True
             Removes previous (non-snapshotted) versions from the database.
         staged : bool, default=False
             Whether to write to a staging area rather than immediately to the library.
         validate_index: bool, default=False
-            If True, will verify that the index of `data` supports date range searches and update operations. This in effect tests that the data is sorted in ascending order. 
-            ArcticDB relies on Pandas to detect if data is sorted - you can call DataFrame.index.is_monotonic_increasing on your input DataFrame to see if Pandas believes the 
+            If True, will verify that the index of `data` supports date range searches and update operations. This in effect tests that the data is sorted in ascending order.
+            ArcticDB relies on Pandas to detect if data is sorted - you can call DataFrame.index.is_monotonic_increasing on your input DataFrame to see if Pandas believes the
             data to be sorted
 
             Note that each unit of staged data must a) be datetime indexed and b) not overlap with any other unit of
             staged data. Note that this will create symbols with Dynamic Schema enabled.
 
         Returns
         -------
@@ -362,15 +363,15 @@
         WritePayload objects can be unpacked and used as parameters:
 
         >>> w = WritePayload("symbol", df, metadata={'the': 'metadata'})
         >>> lib.write(*w, staged=True)
         """
         if not isinstance(data, NORMALIZABLE_TYPES):
             raise ArcticUnsupportedDataTypeException(
-                f"data is of a type that cannot be normalized. Consider using "
+                "data is of a type that cannot be normalized. Consider using "
                 f"write_pickle instead. type(data)=[{type(data)}]"
             )
 
         return self._nvs.write(
             symbol=symbol,
             data=data,
             metadata=metadata,
@@ -464,16 +465,16 @@
         payloads : `List[WritePayload]`
             Symbols and their corresponding data. There must not be any duplicate symbols in `payload`.
         prune_previous_versions: `bool`, default=True
             See `write`.
         staged: `bool`, default=False
             See `write`.
         validate_index: bool, default=False
-            If True, will verify for each entry in the batch hat the index of `data` supports date range searches and update operations. 
-            This in effect tests that the data is sorted in ascending order. ArcticDB relies on Pandas to detect if data is sorted - 
+            If True, will verify for each entry in the batch hat the index of `data` supports date range searches and update operations.
+            This in effect tests that the data is sorted in ascending order. ArcticDB relies on Pandas to detect if data is sorted -
             you can call DataFrame.index.is_monotonic_increasing on your input DataFrame to see if Pandas believes the data to be sorted
 
         Returns
         -------
         List[VersionedItem]
             Structure containing metadata and version number of the written symbols in the store, in the
             same order as `payload`.
@@ -575,15 +576,15 @@
         data: NormalizableType,
         metadata: Any = None,
         prune_previous_versions: bool = False,
         validate_index: bool = True,
     ) -> Optional[VersionedItem]:
         """
         Appends the given data to the existing, stored data. Append always appends along the index. A new version will
-        be created to reference the newly-appended data. Append only accepts data for which the index of the first 
+        be created to reference the newly-appended data. Append only accepts data for which the index of the first
         row is equal to or greater than the index of the last row in the existing data.
 
         Appends containing differing column sets to the existing data are only possible if the library has been
         configured to support dynamic schemas.
 
         Note that `append` is not designed for multiple concurrent writers over a single symbol.
 
@@ -595,16 +596,16 @@
             Data to be written.
         metadata
             Optional metadata to persist along with the new symbol version. Note that the metadata is
             not combined in any way with the metadata stored in the previous version.
         prune_previous_versions
             Removes previous (non-snapshotted) versions from the database when True.
         validate_index: bool, default=False
-            If True, will verify that resulting symbol will support date range searches and update operations. This in effect tests that the previous version of the 
-            data and `data` are both sorted in ascending order. ArcticDB relies on Pandas to detect if data is sorted - you can call DataFrame.index.is_monotonic_increasing 
+            If True, will verify that resulting symbol will support date range searches and update operations. This in effect tests that the previous version of the
+            data and `data` are both sorted in ascending order. ArcticDB relies on Pandas to detect if data is sorted - you can call DataFrame.index.is_monotonic_increasing
             on your input DataFrame to see if Pandas believes the data to be sorted
 
         Returns
         -------
         VersionedItem
             Structure containing metadata and version number of the written symbol in the store.
 
@@ -750,18 +751,17 @@
             Documentation on the ``staged`` parameter explains the concept of staged data in more detail.
         """
         self._nvs.compact_incomplete(symbol, mode == StagedDataFinalizeMethod.APPEND, False)
 
     def sort_and_finalize_staged_data(
         self, symbol: str, mode: Optional[StagedDataFinalizeMethod] = StagedDataFinalizeMethod.WRITE
     ):
-
         """
-        sort_merge will sort and finalize staged data. This differs from `finalize_staged_data` in that it 
-        can support staged segments with interleaved time periods - the end result will be ordered. This requires 
+        sort_merge will sort and finalize staged data. This differs from `finalize_staged_data` in that it
+        can support staged segments with interleaved time periods - the end result will be ordered. This requires
         performing a full sort in memory so can be time consuming.
 
         Parameters
         ----------
         symbol : `str`
             Symbol to finalize data for.
 
@@ -928,15 +928,16 @@
         for s in symbols:
             if isinstance(s, str):
                 handle_symbol(s)
             elif isinstance(s, ReadRequest):
                 handle_read_request(s)
             else:
                 raise ArcticInvalidApiUsageException(
-                    f"Unsupported item in the symbols argument s=[{s}] type(s)=[{type(s)}]. Only [str] and [ReadRequest] are supported."
+                    f"Unsupported item in the symbols argument s=[{s}] type(s)=[{type(s)}]. Only [str] and"
+                    " [ReadRequest] are supported."
                 )
 
         return self._nvs._batch_read_to_versioned_items(
             symbol_strings, as_ofs, date_ranges, columns, query_builder or query_builders
         )
 
     def read_metadata(self, symbol: str, as_of: Optional[AsOf] = None) -> VersionedItem:
@@ -956,39 +957,38 @@
         -------
         VersionedItem
             Structure containing metadata and version number of the affected symbol in the store. The data attribute
             will be None.
         """
         return self._nvs.read_metadata(symbol, as_of)
 
-    def read_metadata_batch(
-        self, symbols: List[Union[str, ReadInfoRequest]]
-    ) -> List[VersionedItem]:
+    def read_metadata_batch(self, symbols: List[Union[str, ReadInfoRequest]]) -> List[VersionedItem]:
         """
         Reads the metadata of multiple symbols.
 
         Parameters
         ----------
         symbols : List[Union[str, ReadInfoRequest]]
             List of symbols to read.
-        
+
         Returns
         -------
         List[VersionedItem]
             A list of the read results, whose i-th element corresponds to the i-th element of the ``symbols`` parameter.
-            A VersionedItem object with the metadata field set as None will be returned if the requested version of the 
+            A VersionedItem object with the metadata field set as None will be returned if the requested version of the
                 symbol exists but there is no metadata
             A None object will be returned if the requested version of the symbol does not exist
-        
+
         See Also
         --------
         read_metadata
         """
         symbol_strings = []
         as_ofs = []
+
         def handle_read_request(s_):
             symbol_strings.append(s_.symbol)
             as_ofs.append(s_.as_of)
 
         def handle_symbol(s_):
             symbol_strings.append(s_)
             as_ofs.append(None)
@@ -1096,15 +1096,15 @@
         if isinstance(versions, int):
             versions = (versions,)
 
         for v in versions:
             self._nvs.delete_version(symbol, v)
 
     def prune_previous_versions(self, symbol):
-        """ Removes all (non-snapshotted) versions from the database for the given symbol, except the latest.
+        """Removes all (non-snapshotted) versions from the database for the given symbol, except the latest.
 
         Parameters
         ----------
         symbol : `str`
             Symbol name to prune.
         """
         self._nvs.prune_previous_versions(symbol)
@@ -1348,17 +1348,15 @@
             index=index,
             row_count=info["rows"],
             last_update_time=last_update_time,
             index_type=info["index_type"],
             date_range=info["date_range"],
         )
 
-    def get_description_batch(
-        self, symbols: List[Union[str, ReadInfoRequest]]
-    ) -> List[SymbolDescription]:
+    def get_description_batch(self, symbols: List[Union[str, ReadInfoRequest]]) -> List[SymbolDescription]:
         """
         Returns descriptive data for a list of ``symbols``.
 
         Parameters
         ----------
         symbols : List[Union[str, ReadInfoRequest]]
             List of symbols to read.
@@ -1372,14 +1370,15 @@
         See Also
         --------
         SymbolDescription
             For documentation on each field.
         """
         symbol_strings = []
         as_ofs = []
+
         def handle_read_request(s):
             symbol_strings.append(s.symbol)
             as_ofs.append(s.as_of)
 
         def handle_symbol(s):
             symbol_strings.append(s)
             as_ofs.append(None)
@@ -1387,15 +1386,16 @@
         for s in symbols:
             if isinstance(s, str):
                 handle_symbol(s)
             elif isinstance(s, ReadInfoRequest):
                 handle_read_request(s)
             else:
                 raise ArcticInvalidApiUsageException(
-                    f"Unsupported item in the symbols argument s=[{s}] type(s)=[{type(s)}]. Only [str] and [ReadInfoRequest] are supported."
+                    f"Unsupported item in the symbols argument s=[{s}] type(s)=[{type(s)}]. Only [str] and"
+                    " [ReadInfoRequest] are supported."
                 )
 
         infos = self._nvs.batch_get_info(symbol_strings, as_ofs)
         list_descriptions = []
         for info in infos:
             last_update_time = pd.to_datetime(info["last_update"])
             columns = tuple(NameWithDType(n, t) for n, t in zip(info["col_names"]["columns"], info["dtype"]))
@@ -1411,15 +1411,18 @@
                     date_range=info["date_range"],
                 )
             )
         return list_descriptions
 
     def reload_symbol_list(self):
         """
-        Forces the symbol list cache to be reloaded
+        Forces the symbol list cache to be reloaded.
+
+        This can take a long time on large libraries or certain S3 implementations, and once started, it cannot be
+        safely interrupted. If the call is interrupted somehow (exception/process killed), please call this again ASAP.
         """
         self._nvs.version_store.reload_symbol_list()
 
     def is_symbol_fragmented(self, symbol: str, segment_size: Optional[int] = None) -> bool:
         """
         Check whether the number of segments that would be reduced by compaction is more than or equal to the
         value specified by the configuration option "SymbolDataCompact.SegmentCount" (defaults to 100).
@@ -1432,31 +1435,31 @@
             Target for maximum no. of rows per segment, after compaction.
             If parameter is not provided, library option for segments's maximum row size will be used
 
         Notes
         ----------
         Config map setting - SymbolDataCompact.SegmentCount will be replaced by a library setting
         in the future. This API will allow overriding the setting as well.
-        
+
         Returns
         -------
         bool
         """
         return self._nvs.is_symbol_fragmented(symbol, segment_size)
 
     def defragment_symbol_data(self, symbol: str, segment_size: Optional[int] = None) -> VersionedItem:
         """
         Compacts fragmented segments by merging row-sliced segments (https://docs.arcticdb.io/technical/on_disk_storage/#data-layer).
-        This method calls `is_symbol_fragmented` to determine whether to proceed with the defragmentation operation. 
+        This method calls `is_symbol_fragmented` to determine whether to proceed with the defragmentation operation.
 
         CAUTION - Please note that a major restriction of this method at present is that any column slicing present on the data will be
-        removed in the new version created as a result of this method. 
-        As a result, if the impacted symbol has more than 127 columns (default value), the performance of selecting individual columns of 
-        the symbol (by using the `columns` parameter) may be negatively impacted in the defragmented version. 
-        If your symbol has less than 127 columns this caveat does not apply. 
+        removed in the new version created as a result of this method.
+        As a result, if the impacted symbol has more than 127 columns (default value), the performance of selecting individual columns of
+        the symbol (by using the `columns` parameter) may be negatively impacted in the defragmented version.
+        If your symbol has less than 127 columns this caveat does not apply.
         For more information, please see `columns_per_segment` here:
 
         https://docs.arcticdb.io/api/arcticdb/arcticdb.LibraryOptions
 
         Parameters
         ----------
         symbol: `str`
```

## arcticdb/version_store/processing.py

```diff
@@ -411,16 +411,16 @@
         self.stages = []
         self._optimisation = _Optimisation.SPEED
 
         self._clause_builder = _ClauseBuilder()
 
     def apply(self, name, expr):
         """
-        Apply enables new columns to be created using supported QueryBuilder numeric operations. See the documentation for the 
-        QueryBuilder class for more information on supported expressions - any expression valid in a filter is valid when using 
+        Apply enables new columns to be created using supported QueryBuilder numeric operations. See the documentation for the
+        QueryBuilder class for more information on supported expressions - any expression valid in a filter is valid when using
         `apply`.
 
         Parameters
         ----------
         name: `str`
             Name of the column to be created
         expr:
@@ -459,21 +459,21 @@
             Modified QueryBuilder object.
         """
         self.stages.append(ProjectClause(name, expr))
         return self
 
     def groupby(self, expr: str):
         """
-        Group symbol by column name. GroupBy operations must be followed by an aggregation operator. Currently the following four aggregation 
+        Group symbol by column name. GroupBy operations must be followed by an aggregation operator. Currently the following four aggregation
         operators are supported:
             * "mean" - compute the mean of the group
             * "sum" - compute the sum of the group
             * "min" - compute the min of the group
             * "max" - compute the max of the group
-        
+
         For usage examples, see below.
 
         Parameters
         ----------
         expr: `str`
             Name of the symbol to group on. Note that currently GroupBy only supports single-column groupings.
```

## Comparing `arcticdb-1.2.1.dist-info/LICENSE.txt` & `arcticdb-1.3.0.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `arcticdb-1.2.1.dist-info/METADATA` & `arcticdb-1.3.0.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 Metadata-Version: 2.1
 Name: arcticdb
-Version: 1.2.1
+Version: 1.3.0
 Summary: ArcticDB DataFrame Database
 Home-page: https://github.com/man-group/arcticdb
 Author: Man Alpha Technology
 Author-email: arcticdb@man.com
+License: Business Source License 1.1 (See LICENSE.txt)
 Classifier: Programming Language :: Python :: 3
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Operating System :: Microsoft :: Windows
 Classifier: Topic :: Database
 Classifier: Topic :: Database :: Database Engines/Servers
 Description-Content-Type: text/markdown
 License-File: LICENSE.txt
```

### html2text {}

```diff
@@ -1,28 +1,28 @@
-Metadata-Version: 2.1 Name: arcticdb Version: 1.2.1 Summary: ArcticDB DataFrame
+Metadata-Version: 2.1 Name: arcticdb Version: 1.3.0 Summary: ArcticDB DataFrame
 Database Home-page: https://github.com/man-group/arcticdb Author: Man Alpha
-Technology Author-email: arcticdb@man.com Classifier: Programming Language ::
-Python :: 3 Classifier: Operating System :: POSIX :: Linux Classifier:
-Operating System :: Microsoft :: Windows Classifier: Topic :: Database
-Classifier: Topic :: Database :: Database Engines/Servers Description-Content-
-Type: text/markdown License-File: LICENSE.txt License-File: NOTICE.txt
-Requires-Dist: numpy Requires-Dist: pandas (<2) Requires-Dist: attrs Requires-
-Dist: enum-compat Requires-Dist: grpcio-tools Requires-Dist: protobuf
-(>=3.5.0.post1) Requires-Dist: xxhash Requires-Dist: six Requires-Dist: msgpack
-Requires-Dist: psutil Requires-Dist: pyyaml Requires-Dist: decorator Requires-
-Dist: prometheus-client Requires-Dist: dataclasses ; python_version < "3.7"
-Provides-Extra: testing Requires-Dist: pytest ; extra == 'testing' Requires-
-Dist: pytest-cpp ; extra == 'testing' Requires-Dist: pytest-timeout ; extra ==
-'testing' Requires-Dist: packaging ; extra == 'testing' Requires-Dist: future ;
-extra == 'testing' Requires-Dist: pytest-server-fixtures ; extra == 'testing'
-Requires-Dist: mock ; extra == 'testing' Requires-Dist: boto3 ; extra ==
-'testing' Requires-Dist: moto ; extra == 'testing' Requires-Dist: flask ; extra
-== 'testing' Requires-Dist: flask-cors ; extra == 'testing' Requires-Dist:
-hypothesis (<6.73) ; extra == 'testing' Requires-Dist: pymongo ; extra ==
-'testing'
+Technology Author-email: arcticdb@man.com License: Business Source License 1.1
+(See LICENSE.txt) Classifier: Programming Language :: Python :: 3 Classifier:
+Operating System :: POSIX :: Linux Classifier: Operating System :: Microsoft ::
+Windows Classifier: Topic :: Database Classifier: Topic :: Database :: Database
+Engines/Servers Description-Content-Type: text/markdown License-File:
+LICENSE.txt License-File: NOTICE.txt Requires-Dist: numpy Requires-Dist: pandas
+(<2) Requires-Dist: attrs Requires-Dist: enum-compat Requires-Dist: grpcio-
+tools Requires-Dist: protobuf (>=3.5.0.post1) Requires-Dist: xxhash Requires-
+Dist: six Requires-Dist: msgpack Requires-Dist: psutil Requires-Dist: pyyaml
+Requires-Dist: decorator Requires-Dist: prometheus-client Requires-Dist:
+dataclasses ; python_version < "3.7" Provides-Extra: testing Requires-Dist:
+pytest ; extra == 'testing' Requires-Dist: pytest-cpp ; extra == 'testing'
+Requires-Dist: pytest-timeout ; extra == 'testing' Requires-Dist: packaging ;
+extra == 'testing' Requires-Dist: future ; extra == 'testing' Requires-Dist:
+pytest-server-fixtures ; extra == 'testing' Requires-Dist: mock ; extra ==
+'testing' Requires-Dist: boto3 ; extra == 'testing' Requires-Dist: moto ; extra
+== 'testing' Requires-Dist: flask ; extra == 'testing' Requires-Dist: flask-
+cors ; extra == 'testing' Requires-Dist: hypothesis (<6.73) ; extra ==
+'testing' Requires-Dist: pymongo ; extra == 'testing'
  [https://github.com/man-group/ArcticDB/raw/master/static/ArcticDBCropped.png]
 ---
      [https://raw.githubusercontent.com/man-group/ArcticDB/master/static/
                              ArcticDBTerminal.gif]
 ---
  ArcticDB_Website | ArcticDB_Blog | Press_Release | Press_Release | Community
```

## Comparing `arcticdb-1.2.1.dist-info/NOTICE.txt` & `arcticdb-1.3.0.dist-info/NOTICE.txt`

 * *Files identical despite different names*

## Comparing `arcticdb-1.2.1.dist-info/RECORD` & `arcticdb-1.3.0.dist-info/RECORD`

 * *Files 7% similar despite different names*

```diff
@@ -1,25 +1,25 @@
-arcticdb_ext.cp39-win_amd64.pyd,sha256=J5X46qajrVYZad0Eguhga1Ov8AV6JDZV5Vi1dFC26Ro,22185472
-arcticc/__init__.py,sha256=h-QyvMVzDNpT3jyVskcSbUVFXxGCRxieFPrvTveZG9k,64
+arcticdb_ext.cp39-win_amd64.pyd,sha256=fqicZdD9D7sd2q7g29_C29WFCgj5d6heqouTMCgsnGs,22063616
+arcticc/__init__.py,sha256=5t8gAo_NnZ269q7FPcp6PwmUczF2UFmS8Zz1KAVbiL0,66
 arcticc/pb2/__init__.py,sha256=pfsson0mxPSudMCFd_HpaUTeFROIFD4yA_XQgqAYMiU,583
-arcticdb/__init__.py,sha256=DCzcLfCzAZozdnqVr4uUCdkKclfjasgBCIJ_F4iLCEw,426
+arcticdb/__init__.py,sha256=t1SAKz0_D9TmtoCAYVS1jfBDTpSZdIfrJ4aysnjDGnY,424
 arcticdb/_msgpack_compat.py,sha256=i_3HluY89KVSXFnxC-UjcdK0zNsIcSBLmY3YpKFeLl8,447
-arcticdb/arctic.py,sha256=p9qG4ZfHo0zI-ZE2OIZOmyNoV3ZI2xoNzcvFlOwwWSs,10405
-arcticdb/config.py,sha256=EzKNvld8Y6dA5R7CkbZ03mMnlOQSYlPen-pzK5e9u-E,7970
+arcticdb/arctic.py,sha256=5ZGA0u0ella3mKc3lDO3YXwNWJ47bNsEb4Jsybvzjqs,10784
+arcticdb/config.py,sha256=ozc4oNPIDue4JoRTpsTJeBuhEXGCbgt27qux2nsao7E,7524
 arcticdb/exceptions.py,sha256=ArcFJwbH8PKD5CfC5Aq5LEKwcjIx-ftsocuGtwZjUrU,765
-arcticdb/flattener.py,sha256=1BBY-zD4FBD40DxhfIwXFpP9Glcgx5xaa2B098OKV7s,9363
-arcticdb/log.py,sha256=HTRQyuTNxr5g9a3vFRpGxgGZlhs9LJRzbtJQ64sP5YY,2071
-arcticdb/options.py,sha256=lYScTfHS-JGUZFomGjkLeLS459v_PrFADZhL9AV24Zw,6436
+arcticdb/flattener.py,sha256=u-rdlqaavqzcMUtyMycjaa9MjUdd-6xcVedkCjW8nJ8,9361
+arcticdb/log.py,sha256=0TGHEHZ6DuUalql6NegQ--m8Q7bjxrPzxB-RniLFwfQ,1809
+arcticdb/options.py,sha256=euSyeMVFtvqgGFSEPyw-3d4n0NzwCe3uJ5iwjDXktz8,6476
 arcticdb/preconditions.py,sha256=85PtbfJEUGvVeRethePECPEhwAY6ZTZ3oy8Fx-KePyY,519
 arcticdb/supported_types.py,sha256=FUr7Slxn5c1v_wQ20H3l5wlK1cwpDoQKJfwDEjtnhEI,1612
 arcticdb/tools.py,sha256=umjEygBZDw6_IWrj5gew-werTfTW-UGsqp4TdtfQt6g,3142
 arcticdb/adapters/__init__.py,sha256=wmFWqLdci5GAilj5KQGwGPYLnDRRKDOxrc80r27lYlg,138
 arcticdb/adapters/arctic_library_adapter.py,sha256=brVvjPTkxUlWCVhpNQp2nl-V368reG_1qNa57QAoG3s,2211
 arcticdb/adapters/lmdb_library_adapter.py,sha256=YmygJR8j1JowEkxfaDO2lFXRp7GRpAuXt-zikw9OSaA,2394
-arcticdb/adapters/s3_library_adapter.py,sha256=6auNlcU_IldvXaYW48j1wRJ9jNyxrdjf_A2ZcE9bFGw,6434
+arcticdb/adapters/s3_library_adapter.py,sha256=RImsx35b7mt-zMtjn3nO_il-GNWwVwiB6VjJt6_5sDs,6433
 arcticdb/authorization/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 arcticdb/authorization/permissions.py,sha256=3E91GMrL6xSEKloOWdIGdaKrHfHoqhkSEqJwjJZJ3HQ,969
 arcticdb/proto/3/arcticc/pb2/config_pb2.py,sha256=W1pb3T1ArzmExVkjgF3xWn5WMWL5l9fR7rR0unnKhxE,8935
 arcticdb/proto/3/arcticc/pb2/descriptors_pb2.py,sha256=VmnoqTZkDtHRVxsPVWqOqoQF2W7lU03KHC6x4Lmk2ys,96954
 arcticdb/proto/3/arcticc/pb2/encoding_pb2.py,sha256=Huer_Ni4ySR6J6JXmObVVDqByoxp7hp2S59EzMrD8Sw,28646
 arcticdb/proto/3/arcticc/pb2/generation_pb2.py,sha256=lnxrzzbxdZ9ooO6BvKwLcntubqOh87YHS-0nzcAJfoI,13225
 arcticdb/proto/3/arcticc/pb2/in_memory_storage_pb2.py,sha256=xZjv7HAzBZIQl72vh5SCewyz7_Q5ISSqIyjx9kn3JFs,1990
@@ -43,29 +43,29 @@
 arcticdb/proto/4/arcticc/pb2/nfs_backed_storage_pb2.py,sha256=jZ0m6j4EuYSsUR_RwMQQpne5OBl4ES6vDridcL_DWBE,1517
 arcticdb/proto/4/arcticc/pb2/processors_pb2.py,sha256=Mbr1IpNNrtI-UcbL5pavdUoIjsmaAMEOLEV45uy6OJs,1259
 arcticdb/proto/4/arcticc/pb2/request_pb2.py,sha256=n_oXwA3vEtkUeNrzMHrC4MEFh6dkvCk6r8sfGQ-rreY,6801
 arcticdb/proto/4/arcticc/pb2/s3_storage_pb2.py,sha256=MK5ZPt9oik4TyWxLke4VC3BM3KfQVwkdcudbd1G7RK4,1496
 arcticdb/proto/4/arcticc/pb2/storage_pb2.py,sha256=liHmX8x8V6jRhFwzALwdYGsCx-npRpkbpg4pRL3mTcM,9907
 arcticdb/proto/4/arcticc/pb2/utils_pb2.py,sha256=ksfhHFjEdosK8mwT1fcaiMfVTal7C9SAviBxFYDbnHI,2310
 arcticdb/toolbox/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-arcticdb/toolbox/library_tool.py,sha256=5jBjyB2-VMczywWrEjeWvnRHQ0pzSU5gCsEdusximQc,5669
+arcticdb/toolbox/library_tool.py,sha256=Ch8zB82NTe_GUnrDD4QxrxkP0PlV9FZMsAv7jYNCpVs,6102
 arcticdb/util/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 arcticdb/util/errors.py,sha256=vmVvTdFvJJP0cofApzcor6Mn6lSmoE5ih3zvEf1Flkc,2705
-arcticdb/util/hypothesis.py,sha256=RA7ty7sFUl_p1B85FFAnQZfFd52f7KYxMkEFLJaMzTs,8769
+arcticdb/util/hypothesis.py,sha256=0nlWCa4zESA8I-x_yy4iJ9CFytAF332q58oJgGhKLHY,8775
 arcticdb/util/memory.py,sha256=-YYUz_ATxFGPov8mAOWsvwgulxZng1hgEB4xkAvUkPg,719
-arcticdb/util/tasks.py,sha256=_tIl4ZfwGyxmnquKcc14TlqMbLBflmctkIIn2uTJAjo,6042
-arcticdb/util/test.py,sha256=AdtnFnoN379lBL2FMlX2hGrYZ6Wp_dzIjDsWmwZcK0Q,18195
+arcticdb/util/tasks.py,sha256=aXNt_CDW9F0rtXCOwQaWIuoG69OMXajmy7NDUtwgJ2w,6013
+arcticdb/util/test.py,sha256=g-KS_3Y_KnsOWbnQUxG85LoetlH5MRP1mC6ZY6_-_3A,18251
 arcticdb/version_store/__init__.py,sha256=uGAsgCsoyGmR-F7zvOx8GuMs1QdnVHdg5l7RSm6XiBY,131
-arcticdb/version_store/_common.py,sha256=XMTAUSUAPWxOZEsnbc44egIX41AAHSIUMT78KignaRY,7142
+arcticdb/version_store/_common.py,sha256=V8eruObk6XhIabaGtFibt7jmzpf0V3nByNOdfb1ftGg,7189
 arcticdb/version_store/_custom_normalizers.py,sha256=EKqxg39qV8BJPjeSCZ9BEOnZkixCXJW796F91bR8wws,4193
-arcticdb/version_store/_normalization.py,sha256=vup8gPRBD53hfrYpAvDr8CxKvTnTQ1KdxPt16zVbgd4,52604
-arcticdb/version_store/_store.py,sha256=TagrLpunpd_4DJYnnTDbYhNsAqvpRVdJY3b0I7y9iH4,113852
-arcticdb/version_store/helper.py,sha256=4e3bmX-6i9UJ8LS48zXsKarQCeFDl3-A8uDd1QgGU2w,10879
-arcticdb/version_store/library.py,sha256=U1P_7Y6su6J95hhEPeKBOsVlOn8ZHuCO8ciK3UlzX7s,59225
-arcticdb/version_store/processing.py,sha256=VSEm8kg-XTG5fhEc_PKpsraTJCjaXVWEOf3yjtbndPk,24592
+arcticdb/version_store/_normalization.py,sha256=l0QU6VKuRcI7ZtH4E8-A4Pa_LBP58z3CP0Wrbzegnbw,52659
+arcticdb/version_store/_store.py,sha256=rVkBxfhEfoyWn6aLjtJ9Gic-Zy6dgn_NOiOdYUk6oEI,113749
+arcticdb/version_store/helper.py,sha256=RvlvYy02ftrY_9ovK632SWR4Ivf-nPq1hcpbl-kssHg,11040
+arcticdb/version_store/library.py,sha256=qqqkPKwkQTwD81gmiH05mMZQkbLgIi73RdaD84xJbKk,59441
+arcticdb/version_store/processing.py,sha256=2eW_r38uAm1_WxymHCu1kSwXFvx3-EuRLnTO5OTjsVc,24581
 arcticdb/version_store/read_result.py,sha256=5HhAJ0Wh01f111qA5XvWqOXABO-2H0jZ78kx-iiQzOk,603
-arcticdb-1.2.1.dist-info/LICENSE.txt,sha256=ruvCXWZm0cgyb-XAEjFcfdJkJ_nGbv9gsYNeps514Ys,4851
-arcticdb-1.2.1.dist-info/METADATA,sha256=JssNSsOBwoY_7-2Ayh6iEEj4WYTVAn3v10bXNrlT8qo,8393
-arcticdb-1.2.1.dist-info/NOTICE.txt,sha256=TsVpAVXueJjRq_zV86A3v_TqJRsXCQtYjrEvycwyaaY,18186
-arcticdb-1.2.1.dist-info/WHEEL,sha256=J_4V_gB-O6Y7Pn6lk91K27JaIhI-q07YM5J8Ufzqla4,100
-arcticdb-1.2.1.dist-info/top_level.txt,sha256=jDxz3uFaLYFuxxPf3h6-sqPQqEFbMkWGP-9vE8Dbi1w,30
-arcticdb-1.2.1.dist-info/RECORD,,
+arcticdb-1.3.0.dist-info/LICENSE.txt,sha256=ruvCXWZm0cgyb-XAEjFcfdJkJ_nGbv9gsYNeps514Ys,4851
+arcticdb-1.3.0.dist-info/METADATA,sha256=Wc-jW5KcIz90EaxZpE8K6N4CiAdetzLzQs-G81547u8,8449
+arcticdb-1.3.0.dist-info/NOTICE.txt,sha256=TsVpAVXueJjRq_zV86A3v_TqJRsXCQtYjrEvycwyaaY,18186
+arcticdb-1.3.0.dist-info/WHEEL,sha256=J_4V_gB-O6Y7Pn6lk91K27JaIhI-q07YM5J8Ufzqla4,100
+arcticdb-1.3.0.dist-info/top_level.txt,sha256=jDxz3uFaLYFuxxPf3h6-sqPQqEFbMkWGP-9vE8Dbi1w,30
+arcticdb-1.3.0.dist-info/RECORD,,
```

