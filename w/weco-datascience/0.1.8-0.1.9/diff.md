# Comparing `tmp/weco-datascience-0.1.8.tar.gz` & `tmp/weco-datascience-0.1.9.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "weco-datascience-0.1.8.tar", last modified: Thu Feb 11 18:52:06 2021, max compression
+gzip compressed data, was "weco-datascience-0.1.9.tar", last modified: Thu Aug 19 13:50:42 2021, max compression
```

## Comparing `weco-datascience-0.1.8.tar` & `weco-datascience-0.1.9.tar`

### file list

```diff
@@ -1,365 +1,425 @@
--rw-r--r--   0        0        0      652 2021-02-11 18:51:05.377286 weco-datascience-0.1.8/.github/workflows/publish.yml
--rw-r--r--   0        0        0     4127 2021-02-11 18:51:05.377286 weco-datascience-0.1.8/.gitignore
--rw-r--r--   0        0        0     1070 2021-02-11 18:51:05.377286 weco-datascience-0.1.8/LICENSE.md
--rw-r--r--   0        0        0      732 2021-02-11 18:51:05.377286 weco-datascience-0.1.8/Makefile
--rw-r--r--   0        0        0      495 2021-02-11 18:51:05.377286 weco-datascience-0.1.8/README.md
--rw-r--r--   0        0        0       95 2021-02-11 18:51:05.377286 weco-datascience-0.1.8/api_interfaces/README.md
--rw-r--r--   0        0        0        0 2021-02-11 18:51:05.377286 weco-datascience-0.1.8/api_interfaces/devise_search/Dockerfile
--rw-r--r--   0        0        0     1935 2021-02-11 18:51:05.377286 weco-datascience-0.1.8/api_interfaces/devise_search/static/index.html
--rw-r--r--   0        0        0     1237 2021-02-11 18:51:05.377286 weco-datascience-0.1.8/api_interfaces/devise_search/static/javascript.js
--rw-r--r--   0        0        0      747 2021-02-11 18:51:05.377286 weco-datascience-0.1.8/api_interfaces/devise_search/static/style.css
--rw-r--r--   0        0        0      305 2021-02-11 18:51:05.377286 weco-datascience-0.1.8/api_interfaces/nerd/.gitignore
--rw-r--r--   0        0        0      400 2021-02-11 18:51:05.377286 weco-datascience-0.1.8/api_interfaces/nerd/Dockerfile
--rw-r--r--   0        0        0   525586 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/api_interfaces/nerd/package-lock.json
--rw-r--r--   0        0        0      614 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/api_interfaces/nerd/package.json
--rw-r--r--   0        0        0    93062 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/api_interfaces/nerd/public/favicon.ico
--rw-r--r--   0        0        0     2006 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/api_interfaces/nerd/public/icon.svg
--rw-r--r--   0        0        0     1558 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/api_interfaces/nerd/public/index.html
--rw-r--r--   0        0        0      332 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/api_interfaces/nerd/public/manifest.json
--rw-r--r--   0        0        0     1681 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/api_interfaces/nerd/src/App.js
--rw-r--r--   0        0        0      248 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/api_interfaces/nerd/src/App.test.js
--rw-r--r--   0        0        0       66 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/api_interfaces/nerd/src/css/custom.css
--rw-r--r--   0        0        0   113587 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/api_interfaces/nerd/src/css/tachyons.css
--rw-r--r--   0        0        0      943 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/api_interfaces/nerd/src/index.js
--rw-r--r--   0        0        0        0 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/api_interfaces/palette_similarity/Dockerfile
--rw-r--r--   0        0        0      296 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/api_interfaces/palette_similarity/palette_dict.pkl
--rw-r--r--   0        0        0     1724 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/api_interfaces/palette_similarity/static/index.html
--rw-r--r--   0        0        0     2336 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/api_interfaces/palette_similarity/static/javascript.js
--rwxr-xr-x   0        0        0    54093 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/api_interfaces/palette_similarity/static/jscolor.js
--rw-r--r--   0        0        0      575 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/api_interfaces/palette_similarity/static/style.css
--rw-r--r--   0        0        0      815 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/apis/README.md
--rw-r--r--   0        0        0      260 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/apis/devise_search/Dockerfile
--rw-r--r--   0        0        0     2656 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/apis/devise_search/README.md
--rw-r--r--   0        0        0        0 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/apis/devise_search/app/__init__.py
--rw-r--r--   0        0        0     1853 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/apis/devise_search/app/api.py
--rw-r--r--   0        0        0      921 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/apis/devise_search/app/aws.py
--rw-r--r--   0        0        0      417 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/apis/devise_search/app/sentence_encoder.py
--rw-r--r--   0        0        0     1792 2021-02-11 18:51:05.381286 weco-datascience-0.1.8/apis/devise_search/app/test_api.py
--rw-r--r--   0        0        0      633 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/devise_search/app/test_utils.py
--rw-r--r--   0        0        0     1792 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/devise_search/app/utils.py
--rw-r--r--   0        0        0      109 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/devise_search/requirements.in
--rw-r--r--   0        0        0      845 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/devise_search/requirements.txt
--rw-r--r--   0        0        0      215 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/feature_similarity/Dockerfile
--rw-r--r--   0        0        0     3385 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/feature_similarity/README.md
--rw-r--r--   0        0        0        0 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/feature_similarity/__init__.py
--rw-r--r--   0        0        0        1 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/feature_similarity/app/__init__.py
--rw-r--r--   0        0        0     1213 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/feature_similarity/app/api.py
--rw-r--r--   0        0        0      438 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/feature_similarity/app/aws.py
--rw-r--r--   0        0        0      957 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/feature_similarity/app/identifiers.py
--rw-r--r--   0        0        0      885 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/feature_similarity/app/neighbours.py
--rw-r--r--   0        0        0       89 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/feature_similarity/requirements.in
--rw-r--r--   0        0        0     1096 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/feature_similarity/requirements.txt
--rw-r--r--   0        0        0      225 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/image_pathways/Dockerfile
--rw-r--r--   0        0        0     2255 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/image_pathways/README.md
--rw-r--r--   0        0        0        0 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/image_pathways/app/__init__.py
--rw-r--r--   0        0        0     1011 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/image_pathways/app/api.py
--rw-r--r--   0        0        0      921 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/image_pathways/app/aws.py
--rw-r--r--   0        0        0     2314 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/image_pathways/app/test_api.py
--rw-r--r--   0        0        0     1510 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/image_pathways/app/test_utils.py
--rw-r--r--   0        0        0     3204 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/image_pathways/app/utils.py
--rw-r--r--   0        0        0      304 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/image_pathways/build_nmslib_index.py
--rw-r--r--   0        0        0       74 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/image_pathways/requirements.in
--rw-r--r--   0        0        0      992 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/image_pathways/requirements.txt
--rw-r--r--   0        0        0      225 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/nerd/Dockerfile
--rw-r--r--   0        0        0     1539 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/nerd/README.md
--rw-r--r--   0        0        0        0 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/nerd/app/__init__.py
--rw-r--r--   0        0        0      986 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/nerd/app/api.py
--rw-r--r--   0        0        0     1581 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/nerd/app/aws.py
--rw-r--r--   0        0        0     2974 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/nerd/app/backbone.py
--rw-r--r--   0        0        0     1659 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/nerd/app/character_level_network.py
--rw-r--r--   0        0        0     1203 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/nerd/app/heads.py
--rw-r--r--   0        0        0     5300 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/nerd/app/model_utils.py
--rw-r--r--   0        0        0     2968 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/nerd/app/nerd.py
--rw-r--r--   0        0        0      170 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/nerd/requirements.in
--rw-r--r--   0        0        0     1063 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/nerd/requirements.txt
--rw-r--r--   0        0        0      214 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/palette_similarity/Dockerfile
--rw-r--r--   0        0        0     6881 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/palette_similarity/README.md
--rw-r--r--   0        0        0        1 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/palette_similarity/app/__init__.py
--rw-r--r--   0        0        0     2415 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/palette_similarity/app/api.py
--rw-r--r--   0        0        0      438 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/palette_similarity/app/aws.py
--rw-r--r--   0        0        0      346 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/palette_similarity/app/colours.py
--rw-r--r--   0        0        0     1465 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/palette_similarity/app/identifiers.py
--rw-r--r--   0        0        0      878 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/palette_similarity/app/neighbours.py
--rw-r--r--   0        0        0     1800 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/palette_similarity/app/palette_embedder.py
--rw-r--r--   0        0        0      122 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/palette_similarity/requirements.in
--rw-r--r--   0        0        0     1489 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/apis/palette_similarity/requirements.txt
--rw-r--r--   0        0        0     1822 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/benchmarking/.gitignore
--rw-r--r--   0        0        0     1822 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/benchmarking/lsh/.gitignore
--rw-r--r--   0        0        0      650 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/benchmarking/lsh/README.md
--rw-r--r--   0        0        0      142 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/benchmarking/lsh/api/Dockerfile
--rw-r--r--   0        0        0      155 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/benchmarking/lsh/api/README.md
--rw-r--r--   0        0        0        0 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/benchmarking/lsh/api/app/__init__.py
--rw-r--r--   0        0        0     2525 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/benchmarking/lsh/api/app/main.py
--rw-r--r--   0        0        0      121 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/benchmarking/lsh/api/requirements.in
--rw-r--r--   0        0        0      506 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/benchmarking/lsh/api/requirements.txt
--rw-r--r--   0        0        0      160 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/benchmarking/lsh/benchmark/Dockerfile
--rw-r--r--   0        0        0      352 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/benchmarking/lsh/benchmark/README.md
--rw-r--r--   0        0        0     2015 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/benchmarking/lsh/benchmark/benchmark_lsh.py
--rw-r--r--   0        0        0      839 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/benchmarking/lsh/benchmark/cli.py
--rw-r--r--   0        0        0       85 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/benchmarking/lsh/benchmark/requirements.in
--rw-r--r--   0        0        0      743 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/benchmarking/lsh/benchmark/requirements.txt
--rw-r--r--   0        0        0       44 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/benchmarking/lsh/compare/.dockerignore
--rw-r--r--   0        0        0      310 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/benchmarking/lsh/compare/.gitignore
--rw-r--r--   0        0        0       67 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/benchmarking/lsh/compare/Dockerfile
--rw-r--r--   0        0        0      316 2021-02-11 18:51:05.385286 weco-datascience-0.1.8/benchmarking/lsh/compare/README.md
--rw-r--r--   0        0        0   761474 2021-02-11 18:51:05.389286 weco-datascience-0.1.8/benchmarking/lsh/compare/Untitled.png
--rw-r--r--   0        0        0      778 2021-02-11 18:51:05.389286 weco-datascience-0.1.8/benchmarking/lsh/compare/package.json
--rw-r--r--   0        0        0     3150 2021-02-11 18:51:05.389286 weco-datascience-0.1.8/benchmarking/lsh/compare/public/favicon.ico
--rw-r--r--   0        0        0      366 2021-02-11 18:51:05.389286 weco-datascience-0.1.8/benchmarking/lsh/compare/public/index.html
--rw-r--r--   0        0        0     5347 2021-02-11 18:51:05.389286 weco-datascience-0.1.8/benchmarking/lsh/compare/public/logo192.png
--rw-r--r--   0        0        0     9664 2021-02-11 18:51:05.389286 weco-datascience-0.1.8/benchmarking/lsh/compare/public/logo512.png
--rw-r--r--   0        0        0      492 2021-02-11 18:51:05.389286 weco-datascience-0.1.8/benchmarking/lsh/compare/public/manifest.json
--rw-r--r--   0        0        0       67 2021-02-11 18:51:05.389286 weco-datascience-0.1.8/benchmarking/lsh/compare/public/robots.txt
--rw-r--r--   0        0        0      992 2021-02-11 18:51:05.389286 weco-datascience-0.1.8/benchmarking/lsh/compare/src/App.js
--rw-r--r--   0        0        0      931 2021-02-11 18:51:05.389286 weco-datascience-0.1.8/benchmarking/lsh/compare/src/button.js
--rw-r--r--   0        0        0      641 2021-02-11 18:51:05.389286 weco-datascience-0.1.8/benchmarking/lsh/compare/src/choice.js
--rw-r--r--   0        0        0      576 2021-02-11 18:51:05.389286 weco-datascience-0.1.8/benchmarking/lsh/compare/src/header.js
--rw-r--r--   0        0        0      197 2021-02-11 18:51:05.389286 weco-datascience-0.1.8/benchmarking/lsh/compare/src/index.js
--rw-r--r--   0        0        0      389 2021-02-11 18:51:05.389286 weco-datascience-0.1.8/benchmarking/lsh/compare/src/wellcome_image.js
--rw-r--r--   0        0        0   471942 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/compare/yarn.lock
--rw-r--r--   0        0        0     1043 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/docker-compose.yml
--rw-r--r--   0        0        0     1732 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/howto.md
--rw-r--r--   0        0        0      155 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/populate/Dockerfile
--rw-r--r--   0        0        0      423 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/populate/README.md
--rw-r--r--   0        0        0     2141 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/populate/cli.py
--rw-r--r--   0        0        0     2522 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/populate/populate.py
--rw-r--r--   0        0        0      109 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/populate/requirements.in
--rw-r--r--   0        0        0      595 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/populate/requirements.txt
--rw-r--r--   0        0        0     1136 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/results.md
--rw-r--r--   0        0        0      158 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/score/Dockerfile
--rw-r--r--   0        0        0      232 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/score/README.md
--rw-r--r--   0        0        0     4359 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/score/elo.py
--rw-r--r--   0        0        0       49 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/score/requirements.in
--rw-r--r--   0        0        0      480 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/score/requirements.txt
--rw-r--r--   0        0        0     1172 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/score/score_table.py
--rw-r--r--   0        0        0      142 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/src/README.md
--rw-r--r--   0        0        0        0 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/src/__init__.py
--rw-r--r--   0        0        0     3288 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/src/elastic.py
--rw-r--r--   0        0        0     2063 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/benchmarking/lsh/src/lsh_encoder.py
--rw-r--r--   0        0        0     3793 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/hal/README.md
--rw-r--r--   0        0        0        0 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/hal/__init__.py
--rw-r--r--   0        0        0    12605 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/hal/controller.py
--rw-r--r--   0        0        0      655 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/hal/enable_ipywidgets
--rwxr-xr-x   0        0        0     3866 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/hal/hal
--rwxr-xr-x   0        0        0      483 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/hal/install.sh
--rw-r--r--   0        0        0       82 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/hal/requirements.in
--rw-r--r--   0        0        0     1090 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/hal/requirements.txt
--rw-r--r--   0        0        0      355 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/jupyter/README.md
--rwxr-xr-x   0        0        0     2379 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/fetch_images_from_api.py
--rwxr-xr-x   0        0        0     1728 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/index_inferrer_output.py
--rw-r--r--   0        0        0      864 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/palette_adapter.py
--rw-r--r--   0        0        0       96 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/requirements.in
--rw-r--r--   0        0        0     1673 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/requirements.txt
--rwxr-xr-x   0        0        0     1754 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/run_inference.py
--rw-r--r--   0        0        0       87 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/simple-viewer/.babelrc
--rw-r--r--   0        0        0      112 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/simple-viewer/.env
--rw-r--r--   0        0        0      392 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/simple-viewer/.gitignore
--rw-r--r--   0        0        0       39 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/simple-viewer/.prettierignore
--rw-r--r--   0        0        0        3 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/simple-viewer/.prettierrc.json
--rw-r--r--   0        0        0     1842 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/simple-viewer/README.md
--rw-r--r--   0        0        0     1076 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/simple-viewer/modules/colorToPalette.ts
--rw-r--r--   0        0        0      133 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/simple-viewer/modules/csv.ts
--rw-r--r--   0        0        0      132 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/simple-viewer/modules/identity.ts
--rw-r--r--   0        0        0      489 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/simple-viewer/modules/index.ts
--rw-r--r--   0        0        0       75 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/simple-viewer/next-env.d.ts
--rw-r--r--   0        0        0      643 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/simple-viewer/package.json
--rw-r--r--   0        0        0      184 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/simple-viewer/pages/_app.tsx
--rw-r--r--   0        0        0     1763 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/simple-viewer/pages/api/images/similar.ts
--rw-r--r--   0        0        0     6674 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/simple-viewer/pages/index.tsx
--rw-r--r--   0        0        0     1863 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/simple-viewer/services/elastic.ts
--rw-r--r--   0        0        0      354 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/simple-viewer/src/RadioInput.tsx
--rw-r--r--   0        0        0      741 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/simple-viewer/src/usePersistedState.ts
--rw-r--r--   0        0        0      514 2021-02-11 18:51:05.393286 weco-datascience-0.1.8/local_inference/simple-viewer/tsconfig.json
--rw-r--r--   0        0        0   184137 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/local_inference/simple-viewer/yarn.lock
--rw-r--r--   0        0        0       11 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/README.md
--rw-r--r--   0        0        0      565 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/feature_generation/README.md
--rw-r--r--   0        0        0     2525 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/feature_generation/generate_feature_index.py
--rw-r--r--   0        0        0      111 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/feature_generation/requirements.in
--rw-r--r--   0        0        0      820 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/feature_generation/requirements.txt
--rw-r--r--   0        0        0        0 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/feature_generation/src/__init__.py
--rw-r--r--   0        0        0      580 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/feature_generation/src/ann.py
--rw-r--r--   0        0        0     2034 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/feature_generation/src/aws.py
--rw-r--r--   0        0        0     1444 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/feature_generation/src/feature_extraction.py
--rw-r--r--   0        0        0      733 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/feature_generation/src/images.py
--rw-r--r--   0        0        0     1521 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/feature_generation/src/utils.py
--rw-r--r--   0        0        0      139 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/images/README.md
--rw-r--r--   0        0        0      122 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/images/feature_extraction/README.md
--rw-r--r--   0        0        0      496 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/images/feature_extraction/extract_features.py
--rw-r--r--   0        0        0       57 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/images/feature_extraction/requirements.in
--rw-r--r--   0        0        0      574 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/images/feature_extraction/requirements.txt
--rw-r--r--   0        0        0     1958 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/images/feature_extraction/src/aws.py
--rw-r--r--   0        0        0      693 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/images/feature_extraction/src/feature_extraction.py
--rw-r--r--   0        0        0      733 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/images/feature_extraction/src/images.py
--rw-r--r--   0        0        0      191 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/images/lsh_inference/README.md
--rw-r--r--   0        0        0     1048 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/images/lsh_inference/infer_hash.py
--rw-r--r--   0        0        0       86 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/images/lsh_inference/requirements.in
--rw-r--r--   0        0        0      699 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/images/lsh_inference/requirements.txt
--rw-r--r--   0        0        0     1958 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/images/lsh_inference/src/aws.py
--rw-r--r--   0        0        0      424 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/images/lsh_training/README.md
--rw-r--r--   0        0        0       86 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/images/lsh_training/requirements.in
--rw-r--r--   0        0        0      699 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/images/lsh_training/requirements.txt
--rw-r--r--   0        0        0     1958 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/images/lsh_training/src/aws.py
--rw-r--r--   0        0        0      602 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/images/lsh_training/src/lsh.py
--rw-r--r--   0        0        0     2134 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/images/lsh_training/train_lsh.py
--rw-r--r--   0        0        0        5 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/nerd/.gitignore
--rw-r--r--   0        0        0      355 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/nerd/Dockerfile
--rw-r--r--   0        0        0        0 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/nerd/nerd/__init__.py
--rw-r--r--   0        0        0     2815 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/nerd/nerd/disambiguate.py
--rw-r--r--   0        0        0     1722 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/nerd/nerd/extract.py
--rw-r--r--   0        0        0     4121 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/nerd/nerd/model.py
--rw-r--r--   0        0        0     2315 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/nerd/nerd/poll_queue.py
--rw-r--r--   0        0        0     1462 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/nerd/nerd/tokenise.py
--rw-r--r--   0        0        0      112 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/nerd/requirements.in
--rw-r--r--   0        0        0      691 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/nerd/requirements.txt
--rw-r--r--   0        0        0     1934 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/nerd/run_cli.py
--rw-r--r--   0        0        0     1206 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pipelines/nerd/run_ecs.py
--rw-r--r--   0        0        0      747 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/pyproject.toml
--rw-r--r--   0        0        0       11 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/README.md
--rw-r--r--   0        0        0       29 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/archive_exploration/.gitignore
--rw-r--r--   0        0        0    13887 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/archive_exploration/01 - subject coocurrence.ipynb
--rw-r--r--   0        0        0    10698 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/archive_exploration/02 - exploring trees.ipynb
--rw-r--r--   0        0        0    10245 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/archive_exploration/03 - wikipedia linking.ipynb
--rw-r--r--   0        0        0      919 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/archive_exploration/README.md
--rw-r--r--   0        0        0      235 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/arranging_images/Dockerfile
--rw-r--r--   0        0        0      185 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/arranging_images/README.md
--rw-r--r--   0        0        0     1464 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/arranging_images/main.py
--rw-r--r--   0        0        0      129 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/arranging_images/requirements.in
--rw-r--r--   0        0        0     1399 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/arranging_images/requirements.txt
--rw-r--r--   0        0        0        0 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/arranging_images/src/__init__.py
--rw-r--r--   0        0        0     1784 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/arranging_images/src/feature_extraction.py
--rw-r--r--   0        0        0      384 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/arranging_images/src/file_utils.py
--rw-r--r--   0        0        0     2286 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/arranging_images/src/images.py
--rw-r--r--   0        0        0     1027 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/arranging_images/src/spaces.py
--rw-r--r--   0        0        0    19803 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/breadth_metric/Breadth metric v3.ipynb
--rw-r--r--   0        0        0     2227 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/devise/README.md
--rw-r--r--   0        0        0    20926 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/devise/notebooks/01 - recreating the original paper with tiny imagenet.ipynb
--rw-r--r--   0        0        0    15188 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/devise/notebooks/02 - scaling up to imagenet.ipynb
--rw-r--r--   0        0        0    16426 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/devise/notebooks/03 - broadening the scope of our classes.ipynb
--rw-r--r--   0        0        0     9577 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/devise/notebooks/04 - testing on wellcome images.ipynb
--rw-r--r--   0        0        0     8075 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/devise/notebooks/05 - sentence embeddings with infersent.ipynb
--rw-r--r--   0        0        0    14137 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/devise/notebooks/06 - devise against sentence embeddings.ipynb
--rw-r--r--   0        0        0     9458 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/devise/notebooks/07 - sentence embeddings via multi-task learning.ipynb
--rw-r--r--   0        0        0    25606 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/devise/notebooks/08 - sentence embeddings via NLI from scratch.ipynb
--rw-r--r--   0        0        0    28177 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/devise/notebooks/09 - devise against custom sentence embedding.ipynb
--rw-r--r--   0        0        0     8560 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/devise/notebooks/10 - inference only demo.ipynb
--rw-r--r--   0        0        0     3411 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/elastic_lsh/README.md
--rw-r--r--   0        0        0    11354 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/elastic_lsh/notebooks/01 - feature hashing.ipynb
--rw-r--r--   0        0        0     7832 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/elastic_lsh/notebooks/02 - palette hashing.ipynb
--rw-r--r--   0        0        0     5884 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/elastic_lsh/notebooks/03 - direct cosine similarity calculation.ipynb
--rw-r--r--   0        0        0     4531 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/elastic_lsh/notebooks/04 - finding correlated groups of features.ipynb
--rw-r--r--   0        0        0     9436 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/elastic_lsh/notebooks/05 - feature hashing with grouped features.ipynb
--rw-r--r--   0        0        0     5261 2021-02-11 18:51:05.397287 weco-datascience-0.1.8/research_notebooks/elastic_lsh/notebooks/06 - exact evaluation set.ipynb
--rw-r--r--   0        0        0     5816 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/image_pathways/Image pathways project.md
--rw-r--r--   0        0        0    11937 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/1. Preprocess_images.ipynb
--rw-r--r--   0        0        0    14476 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/2. Get_feature_vectors.ipynb
--rw-r--r--   0        0        0     9607 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/3. Graph_pathways.ipynb
--rw-r--r--   0        0        0    22163 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/4. Graph_pathways_experiments.ipynb
--rw-r--r--   0        0        0    10613 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/5. Graph_pathways_focused.ipynb
--rw-r--r--   0        0        0     5984 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/6. Spaced_pathway.ipynb
--rw-r--r--   0        0        0     4452 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/7. Save_all_FV_pathways.ipynb
--rw-r--r--   0        0        0    21016 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/8. Use_all_FV.ipynb
--rw-r--r--   0        0        0    14985 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/medium_blog_images.ipynb
--rw-r--r--   0        0        0    14102 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/old_Make_network_testing.ipynb
--rw-r--r--   0        0        0    14803 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/old_basic_flow_s3sample.ipynb
--rw-r--r--   0        0        0        0 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/image_pathways/src/__init__.py
--rw-r--r--   0        0        0    20216 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/image_pathways/src/network_functions.py
--rw-r--r--   0        0        0       18 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/palette/.gitignore
--rw-r--r--   0        0        0       44 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/palette/README.md
--rw-r--r--   0        0        0     6097 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/palette/notebooks/00 - RGB, LAB, and human colour perception.ipynb
--rw-r--r--   0        0        0    10359 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/palette/notebooks/01 - obtaining palettes.ipynb
--rw-r--r--   0        0        0    19078 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/palette/notebooks/02 - histogram similarity.ipynb
--rw-r--r--   0        0        0     8843 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/palette/notebooks/03 - comparing histograms in 3d.ipynb
--rw-r--r--   0        0        0    90399 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/palette/notebooks/04 - palette based search.ipynb
--rw-r--r--   0        0        0    12990 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/palette/notebooks/05 - vectorised palette distance.ipynb
--rw-r--r--   0        0        0     5416 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/palette/notebooks/06 - vectorised assignment switching.ipynb
--rw-r--r--   0        0        0     3940 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/palette/notebooks/07 - data generation for palette explorer app.ipynb
--rw-r--r--   0        0        0     7478 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/palette/notebooks/08 - get palettes.ipynb
--rw-r--r--   0        0        0     5266 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/palette/notebooks/09 - merge palettes.ipynb
--rw-r--r--   0        0        0     3090 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/palette/notebooks/10 - dataset for palette embedding network.ipynb
--rw-r--r--   0        0        0    10391 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/palette/notebooks/11 - siamese network.ipynb
--rw-r--r--   0        0        0     5828 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/palette/notebooks/12 - embed all palettes.ipynb
--rw-r--r--   0        0        0     3402 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/palette/notebooks/13 - create an nmslib index.ipynb
--rw-r--r--   0        0        0     3347 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/palette/src/utils.py
--rw-r--r--   0        0        0     3971 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/search_intentions/Progression_of_search.ipynb
--rw-r--r--   0        0        0     4268 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/search_intentions/Top_workIds_2days.ipynb
--rw-r--r--   0        0        0     4264 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/search_intentions/Top_workIds_3days.ipynb
--rw-r--r--   0        0        0        0 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/.s3keep
--rw-r--r--   0        0        0    10057 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/01 - knowledge graph embeddings.ipynb
--rw-r--r--   0        0        0    14699 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/02 - stealing links from wikipedia.ipynb
--rw-r--r--   0        0        0     3144 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/03 - graph stuff.ipynb
--rw-r--r--   0        0        0     3819 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/04 - collaborative filtering.ipynb
--rw-r--r--   0        0        0    11529 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/05 - autoencoders.ipynb
--rw-r--r--   0        0        0     5563 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/06 - Singular Value Decomposition.ipynb
--rw-r--r--   0        0        0     6035 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/07 - labelling existing links in wikipedia text.ipynb
--rw-r--r--   0        0        0    16823 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/08 - word-level BiLSTM link labeller.ipynb
--rw-r--r--   0        0        0    16088 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/09 - character based language model, targeting tokens.ipynb
--rw-r--r--   0        0        0    19724 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/10 - character level language model, targeting wvs.ipynb
--rw-r--r--   0        0        0    21038 2021-02-11 18:51:05.401286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/11- making the character-level network bidirectional.ipynb
--rw-r--r--   0        0        0    24048 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/12 - concatenating word- and character-level embeddings.ipynb
--rw-r--r--   0        0        0    35208 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/13 - adding context vectors.ipynb
--rw-r--r--   0        0        0     6706 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/14 - disambiguating.ipynb
--rw-r--r--   0        0        0    25756 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/final network.ipynb
--rw-r--r--   0        0        0    21367 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/pretrained bert.ipynb
--rw-r--r--   0        0        0     1449 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/reading from VHS.ipynb
--rw-r--r--   0        0        0     5873 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/scratch.ipynb
--rw-r--r--   0        0        0     6636 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/test bpe.ipynb
--rw-r--r--   0        0        0        0 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/research_notebooks/wikipedia/src/.s3keep
--rw-r--r--   0        0        0     1608 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/research_notebooks/wikipedia/src/utils.py
--rw-r--r--   0        0        0     1755 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/scripts/download_all_images.py
--rwxr-xr-x   0        0        0     2026 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/scripts/download_oai_harvest.py
--rw-r--r--   0        0        0     4119 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/scripts/generate_api_identifiers.py
--rw-r--r--   0        0        0     2206 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/scripts/populate_dummy_calm_vhs.py
--rw-r--r--   0        0        0     1729 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/scripts/put_wikidata_embeddings_in_dynamodb.py
--rw-r--r--   0        0        0      430 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/terraform/apis/api_gateway.tf
--rw-r--r--   0        0        0       57 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/terraform/apis/ecs.tf
--rw-r--r--   0        0        0      718 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/terraform/apis/locals.tf
--rw-r--r--   0        0        0     2844 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/terraform/apis/main.tf
--rw-r--r--   0        0        0     1882 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/terraform/apis/modules/service/api/api_gateway.tf
--rw-r--r--   0        0        0     1492 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/terraform/apis/modules/service/api/iam.tf
--rw-r--r--   0        0        0       63 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/terraform/apis/modules/service/api/locals.tf
--rw-r--r--   0        0        0     1550 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/terraform/apis/modules/service/api/main.tf
--rw-r--r--   0        0        0      202 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/terraform/apis/modules/service/api/outputs.tf
--rw-r--r--   0        0        0     1087 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/terraform/apis/modules/service/api/variables.tf
--rw-r--r--   0        0        0      207 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/terraform/apis/nlb.tf
--rw-r--r--   0        0        0      639 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/terraform/apis/security_groups.tf
--rw-r--r--   0        0        0      121 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/terraform/apis/service_discovery.tf
--rw-r--r--   0        0        0      782 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/terraform/apis/terraform.tf
--rw-r--r--   0        0        0      403 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/terraform/data_stores/outputs.tf
--rw-r--r--   0        0        0      824 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/terraform/data_stores/s3.tf
--rw-r--r--   0        0        0      470 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/terraform/data_stores/terraform.tf
--rw-r--r--   0        0        0      271 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/ui/.gitignore
--rw-r--r--   0        0        0       20 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/ui/.nowignore
--rw-r--r--   0        0        0      853 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/ui/README.md
--rw-r--r--   0        0        0     2139 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/ui/components/Header.tsx
--rw-r--r--   0        0        0       75 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/ui/next-env.d.ts
--rw-r--r--   0        0        0      164 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/ui/next.config.js
--rw-r--r--   0        0        0   226587 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/ui/package-lock.json
--rw-r--r--   0        0        0      414 2021-02-11 18:51:05.405286 weco-datascience-0.1.8/ui/package.json
--rw-r--r--   0        0        0      548 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/ui/pages/_app.tsx
--rw-r--r--   0        0        0       72 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/ui/pages/index.tsx
--rw-r--r--   0        0        0     1300 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/ui/pages/palette.tsx
--rw-r--r--   0        0        0    15086 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/ui/static/favicon.ico
--rw-r--r--   0        0        0      533 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/ui/tsconfig.json
--rw-r--r--   0        0        0   213657 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/ui/yarn.lock
--rw-r--r--   0        0        0       94 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/README.md
--rw-r--r--   0        0        0      105 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/__init__.py
--rw-r--r--   0        0        0       89 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/api/__init__.py
--rw-r--r--   0        0        0     1494 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/api/catalogue.py
--rw-r--r--   0        0        0     1127 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/api/image.py
--rw-r--r--   0        0        0      671 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/api/snapshot.py
--rw-r--r--   0        0        0      573 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/aws.py
--rw-r--r--   0        0        0     5915 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/batching.py
--rw-r--r--   0        0        0     1201 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/credentials.py
--rw-r--r--   0        0        0     1720 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/http.py
--rw-r--r--   0        0        0     2813 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/image.py
--rw-r--r--   0        0        0      399 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/logging.py
--rw-r--r--   0        0        0     3446 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/reporting.py
--rw-r--r--   0        0        0    14311 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/test/V0002882.jpg
--rw-r--r--   0        0        0      382 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/test/__init__.py
--rw-r--r--   0        0        0       43 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/test/api/__init__.py
--rw-r--r--   0        0        0      690 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/test/api/test_catalogue.py
--rw-r--r--   0        0        0      681 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/test/api/test_images.py
--rw-r--r--   0        0        0     1564 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/test/test_http.py
--rw-r--r--   0        0        0     2477 2021-02-11 18:51:05.409286 weco-datascience-0.1.8/weco_datascience/test/test_image.py
--rw-r--r--   0        0        0     1032 1970-01-01 00:00:00.000000 weco-datascience-0.1.8/setup.py
--rw-r--r--   0        0        0      261 1970-01-01 00:00:00.000000 weco-datascience-0.1.8/PKG-INFO
+-rw-r--r--   0        0        0      652 2021-08-19 13:49:40.734631 weco-datascience-0.1.9/.github/workflows/publish.yml
+-rw-r--r--   0        0        0     4161 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/.gitignore
+-rw-r--r--   0        0        0     1070 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/LICENSE.md
+-rw-r--r--   0        0        0      871 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/Makefile
+-rw-r--r--   0        0        0      495 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/README.md
+-rw-r--r--   0        0        0       95 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/README.md
+-rw-r--r--   0        0        0     1822 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/benchmarking/.gitignore
+-rw-r--r--   0        0        0     1822 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/.gitignore
+-rw-r--r--   0        0        0      650 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/README.md
+-rw-r--r--   0        0        0      142 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/api/Dockerfile
+-rw-r--r--   0        0        0      155 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/api/README.md
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/api/app/__init__.py
+-rw-r--r--   0        0        0     2525 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/api/app/main.py
+-rw-r--r--   0        0        0      121 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/api/requirements.in
+-rw-r--r--   0        0        0      533 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/api/requirements.txt
+-rw-r--r--   0        0        0      160 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/benchmark/Dockerfile
+-rw-r--r--   0        0        0      352 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/benchmark/README.md
+-rw-r--r--   0        0        0     2015 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/benchmark/benchmark_lsh.py
+-rw-r--r--   0        0        0      839 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/benchmark/cli.py
+-rw-r--r--   0        0        0       85 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/benchmark/requirements.in
+-rw-r--r--   0        0        0      743 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/benchmark/requirements.txt
+-rw-r--r--   0        0        0       44 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/.dockerignore
+-rw-r--r--   0        0        0      310 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/.gitignore
+-rw-r--r--   0        0        0       67 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/Dockerfile
+-rw-r--r--   0        0        0      316 2021-08-19 13:49:40.738631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/README.md
+-rw-r--r--   0        0        0   761474 2021-08-19 13:49:40.742631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/Untitled.png
+-rw-r--r--   0        0        0      778 2021-08-19 13:49:40.742631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/package.json
+-rw-r--r--   0        0        0     3150 2021-08-19 13:49:40.742631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/public/favicon.ico
+-rw-r--r--   0        0        0      366 2021-08-19 13:49:40.742631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/public/index.html
+-rw-r--r--   0        0        0     5347 2021-08-19 13:49:40.742631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/public/logo192.png
+-rw-r--r--   0        0        0     9664 2021-08-19 13:49:40.742631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/public/logo512.png
+-rw-r--r--   0        0        0      492 2021-08-19 13:49:40.742631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/public/manifest.json
+-rw-r--r--   0        0        0       67 2021-08-19 13:49:40.742631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/public/robots.txt
+-rw-r--r--   0        0        0      992 2021-08-19 13:49:40.742631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/src/App.js
+-rw-r--r--   0        0        0      931 2021-08-19 13:49:40.742631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/src/button.js
+-rw-r--r--   0        0        0      641 2021-08-19 13:49:40.742631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/src/choice.js
+-rw-r--r--   0        0        0      576 2021-08-19 13:49:40.742631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/src/header.js
+-rw-r--r--   0        0        0      197 2021-08-19 13:49:40.742631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/src/index.js
+-rw-r--r--   0        0        0      389 2021-08-19 13:49:40.742631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/src/wellcome_image.js
+-rw-r--r--   0        0        0   471942 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/yarn.lock
+-rw-r--r--   0        0        0     1043 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/docker-compose.yml
+-rw-r--r--   0        0        0     1732 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/howto.md
+-rw-r--r--   0        0        0      155 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/populate/Dockerfile
+-rw-r--r--   0        0        0      423 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/populate/README.md
+-rw-r--r--   0        0        0     2141 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/populate/cli.py
+-rw-r--r--   0        0        0     2522 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/populate/populate.py
+-rw-r--r--   0        0        0      109 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/populate/requirements.in
+-rw-r--r--   0        0        0      595 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/populate/requirements.txt
+-rw-r--r--   0        0        0     1136 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/results.md
+-rw-r--r--   0        0        0      158 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/score/Dockerfile
+-rw-r--r--   0        0        0      232 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/score/README.md
+-rw-r--r--   0        0        0     4359 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/score/elo.py
+-rw-r--r--   0        0        0       49 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/score/requirements.in
+-rw-r--r--   0        0        0      480 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/score/requirements.txt
+-rw-r--r--   0        0        0     1172 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/score/score_table.py
+-rw-r--r--   0        0        0      142 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/src/README.md
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/src/__init__.py
+-rw-r--r--   0        0        0     3288 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/src/elastic.py
+-rw-r--r--   0        0        0     2063 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/src/lsh_encoder.py
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/devise_search/Dockerfile
+-rw-r--r--   0        0        0     1935 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/devise_search/static/index.html
+-rw-r--r--   0        0        0     1237 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/devise_search/static/javascript.js
+-rw-r--r--   0        0        0      747 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/devise_search/static/style.css
+-rwxr-xr-x   0        0        0     2379 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/fetch_images_from_api.py
+-rwxr-xr-x   0        0        0     1728 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/index_inferrer_output.py
+-rw-r--r--   0        0        0      864 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/palette_adapter.py
+-rw-r--r--   0        0        0       96 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/requirements.in
+-rw-r--r--   0        0        0     1673 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/requirements.txt
+-rwxr-xr-x   0        0        0     1754 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/run_inference.py
+-rw-r--r--   0        0        0       87 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/.babelrc
+-rw-r--r--   0        0        0      112 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/.env
+-rw-r--r--   0        0        0      392 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/.gitignore
+-rw-r--r--   0        0        0       39 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/.prettierignore
+-rw-r--r--   0        0        0        3 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/.prettierrc.json
+-rw-r--r--   0        0        0     1842 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/README.md
+-rw-r--r--   0        0        0     1076 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/modules/colorToPalette.ts
+-rw-r--r--   0        0        0      133 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/modules/csv.ts
+-rw-r--r--   0        0        0      132 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/modules/identity.ts
+-rw-r--r--   0        0        0      489 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/modules/index.ts
+-rw-r--r--   0        0        0       75 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/next-env.d.ts
+-rw-r--r--   0        0        0      643 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/package.json
+-rw-r--r--   0        0        0      184 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/pages/_app.tsx
+-rw-r--r--   0        0        0     1763 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/pages/api/images/similar.ts
+-rw-r--r--   0        0        0     6674 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/pages/index.tsx
+-rw-r--r--   0        0        0     1863 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/services/elastic.ts
+-rw-r--r--   0        0        0      354 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/src/RadioInput.tsx
+-rw-r--r--   0        0        0      741 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/src/usePersistedState.ts
+-rw-r--r--   0        0        0      514 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/tsconfig.json
+-rw-r--r--   0        0        0   184121 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/yarn.lock
+-rw-r--r--   0        0        0      305 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/nerd/.gitignore
+-rw-r--r--   0        0        0      400 2021-08-19 13:49:40.746631 weco-datascience-0.1.9/api_interfaces/nerd/Dockerfile
+-rw-r--r--   0        0        0   525586 2021-08-19 13:49:40.750631 weco-datascience-0.1.9/api_interfaces/nerd/package-lock.json
+-rw-r--r--   0        0        0      614 2021-08-19 13:49:40.750631 weco-datascience-0.1.9/api_interfaces/nerd/package.json
+-rw-r--r--   0        0        0    93062 2021-08-19 13:49:40.750631 weco-datascience-0.1.9/api_interfaces/nerd/public/favicon.ico
+-rw-r--r--   0        0        0     2006 2021-08-19 13:49:40.750631 weco-datascience-0.1.9/api_interfaces/nerd/public/icon.svg
+-rw-r--r--   0        0        0     1558 2021-08-19 13:49:40.750631 weco-datascience-0.1.9/api_interfaces/nerd/public/index.html
+-rw-r--r--   0        0        0      332 2021-08-19 13:49:40.750631 weco-datascience-0.1.9/api_interfaces/nerd/public/manifest.json
+-rw-r--r--   0        0        0     1681 2021-08-19 13:49:40.750631 weco-datascience-0.1.9/api_interfaces/nerd/src/App.js
+-rw-r--r--   0        0        0      248 2021-08-19 13:49:40.750631 weco-datascience-0.1.9/api_interfaces/nerd/src/App.test.js
+-rw-r--r--   0        0        0       66 2021-08-19 13:49:40.750631 weco-datascience-0.1.9/api_interfaces/nerd/src/css/custom.css
+-rw-r--r--   0        0        0   113587 2021-08-19 13:49:40.750631 weco-datascience-0.1.9/api_interfaces/nerd/src/css/tachyons.css
+-rw-r--r--   0        0        0      943 2021-08-19 13:49:40.750631 weco-datascience-0.1.9/api_interfaces/nerd/src/index.js
+-rw-r--r--   0        0        0      271 2021-08-19 13:49:40.750631 weco-datascience-0.1.9/api_interfaces/palette/.gitignore
+-rw-r--r--   0        0        0       20 2021-08-19 13:49:40.750631 weco-datascience-0.1.9/api_interfaces/palette/.nowignore
+-rw-r--r--   0        0        0      853 2021-08-19 13:49:40.750631 weco-datascience-0.1.9/api_interfaces/palette/README.md
+-rw-r--r--   0        0        0     2139 2021-08-19 13:49:40.750631 weco-datascience-0.1.9/api_interfaces/palette/components/Header.tsx
+-rw-r--r--   0        0        0       75 2021-08-19 13:49:40.750631 weco-datascience-0.1.9/api_interfaces/palette/next-env.d.ts
+-rw-r--r--   0        0        0      164 2021-08-19 13:49:40.750631 weco-datascience-0.1.9/api_interfaces/palette/next.config.js
+-rw-r--r--   0        0        0   355162 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/api_interfaces/palette/package-lock.json
+-rw-r--r--   0        0        0      414 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/api_interfaces/palette/package.json
+-rw-r--r--   0        0        0      548 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/api_interfaces/palette/pages/_app.tsx
+-rw-r--r--   0        0        0       72 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/api_interfaces/palette/pages/index.tsx
+-rw-r--r--   0        0        0     1300 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/api_interfaces/palette/pages/palette.tsx
+-rw-r--r--   0        0        0    15086 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/api_interfaces/palette/static/favicon.ico
+-rw-r--r--   0        0        0      533 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/api_interfaces/palette/tsconfig.json
+-rw-r--r--   0        0        0   290856 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/api_interfaces/palette/yarn.lock
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/api_interfaces/palette_similarity/Dockerfile
+-rw-r--r--   0        0        0      296 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/api_interfaces/palette_similarity/palette_dict.pkl
+-rw-r--r--   0        0        0     1724 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/api_interfaces/palette_similarity/static/index.html
+-rw-r--r--   0        0        0     2336 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/api_interfaces/palette_similarity/static/javascript.js
+-rwxr-xr-x   0        0        0    54093 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/api_interfaces/palette_similarity/static/jscolor.js
+-rw-r--r--   0        0        0      575 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/api_interfaces/palette_similarity/static/style.css
+-rw-r--r--   0        0        0      815 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/README.md
+-rw-r--r--   0        0        0      260 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/devise_search/Dockerfile
+-rw-r--r--   0        0        0     2656 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/devise_search/README.md
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/devise_search/app/__init__.py
+-rw-r--r--   0        0        0     1853 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/devise_search/app/api.py
+-rw-r--r--   0        0        0      921 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/devise_search/app/aws.py
+-rw-r--r--   0        0        0      417 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/devise_search/app/sentence_encoder.py
+-rw-r--r--   0        0        0     1792 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/devise_search/app/test_api.py
+-rw-r--r--   0        0        0      633 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/devise_search/app/test_utils.py
+-rw-r--r--   0        0        0     1792 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/devise_search/app/utils.py
+-rw-r--r--   0        0        0      109 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/devise_search/requirements.in
+-rw-r--r--   0        0        0      845 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/devise_search/requirements.txt
+-rw-r--r--   0        0        0      215 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/feature_similarity/Dockerfile
+-rw-r--r--   0        0        0     3385 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/feature_similarity/README.md
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/feature_similarity/__init__.py
+-rw-r--r--   0        0        0        1 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/feature_similarity/app/__init__.py
+-rw-r--r--   0        0        0     1213 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/feature_similarity/app/api.py
+-rw-r--r--   0        0        0      438 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/feature_similarity/app/aws.py
+-rw-r--r--   0        0        0      957 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/feature_similarity/app/identifiers.py
+-rw-r--r--   0        0        0      885 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/feature_similarity/app/neighbours.py
+-rw-r--r--   0        0        0       89 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/feature_similarity/requirements.in
+-rw-r--r--   0        0        0     1070 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/feature_similarity/requirements.txt
+-rw-r--r--   0        0        0      225 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/image_pathways/Dockerfile
+-rw-r--r--   0        0        0     2255 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/image_pathways/README.md
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/image_pathways/app/__init__.py
+-rw-r--r--   0        0        0     1011 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/image_pathways/app/api.py
+-rw-r--r--   0        0        0      921 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/image_pathways/app/aws.py
+-rw-r--r--   0        0        0     2314 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/image_pathways/app/test_api.py
+-rw-r--r--   0        0        0     1510 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/image_pathways/app/test_utils.py
+-rw-r--r--   0        0        0     3204 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/image_pathways/app/utils.py
+-rw-r--r--   0        0        0      304 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/image_pathways/build_nmslib_index.py
+-rw-r--r--   0        0        0       74 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/image_pathways/requirements.in
+-rw-r--r--   0        0        0      960 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/image_pathways/requirements.txt
+-rw-r--r--   0        0        0      225 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/nerd/Dockerfile
+-rw-r--r--   0        0        0     1539 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/nerd/README.md
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/nerd/app/__init__.py
+-rw-r--r--   0        0        0      986 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/nerd/app/api.py
+-rw-r--r--   0        0        0     1581 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/nerd/app/aws.py
+-rw-r--r--   0        0        0     2974 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/nerd/app/backbone.py
+-rw-r--r--   0        0        0     1659 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/nerd/app/character_level_network.py
+-rw-r--r--   0        0        0     1203 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/nerd/app/heads.py
+-rw-r--r--   0        0        0     5300 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/nerd/app/model_utils.py
+-rw-r--r--   0        0        0     2968 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/nerd/app/nerd.py
+-rw-r--r--   0        0        0      170 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/nerd/requirements.in
+-rw-r--r--   0        0        0     1063 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/nerd/requirements.txt
+-rw-r--r--   0        0        0      214 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/palette_similarity/Dockerfile
+-rw-r--r--   0        0        0     6881 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/palette_similarity/README.md
+-rw-r--r--   0        0        0        1 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/palette_similarity/app/__init__.py
+-rw-r--r--   0        0        0     2415 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/palette_similarity/app/api.py
+-rw-r--r--   0        0        0      438 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/palette_similarity/app/aws.py
+-rw-r--r--   0        0        0      346 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/palette_similarity/app/colours.py
+-rw-r--r--   0        0        0     1465 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/palette_similarity/app/identifiers.py
+-rw-r--r--   0        0        0      878 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/palette_similarity/app/neighbours.py
+-rw-r--r--   0        0        0     1800 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/palette_similarity/app/palette_embedder.py
+-rw-r--r--   0        0        0      122 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/palette_similarity/requirements.in
+-rw-r--r--   0        0        0     1489 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/apis/palette_similarity/requirements.txt
+-rw-r--r--   0        0        0      235 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/notebooks/Dockerfile
+-rw-r--r--   0        0        0     1850 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/notebooks/README.md
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/notebooks/analysis/README.md
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/notebooks/analysis/data/.gitkeep
+-rw-r--r--   0        0        0     3026 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/notebooks/analysis/notebooks/Bernard_de_Gordon_query.ipynb
+-rw-r--r--   0        0        0     2958 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/notebooks/analysis/notebooks/art_of_science_query.ipynb
+-rw-r--r--   0        0        0     4382 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/notebooks/analysis/notebooks/breadth_metric.ipynb
+-rw-r--r--   0        0        0     4352 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/notebooks/analysis/notebooks/manifest_usage.ipynb
+-rw-r--r--   0        0        0     2887 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/notebooks/analysis/notebooks/search_dupes.ipynb
+-rw-r--r--   0        0        0       39 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/notebooks/analysis/requirements.common
+-rw-r--r--   0        0        0       44 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/notebooks/analysis/requirements.in
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/notebooks/analysis/src/.gitkeep
+-rw-r--r--   0        0        0       29 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/notebooks/archive_exploration/.gitignore
+-rw-r--r--   0        0        0      919 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/notebooks/archive_exploration/README.md
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.754631 weco-datascience-0.1.9/notebooks/archive_exploration/data/.gitkeep
+-rw-r--r--   0        0        0    13843 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/archive_exploration/notebooks/01 - subject coocurrence.ipynb
+-rw-r--r--   0        0        0    10472 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/archive_exploration/notebooks/02 - exploring trees.ipynb
+-rw-r--r--   0        0        0    10118 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/archive_exploration/notebooks/03 - wikipedia linking.ipynb
+-rw-r--r--   0        0        0       39 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/archive_exploration/requirements.common
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/archive_exploration/requirements.in
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/archive_exploration/src/.gitkeep
+-rw-r--r--   0        0        0     2227 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/devise/README.md
+-rw-r--r--   0        0        0    20745 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/devise/notebooks/01 - recreating the original paper with tiny imagenet.ipynb
+-rw-r--r--   0        0        0    14867 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/devise/notebooks/02 - scaling up to imagenet.ipynb
+-rw-r--r--   0        0        0    16391 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/devise/notebooks/03 - broadening the scope of our classes.ipynb
+-rw-r--r--   0        0        0     9580 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/devise/notebooks/04 - testing on wellcome images.ipynb
+-rw-r--r--   0        0        0     8082 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/devise/notebooks/05 - sentence embeddings with infersent.ipynb
+-rw-r--r--   0        0        0    14041 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/devise/notebooks/06 - devise against sentence embeddings.ipynb
+-rw-r--r--   0        0        0     9350 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/devise/notebooks/07 - sentence embeddings via multi-task learning.ipynb
+-rw-r--r--   0        0        0    25200 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/devise/notebooks/08 - sentence embeddings via NLI from scratch.ipynb
+-rw-r--r--   0        0        0    27840 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/devise/notebooks/09 - devise against custom sentence embedding.ipynb
+-rw-r--r--   0        0        0     8432 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/devise/notebooks/10 - inference only demo.ipynb
+-rw-r--r--   0        0        0       39 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/devise/requirements.common
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/devise/requirements.in
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/devise/src/.gitkeep
+-rw-r--r--   0        0        0     4383 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/docker-compose.yml
+-rw-r--r--   0        0        0     3411 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/elastic_lsh/README.md
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/elastic_lsh/data/.gitkeep
+-rw-r--r--   0        0        0    11231 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/elastic_lsh/notebooks/01 - feature hashing.ipynb
+-rw-r--r--   0        0        0     7803 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/elastic_lsh/notebooks/02 - palette hashing.ipynb
+-rw-r--r--   0        0        0     5832 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/elastic_lsh/notebooks/03 - direct cosine similarity calculation.ipynb
+-rw-r--r--   0        0        0     4533 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/elastic_lsh/notebooks/04 - finding correlated groups of features.ipynb
+-rw-r--r--   0        0        0     9391 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/elastic_lsh/notebooks/05 - feature hashing with grouped features.ipynb
+-rw-r--r--   0        0        0     5277 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/elastic_lsh/notebooks/06 - exact evaluation set.ipynb
+-rw-r--r--   0        0        0       39 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/elastic_lsh/requirements.common
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/elastic_lsh/requirements.in
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/elastic_lsh/src/.gitkeep
+-rw-r--r--   0        0        0     5816 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/image_pathways/README.md
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/image_pathways/data/.gitkeep
+-rw-r--r--   0        0        0    12032 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/image_pathways/notebooks/1. Preprocess_images.ipynb
+-rw-r--r--   0        0        0    14178 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/image_pathways/notebooks/2. Get_feature_vectors.ipynb
+-rw-r--r--   0        0        0     9530 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/image_pathways/notebooks/3. Graph_pathways.ipynb
+-rw-r--r--   0        0        0    22787 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/image_pathways/notebooks/4. Graph_pathways_experiments.ipynb
+-rw-r--r--   0        0        0    10856 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/image_pathways/notebooks/5. Graph_pathways_focused.ipynb
+-rw-r--r--   0        0        0     6228 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/image_pathways/notebooks/6. Spaced_pathway.ipynb
+-rw-r--r--   0        0        0     4484 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/image_pathways/notebooks/7. Save_all_FV_pathways.ipynb
+-rw-r--r--   0        0        0    21750 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/image_pathways/notebooks/8. Use_all_FV.ipynb
+-rw-r--r--   0        0        0    15600 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/image_pathways/notebooks/medium_blog_images.ipynb
+-rw-r--r--   0        0        0    13769 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/image_pathways/notebooks/old_Make_network_testing.ipynb
+-rw-r--r--   0        0        0    14636 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/image_pathways/notebooks/old_basic_flow_s3sample.ipynb
+-rw-r--r--   0        0        0       39 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/image_pathways/requirements.common
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/image_pathways/requirements.in
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/image_pathways/src/.gitkeep
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/image_pathways/src/__init__.py
+-rw-r--r--   0        0        0    20216 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/image_pathways/src/network_functions.py
+-rw-r--r--   0        0        0       18 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/palette/.gitignore
+-rw-r--r--   0        0        0       44 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/palette/README.md
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/palette/data/.gitkeep
+-rw-r--r--   0        0        0     6134 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/palette/notebooks/00 - RGB, LAB, and human colour perception.ipynb
+-rw-r--r--   0        0        0    10387 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/palette/notebooks/01 - obtaining palettes.ipynb
+-rw-r--r--   0        0        0    18704 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/palette/notebooks/02 - histogram similarity.ipynb
+-rw-r--r--   0        0        0     8668 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/palette/notebooks/03 - comparing histograms in 3d.ipynb
+-rw-r--r--   0        0        0    90322 2021-08-19 13:49:40.758631 weco-datascience-0.1.9/notebooks/palette/notebooks/04 - palette based search.ipynb
+-rw-r--r--   0        0        0    12913 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/palette/notebooks/05 - vectorised palette distance.ipynb
+-rw-r--r--   0        0        0     5344 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/palette/notebooks/06 - vectorised assignment switching.ipynb
+-rw-r--r--   0        0        0     3780 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/palette/notebooks/07 - data generation for palette explorer app.ipynb
+-rw-r--r--   0        0        0     7595 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/palette/notebooks/08 - get palettes.ipynb
+-rw-r--r--   0        0        0     5191 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/palette/notebooks/09 - merge palettes.ipynb
+-rw-r--r--   0        0        0     3078 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/palette/notebooks/10 - dataset for palette embedding network.ipynb
+-rw-r--r--   0        0        0    10448 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/palette/notebooks/11 - siamese network.ipynb
+-rw-r--r--   0        0        0     6037 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/palette/notebooks/12 - embed all palettes.ipynb
+-rw-r--r--   0        0        0     3478 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/palette/notebooks/13 - create an nmslib index.ipynb
+-rw-r--r--   0        0        0       39 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/palette/requirements.common
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/palette/requirements.in
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/palette/src/.gitkeep
+-rw-r--r--   0        0        0     3347 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/palette/src/utils.py
+-rw-r--r--   0        0        0      678 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/rank_analysis/README.md
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/rank_analysis/data/.gitkeep
+-rw-r--r--   0        0        0     5473 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/rank_analysis/notebooks/01 - building a dataset.ipynb
+-rw-r--r--   0        0        0     3152 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/rank_analysis/notebooks/02 - simple analysis.ipynb
+-rw-r--r--   0        0        0     9406 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/rank_analysis/notebooks/03 - time travel.ipynb
+-rw-r--r--   0        0        0       39 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/rank_analysis/requirements.common
+-rw-r--r--   0        0        0       22 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/rank_analysis/requirements.in
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/rank_analysis/src/.gitkeep
+-rw-r--r--   0        0        0       39 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/requirements.common
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/search_intentions/README.md
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/search_intentions/data/.gitkeep
+-rw-r--r--   0        0        0     3770 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/search_intentions/notebooks/Progression_of_search.ipynb
+-rw-r--r--   0        0        0     4200 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/search_intentions/notebooks/Top_workIds_2days.ipynb
+-rw-r--r--   0        0        0     4196 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/search_intentions/notebooks/Top_workIds_3days.ipynb
+-rw-r--r--   0        0        0       39 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/search_intentions/requirements.common
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/search_intentions/requirements.in
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/search_intentions/src/.gitkeep
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/README.md
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/data/.gitkeep
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/.s3keep
+-rw-r--r--   0        0        0    10049 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/01 - knowledge graph embeddings.ipynb
+-rw-r--r--   0        0        0    14507 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/02 - stealing links from wikipedia.ipynb
+-rw-r--r--   0        0        0     3162 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/03 - graph stuff.ipynb
+-rw-r--r--   0        0        0     3787 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/04 - collaborative filtering.ipynb
+-rw-r--r--   0        0        0    11204 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/05 - autoencoders.ipynb
+-rw-r--r--   0        0        0     5412 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/06 - Singular Value Decomposition.ipynb
+-rw-r--r--   0        0        0     6051 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/07 - labelling existing links in wikipedia text.ipynb
+-rw-r--r--   0        0        0    16407 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/08 - word-level BiLSTM link labeller.ipynb
+-rw-r--r--   0        0        0    15624 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/09 - character based language model, targeting tokens.ipynb
+-rw-r--r--   0        0        0    19302 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/10 - character level language model, targeting wvs.ipynb
+-rw-r--r--   0        0        0    20621 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/11- making the character-level network bidirectional.ipynb
+-rw-r--r--   0        0        0    23293 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/12 - concatenating word- and character-level embeddings.ipynb
+-rw-r--r--   0        0        0    34615 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/13 - adding context vectors.ipynb
+-rw-r--r--   0        0        0     6713 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/14 - disambiguating.ipynb
+-rw-r--r--   0        0        0    24956 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/final network.ipynb
+-rw-r--r--   0        0        0    21226 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/pretrained bert.ipynb
+-rw-r--r--   0        0        0     1433 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/reading from VHS.ipynb
+-rw-r--r--   0        0        0     5891 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/scratch.ipynb
+-rw-r--r--   0        0        0     6924 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/notebooks/test bpe.ipynb
+-rw-r--r--   0        0        0       39 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/requirements.common
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/requirements.in
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/src/.gitkeep
+-rw-r--r--   0        0        0     1608 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/notebooks/wikipedia/src/utils.py
+-rw-r--r--   0        0        0       11 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/README.md
+-rw-r--r--   0        0        0      235 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/arranging_images/Dockerfile
+-rw-r--r--   0        0        0      185 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/arranging_images/README.md
+-rw-r--r--   0        0        0     1464 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/arranging_images/main.py
+-rw-r--r--   0        0        0      129 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/arranging_images/requirements.in
+-rw-r--r--   0        0        0     1399 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/arranging_images/requirements.txt
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/arranging_images/src/__init__.py
+-rw-r--r--   0        0        0     1784 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/arranging_images/src/feature_extraction.py
+-rw-r--r--   0        0        0      384 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/arranging_images/src/file_utils.py
+-rw-r--r--   0        0        0     2286 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/arranging_images/src/images.py
+-rw-r--r--   0        0        0     1027 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/arranging_images/src/spaces.py
+-rw-r--r--   0        0        0      565 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/feature_generation/README.md
+-rw-r--r--   0        0        0     2525 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/feature_generation/generate_feature_index.py
+-rw-r--r--   0        0        0      111 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/feature_generation/requirements.in
+-rw-r--r--   0        0        0      820 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/feature_generation/requirements.txt
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/feature_generation/src/__init__.py
+-rw-r--r--   0        0        0      580 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/feature_generation/src/ann.py
+-rw-r--r--   0        0        0     2034 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/feature_generation/src/aws.py
+-rw-r--r--   0        0        0     1444 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/feature_generation/src/feature_extraction.py
+-rw-r--r--   0        0        0      733 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/feature_generation/src/images.py
+-rw-r--r--   0        0        0     1521 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/feature_generation/src/utils.py
+-rw-r--r--   0        0        0      139 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/images/README.md
+-rw-r--r--   0        0        0      122 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/images/feature_extraction/README.md
+-rw-r--r--   0        0        0      496 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/images/feature_extraction/extract_features.py
+-rw-r--r--   0        0        0       57 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/images/feature_extraction/requirements.in
+-rw-r--r--   0        0        0      574 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/images/feature_extraction/requirements.txt
+-rw-r--r--   0        0        0     1958 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/images/feature_extraction/src/aws.py
+-rw-r--r--   0        0        0      693 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/images/feature_extraction/src/feature_extraction.py
+-rw-r--r--   0        0        0      733 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/images/feature_extraction/src/images.py
+-rw-r--r--   0        0        0      191 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/images/lsh_inference/README.md
+-rw-r--r--   0        0        0     1048 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/images/lsh_inference/infer_hash.py
+-rw-r--r--   0        0        0       86 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/images/lsh_inference/requirements.in
+-rw-r--r--   0        0        0      699 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/images/lsh_inference/requirements.txt
+-rw-r--r--   0        0        0     1958 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/images/lsh_inference/src/aws.py
+-rw-r--r--   0        0        0      424 2021-08-19 13:49:40.762631 weco-datascience-0.1.9/pipelines/images/lsh_training/README.md
+-rw-r--r--   0        0        0       86 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/pipelines/images/lsh_training/requirements.in
+-rw-r--r--   0        0        0      699 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/pipelines/images/lsh_training/requirements.txt
+-rw-r--r--   0        0        0     1958 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/pipelines/images/lsh_training/src/aws.py
+-rw-r--r--   0        0        0      602 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/pipelines/images/lsh_training/src/lsh.py
+-rw-r--r--   0        0        0     2134 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/pipelines/images/lsh_training/train_lsh.py
+-rw-r--r--   0        0        0        5 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/pipelines/nerd/.gitignore
+-rw-r--r--   0        0        0      355 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/pipelines/nerd/Dockerfile
+-rw-r--r--   0        0        0        0 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/pipelines/nerd/nerd/__init__.py
+-rw-r--r--   0        0        0     2815 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/pipelines/nerd/nerd/disambiguate.py
+-rw-r--r--   0        0        0     1722 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/pipelines/nerd/nerd/extract.py
+-rw-r--r--   0        0        0     4121 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/pipelines/nerd/nerd/model.py
+-rw-r--r--   0        0        0     2315 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/pipelines/nerd/nerd/poll_queue.py
+-rw-r--r--   0        0        0     1462 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/pipelines/nerd/nerd/tokenise.py
+-rw-r--r--   0        0        0      112 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/pipelines/nerd/requirements.in
+-rw-r--r--   0        0        0      691 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/pipelines/nerd/requirements.txt
+-rw-r--r--   0        0        0     1934 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/pipelines/nerd/run_cli.py
+-rw-r--r--   0        0        0     1206 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/pipelines/nerd/run_ecs.py
+-rw-r--r--   0        0        0      774 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/pyproject.toml
+-rw-r--r--   0        0        0     1755 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/scripts/download_all_images.py
+-rwxr-xr-x   0        0        0     2026 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/scripts/download_oai_harvest.py
+-rw-r--r--   0        0        0     4119 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/scripts/generate_api_identifiers.py
+-rw-r--r--   0        0        0     2206 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/scripts/populate_dummy_calm_vhs.py
+-rw-r--r--   0        0        0     1729 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/scripts/put_wikidata_embeddings_in_dynamodb.py
+-rw-r--r--   0        0        0      401 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/.terraform.lock.hcl
+-rw-r--r--   0        0        0      438 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/api_gateway.tf
+-rw-r--r--   0        0        0       57 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/ecs.tf
+-rw-r--r--   0        0        0      722 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/locals.tf
+-rw-r--r--   0        0        0     2777 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/main.tf
+-rw-r--r--   0        0        0     1596 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/ecs/modules/service/prebuilt/rest/tcp/iam.tf
+-rw-r--r--   0        0        0     1066 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/ecs/modules/service/prebuilt/rest/tcp/main.tf
+-rw-r--r--   0        0        0      126 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/ecs/modules/service/prebuilt/rest/tcp/outputs.tf
+-rw-r--r--   0        0        0      581 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/ecs/modules/service/prebuilt/rest/tcp/target_group.tf
+-rw-r--r--   0        0        0      632 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/ecs/modules/service/prebuilt/rest/tcp/variables.tf
+-rw-r--r--   0        0        0     1914 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/ecs/modules/task/modules/container_definition/container_with_sidecar/main.tf
+-rw-r--r--   0        0        0      215 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/ecs/modules/task/modules/container_definition/container_with_sidecar/outputs.tf
+-rw-r--r--   0        0        0     1128 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/ecs/modules/task/modules/container_definition/container_with_sidecar/task_definition.json.template
+-rw-r--r--   0        0        0     1012 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/ecs/modules/task/modules/container_definition/container_with_sidecar/variables.tf
+-rw-r--r--   0        0        0     2564 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/ecs/modules/task/modules/env_vars/main.tf
+-rw-r--r--   0        0        0       60 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/ecs/modules/task/modules/env_vars/outputs.tf
+-rw-r--r--   0        0        0      148 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/ecs/modules/task/modules/env_vars/variables.tf
+-rw-r--r--   0        0        0      864 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/ecs/modules/task/modules/port_mappings/main.tf
+-rw-r--r--   0        0        0      558 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/ecs/modules/task/modules/task_definition/default/main.tf
+-rw-r--r--   0        0        0      137 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/ecs/modules/task/modules/task_definition/default/outputs.tf
+-rw-r--r--   0        0        0      344 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/ecs/modules/task/modules/task_definition/default/variables.tf
+-rw-r--r--   0        0        0     1333 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/ecs/modules/task/prebuilt/container_with_sidecar/main.tf
+-rw-r--r--   0        0        0      471 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/ecs/modules/task/prebuilt/container_with_sidecar/outputs.tf
+-rw-r--r--   0        0        0      917 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/ecs/modules/task/prebuilt/container_with_sidecar/variables.tf
+-rw-r--r--   0        0        0     1799 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/service/api/api_gateway.tf
+-rw-r--r--   0        0        0     1438 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/service/api/iam.tf
+-rw-r--r--   0        0        0       63 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/service/api/locals.tf
+-rw-r--r--   0        0        0     1293 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/service/api/main.tf
+-rw-r--r--   0        0        0      187 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/service/api/outputs.tf
+-rw-r--r--   0        0        0     1111 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/modules/service/api/variables.tf
+-rw-r--r--   0        0        0      198 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/nlb.tf
+-rw-r--r--   0        0        0      624 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/security_groups.tf
+-rw-r--r--   0        0        0      116 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/service_discovery.tf
+-rw-r--r--   0        0        0      764 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/apis/terraform.tf
+-rw-r--r--   0        0        0      252 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/data_stores/.terraform.lock.hcl
+-rw-r--r--   0        0        0      383 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/data_stores/outputs.tf
+-rw-r--r--   0        0        0      807 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/data_stores/s3.tf
+-rw-r--r--   0        0        0      448 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/terraform/data_stores/terraform.tf
+-rw-r--r--   0        0        0       94 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/weco_datascience/README.md
+-rw-r--r--   0        0        0      105 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/weco_datascience/__init__.py
+-rw-r--r--   0        0        0       89 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/weco_datascience/api/__init__.py
+-rw-r--r--   0        0        0     1495 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/weco_datascience/api/catalogue.py
+-rw-r--r--   0        0        0     1128 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/weco_datascience/api/image.py
+-rw-r--r--   0        0        0      671 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/weco_datascience/api/snapshot.py
+-rw-r--r--   0        0        0     1345 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/weco_datascience/aws.py
+-rw-r--r--   0        0        0     5915 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/weco_datascience/batching.py
+-rw-r--r--   0        0        0     1201 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/weco_datascience/credentials.py
+-rw-r--r--   0        0        0     1720 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/weco_datascience/http.py
+-rw-r--r--   0        0        0     2813 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/weco_datascience/image.py
+-rw-r--r--   0        0        0      399 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/weco_datascience/logging.py
+-rw-r--r--   0        0        0     4450 2021-08-19 13:50:37.275503 weco-datascience-0.1.9/weco_datascience/reporting.py
+-rw-r--r--   0        0        0    14311 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/weco_datascience/test/V0002882.jpg
+-rw-r--r--   0        0        0      382 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/weco_datascience/test/__init__.py
+-rw-r--r--   0        0        0       43 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/weco_datascience/test/api/__init__.py
+-rw-r--r--   0        0        0      690 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/weco_datascience/test/api/test_catalogue.py
+-rw-r--r--   0        0        0      681 2021-08-19 13:49:40.766631 weco-datascience-0.1.9/weco_datascience/test/api/test_images.py
+-rw-r--r--   0        0        0     1516 2021-08-19 13:50:36.183480 weco-datascience-0.1.9/weco_datascience/test/test_http.py
+-rw-r--r--   0        0        0     2478 2021-08-19 13:50:36.223481 weco-datascience-0.1.9/weco_datascience/test/test_image.py
+-rw-r--r--   0        0        0     1056 1970-01-01 00:00:00.000000 weco-datascience-0.1.9/setup.py
+-rw-r--r--   0        0        0      944 1970-01-01 00:00:00.000000 weco-datascience-0.1.9/PKG-INFO
```

### Comparing `weco-datascience-0.1.8/.github/workflows/publish.yml` & `weco-datascience-0.1.9/.github/workflows/publish.yml`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/.gitignore` & `weco-datascience-0.1.9/.gitignore`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,8 @@
 ### Project 
-**data/
-
 .idea
 *.iml
 .lambda_hashes.json
 terraform.plan
 .terraform
 shared_infra/.releases
 *.tfvars
@@ -100,14 +98,15 @@
 # PyBuilder
 target/
 
 # Jupyter Notebook
 .ipynb_checkpoints
 ignore_this_code.ipynb
 tst_ignore.ipynb
+.jupyter/
 
 # IPython
 profile_default/
 ipython_config.py
 
 # pyenv
 .python-version
@@ -243,7 +242,10 @@
 # Include tfplan files to ignore the plan output of command: terraform plan -out=tfplan
 # example: *tfplan*
 
 # End of https://www.gitignore.io/api/flask,python,terraform,jupyternotebook
 
 node_modules/
 .next/
+**/data/**/*
+!.gitkeep
+.DS_Store
```

### Comparing `weco-datascience-0.1.8/LICENSE.md` & `weco-datascience-0.1.9/LICENSE.md`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/api_interfaces/devise_search/static/index.html` & `weco-datascience-0.1.9/api_interfaces/devise_search/static/index.html`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/api_interfaces/devise_search/static/javascript.js` & `weco-datascience-0.1.9/api_interfaces/devise_search/static/javascript.js`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/api_interfaces/devise_search/static/style.css` & `weco-datascience-0.1.9/api_interfaces/devise_search/static/style.css`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/api_interfaces/nerd/package-lock.json` & `weco-datascience-0.1.9/api_interfaces/nerd/package-lock.json`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/api_interfaces/nerd/package.json` & `weco-datascience-0.1.9/api_interfaces/nerd/package.json`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/api_interfaces/nerd/public/favicon.ico` & `weco-datascience-0.1.9/api_interfaces/nerd/public/favicon.ico`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/api_interfaces/nerd/public/icon.svg` & `weco-datascience-0.1.9/api_interfaces/nerd/public/icon.svg`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/api_interfaces/nerd/public/index.html` & `weco-datascience-0.1.9/api_interfaces/nerd/public/index.html`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/api_interfaces/nerd/src/App.js` & `weco-datascience-0.1.9/api_interfaces/nerd/src/App.js`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/api_interfaces/nerd/src/css/tachyons.css` & `weco-datascience-0.1.9/api_interfaces/nerd/src/css/tachyons.css`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/api_interfaces/nerd/src/index.js` & `weco-datascience-0.1.9/api_interfaces/nerd/src/index.js`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/api_interfaces/palette_similarity/static/index.html` & `weco-datascience-0.1.9/api_interfaces/palette_similarity/static/index.html`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/api_interfaces/palette_similarity/static/javascript.js` & `weco-datascience-0.1.9/api_interfaces/palette_similarity/static/javascript.js`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/api_interfaces/palette_similarity/static/jscolor.js` & `weco-datascience-0.1.9/api_interfaces/palette_similarity/static/jscolor.js`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/api_interfaces/palette_similarity/static/style.css` & `weco-datascience-0.1.9/api_interfaces/palette_similarity/static/style.css`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/README.md` & `weco-datascience-0.1.9/apis/README.md`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/devise_search/README.md` & `weco-datascience-0.1.9/apis/devise_search/README.md`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/devise_search/app/api.py` & `weco-datascience-0.1.9/apis/devise_search/app/api.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/devise_search/app/aws.py` & `weco-datascience-0.1.9/apis/devise_search/app/aws.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/devise_search/app/test_api.py` & `weco-datascience-0.1.9/apis/devise_search/app/test_api.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/devise_search/app/test_utils.py` & `weco-datascience-0.1.9/apis/devise_search/app/test_utils.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/devise_search/app/utils.py` & `weco-datascience-0.1.9/apis/devise_search/app/utils.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/devise_search/requirements.txt` & `weco-datascience-0.1.9/apis/devise_search/requirements.txt`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/feature_similarity/README.md` & `weco-datascience-0.1.9/apis/feature_similarity/README.md`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/feature_similarity/app/api.py` & `weco-datascience-0.1.9/apis/feature_similarity/app/api.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/feature_similarity/app/identifiers.py` & `weco-datascience-0.1.9/apis/feature_similarity/app/identifiers.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/feature_similarity/app/neighbours.py` & `weco-datascience-0.1.9/apis/feature_similarity/app/neighbours.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/feature_similarity/requirements.txt` & `weco-datascience-0.1.9/apis/feature_similarity/requirements.txt`

 * *Files 17% similar despite different names*

```diff
@@ -1,28 +1,61 @@
 #
 # This file is autogenerated by pip-compile
 # To update, run:
 #
 #    pip-compile requirements.in
 #
-boto3==1.9.198            # via -r requirements.in
-botocore==1.12.223        # via boto3, s3transfer
-click==7.0                # via uvicorn
-docutils==0.15.2          # via botocore
-fastapi==0.36.0           # via -r requirements.in
-h11==0.8.1                # via uvicorn
-httptools==0.1.1          # via uvicorn
-jmespath==0.9.4           # via boto3, botocore
-nmslib==1.8.1             # via -r requirements.in
-numpy==1.17.0             # via -r requirements.in, nmslib, pandas
-pandas==0.23.4            # via -r requirements.in
-pybind11==2.3.0           # via nmslib
-pydantic==0.30            # via fastapi
-python-dateutil==2.8.0    # via botocore, pandas
-pytz==2019.3              # via pandas
-s3transfer==0.2.1         # via boto3
-six==1.12.0               # via python-dateutil
-starlette==0.12.7         # via fastapi
-urllib3==1.25.3           # via botocore
-uvicorn==0.11.7           # via -r requirements.in
-uvloop==0.14.0            # via uvicorn
-websockets==8.1           # via uvicorn
+boto3==1.9.198
+    # via -r requirements.in
+botocore==1.12.223
+    # via
+    #   boto3
+    #   s3transfer
+click==7.0
+    # via uvicorn
+docutils==0.15.2
+    # via botocore
+fastapi==0.65.2
+    # via -r requirements.in
+h11==0.8.1
+    # via uvicorn
+httptools==0.1.1
+    # via uvicorn
+jmespath==0.9.4
+    # via
+    #   boto3
+    #   botocore
+nmslib==1.8.1
+    # via -r requirements.in
+numpy==1.17.0
+    # via
+    #   -r requirements.in
+    #   nmslib
+    #   pandas
+pandas==0.23.4
+    # via -r requirements.in
+pybind11==2.3.0
+    # via nmslib
+pydantic==1.8.2
+    # via fastapi
+python-dateutil==2.8.0
+    # via
+    #   botocore
+    #   pandas
+pytz==2019.3
+    # via pandas
+s3transfer==0.2.1
+    # via boto3
+six==1.12.0
+    # via python-dateutil
+starlette==0.14.2
+    # via fastapi
+typing-extensions==3.10.0.0
+    # via pydantic
+urllib3==1.25.3
+    # via botocore
+uvicorn==0.11.7
+    # via -r requirements.in
+uvloop==0.14.0
+    # via uvicorn
+websockets==8.1
+    # via uvicorn
```

### Comparing `weco-datascience-0.1.8/apis/image_pathways/README.md` & `weco-datascience-0.1.9/apis/image_pathways/README.md`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/image_pathways/app/api.py` & `weco-datascience-0.1.9/apis/image_pathways/app/api.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/image_pathways/app/aws.py` & `weco-datascience-0.1.9/apis/image_pathways/app/aws.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/image_pathways/app/test_api.py` & `weco-datascience-0.1.9/apis/image_pathways/app/test_api.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/image_pathways/app/test_utils.py` & `weco-datascience-0.1.9/apis/image_pathways/app/test_utils.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/image_pathways/app/utils.py` & `weco-datascience-0.1.9/apis/image_pathways/app/utils.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/nerd/README.md` & `weco-datascience-0.1.9/apis/nerd/README.md`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/nerd/app/api.py` & `weco-datascience-0.1.9/apis/nerd/app/api.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/nerd/app/aws.py` & `weco-datascience-0.1.9/apis/nerd/app/aws.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/nerd/app/backbone.py` & `weco-datascience-0.1.9/apis/nerd/app/backbone.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/nerd/app/character_level_network.py` & `weco-datascience-0.1.9/apis/nerd/app/character_level_network.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/nerd/app/heads.py` & `weco-datascience-0.1.9/apis/nerd/app/heads.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/nerd/app/model_utils.py` & `weco-datascience-0.1.9/apis/nerd/app/model_utils.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/nerd/app/nerd.py` & `weco-datascience-0.1.9/apis/nerd/app/nerd.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/nerd/requirements.txt` & `weco-datascience-0.1.9/apis/nerd/requirements.txt`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/palette_similarity/README.md` & `weco-datascience-0.1.9/apis/palette_similarity/README.md`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/palette_similarity/app/api.py` & `weco-datascience-0.1.9/apis/palette_similarity/app/api.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/palette_similarity/app/identifiers.py` & `weco-datascience-0.1.9/apis/palette_similarity/app/identifiers.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/palette_similarity/app/neighbours.py` & `weco-datascience-0.1.9/apis/palette_similarity/app/neighbours.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/palette_similarity/app/palette_embedder.py` & `weco-datascience-0.1.9/apis/palette_similarity/app/palette_embedder.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/apis/palette_similarity/requirements.txt` & `weco-datascience-0.1.9/apis/palette_similarity/requirements.txt`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/.gitignore` & `weco-datascience-0.1.9/api_interfaces/benchmarking/.gitignore`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/.gitignore` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/.gitignore`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/README.md` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/README.md`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/api/app/main.py` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/api/app/main.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/benchmark/benchmark_lsh.py` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/benchmark/benchmark_lsh.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/benchmark/cli.py` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/benchmark/cli.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/benchmark/requirements.txt` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/benchmark/requirements.txt`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/compare/Untitled.png` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/Untitled.png`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/compare/package.json` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/package.json`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/compare/public/favicon.ico` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/public/favicon.ico`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/compare/public/logo192.png` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/public/logo192.png`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/compare/public/logo512.png` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/public/logo512.png`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/compare/src/App.js` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/src/App.js`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/compare/src/button.js` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/src/button.js`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/compare/src/choice.js` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/src/choice.js`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/compare/src/header.js` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/src/header.js`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/compare/yarn.lock` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/compare/yarn.lock`

 * *Files 0% similar despite different names*

```diff
@@ -2967,17 +2967,17 @@
 
 color-name@^1.0.0, color-name@~1.1.4:
   version "1.1.4"
   resolved "https://registry.yarnpkg.com/color-name/-/color-name-1.1.4.tgz#c2a09a87acbde69543de6f63fa3995c826c536a2"
   integrity sha512-dOy+3AuW3a2wNbZHIuMZpTcgjGuLU/uBL/ubcZF9OXbDo8ff4O8yVp5Bf0efS8uEoYo5q4Fx7dY9OgQGXgAsQA==
 
 color-string@^1.5.2:
-  version "1.5.3"
-  resolved "https://registry.yarnpkg.com/color-string/-/color-string-1.5.3.tgz#c9bbc5f01b58b5492f3d6857459cb6590ce204cc"
-  integrity sha512-dC2C5qeWoYkxki5UAXapdjqO672AM4vZuPGRQfO8b5HKuKGBbKWpITyDYN7TOFKvRW7kOgAn3746clDBMDJyQw==
+  version "1.6.0"
+  resolved "https://registry.yarnpkg.com/color-string/-/color-string-1.6.0.tgz#c3915f61fe267672cb7e1e064c9d692219f6c312"
+  integrity sha512-c/hGS+kRWJutUBEngKKmk4iH3sD59MBkoxVapS/0wgpCz2u7XsNloxknyvBhzwEs1IbV36D9PwqLPJ2DTu3vMA==
   dependencies:
     color-name "^1.0.0"
     simple-swizzle "^0.2.2"
 
 color@^3.0.0:
   version "3.1.2"
   resolved "https://registry.yarnpkg.com/color/-/color-3.1.2.tgz#68148e7f85d41ad7649c5fa8c8106f098d229e10"
@@ -3668,17 +3668,17 @@
 
 dns-equal@^1.0.0:
   version "1.0.0"
   resolved "https://registry.yarnpkg.com/dns-equal/-/dns-equal-1.0.0.tgz#b39e7f1da6eb0a75ba9c17324b34753c47e0654d"
   integrity sha1-s55/HabrCnW6nBcySzR1PEfgZU0=
 
 dns-packet@^1.3.1:
-  version "1.3.1"
-  resolved "https://registry.yarnpkg.com/dns-packet/-/dns-packet-1.3.1.tgz#12aa426981075be500b910eedcd0b47dd7deda5a"
-  integrity sha512-0UxfQkMhYAUaZI+xrNZOz/as5KgDU0M/fQ9b6SpkyLbk3GEswDi6PADJVaYJradtRVsRIlF1zLyOodbcTCDzUg==
+  version "1.3.4"
+  resolved "https://registry.yarnpkg.com/dns-packet/-/dns-packet-1.3.4.tgz#e3455065824a2507ba886c55a89963bb107dec6f"
+  integrity sha512-BQ6F4vycLXBvdrJZ6S3gZewt6rcrks9KBgM9vrhW+knGRqc8uEdT7fuCwloc7nny5xNoMJ17HGH0R/6fpo8ECA==
   dependencies:
     ip "^1.1.0"
     safe-buffer "^5.0.1"
 
 dns-txt@^2.0.2:
   version "2.0.2"
   resolved "https://registry.yarnpkg.com/dns-txt/-/dns-txt-2.0.2.tgz#b91d806f5d27188e4ab3e7d107d881a1cc4642b6"
@@ -4413,17 +4413,17 @@
   version "2.0.1"
   resolved "https://registry.yarnpkg.com/fb-watchman/-/fb-watchman-2.0.1.tgz#fc84fb39d2709cf3ff6d743706157bb5708a8a85"
   integrity sha512-DkPJKQeY6kKwmuMretBhr7G6Vodr7bFwDYTXIkfG1gjvNpaxBTQV3PbXg6bR1c1UP4jPOX0jHUbbHANL9vRjVg==
   dependencies:
     bser "2.1.1"
 
 figgy-pudding@^3.5.1:
-  version "3.5.1"
-  resolved "https://registry.yarnpkg.com/figgy-pudding/-/figgy-pudding-3.5.1.tgz#862470112901c727a0e495a80744bd5baa1d6790"
-  integrity sha512-vNKxJHTEKNThjfrdJwHc7brvM6eVevuO5nTj6ez8ZQ1qbXTvGthucRF7S4vf2cr71QVnT70V34v0S1DyQsti0w==
+  version "3.5.2"
+  resolved "https://registry.yarnpkg.com/figgy-pudding/-/figgy-pudding-3.5.2.tgz#b4eee8148abb01dcf1d1ac34367d59e12fa61d6e"
+  integrity sha512-0btnI/H8f2pavGMN8w40mlSKOfTK2SVJmBfBeVIj3kNw0swwgzyRq0d5TJVOwodFmtvpPeWPN/MCcfuWF0Ezbw==
 
 figures@^3.0.0:
   version "3.2.0"
   resolved "https://registry.yarnpkg.com/figures/-/figures-3.2.0.tgz#625c18bd293c604dc4a8ddb2febf0c88341746af"
   integrity sha512-yaduQFRKLXYOGgEn6AZau90j3ggSOyiqXU0F9JZfeXYhNa+Jk4X+s45A2zg5jns87GAFa34BBm2kXw4XpNcbdg==
   dependencies:
     escape-string-regexp "^1.0.5"
@@ -4972,17 +4972,17 @@
   integrity sha1-0nRXAQJabHdabFRXk+1QL8DGSaE=
   dependencies:
     hash.js "^1.0.3"
     minimalistic-assert "^1.0.0"
     minimalistic-crypto-utils "^1.0.1"
 
 hosted-git-info@^2.1.4:
-  version "2.8.8"
-  resolved "https://registry.yarnpkg.com/hosted-git-info/-/hosted-git-info-2.8.8.tgz#7539bd4bc1e0e0a895815a2e0262420b12858488"
-  integrity sha512-f/wzC2QaWBs7t9IYqB4T3sR1xviIViXJRJTWBlx2Gf3g0Xi5vI7Yy4koXQ1c9OYDGHN9sBy1DQ2AB8fqZBWhUg==
+  version "2.8.9"
+  resolved "https://registry.yarnpkg.com/hosted-git-info/-/hosted-git-info-2.8.9.tgz#dffc0bf9a21c02209090f2aa69429e1414daf3f9"
+  integrity sha512-mxIDAb9Lsm6DoOJ7xH+5+X4y1LU/4Hi50L9C5sIswK3JzULS4bwk1FvjdBgvYR4bzT4tuUQiC15FE2f5HbLvYw==
 
 hpack.js@^2.1.6:
   version "2.1.6"
   resolved "https://registry.yarnpkg.com/hpack.js/-/hpack.js-2.1.6.tgz#87774c0949e513f42e84575b3c45681fade2a0b2"
   integrity sha1-h3dMCUnlE/QuhFdbPEVoH63ioLI=
   dependencies:
     inherits "^2.0.1"
@@ -6614,17 +6614,17 @@
   resolved "https://registry.yarnpkg.com/memory-fs/-/memory-fs-0.5.0.tgz#324c01288b88652966d161db77838720845a8e3c"
   integrity sha512-jA0rdU5KoQMC0e6ppoNRtpp6vjFq6+NY7r8hywnC7V+1Xj/MtHwGIbB1QaK/dunyjWteJzmkpd7ooeWg10T7GA==
   dependencies:
     errno "^0.1.3"
     readable-stream "^2.0.1"
 
 merge-deep@^3.0.2:
-  version "3.0.2"
-  resolved "https://registry.yarnpkg.com/merge-deep/-/merge-deep-3.0.2.tgz#f39fa100a4f1bd34ff29f7d2bf4508fbb8d83ad2"
-  integrity sha512-T7qC8kg4Zoti1cFd8Cr0M+qaZfOwjlPDEdZIIPPB2JZctjaPM4fX+i7HOId69tAti2fvO6X5ldfYUONDODsrkA==
+  version "3.0.3"
+  resolved "https://registry.yarnpkg.com/merge-deep/-/merge-deep-3.0.3.tgz#1a2b2ae926da8b2ae93a0ac15d90cd1922766003"
+  integrity sha512-qtmzAS6t6grwEkNrunqTBdn0qKwFgNWvlxUbAV8es9M7Ot1EbyApytCnvE0jALPa46ZpKDUo527kKiaWplmlFA==
   dependencies:
     arr-union "^3.1.0"
     clone-deep "^0.2.4"
     kind-of "^3.0.2"
 
 merge-descriptors@1.0.1:
   version "1.0.1"
@@ -8482,17 +8482,17 @@
 
 querystring@0.2.0:
   version "0.2.0"
   resolved "https://registry.yarnpkg.com/querystring/-/querystring-0.2.0.tgz#b209849203bb25df820da756e747005878521620"
   integrity sha1-sgmEkgO7Jd+CDadW50cAWHhSFiA=
 
 querystringify@^2.1.1:
-  version "2.1.1"
-  resolved "https://registry.yarnpkg.com/querystringify/-/querystringify-2.1.1.tgz#60e5a5fd64a7f8bfa4d2ab2ed6fdf4c85bad154e"
-  integrity sha512-w7fLxIRCRT7U8Qu53jQnJyPkYZIaR4n5151KMfcJlO/A9397Wxb1amJvROTK6TOnp7PfoAmg/qXiNHI+08jRfA==
+  version "2.2.0"
+  resolved "https://registry.yarnpkg.com/querystringify/-/querystringify-2.2.0.tgz#3345941b4153cb9d082d8eee4cda2016a9aef7f6"
+  integrity sha512-FIqgj2EUvTa7R50u0rGsyTftzjYmv/a3hO345bZNrqabNqjtgiDMgmo4mkUjd+nzU5oF3dClKqFIPUKybUyqoQ==
 
 raf@^3.4.1:
   version "3.4.1"
   resolved "https://registry.yarnpkg.com/raf/-/raf-3.4.1.tgz#0742e99a4a6552f445d73e3ee0328af0ff1ede39"
   integrity sha512-Sq4CW4QhwOHE8ucn6J34MqtZCeWFP2aQSmrlroYgqAV1PjStIhJXxYuTgUIfkEk7zTLjmIjLmU5q+fbD1NnOJA==
   dependencies:
     performance-now "^2.1.0"
@@ -9078,17 +9078,17 @@
 
 safe-buffer@5.1.2, safe-buffer@~5.1.0, safe-buffer@~5.1.1:
   version "5.1.2"
   resolved "https://registry.yarnpkg.com/safe-buffer/-/safe-buffer-5.1.2.tgz#991ec69d296e0313747d59bdfd2b745c35f8828d"
   integrity sha512-Gd2UZBJDkXlY7GbJxfsE8/nvKkUEU1G38c1siN6QP6a9PT9MmHB8GnpscSmMJSoF8LOIrt8ud/wPtojys4G6+g==
 
 safe-buffer@>=5.1.0, safe-buffer@^5.0.1, safe-buffer@^5.1.0, safe-buffer@^5.1.1, safe-buffer@^5.1.2, safe-buffer@~5.2.0:
-  version "5.2.0"
-  resolved "https://registry.yarnpkg.com/safe-buffer/-/safe-buffer-5.2.0.tgz#b74daec49b1148f88c64b68d49b1e815c1f2f519"
-  integrity sha512-fZEwUGbVl7kouZs1jCdMLdt95hdIv0ZeHg6L7qPeciMZhZ+/gdesW4wgTARkrFWEpspjEATAzUGPG8N2jJiwbg==
+  version "5.2.1"
+  resolved "https://registry.yarnpkg.com/safe-buffer/-/safe-buffer-5.2.1.tgz#1eaf9fa9bdb1fdd4ec75f58f9cdb4e6b7827eec6"
+  integrity sha512-rp3So07KcdmmKbGvgaNxQSJr7bGVSVk5S9Eq1F+ppbRo70+YeaDxkw5Dd8NPN+GD6bjnYm2VuPuCXmpuYvmCXQ==
 
 safe-regex@^1.1.0:
   version "1.1.0"
   resolved "https://registry.yarnpkg.com/safe-regex/-/safe-regex-1.1.0.tgz#40a3669f3b077d1e943d44629e157dd48023bf2e"
   integrity sha1-QKNmnzsHfR6UPURinhV91IAjvy4=
   dependencies:
     ret "~0.1.10"
@@ -9547,17 +9547,17 @@
     ecc-jsbn "~0.1.1"
     getpass "^0.1.1"
     jsbn "~0.1.0"
     safer-buffer "^2.0.2"
     tweetnacl "~0.14.0"
 
 ssri@^6.0.1:
-  version "6.0.1"
-  resolved "https://registry.yarnpkg.com/ssri/-/ssri-6.0.1.tgz#2a3c41b28dd45b62b63676ecb74001265ae9edd8"
-  integrity sha512-3Wge10hNcT1Kur4PDFwEieXSCMCJs/7WvSACcrMYrNp+b8kDL1/0wJch5Ni2WrtwEa2IO8OsVfeKIciKCDx/QA==
+  version "6.0.2"
+  resolved "https://registry.yarnpkg.com/ssri/-/ssri-6.0.2.tgz#157939134f20464e7301ddba3e90ffa8f7728ac5"
+  integrity sha512-cepbSq/neFK7xB6A50KHN0xHDotYzq58wWCa5LeWqnPrHG8GzfEjO/4O8kpmcGW+oaxkvhEJCWgbgNk4/ZV93Q==
   dependencies:
     figgy-pudding "^3.5.1"
 
 ssri@^7.0.0:
   version "7.1.0"
   resolved "https://registry.yarnpkg.com/ssri/-/ssri-7.1.0.tgz#92c241bf6de82365b5c7fb4bd76e975522e1294d"
   integrity sha512-77/WrDZUWocK0mvA5NTRQyveUf+wsrIc6vyrxpS8tVvYBcX215QbafrJR3KtkpskIzoFLqqNuuYQvxaMjXJ/0g==
@@ -10217,17 +10217,17 @@
   integrity sha512-goSdg8VY+7nPZKUEChZSEtW5gjbS66USIGCeSJ1OVOJ7Yfuh/36YxCwMi5HVEJh6mqUYOoy3NJ0vlOMrWsSHog==
   dependencies:
     loader-utils "^1.2.3"
     mime "^2.4.4"
     schema-utils "^2.5.0"
 
 url-parse@^1.4.3:
-  version "1.4.7"
-  resolved "https://registry.yarnpkg.com/url-parse/-/url-parse-1.4.7.tgz#a8a83535e8c00a316e403a5db4ac1b9b853ae278"
-  integrity sha512-d3uaVyzDB9tQoSXFvuSUNFibTd9zxd2bkVrDRvF5TmvWWQwqE4lgYJ5m+x1DbecWkw+LK4RNl2CU1hHuOKPVlg==
+  version "1.5.1"
+  resolved "https://registry.yarnpkg.com/url-parse/-/url-parse-1.5.1.tgz#d5fa9890af8a5e1f274a2c98376510f6425f6e3b"
+  integrity sha512-HOfCOUJt7iSYzEx/UqgtwKRMC6EU91NFhsCHMv9oM03VJcVo2Qrp8T8kI9D7amFf1cu+/3CEhgb3rF9zL7k85Q==
   dependencies:
     querystringify "^2.1.1"
     requires-port "^1.0.0"
 
 url@^0.11.0:
   version "0.11.0"
   resolved "https://registry.yarnpkg.com/url/-/url-0.11.0.tgz#3838e97cfc60521eb73c525a8e55bfdd9e2e28f1"
@@ -10741,17 +10741,17 @@
   version "1.0.3"
   resolved "https://registry.yarnpkg.com/write/-/write-1.0.3.tgz#0800e14523b923a387e415123c865616aae0f5c3"
   integrity sha512-/lg70HAjtkUgWPVZhZcm+T4hkL8Zbtp1nFNOn3lRrxnlv50SRBv7cR7RqR+GMsd3hUXy9hWBo4CHTbFTcOYwig==
   dependencies:
     mkdirp "^0.5.1"
 
 ws@^5.2.0:
-  version "5.2.2"
-  resolved "https://registry.yarnpkg.com/ws/-/ws-5.2.2.tgz#dffef14866b8e8dc9133582514d1befaf96e980f"
-  integrity sha512-jaHFD6PFv6UgoIVda6qZllptQsMlDEJkTQcybzzXDYM1XO9Y8em691FGMPmM46WGyLU4z9KMgQN+qrux/nhlHA==
+  version "5.2.3"
+  resolved "https://registry.yarnpkg.com/ws/-/ws-5.2.3.tgz#05541053414921bc29c63bee14b8b0dd50b07b3d"
+  integrity sha512-jZArVERrMsKUatIdnLzqvcfydI85dvd/Fp1u/VOpfdDWQ4c9qWXe+VIeAbQ5FrDwciAkr+lzofXLz3Kuf26AOA==
   dependencies:
     async-limiter "~1.0.0"
 
 ws@^6.1.2, ws@^6.2.1:
   version "6.2.1"
   resolved "https://registry.yarnpkg.com/ws/-/ws-6.2.1.tgz#442fdf0a47ed64f59b6a5d8ff130f4748ed524fb"
   integrity sha512-GIyAXC2cB7LjvpgMt9EKS2ldqr0MTrORaleiOno6TweZ6r3TKtoFQWay/2PceJ3RuBasOHzXNn5Lrw1X0bEjqA==
```

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/docker-compose.yml` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/docker-compose.yml`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/howto.md` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/howto.md`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/populate/cli.py` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/populate/cli.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/populate/populate.py` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/populate/populate.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/populate/requirements.txt` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/populate/requirements.txt`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/results.md` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/results.md`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/score/elo.py` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/score/elo.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/score/score_table.py` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/score/score_table.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/src/elastic.py` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/src/elastic.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/benchmarking/lsh/src/lsh_encoder.py` & `weco-datascience-0.1.9/api_interfaces/benchmarking/lsh/src/lsh_encoder.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/hal/requirements.txt` & `weco-datascience-0.1.9/pipelines/feature_generation/requirements.txt`

 * *Files 20% similar despite different names*

```diff
@@ -1,32 +1,28 @@
 #
 # This file is autogenerated by pip-compile
 # To update, run:
 #
-#    pip-compile ./requirements.in
+#    pip-compile requirements.in
 #
-asn1crypto==0.24.0        # via cryptography
-bcrypt==3.1.7             # via paramiko
 boto3==1.9.198
-botocore==1.12.198        # via boto3, s3transfer
-cffi==1.12.3              # via bcrypt, cryptography, pynacl
+botocore==1.12.253        # via boto3, s3transfer
 click==7.0
 colorama==0.3.9           # via halo, log-symbols
-cryptography==2.7         # via paramiko
 cursor==1.2.0             # via halo
-docutils==0.14            # via botocore
-fire==0.2.1
+docutils==0.15.2          # via botocore
 halo==0.0.26
 jmespath==0.9.4           # via boto3, botocore
 log-symbols==0.0.13       # via halo
-numpy==1.17.0             # via pandas
-pandas==0.23.4
-paramiko==2.6.0
-pycparser==2.19           # via cffi
-pynacl==1.3.0             # via paramiko
-python-dateutil==2.8.0    # via botocore, pandas
-pytz==2019.1              # via pandas
+nmslib==1.8.1
+numpy==1.17.3
+pillow==7.0.0             # via torchvision
+pybind11==2.4.3           # via nmslib
+python-dateutil==2.8.1    # via botocore
 s3transfer==0.2.1         # via boto3
-six==1.12.0               # via bcrypt, cryptography, fire, halo, pynacl, python-dateutil
+six==1.12.0               # via halo, python-dateutil, torchvision
 spinners==0.0.23          # via halo
-termcolor==1.1.0          # via fire, halo
-urllib3==1.25.3           # via botocore
+termcolor==1.1.0          # via halo
+torch==1.3.1
+torchvision==0.4.2
+tqdm==4.26.0
+urllib3==1.25.8           # via botocore
```

### Comparing `weco-datascience-0.1.8/local_inference/fetch_images_from_api.py` & `weco-datascience-0.1.9/api_interfaces/local_inference/fetch_images_from_api.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/local_inference/index_inferrer_output.py` & `weco-datascience-0.1.9/api_interfaces/local_inference/index_inferrer_output.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/local_inference/palette_adapter.py` & `weco-datascience-0.1.9/api_interfaces/local_inference/palette_adapter.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/local_inference/requirements.txt` & `weco-datascience-0.1.9/api_interfaces/local_inference/requirements.txt`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/local_inference/run_inference.py` & `weco-datascience-0.1.9/api_interfaces/local_inference/run_inference.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/local_inference/simple-viewer/README.md` & `weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/README.md`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/local_inference/simple-viewer/modules/colorToPalette.ts` & `weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/modules/colorToPalette.ts`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/local_inference/simple-viewer/package.json` & `weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/package.json`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/local_inference/simple-viewer/pages/api/images/similar.ts` & `weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/pages/api/images/similar.ts`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/local_inference/simple-viewer/pages/index.tsx` & `weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/pages/index.tsx`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/local_inference/simple-viewer/services/elastic.ts` & `weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/services/elastic.ts`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/local_inference/simple-viewer/src/usePersistedState.ts` & `weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/src/usePersistedState.ts`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/local_inference/simple-viewer/tsconfig.json` & `weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/tsconfig.json`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/local_inference/simple-viewer/yarn.lock` & `weco-datascience-0.1.9/api_interfaces/local_inference/simple-viewer/yarn.lock`

 * *Files 0% similar despite different names*

```diff
@@ -739,18 +739,18 @@
     readable-stream "^3.4.0"
 
 bluebird@^3.5.5:
   version "3.7.2"
   resolved "https://registry.yarnpkg.com/bluebird/-/bluebird-3.7.2.tgz#9f229c15be272454ffa973ace0dbee79a1b0c36f"
   integrity sha512-XpNj6GDQzdfW+r2Wnn7xiSAd7TM3jzkxGXBGTtWKuSXv1xUV+azxAm8jdWZN06QTQk+2N2XB9jRDkvbmQmcRtg==
 
-bn.js@^4.0.0, bn.js@^4.1.0, bn.js@^4.4.0:
-  version "4.11.9"
-  resolved "https://registry.yarnpkg.com/bn.js/-/bn.js-4.11.9.tgz#26d556829458f9d1e81fc48952493d0ba3507828"
-  integrity sha512-E6QoYqCKZfgatHTdHzs1RRKP7ip4vvm+EyRUeE2RF0NblwVvb0p6jSVeNTOFxPn26QXN2o6SMfNxKp6kU8zQaw==
+bn.js@^4.0.0, bn.js@^4.1.0, bn.js@^4.11.9:
+  version "4.12.0"
+  resolved "https://registry.yarnpkg.com/bn.js/-/bn.js-4.12.0.tgz#775b3f278efbb9718eec7361f483fb36fbbfea88"
+  integrity sha512-c98Bf3tPniI+scsdk237ku1Dc3ujXQTSgyiPUDEOe7tRkhrqridvh8klBv0HCEso1OLOYcHuCv/cS6DNxKH+ZA==
 
 bn.js@^5.0.0, bn.js@^5.1.1:
   version "5.1.3"
   resolved "https://registry.yarnpkg.com/bn.js/-/bn.js-5.1.3.tgz#beca005408f642ebebea80b042b4d18d2ac0ee6b"
   integrity sha512-GkTiFpjFtUzU9CbMeJ5iazkCzGL3jrhzerzZIuqLABjbwRaFt33I9tUdSNryIptM+RxDet6OKm2WnLXzW51KsQ==
 
 brace-expansion@^1.1.7:
@@ -780,15 +780,15 @@
 braces@~3.0.2:
   version "3.0.2"
   resolved "https://registry.yarnpkg.com/braces/-/braces-3.0.2.tgz#3454e1a462ee8d599e236df336cd9ea4f8afe107"
   integrity sha512-b8um+L1RzM3WDSzvhm6gIz1yfTbBt6YTlcEKAvsmqCZZFw46z626lVj9j1yEPW33H5H+lBQpZMP1k8l+78Ha0A==
   dependencies:
     fill-range "^7.0.1"
 
-brorand@^1.0.1:
+brorand@^1.0.1, brorand@^1.1.0:
   version "1.1.0"
   resolved "https://registry.yarnpkg.com/brorand/-/brorand-1.1.0.tgz#12c25efe40a45e3c323eb8675a0a0ce57b22371f"
   integrity sha1-EsJe/kCkXjwyPrhnWgoM5XsiNx8=
 
 browserify-aes@^1.0.0, browserify-aes@^1.0.4:
   version "1.2.0"
   resolved "https://registry.yarnpkg.com/browserify-aes/-/browserify-aes-1.2.0.tgz#326734642f403dabc3003209853bb70ad428ef48"
@@ -1096,17 +1096,17 @@
 
 color-name@^1.0.0, color-name@~1.1.4:
   version "1.1.4"
   resolved "https://registry.yarnpkg.com/color-name/-/color-name-1.1.4.tgz#c2a09a87acbde69543de6f63fa3995c826c536a2"
   integrity sha512-dOy+3AuW3a2wNbZHIuMZpTcgjGuLU/uBL/ubcZF9OXbDo8ff4O8yVp5Bf0efS8uEoYo5q4Fx7dY9OgQGXgAsQA==
 
 color-string@^1.5.4:
-  version "1.5.4"
-  resolved "https://registry.yarnpkg.com/color-string/-/color-string-1.5.4.tgz#dd51cd25cfee953d138fe4002372cc3d0e504cb6"
-  integrity sha512-57yF5yt8Xa3czSEW1jfQDE79Idk0+AkN/4KWad6tbdxUmAs3MvjxlWSWD4deYytcRfoZ9nhKyFl1kj5tBvidbw==
+  version "1.6.0"
+  resolved "https://registry.yarnpkg.com/color-string/-/color-string-1.6.0.tgz#c3915f61fe267672cb7e1e064c9d692219f6c312"
+  integrity sha512-c/hGS+kRWJutUBEngKKmk4iH3sD59MBkoxVapS/0wgpCz2u7XsNloxknyvBhzwEs1IbV36D9PwqLPJ2DTu3vMA==
   dependencies:
     color-name "^1.0.0"
     simple-swizzle "^0.2.2"
 
 color@^3.1.3:
   version "3.1.3"
   resolved "https://registry.yarnpkg.com/color/-/color-3.1.3.tgz#ca67fb4e7b97d611dcde39eceed422067d91596e"
@@ -1516,25 +1516,25 @@
 
 electron-to-chromium@^1.3.585:
   version "1.3.634"
   resolved "https://registry.yarnpkg.com/electron-to-chromium/-/electron-to-chromium-1.3.634.tgz#82ea400f520f739c4f6ff00c1f7524827a917d25"
   integrity sha512-QPrWNYeE/A0xRvl/QP3E0nkaEvYUvH3gM04ZWYtIa6QlSpEetRlRI1xvQ7hiMIySHHEV+mwDSX8Kj4YZY6ZQAw==
 
 elliptic@^6.5.3:
-  version "6.5.3"
-  resolved "https://registry.yarnpkg.com/elliptic/-/elliptic-6.5.3.tgz#cb59eb2efdaf73a0bd78ccd7015a62ad6e0f93d6"
-  integrity sha512-IMqzv5wNQf+E6aHeIqATs0tOLeOTwj1QKbRcS3jBbYkl5oLAserA8yJTT7/VyHUYG91PRmPyeQDObKLPpeS4dw==
+  version "6.5.4"
+  resolved "https://registry.yarnpkg.com/elliptic/-/elliptic-6.5.4.tgz#da37cebd31e79a1367e941b592ed1fbebd58abbb"
+  integrity sha512-iLhC6ULemrljPZb+QutR5TQGB+pdW6KGD5RSegS+8sorOZT+rdQFbsQFJgvN3eRqNALqJer4oQ16YvJHlU8hzQ==
   dependencies:
-    bn.js "^4.4.0"
-    brorand "^1.0.1"
+    bn.js "^4.11.9"
+    brorand "^1.1.0"
     hash.js "^1.0.0"
-    hmac-drbg "^1.0.0"
-    inherits "^2.0.1"
-    minimalistic-assert "^1.0.0"
-    minimalistic-crypto-utils "^1.0.0"
+    hmac-drbg "^1.0.1"
+    inherits "^2.0.4"
+    minimalistic-assert "^1.0.1"
+    minimalistic-crypto-utils "^1.0.1"
 
 emojis-list@^2.0.0:
   version "2.1.0"
   resolved "https://registry.yarnpkg.com/emojis-list/-/emojis-list-2.1.0.tgz#4daa4d9db00f9819880c79fa457ae5b09a1fd389"
   integrity sha1-TapNnbAPmBmIDHn6RXrlsJof04k=
 
 emojis-list@^3.0.0:
@@ -1979,15 +1979,15 @@
     minimalistic-assert "^1.0.1"
 
 he@1.2.0:
   version "1.2.0"
   resolved "https://registry.yarnpkg.com/he/-/he-1.2.0.tgz#84ae65fa7eafb165fddb61566ae14baf05664f0f"
   integrity sha512-F/1DnUGPopORZi0ni+CvrCgHQ5FyEAHRLSApuYWMmrbSwoN2Mn/7k+Gl38gJnR7yyDZk6WLXwiGod1JOWNDKGw==
 
-hmac-drbg@^1.0.0:
+hmac-drbg@^1.0.1:
   version "1.0.1"
   resolved "https://registry.yarnpkg.com/hmac-drbg/-/hmac-drbg-1.0.1.tgz#d2745701025a6c775a6c545793ed502fc0c649a1"
   integrity sha1-0nRXAQJabHdabFRXk+1QL8DGSaE=
   dependencies:
     hash.js "^1.0.3"
     minimalistic-assert "^1.0.0"
     minimalistic-crypto-utils "^1.0.1"
@@ -2392,17 +2392,17 @@
 
 lodash.sortby@^4.7.0:
   version "4.7.0"
   resolved "https://registry.yarnpkg.com/lodash.sortby/-/lodash.sortby-4.7.0.tgz#edd14c824e2cc9c1e0b0a1b42bb5210516a42438"
   integrity sha1-7dFMgk4sycHgsKG0K7UhBRakJDg=
 
 lodash@^4.17.11, lodash@^4.17.13, lodash@^4.17.19:
-  version "4.17.20"
-  resolved "https://registry.yarnpkg.com/lodash/-/lodash-4.17.20.tgz#b44a9b6297bcb698f1c51a3545a2b3b368d59c52"
-  integrity sha512-PlhdFcillOINfeV7Ni6oF1TAEayyZBoZ8bcshTHqOYJYlrqzRK5hagpagky5o4HfCzzd1TRkXPMFq6cKk9rGmA==
+  version "4.17.21"
+  resolved "https://registry.yarnpkg.com/lodash/-/lodash-4.17.21.tgz#679591c564c3bffaae8454cf0b3df370c3d6911c"
+  integrity sha512-v2kDEe57lecTulaDIuNTPy3Ry4gLGJ6Z1O3vE1krgXZNrsQ+LFTGHVxVjcXPs17LhbZVGedAJv8XZ1tvj5FvSg==
 
 loose-envify@^1.1.0, loose-envify@^1.4.0:
   version "1.4.0"
   resolved "https://registry.yarnpkg.com/loose-envify/-/loose-envify-1.4.0.tgz#71ee51fa7be4caec1a63839f7e682d8132d30caf"
   integrity sha512-lyuxPGr/Wfhrlem2CL/UcnUc1zcqKAImBDzukY7Y5F/yQiNdko6+fRLevlw1HgMySw7f611UIY408EtxRSoK3Q==
   dependencies:
     js-tokens "^3.0.0 || ^4.0.0"
@@ -2516,15 +2516,15 @@
   integrity sha512-z0yWI+4FDrrweS8Zmt4Ej5HdJmky15+L2e6Wgn3+iK5fWzb6T3fhNFq2+MeTRb064c6Wr4N/wv0DzQTjNzHNGQ==
 
 minimalistic-assert@^1.0.0, minimalistic-assert@^1.0.1:
   version "1.0.1"
   resolved "https://registry.yarnpkg.com/minimalistic-assert/-/minimalistic-assert-1.0.1.tgz#2e194de044626d4a10e7f7fbc00ce73e83e4d5c7"
   integrity sha512-UtJcAD4yEaGtjPezWuO9wC4nwUnVH/8/Im3yEHQP4b67cXlD/Qr9hdITCU1xDbSEXg2XKNaP8jsReV7vQd00/A==
 
-minimalistic-crypto-utils@^1.0.0, minimalistic-crypto-utils@^1.0.1:
+minimalistic-crypto-utils@^1.0.1:
   version "1.0.1"
   resolved "https://registry.yarnpkg.com/minimalistic-crypto-utils/-/minimalistic-crypto-utils-1.0.1.tgz#f6c00c1c0b082246e5c4d99dfb8c7c083b2b582a"
   integrity sha1-9sAMHAsIIkblxNmd+4x8CDsrWCo=
 
 minimatch@^3.0.4:
   version "3.0.4"
   resolved "https://registry.yarnpkg.com/minimatch/-/minimatch-3.0.4.tgz#5166e286457f03306064be5497e8dbb0c3d32083"
@@ -3662,17 +3662,17 @@
   version "3.1.0"
   resolved "https://registry.yarnpkg.com/split-string/-/split-string-3.1.0.tgz#7cb09dda3a86585705c64b39a6466038682e8fe2"
   integrity sha512-NzNVhJDYpwceVVii8/Hu6DKfD2G+NrQHlS/V/qgv763EYudVwEcMQNxd2lh+0VrUByXN/oJkl5grOhYWvQUYiw==
   dependencies:
     extend-shallow "^3.0.0"
 
 ssri@^6.0.1:
-  version "6.0.1"
-  resolved "https://registry.yarnpkg.com/ssri/-/ssri-6.0.1.tgz#2a3c41b28dd45b62b63676ecb74001265ae9edd8"
-  integrity sha512-3Wge10hNcT1Kur4PDFwEieXSCMCJs/7WvSACcrMYrNp+b8kDL1/0wJch5Ni2WrtwEa2IO8OsVfeKIciKCDx/QA==
+  version "6.0.2"
+  resolved "https://registry.yarnpkg.com/ssri/-/ssri-6.0.2.tgz#157939134f20464e7301ddba3e90ffa8f7728ac5"
+  integrity sha512-cepbSq/neFK7xB6A50KHN0xHDotYzq58wWCa5LeWqnPrHG8GzfEjO/4O8kpmcGW+oaxkvhEJCWgbgNk4/ZV93Q==
   dependencies:
     figgy-pudding "^3.5.1"
 
 stacktrace-parser@0.1.10:
   version "0.1.10"
   resolved "https://registry.yarnpkg.com/stacktrace-parser/-/stacktrace-parser-0.1.10.tgz#29fb0cae4e0d0b85155879402857a1639eb6051a"
   integrity sha512-KJP1OCML99+8fhOHxwwzyWrlUuVX5GQ0ZpJTd1DFXhdkrvg1szxfHhawXUZ3g9TkXORQd4/WG68jMlQZ2p8wlg==
```

### Comparing `weco-datascience-0.1.8/pipelines/feature_generation/README.md` & `weco-datascience-0.1.9/pipelines/feature_generation/README.md`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/feature_generation/generate_feature_index.py` & `weco-datascience-0.1.9/pipelines/feature_generation/generate_feature_index.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/feature_generation/requirements.txt` & `weco-datascience-0.1.9/pipelines/images/feature_extraction/requirements.txt`

 * *Files 27% similar despite different names*

```diff
@@ -3,26 +3,17 @@
 # To update, run:
 #
 #    pip-compile requirements.in
 #
 boto3==1.9.198
 botocore==1.12.253        # via boto3, s3transfer
 click==7.0
-colorama==0.3.9           # via halo, log-symbols
-cursor==1.2.0             # via halo
 docutils==0.15.2          # via botocore
-halo==0.0.26
 jmespath==0.9.4           # via boto3, botocore
-log-symbols==0.0.13       # via halo
-nmslib==1.8.1
-numpy==1.17.3
+numpy==1.18.1             # via torch, torchvision
 pillow==7.0.0             # via torchvision
-pybind11==2.4.3           # via nmslib
 python-dateutil==2.8.1    # via botocore
 s3transfer==0.2.1         # via boto3
-six==1.12.0               # via halo, python-dateutil, torchvision
-spinners==0.0.23          # via halo
-termcolor==1.1.0          # via halo
+six==1.14.0               # via python-dateutil, torchvision
 torch==1.3.1
 torchvision==0.4.2
-tqdm==4.26.0
-urllib3==1.25.8           # via botocore
+urllib3==1.25.7           # via botocore
```

### Comparing `weco-datascience-0.1.8/pipelines/feature_generation/src/ann.py` & `weco-datascience-0.1.9/pipelines/feature_generation/src/ann.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/feature_generation/src/aws.py` & `weco-datascience-0.1.9/pipelines/feature_generation/src/aws.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/feature_generation/src/feature_extraction.py` & `weco-datascience-0.1.9/pipelines/feature_generation/src/feature_extraction.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/feature_generation/src/images.py` & `weco-datascience-0.1.9/pipelines/feature_generation/src/images.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/feature_generation/src/utils.py` & `weco-datascience-0.1.9/pipelines/feature_generation/src/utils.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/images/feature_extraction/src/aws.py` & `weco-datascience-0.1.9/pipelines/images/feature_extraction/src/aws.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/images/feature_extraction/src/feature_extraction.py` & `weco-datascience-0.1.9/pipelines/images/feature_extraction/src/feature_extraction.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/images/feature_extraction/src/images.py` & `weco-datascience-0.1.9/pipelines/images/feature_extraction/src/images.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/images/lsh_inference/infer_hash.py` & `weco-datascience-0.1.9/pipelines/images/lsh_inference/infer_hash.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/images/lsh_inference/requirements.txt` & `weco-datascience-0.1.9/pipelines/images/lsh_inference/requirements.txt`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/images/lsh_inference/src/aws.py` & `weco-datascience-0.1.9/pipelines/images/lsh_inference/src/aws.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/images/lsh_training/requirements.txt` & `weco-datascience-0.1.9/pipelines/images/lsh_training/requirements.txt`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/images/lsh_training/src/aws.py` & `weco-datascience-0.1.9/pipelines/images/lsh_training/src/aws.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/images/lsh_training/src/lsh.py` & `weco-datascience-0.1.9/pipelines/images/lsh_training/src/lsh.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/images/lsh_training/train_lsh.py` & `weco-datascience-0.1.9/pipelines/images/lsh_training/train_lsh.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/nerd/nerd/disambiguate.py` & `weco-datascience-0.1.9/pipelines/nerd/nerd/disambiguate.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/nerd/nerd/extract.py` & `weco-datascience-0.1.9/pipelines/nerd/nerd/extract.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/nerd/nerd/model.py` & `weco-datascience-0.1.9/pipelines/nerd/nerd/model.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/nerd/nerd/poll_queue.py` & `weco-datascience-0.1.9/pipelines/nerd/nerd/poll_queue.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/nerd/nerd/tokenise.py` & `weco-datascience-0.1.9/pipelines/nerd/nerd/tokenise.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/nerd/requirements.txt` & `weco-datascience-0.1.9/pipelines/nerd/requirements.txt`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/nerd/run_cli.py` & `weco-datascience-0.1.9/pipelines/nerd/run_cli.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pipelines/nerd/run_ecs.py` & `weco-datascience-0.1.9/pipelines/nerd/run_ecs.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/pyproject.toml` & `weco-datascience-0.1.9/pyproject.toml`

 * *Files 2% similar despite different names*

```diff
@@ -15,14 +15,15 @@
     "aiofile==3.1.0",
     "aiohttp[speedups]==3.6.2",
     "async-timeout==3.0.1",
     "boto3==1.12.14",
     "Pillow==7.0.0",
     "piffle==0.3.0",
     "urlpath==1.1.7",
+    "elasticsearch==7.14",
 ]
 
 [tool.flit.metadata.requires-extra]
 dev = [
     "black==19.10b0",
     "flake8==3.8.1",
     "isort==4.3.21",
```

### Comparing `weco-datascience-0.1.8/research_notebooks/archive_exploration/01 - subject coocurrence.ipynb` & `weco-datascience-0.1.9/notebooks/archive_exploration/notebooks/01 - subject coocurrence.ipynb`

 * *Files 5% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9967273447840674%*

 * *Differences: {"'cells'": '{1: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("white")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\')], delete: [4, 3]}}, 2: {\'source\': '*

 * *            "{insert: [(1, '    return [item for sublist in input_list for item in sublist]\\n'), "*

 * *            '(2, \'\\n\'), (6, \'\\n\'), (9, \'    return subject.strip().lower().replace("<p>", '*

 * *            '"")\')], delete: [9, 3, 2, 1]}}, 4: {\'source\': [\'df = '*

 * *            'pd.read_json("data/calm_records […]*

```diff
@@ -16,16 +16,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('white')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"white\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import pandas as pd\n",
                 "import numpy as np\n",
                 "import networkx as nx\n",
                 "\n",
                 "from sklearn.cluster import AgglomerativeClustering\n",
                 "\n",
@@ -36,23 +37,23 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def flatten(input_list):\n",
-                "    return [item \n",
-                "            for sublist in input_list \n",
-                "            for item in sublist]\n",
+                "    return [item for sublist in input_list for item in sublist]\n",
+                "\n",
                 "\n",
                 "def cartesian(*arrays):\n",
                 "    return np.array([x.reshape(-1) for x in np.meshgrid(*arrays)]).T\n",
                 "\n",
+                "\n",
                 "def clean(subject):\n",
-                "    return subject.strip().lower().replace('<p>', '')"
+                "    return subject.strip().lower().replace(\"<p>\", \"\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "let's load up our CALM data. The data has been exported in its entirety as a single `.json`  where each line is a record.  \n",
@@ -61,15 +62,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df = pd.read_json('data/calm_records.json')"
+                "df = pd.read_json(\"data/calm_records.json\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -112,15 +113,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df['Subject']"
+                "df[\"Subject\"]"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### After much trial and error...\n",
@@ -131,15 +132,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "subjects = flatten(df['Subject'].dropna().tolist())\n",
+                "subjects = flatten(df[\"Subject\"].dropna().tolist())\n",
                 "print(len(subjects))\n",
                 "subjects = list(set(map(clean, subjects)))\n",
                 "print(len(subjects))"
             ]
         },
         {
             "cell_type": "markdown",
@@ -167,16 +168,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "adjacency = np.empty((len(subjects), len(subjects)), \n",
-                "                     dtype=np.uint16)"
+                "adjacency = np.empty((len(subjects), len(subjects)), dtype=np.uint16)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "To populate the matrix, we want to find every possible combination of subject in each sub-list from our original column, ie if we had the subjects\n",
@@ -210,15 +210,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "for row_of_subjects in tqdm(df['Subject'].dropna()):\n",
+                "for row_of_subjects in tqdm(df[\"Subject\"].dropna()):\n",
                 "    for subject_pair in cartesian(row_of_subjects, row_of_subjects):\n",
                 "        subject_index_1 = subject_to_index[clean(subject_pair[0])]\n",
                 "        subject_index_2 = subject_to_index[clean(subject_pair[1])]\n",
                 "\n",
                 "        adjacency[subject_index_1, subject_index_2] += 1"
             ]
         },
@@ -232,16 +232,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "embedding_2d = pd.DataFrame(UMAP(n_components=2)\n",
-                "                            .fit_transform(adjacency))"
+                "embedding_2d = pd.DataFrame(UMAP(n_components=2).fit_transform(adjacency))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -260,20 +259,19 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "n_clusters = 15\n",
                 "\n",
-                "embedding_2d['labels'] = (AgglomerativeClustering(n_clusters)\n",
-                "                          .fit_predict(embedding_2d.values))\n",
-                "                          \n",
-                "embedding_2d.plot.scatter(x=0, y=1, \n",
-                "                          c='labels', \n",
-                "                          cmap='Paired');"
+                "embedding_2d[\"labels\"] = AgglomerativeClustering(n_clusters).fit_predict(\n",
+                "    embedding_2d.values\n",
+                ")\n",
+                "\n",
+                "embedding_2d.plot.scatter(x=0, y=1, c=\"labels\", cmap=\"Paired\");"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "We can now use the `index_to_subject` mapping that we created earlier to examine which subjects have been grouped together into clusters"
@@ -282,18 +280,24 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "for i in range(n_clusters):\n",
-                "    print(str(i) + ' ' + '-'*80 + '\\n')\n",
-                "    print(np.sort([index_to_subject[index]\n",
-                "                   for index in embedding_2d[embedding_2d['labels'] == i].index.values]))\n",
-                "    print('\\n')"
+                "    print(str(i) + \" \" + \"-\" * 80 + \"\\n\")\n",
+                "    print(\n",
+                "        np.sort(\n",
+                "            [\n",
+                "                index_to_subject[index]\n",
+                "                for index in embedding_2d[embedding_2d[\"labels\"] == i].index.values\n",
+                "            ]\n",
+                "        )\n",
+                "    )\n",
+                "    print(\"\\n\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "Interesting! Taking a look at some of the smaller clusters of subjects (for the sake of space and your willingness to read lists of 100s of subjects):\n",
```

### Comparing `weco-datascience-0.1.8/research_notebooks/archive_exploration/02 - exploring trees.ipynb` & `weco-datascience-0.1.9/notebooks/archive_exploration/notebooks/02 - exploring trees.ipynb`

 * *Files 9% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9958508801247772%*

 * *Differences: {"'cells'": '{1: {\'source\': {insert: [(5, \'\\n\'), (6, \'sns.set_style("white")\\n\'), (7, '*

 * *            '\'plt.rcParams["figure.figsize"] = (30, 30)\\n\')], delete: [6, 5]}}, 2: {\'source\': '*

 * *            '[\'df = pd.read_json("data/calm_records.json")\']}, 3: {\'source\': '*

 * *            '[\'df["AltRefNo"]\']}, 5: {\'source\': '*

 * *            '[\'display(HTML(df["Arrangement"][269057][0]))\']}, 7: {\'source\': [\'df["AltRefNo"] '*

 * *            '= df["AltRefNo"].dropna().apply(lambda x: x[0])\\n\', \'df["Leve […]*

```diff
@@ -15,16 +15,17 @@
             "outputs": [],
             "source": [
                 "from IPython.core.display import display, HTML\n",
                 "\n",
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('white')\n",
-                "plt.rcParams['figure.figsize'] = (30, 30)\n",
+                "\n",
+                "sns.set_style(\"white\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (30, 30)\n",
                 "\n",
                 "import pandas as pd\n",
                 "import numpy as np\n",
                 "import networkx as nx\n",
                 "\n",
                 "import re\n",
                 "\n",
@@ -37,24 +38,24 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df = pd.read_json('data/calm_records.json')"
+                "df = pd.read_json(\"data/calm_records.json\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df['AltRefNo']"
+                "df[\"AltRefNo\"]"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "The `Level` field tells us whether the record is an item, a series, a subsubsection etc. However, I'm going to ignore this as the choice of when to use these hierarchy descriptors is somewhat arbitrary, chosen by the archivist in question to best fit the archive. I'm told that what _really_ matters is the _structure_ of the archive, which is all contained in the `AltRefNo`s.  \n",
@@ -63,15 +64,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "display(HTML(df['Arrangement'][269057][0]))"
+                "display(HTML(df[\"Arrangement\"][269057][0]))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "Let's quickly clean up some of the json-ified data"
@@ -79,16 +80,16 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df['AltRefNo'] = df['AltRefNo'].dropna().apply(lambda x: x[0])\n",
-                "df['Level'] = df['Level'].dropna().apply(lambda x: x[0])"
+                "df[\"AltRefNo\"] = df[\"AltRefNo\"].dropna().apply(lambda x: x[0])\n",
+                "df[\"Level\"] = df[\"Level\"].dropna().apply(lambda x: x[0])"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### building a tree\n",
@@ -97,25 +98,25 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "for collection_id in sorted(df['AltRefNo'][df['Level'] == 'Collection'].values):\n",
+                "for collection_id in sorted(df[\"AltRefNo\"][df[\"Level\"] == \"Collection\"].values):\n",
                 "    print(collection_id)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "alt_ref_no = 'PENROSE'"
+                "alt_ref_no = \"PENROSE\""
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "We want to find every record in the dataframe whose `AltRefNo`s start with the chosen string above:"
@@ -123,18 +124,17 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "codes_as_str = sorted(df['AltRefNo'][df['AltRefNo']\n",
-                "                                     .str.startswith(alt_ref_no)\n",
-                "                                     .fillna(False)]\n",
-                "                      .tolist())\n",
+                "codes_as_str = sorted(\n",
+                "    df[\"AltRefNo\"][df[\"AltRefNo\"].str.startswith(alt_ref_no).fillna(False)].tolist()\n",
+                ")\n",
                 "codes_as_str"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -143,16 +143,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "codes_as_list = sorted(list(map(lambda x: re.split('/|\\.', x.strip()), \n",
-                "                                codes_as_str)))\n",
+                "codes_as_list = sorted(list(map(lambda x: re.split(\"/|\\.\", x.strip()), codes_as_str)))\n",
                 "codes_as_list"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -180,17 +179,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "adjacency = pd.DataFrame(data=0, \n",
-                "                         index=codes_as_str, \n",
-                "                         columns=codes_as_str)"
+                "adjacency = pd.DataFrame(data=0, index=codes_as_str, columns=codes_as_str)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "We'll now iterate through all of the possible `AltRefNo` string/list pairs. If the pair look like they have a parent/child relationship (the child's `AltRefNo` list is one element longer than the parent's and contains the full parent `AltRefNo`), then we change the element from a `0` to a `1`, drawing a connection between them."
@@ -200,16 +197,15 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "for str_1, list_1 in tqdm(codes.items()):\n",
                 "    for str_2, list_2 in codes.items():\n",
-                "        if ((len(list_2) == len(list_1) + 1) & \n",
-                "            (list_1 == list_2[:len(list_1)])):\n",
+                "        if (len(list_2) == len(list_1) + 1) & (list_1 == list_2[: len(list_1)]):\n",
                 "            adjacency[str_1][str_2] = 1"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -266,16 +262,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "embedding_2d = pd.DataFrame(UMAP(n_components=2)\n",
-                "                            .fit_transform(adjacency))\n",
+                "embedding_2d = pd.DataFrame(UMAP(n_components=2).fit_transform(adjacency))\n",
                 "\n",
                 "embedding_2d.plot.scatter(x=0, y=1);"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -288,16 +283,18 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "from sklearn.cluster import AgglomerativeClustering\n",
                 "\n",
                 "n_clusters = 15\n",
-                "embedding_2d['labels'] = AgglomerativeClustering(n_clusters).fit_predict(embedding_2d.values)\n",
-                "embedding_2d.plot.scatter(x=0, y=1, c='labels', cmap='Paired');"
+                "embedding_2d[\"labels\"] = AgglomerativeClustering(n_clusters).fit_predict(\n",
+                "    embedding_2d.values\n",
+                ")\n",
+                "embedding_2d.plot.scatter(x=0, y=1, c=\"labels\", cmap=\"Paired\");"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "now we can map those cluster labels back onto our graph nodes, colouring them according to the points with which they share a common feature space."
@@ -305,16 +302,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "nx.draw_spring(G, node_size=100, \n",
-                "               node_color=embedding_2d['labels'])"
+                "nx.draw_spring(G, node_size=100, node_color=embedding_2d[\"labels\"])"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "beautiful!"
```

### Comparing `weco-datascience-0.1.8/research_notebooks/archive_exploration/03 - wikipedia linking.ipynb` & `weco-datascience-0.1.9/notebooks/archive_exploration/notebooks/03 - wikipedia linking.ipynb`

 * *Files 5% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9963461538461538%*

 * *Differences: {"'cells'": '{2: {\'source\': [\'df = pd.read_json("data/calm_records.json")\']}, 4: {\'source\': '*

 * *            '{insert: [(0, \'record = df.loc[269057]["AdminHistory"][0]\\n\')], delete: [0]}}, 6: '*

 * *            '{\'source\': {insert: [(0, \'soup = BeautifulSoup(record, "html.parser")\\n\')], '*

 * *            'delete: [0]}}, 8: {\'source\': {insert: [(0, \'nlp = spacy.load("en")\\n\')], delete: '*

 * *            '[0]}}, 10: {\'source\': [\'ent_types = [\\n\', \'    "PERSON",\\n\', \'    '*

 * *            '"NORP",\\n\' […]*

```diff
@@ -37,15 +37,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df = pd.read_json('data/calm_records.json')"
+                "df = pd.read_json(\"data/calm_records.json\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### Abortion Laws Reform Act\n",
@@ -54,15 +54,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "record = df.loc[269057]['AdminHistory'][0]\n",
+                "record = df.loc[269057][\"AdminHistory\"][0]\n",
                 "record[:1000]"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -71,15 +71,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "soup = BeautifulSoup(record, 'html.parser')\n",
+                "soup = BeautifulSoup(record, \"html.parser\")\n",
                 "plain_text = soup.get_text()\n",
                 "\n",
                 "print(plain_text)"
             ]
         },
         {
             "cell_type": "markdown",
@@ -90,15 +90,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "nlp = spacy.load('en')\n",
+                "nlp = spacy.load(\"en\")\n",
                 "doc = nlp(plain_text)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -109,42 +109,42 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "ent_types = ['PERSON', \n",
-                "             'NORP', \n",
-                "             'FACILITY', \n",
-                "             'ORG', \n",
-                "             'GPE', \n",
-                "             'LOC', \n",
-                "             'PRODUCT', \n",
-                "             'EVENT', \n",
-                "             'WORK_OF_ART', \n",
-                "             'LAW', \n",
-                "             'LANGUAGE']"
+                "ent_types = [\n",
+                "    \"PERSON\",\n",
+                "    \"NORP\",\n",
+                "    \"FACILITY\",\n",
+                "    \"ORG\",\n",
+                "    \"GPE\",\n",
+                "    \"LOC\",\n",
+                "    \"PRODUCT\",\n",
+                "    \"EVENT\",\n",
+                "    \"WORK_OF_ART\",\n",
+                "    \"LAW\",\n",
+                "    \"LANGUAGE\",\n",
+                "]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "for ent in doc.ents:\n",
                 "    if ent.label_ in ent_types and len(ent.text.split()) > 1:\n",
                 "        words = ent.text.lower().split()\n",
-                "        words = [word.replace(\"'s\", '') for word in words]\n",
-                "        words = [word.translate(str.maketrans('', '', punctuation)) \n",
-                "                 for word in words]\n",
+                "        words = [word.replace(\"'s\", \"\") for word in words]\n",
+                "        words = [word.translate(str.maketrans(\"\", \"\", punctuation)) for word in words]\n",
                 "\n",
-                "        print('https://en.wikipedia.org/w/index.php?search=' + \n",
-                "              '+'.join(words))"
+                "        print(\"https://en.wikipedia.org/w/index.php?search=\" + \"+\".join(words))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "Wikipedia's search URLs are great. If wikipedia spots that the search returns a unique result, the user is seamlessly redirected to that result's page. If multiple results are close to the search string, a disambiguation page is returned. If the search is rubbish, the raw search page is returned with a typical list of search results. Try a few of the links above and see which kinds of search work better than others.  \n",
@@ -158,18 +158,16 @@
             "outputs": [],
             "source": [
                 "links = {}\n",
                 "\n",
                 "for ent in doc.ents:\n",
                 "    if ent.label_ in ent_types and len(ent.text.split()) > 1:\n",
                 "        words = ent.text.lower().split()\n",
-                "        words = [word.translate(str.maketrans('', '', punctuation)) \n",
-                "                 for word in words]\n",
-                "        url = ('https://en.wikipedia.org/w/index.php?search=' + \n",
-                "                '+'.join(words))\n",
+                "        words = [word.translate(str.maketrans(\"\", \"\", punctuation)) for word in words]\n",
+                "        url = \"https://en.wikipedia.org/w/index.php?search=\" + \"+\".join(words)\n",
                 "        link = '<a href=\"{}\">{}</a>'.format(url, ent.text.strip())\n",
                 "        links[ent.text.strip()] = link"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -179,15 +177,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "pattern = re.compile(r'\\b(' + '|'.join(links.keys()) + r')\\b')\n",
+                "pattern = re.compile(r\"\\b(\" + \"|\".join(links.keys()) + r\")\\b\")\n",
                 "result = pattern.sub(lambda x: links[x.group()], str(soup))\n",
                 "\n",
                 "display(HTML(result))"
             ]
         },
         {
             "cell_type": "markdown",
```

### Comparing `weco-datascience-0.1.8/research_notebooks/archive_exploration/README.md` & `weco-datascience-0.1.9/notebooks/archive_exploration/README.md`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/research_notebooks/arranging_images/main.py` & `weco-datascience-0.1.9/pipelines/arranging_images/main.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/research_notebooks/arranging_images/requirements.txt` & `weco-datascience-0.1.9/pipelines/arranging_images/requirements.txt`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/research_notebooks/arranging_images/src/feature_extraction.py` & `weco-datascience-0.1.9/pipelines/arranging_images/src/feature_extraction.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/research_notebooks/arranging_images/src/images.py` & `weco-datascience-0.1.9/pipelines/arranging_images/src/images.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/research_notebooks/arranging_images/src/spaces.py` & `weco-datascience-0.1.9/pipelines/arranging_images/src/spaces.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/research_notebooks/breadth_metric/Breadth metric v3.ipynb` & `weco-datascience-0.1.9/notebooks/image_pathways/notebooks/4. Graph_pathways_experiments.ipynb`

 * *Files 26% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.7549437718079022%*

 * *Differences: {"'cells'": "{1: {'source': ['%load_ext autoreload\\n', '%autoreload 2']}, 2: {'execution_count': "*

 * *            "None, 'source': {insert: [(0, 'from tqdm import tqdm\\n'), (1, 'import os\\n'), (2, "*

 * *            "'from io import BytesIO\\n'), (3, 'import ast\\n'), (5, 'import pickle\\n'), (6, "*

 * *            "'from itertools import compress\\n'), (7, 'from collections import Counter\\n'), (8, "*

 * *            "'import operator\\n'), (9, 'from functools import partial\\n'), (11, 'from PIL import "*

 * *            "Ima […]*

```diff
@@ -1,598 +1,806 @@
 {
     "cells": [
         {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "In this notebook we:\n",
+                "- Download the feature vectors/reduced dim feature vectors from S3 (7 options)\n",
+                "- Get the distance matrices for each of these\n",
+                "- Pick the nodes you are going to go between in the network\n",
+                "- Build the graphs using 3 types of neighbour definitions (top neighbours, or neighbours close defined by a threshold, a mixture of these or a fully connected graph)\n",
+                "- Run different pathways (dijkstra path, the a* path or my defined path) using these graphs\n",
+                "\n",
+                "The outcome of this notebook points to using the __raw feature vectors, and with a network where each node is connected to its top 3 neighbours__."
+            ]
+        },
+        {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open (\u201c./passwords.json\u201d, \u201cr\u201d)\n",
-                "as f:\n",
-                "sensitive_data = json.load(f)\n"
+                "%load_ext autoreload\n",
+                "%autoreload 2"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 16,
+            "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "from elasticsearch import helpers \n",
-                "from elasticsearch.helpers import scan\n",
-                "from collections.abc import MutableMapping\n",
-                "import pandas as pd\n",
+                "from tqdm import tqdm\n",
+                "import os\n",
+                "from io import BytesIO\n",
+                "import ast\n",
                 "import numpy as np\n",
+                "import pickle\n",
+                "from itertools import compress\n",
+                "from collections import Counter\n",
+                "import operator\n",
+                "from functools import partial\n",
+                "\n",
+                "from PIL import Image\n",
+                "import torch\n",
+                "import boto3\n",
+                "from scipy.spatial.distance import cdist\n",
+                "import networkx as nx\n",
+                "import matplotlib.pyplot as plt\n",
+                "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
+                "from itertools import combinations\n",
+                "import umap.umap_ as umap"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "cd .."
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "from src.network_functions import (\n",
+                "    import_feature_vectors,\n",
+                "    get_all_s3_keys,\n",
+                "    get_distances,\n",
+                "    image_pathway_plot,\n",
+                "    get_top_neighbours,\n",
+                "    get_high_neighbours,\n",
+                "    get_top_high_neighbours,\n",
+                "    create_network_graph,\n",
+                "    plot_graph,\n",
+                "    defined_path,\n",
+                ")"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "images_dir = \"data/\"\n",
+                "image_type = \".png\""
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "### 1. Get the names of the ~5000 feature vectors which I found different dimensionality reductions\n",
+                "\n",
+                "Pick a sample if you want to make it quicker"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "bucket_name = \"miro-images-feature-vectors\"\n",
+                "bucket_name = bucket_name\n",
+                "s3 = boto3.client(\"s3\")"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "keys = get_all_s3_keys(bucket_name, s3)"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "folder_name = \"reduced_feature_vectors_100_dims\"\n",
+                "\n",
+                "image_names = [os.path.split(k)[1] for k in keys if folder_name in k]"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "n_sample = 1000\n",
+                "np.random.seed(0)  # For dev\n",
+                "image_names = np.random.choice(image_names, n_sample, replace=False)"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "len(image_names)"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "### 2. Download the feature vectors/reduced dim feature vectors from S3"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "feature_vectors, _ = import_feature_vectors(\n",
+                "    s3, bucket_name, \"feature_vectors\", image_names\n",
+                ")\n",
+                "feature_vectors_2_dims, _ = import_feature_vectors(\n",
+                "    s3, bucket_name, \"reduced_feature_vectors_2_dims\", image_names\n",
+                ")\n",
+                "feature_vectors_20_dims, _ = import_feature_vectors(\n",
+                "    s3, bucket_name, \"reduced_feature_vectors_20_dims\", image_names\n",
+                ")\n",
+                "feature_vectors_80_dims, _ = import_feature_vectors(\n",
+                "    s3, bucket_name, \"reduced_feature_vectors_80_dims\", image_names\n",
+                ")\n",
+                "feature_vectors_100_dims, _ = import_feature_vectors(\n",
+                "    s3, bucket_name, \"reduced_feature_vectors_100_dims\", image_names\n",
+                ")\n",
+                "feature_vectors_500_dims, _ = import_feature_vectors(\n",
+                "    s3, bucket_name, \"reduced_feature_vectors_500_dims\", image_names\n",
+                ")\n",
+                "feature_vectors_1000_dims, _ = import_feature_vectors(\n",
+                "    s3, bucket_name, \"reduced_feature_vectors_1000_dims\", image_names\n",
+                ")"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Remove the name of this image from the list if no feature vector data was found for it\n",
+                "image_names = [x for x in image_names if x in list(feature_vectors.keys())]\n",
+                "image_names = [x for x in image_names if x in list(feature_vectors_100_dims.keys())]\n",
+                "len(image_names)"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "image_names_dict = {k: v for k, v in enumerate(image_names)}"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "### 3. Get the distance matrices"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "dist_mat_fv = get_distances(feature_vectors)\n",
+                "dist_mat_fv2 = get_distances(feature_vectors_2_dims)\n",
+                "dist_mat_fv20 = get_distances(feature_vectors_20_dims)\n",
+                "dist_mat_fv80 = get_distances(feature_vectors_80_dims)\n",
+                "dist_mat_fv100 = get_distances(feature_vectors_100_dims)\n",
+                "dist_mat_fv500 = get_distances(feature_vectors_500_dims)\n",
+                "dist_mat_fv1000 = get_distances(feature_vectors_1000_dims)"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "### 4. To save running time, build the graphs, then mess with pathway algos.\n",
+                "\n",
+                "I build four types of graphs using the parameters (when applicable):\n",
+                "- number_neighbours = 3\n",
+                "- dist_threshold = 0.35\n",
+                "\n",
+                "Types of graphs:\n",
+                "1. Using the top n neighbours : each node is connected to its n closest neighbours\n",
+                "2. Using all connections < threshold distance : each node is connected to all it's closest neighbours, defined by a threshold\n",
+                "3. Using all connections < threshold distance or top n : each node is connected to all it's closest neighbours, defined by a threshold, and if there are no 'close' neighbours, then the top n\n",
+                "4. Fully connected graph : every node is connected to each other\n"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "def run_graph(\n",
+                "    dist_mat, neighbour_function, number_neighbours=None, dist_threshold=None\n",
+                "):\n",
+                "\n",
+                "    if neighbour_function == get_top_neighbours:\n",
+                "        dist_mat_neighbours = neighbour_function(dist_mat=dist_mat, n=number_neighbours)\n",
+                "    elif neighbour_function == get_high_neighbours:\n",
+                "        dist_mat_neighbours = neighbour_function(\n",
+                "            dist_mat=dist_mat, dist_threshold=dist_threshold\n",
+                "        )\n",
+                "    elif neighbour_function == get_top_high_neighbours:\n",
+                "        dist_mat_neighbours = neighbour_function(\n",
+                "            dist_mat=dist_mat, n=number_neighbours, dist_threshold=dist_threshold\n",
+                "        )\n",
+                "\n",
+                "    G = create_network_graph(dist_mat_neighbours)\n",
+                "\n",
+                "    return G"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "neighbour_function = get_top_neighbours\n",
+                "number_neighbours = 3\n",
+                "\n",
+                "run_graph_partial = partial(\n",
+                "    run_graph,\n",
+                "    neighbour_function=neighbour_function,\n",
+                "    number_neighbours=number_neighbours,\n",
+                ")\n",
+                "\n",
+                "G_top_fv = run_graph_partial(dist_mat_fv)\n",
+                "G_top_fv2 = run_graph_partial(dist_mat_fv2)\n",
+                "G_top_fv20 = run_graph_partial(dist_mat_fv20)\n",
+                "G_top_fv80 = run_graph_partial(dist_mat_fv80)\n",
+                "G_top_fv100 = run_graph_partial(dist_mat_fv100)\n",
+                "G_top_fv500 = run_graph_partial(dist_mat_fv500)\n",
+                "G_top_fv1000 = run_graph_partial(dist_mat_fv1000)"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "G_top_dict = {\n",
+                "    \"G_top_fv\": G_top_fv,\n",
+                "    \"G_top_fv2\": G_top_fv2,\n",
+                "    \"G_top_fv20\": G_top_fv20,\n",
+                "    \"G_top_fv80\": G_top_fv80,\n",
+                "    \"G_top_fv100\": G_top_fv100,\n",
+                "    \"G_top_fv500\": G_top_fv500,\n",
+                "    \"G_top_fv1000\": G_top_fv1000,\n",
+                "}"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "_ = plot_graph(G_top_fv, figsize=(3, 3))\n",
+                "_ = plot_graph(G_top_fv2, figsize=(3, 3))\n",
+                "_ = plot_graph(G_top_fv20, figsize=(3, 3))\n",
+                "_ = plot_graph(G_top_fv80, figsize=(3, 3))\n",
+                "_ = plot_graph(G_top_fv100, figsize=(3, 3))\n",
+                "_ = plot_graph(G_top_fv500, figsize=(3, 3))\n",
+                "_ = plot_graph(G_top_fv1000, figsize=(3, 3))"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "neighbour_function = get_high_neighbours\n",
+                "dist_threshold = 0.8\n",
                 "\n",
-                "def flatten(nested_dict, parent_key=''):\n",
-                "    items = []\n",
-                "    for k, v in nested_dict.items():\n",
-                "        if isinstance(v, MutableMapping):\n",
-                "            items.extend(flatten(v, k).items())\n",
-                "        else:\n",
-                "            items.append((k, v))\n",
-                "    return dict(items)\n",
-                "\n",
-                "query = {\n",
-                "  \"sort\": [\n",
-                "    {\n",
-                "      \"timestamp\": \"desc\"\n",
-                "    }\n",
-                "  ],\n",
-                "  \"query\": {\n",
-                "    \"match_phrase\": {\n",
-                "      \"event\": \"Search result selected\"\n",
-                "    }\n",
-                "  }\n",
-                " }\n",
-                "\n",
-                "\n",
-                "#note: scan works fast because it grabs data unsorted.\n",
-                "#grabs 100,000 without scan. seems to have trouble past 500,000\n",
-                "response = helpers.scan(es,\n",
-                "    query=query, preserve_order=True, \n",
-                "    index=\"search_relevance_implicit\",\n",
+                "run_graph_partial = partial(\n",
+                "    run_graph, neighbour_function=neighbour_function, dist_threshold=dist_threshold\n",
                 ")\n",
-                "n_events_to_fetch = 250000\n",
                 "\n",
+                "G_high_fv = run_graph_partial(dist_mat_fv)\n",
+                "G_high_fv2 = run_graph_partial(dist_mat_fv2)\n",
+                "G_high_fv20 = run_graph_partial(dist_mat_fv20)\n",
+                "G_high_fv80 = run_graph_partial(dist_mat_fv80)\n",
+                "G_high_fv100 = run_graph_partial(dist_mat_fv100)\n",
+                "G_high_fv500 = run_graph_partial(dist_mat_fv500)\n",
+                "G_high_fv1000 = run_graph_partial(dist_mat_fv1000)"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "G_high_dict = {\n",
+                "    \"G_high_fv\": G_high_fv,\n",
+                "    \"G_high_fv2\": G_high_fv2,\n",
+                "    \"G_high_fv20\": G_high_fv20,\n",
+                "    \"G_high_fv80\": G_high_fv80,\n",
+                "    \"G_high_fv100\": G_high_fv100,\n",
+                "    \"G_high_fv500\": G_high_fv500,\n",
+                "    \"G_high_fv1000\": G_high_fv1000,\n",
+                "}"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "_ = plot_graph(G_high_fv, figsize=(3, 3))\n",
+                "_ = plot_graph(G_high_fv2, figsize=(3, 3))\n",
+                "_ = plot_graph(G_high_fv20, figsize=(3, 3))\n",
+                "_ = plot_graph(G_high_fv80, figsize=(3, 3))\n",
+                "_ = plot_graph(G_high_fv100, figsize=(3, 3))\n",
+                "_ = plot_graph(G_high_fv500, figsize=(3, 3))\n",
+                "_ = plot_graph(G_high_fv1000, figsize=(3, 3))"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "neighbour_function = get_top_high_neighbours\n",
+                "dist_threshold = 0.8\n",
+                "number_neighbours = 3\n",
+                "\n",
+                "run_graph_partial = partial(\n",
+                "    run_graph,\n",
+                "    neighbour_function=neighbour_function,\n",
+                "    number_neighbours=number_neighbours,\n",
+                "    dist_threshold=dist_threshold,\n",
+                ")\n",
                 "\n",
-                "df = pd.DataFrame([\n",
-                "    flatten(next(response)['_source']) \n",
-                "    for _ in range(n_events_to_fetch)\n",
-                "])\n"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 17,
-            "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/html": [
-                            "<div>\n",
-                            "<style scoped>\n",
-                            "    .dataframe tbody tr th:only-of-type {\n",
-                            "        vertical-align: middle;\n",
-                            "    }\n",
-                            "\n",
-                            "    .dataframe tbody tr th {\n",
-                            "        vertical-align: top;\n",
-                            "    }\n",
-                            "\n",
-                            "    .dataframe thead th {\n",
-                            "        text-align: right;\n",
-                            "    }\n",
-                            "</style>\n",
-                            "<table border=\"1\" class=\"dataframe\">\n",
-                            "  <thead>\n",
-                            "    <tr style=\"text-align: right;\">\n",
-                            "      <th></th>\n",
-                            "      <th>event</th>\n",
-                            "      <th>anonymousId</th>\n",
-                            "      <th>timestamp</th>\n",
-                            "      <th>network</th>\n",
-                            "      <th>toggles</th>\n",
-                            "      <th>aggregations</th>\n",
-                            "      <th>items.locations.locationType</th>\n",
-                            "      <th>items.locations.type</th>\n",
-                            "      <th>page</th>\n",
-                            "      <th>production.dates.from</th>\n",
-                            "      <th>...</th>\n",
-                            "      <th>position</th>\n",
-                            "      <th>resultIdentifiers</th>\n",
-                            "      <th>resultLanguage</th>\n",
-                            "      <th>resultSubjects</th>\n",
-                            "      <th>resultWorkType</th>\n",
-                            "      <th>source</th>\n",
-                            "      <th>_queryType</th>\n",
-                            "      <th>color</th>\n",
-                            "      <th>items.locations.accessConditions.status</th>\n",
-                            "      <th>genres.label</th>\n",
-                            "    </tr>\n",
-                            "  </thead>\n",
-                            "  <tbody>\n",
-                            "    <tr>\n",
-                            "      <td>199999</td>\n",
-                            "      <td>Search result selected</td>\n",
-                            "      <td>acda18fb-b1e0-4419-ae57-78fab56fc022</td>\n",
-                            "      <td>2020-06-17T05:49:00.747Z</td>\n",
-                            "      <td>None</td>\n",
-                            "      <td>[]</td>\n",
-                            "      <td>[workType]</td>\n",
-                            "      <td>[iiif-image, iiif-presentation]</td>\n",
-                            "      <td>NaN</td>\n",
-                            "      <td>1</td>\n",
-                            "      <td>None</td>\n",
-                            "      <td>...</td>\n",
-                            "      <td>15</td>\n",
-                            "      <td>[b14659621, 1465962, V0025386]</td>\n",
-                            "      <td>English</td>\n",
-                            "      <td>[Optics., Lenses.]</td>\n",
-                            "      <td>Pictures</td>\n",
-                            "      <td>work_result</td>\n",
-                            "      <td>BoolBoosted</td>\n",
-                            "      <td>NaN</td>\n",
-                            "      <td>NaN</td>\n",
-                            "      <td>NaN</td>\n",
-                            "    </tr>\n",
-                            "    <tr>\n",
-                            "      <td>199998</td>\n",
-                            "      <td>Search result selected</td>\n",
-                            "      <td>acda18fb-b1e0-4419-ae57-78fab56fc022</td>\n",
-                            "      <td>2020-06-17T05:53:02.425Z</td>\n",
-                            "      <td>None</td>\n",
-                            "      <td>[]</td>\n",
-                            "      <td>[workType]</td>\n",
-                            "      <td>[iiif-image, iiif-presentation]</td>\n",
-                            "      <td>NaN</td>\n",
-                            "      <td>2</td>\n",
-                            "      <td>None</td>\n",
-                            "      <td>...</td>\n",
-                            "      <td>3</td>\n",
-                            "      <td>[B0008487]</td>\n",
-                            "      <td>NaN</td>\n",
-                            "      <td>[]</td>\n",
-                            "      <td>Digital Images</td>\n",
-                            "      <td>work_result</td>\n",
-                            "      <td>BoolBoosted</td>\n",
-                            "      <td>NaN</td>\n",
-                            "      <td>NaN</td>\n",
-                            "      <td>NaN</td>\n",
-                            "    </tr>\n",
-                            "    <tr>\n",
-                            "      <td>199997</td>\n",
-                            "      <td>Search result selected</td>\n",
-                            "      <td>99268b47-0e41-4090-b703-6e9817470d3d</td>\n",
-                            "      <td>2020-06-17T05:53:20.727Z</td>\n",
-                            "      <td>None</td>\n",
-                            "      <td>[]</td>\n",
-                            "      <td>[workType]</td>\n",
-                            "      <td>[iiif-image, iiif-presentation]</td>\n",
-                            "      <td>NaN</td>\n",
-                            "      <td>1</td>\n",
-                            "      <td>None</td>\n",
-                            "      <td>...</td>\n",
-                            "      <td>3</td>\n",
-                            "      <td>[b12040502, 1204050, V0024748]</td>\n",
-                            "      <td>English</td>\n",
-                            "      <td>[Astronomy., Trees., Moon.]</td>\n",
-                            "      <td>Pictures</td>\n",
-                            "      <td>work_result</td>\n",
-                            "      <td>PhraserBeam</td>\n",
-                            "      <td>NaN</td>\n",
-                            "      <td>NaN</td>\n",
-                            "      <td>NaN</td>\n",
-                            "    </tr>\n",
-                            "    <tr>\n",
-                            "      <td>199996</td>\n",
-                            "      <td>Search result selected</td>\n",
-                            "      <td>206c77da-b74f-4994-bf7d-e919654a4837</td>\n",
-                            "      <td>2020-06-17T05:55:09.206Z</td>\n",
-                            "      <td>None</td>\n",
-                            "      <td>[]</td>\n",
-                            "      <td>[workType]</td>\n",
-                            "      <td>[iiif-image, iiif-presentation]</td>\n",
-                            "      <td>NaN</td>\n",
-                            "      <td>1</td>\n",
-                            "      <td>None</td>\n",
-                            "      <td>...</td>\n",
-                            "      <td>10</td>\n",
-                            "      <td>[L0040553, b16554097, EPH 557:5]</td>\n",
-                            "      <td>NaN</td>\n",
-                            "      <td>[Advertisement, Product]</td>\n",
-                            "      <td>Digital Images</td>\n",
-                            "      <td>work_result</td>\n",
-                            "      <td>BoolBoosted</td>\n",
-                            "      <td>NaN</td>\n",
-                            "      <td>NaN</td>\n",
-                            "      <td>NaN</td>\n",
-                            "    </tr>\n",
-                            "    <tr>\n",
-                            "      <td>199995</td>\n",
-                            "      <td>Search result selected</td>\n",
-                            "      <td>c5cca18e-58ed-4233-8c0c-d06f7f21710e</td>\n",
-                            "      <td>2020-06-17T05:56:33.866Z</td>\n",
-                            "      <td>None</td>\n",
-                            "      <td>[]</td>\n",
-                            "      <td>[workType]</td>\n",
-                            "      <td>[iiif-image, iiif-presentation]</td>\n",
-                            "      <td>NaN</td>\n",
-                            "      <td>4</td>\n",
-                            "      <td>None</td>\n",
-                            "      <td>...</td>\n",
-                            "      <td>1</td>\n",
-                            "      <td>[b11744066, 1174406, V0015904]</td>\n",
-                            "      <td>French</td>\n",
-                            "      <td>[Blind., Street musicians., Dogs., Hats., Drum...</td>\n",
-                            "      <td>Pictures</td>\n",
-                            "      <td>work_result</td>\n",
-                            "      <td>BoolBoosted</td>\n",
-                            "      <td>NaN</td>\n",
-                            "      <td>NaN</td>\n",
-                            "      <td>NaN</td>\n",
-                            "    </tr>\n",
-                            "  </tbody>\n",
-                            "</table>\n",
-                            "<p>5 rows \u00d7 26 columns</p>\n",
-                            "</div>"
-                        ],
-                        "text/plain": [
-                            "                         event                           anonymousId  \\\n",
-                            "199999  Search result selected  acda18fb-b1e0-4419-ae57-78fab56fc022   \n",
-                            "199998  Search result selected  acda18fb-b1e0-4419-ae57-78fab56fc022   \n",
-                            "199997  Search result selected  99268b47-0e41-4090-b703-6e9817470d3d   \n",
-                            "199996  Search result selected  206c77da-b74f-4994-bf7d-e919654a4837   \n",
-                            "199995  Search result selected  c5cca18e-58ed-4233-8c0c-d06f7f21710e   \n",
-                            "\n",
-                            "                       timestamp network toggles aggregations  \\\n",
-                            "199999  2020-06-17T05:49:00.747Z    None      []   [workType]   \n",
-                            "199998  2020-06-17T05:53:02.425Z    None      []   [workType]   \n",
-                            "199997  2020-06-17T05:53:20.727Z    None      []   [workType]   \n",
-                            "199996  2020-06-17T05:55:09.206Z    None      []   [workType]   \n",
-                            "199995  2020-06-17T05:56:33.866Z    None      []   [workType]   \n",
-                            "\n",
-                            "           items.locations.locationType items.locations.type  page  \\\n",
-                            "199999  [iiif-image, iiif-presentation]                  NaN     1   \n",
-                            "199998  [iiif-image, iiif-presentation]                  NaN     2   \n",
-                            "199997  [iiif-image, iiif-presentation]                  NaN     1   \n",
-                            "199996  [iiif-image, iiif-presentation]                  NaN     1   \n",
-                            "199995  [iiif-image, iiif-presentation]                  NaN     4   \n",
-                            "\n",
-                            "       production.dates.from  ... position                 resultIdentifiers  \\\n",
-                            "199999                  None  ...       15    [b14659621, 1465962, V0025386]   \n",
-                            "199998                  None  ...        3                        [B0008487]   \n",
-                            "199997                  None  ...        3    [b12040502, 1204050, V0024748]   \n",
-                            "199996                  None  ...       10  [L0040553, b16554097, EPH 557:5]   \n",
-                            "199995                  None  ...        1    [b11744066, 1174406, V0015904]   \n",
-                            "\n",
-                            "       resultLanguage                                     resultSubjects  \\\n",
-                            "199999        English                                 [Optics., Lenses.]   \n",
-                            "199998            NaN                                                 []   \n",
-                            "199997        English                        [Astronomy., Trees., Moon.]   \n",
-                            "199996            NaN                           [Advertisement, Product]   \n",
-                            "199995         French  [Blind., Street musicians., Dogs., Hats., Drum...   \n",
-                            "\n",
-                            "        resultWorkType       source   _queryType color  \\\n",
-                            "199999        Pictures  work_result  BoolBoosted   NaN   \n",
-                            "199998  Digital Images  work_result  BoolBoosted   NaN   \n",
-                            "199997        Pictures  work_result  PhraserBeam   NaN   \n",
-                            "199996  Digital Images  work_result  BoolBoosted   NaN   \n",
-                            "199995        Pictures  work_result  BoolBoosted   NaN   \n",
-                            "\n",
-                            "       items.locations.accessConditions.status genres.label  \n",
-                            "199999                                     NaN          NaN  \n",
-                            "199998                                     NaN          NaN  \n",
-                            "199997                                     NaN          NaN  \n",
-                            "199996                                     NaN          NaN  \n",
-                            "199995                                     NaN          NaN  \n",
-                            "\n",
-                            "[5 rows x 26 columns]"
-                        ]
-                    },
-                    "execution_count": 17,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
-            "source": [
-                "#note: Python client automatically indexes from latest to earliest.  \n",
-                "\n",
-                "#\"timestamp\": datetime(2010, 10, 10, 10, 10, 10)\n",
-                "\n",
-                "#To check start date:\n",
-                "\n",
-                "sorted=df.sort_values(by=['timestamp'], ascending=True) \n",
-                "sorted.head()"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 26,
-            "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/html": [
-                            "<div>\n",
-                            "<style scoped>\n",
-                            "    .dataframe tbody tr th:only-of-type {\n",
-                            "        vertical-align: middle;\n",
-                            "    }\n",
-                            "\n",
-                            "    .dataframe tbody tr th {\n",
-                            "        vertical-align: top;\n",
-                            "    }\n",
-                            "\n",
-                            "    .dataframe thead th {\n",
-                            "        text-align: right;\n",
-                            "    }\n",
-                            "</style>\n",
-                            "<table border=\"1\" class=\"dataframe\">\n",
-                            "  <thead>\n",
-                            "    <tr style=\"text-align: right;\">\n",
-                            "      <th></th>\n",
-                            "      <th>id</th>\n",
-                            "      <th>resultWorkType</th>\n",
-                            "      <th>anonymousId</th>\n",
-                            "      <th>timestamp</th>\n",
-                            "    </tr>\n",
-                            "  </thead>\n",
-                            "  <tbody>\n",
-                            "    <tr>\n",
-                            "      <td>159678</td>\n",
-                            "      <td>cyzuf9pb</td>\n",
-                            "      <td>Digital Images</td>\n",
-                            "      <td>00006b57-6b25-44cd-a8a2-2ca62671fcc5</td>\n",
-                            "      <td>2020-07-13T21:15:48.184Z</td>\n",
-                            "    </tr>\n",
-                            "    <tr>\n",
-                            "      <td>114836</td>\n",
-                            "      <td>smxpuwj3</td>\n",
-                            "      <td>Pictures</td>\n",
-                            "      <td>000223d7-c1db-4852-aca1-860be7b66899</td>\n",
-                            "      <td>2020-08-09T22:08:45.139Z</td>\n",
-                            "    </tr>\n",
-                            "    <tr>\n",
-                            "      <td>96399</td>\n",
-                            "      <td>s8ckh9eu</td>\n",
-                            "      <td>Pictures</td>\n",
-                            "      <td>00038646-43b1-49d9-bb75-547e655c7ee7</td>\n",
-                            "      <td>2020-08-23T17:09:33.722Z</td>\n",
-                            "    </tr>\n",
-                            "    <tr>\n",
-                            "      <td>102836</td>\n",
-                            "      <td>nv9pfazp</td>\n",
-                            "      <td>Pictures</td>\n",
-                            "      <td>0003e566-dae1-44b7-a22c-cf5a27043e34</td>\n",
-                            "      <td>2020-08-18T17:41:38.969Z</td>\n",
-                            "    </tr>\n",
-                            "    <tr>\n",
-                            "      <td>102837</td>\n",
-                            "      <td>uudv6zd7</td>\n",
-                            "      <td>Pictures</td>\n",
-                            "      <td>0003e566-dae1-44b7-a22c-cf5a27043e34</td>\n",
-                            "      <td>2020-08-18T17:40:44.327Z</td>\n",
-                            "    </tr>\n",
-                            "  </tbody>\n",
-                            "</table>\n",
-                            "</div>"
-                        ],
-                        "text/plain": [
-                            "              id  resultWorkType                           anonymousId  \\\n",
-                            "159678  cyzuf9pb  Digital Images  00006b57-6b25-44cd-a8a2-2ca62671fcc5   \n",
-                            "114836  smxpuwj3        Pictures  000223d7-c1db-4852-aca1-860be7b66899   \n",
-                            "96399   s8ckh9eu        Pictures  00038646-43b1-49d9-bb75-547e655c7ee7   \n",
-                            "102836  nv9pfazp        Pictures  0003e566-dae1-44b7-a22c-cf5a27043e34   \n",
-                            "102837  uudv6zd7        Pictures  0003e566-dae1-44b7-a22c-cf5a27043e34   \n",
-                            "\n",
-                            "                       timestamp  \n",
-                            "159678  2020-07-13T21:15:48.184Z  \n",
-                            "114836  2020-08-09T22:08:45.139Z  \n",
-                            "96399   2020-08-23T17:09:33.722Z  \n",
-                            "102836  2020-08-18T17:41:38.969Z  \n",
-                            "102837  2020-08-18T17:40:44.327Z  "
-                        ]
-                    },
-                    "execution_count": 26,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
-            "source": [
-                "#remove staff usage, limit time frame to 1/7/20 - 30/9/20\n",
-                "df2=df.loc[(df['network'] != 'StaffCorporateDevices') & (df['timestamp'] >= '2020-07-01') & (df['timestamp'] < '2020-10-01')] \n",
-                "\n",
-                "# grab only the columns needed\n",
-                "df2=df2[['id', 'resultWorkType', 'anonymousId', 'timestamp']]\n",
-                "\n",
-                "#sort the dataframe\n",
-                "df2.sort_values(by=['anonymousId','id'], inplace=True)\n",
-                "df2.head(5)\n"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 27,
-            "metadata": {},
-            "outputs": [],
-            "source": [
-                "#dedupe \n",
-                "\n",
-                "df2.sort_values(by=['id','anonymousId'])\n",
-                "df3=df2.drop_duplicates(subset=['anonymousId','id'], keep='first')\n"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 28,
-            "metadata": {},
-            "outputs": [
-                {
-                    "name": "stdout",
-                    "output_type": "stream",
-                    "text": [
-                        "          anonymousId\n",
-                        "id                   \n",
-                        "a2239muq            2\n",
-                        "a227dajt           15\n",
-                        "a227y9ye            3\n",
-                        "a22au6yn           95\n",
-                        "a22g3y27            5\n",
-                        "...               ...\n",
-                        "zzyxusav            1\n",
-                        "zzzaaraw            2\n",
-                        "zzzsptk9            2\n",
-                        "zzzx2g6s            1\n",
-                        "zzzyukhp            5\n",
-                        "\n",
-                        "[56338 rows x 1 columns]\n"
-                    ]
-                }
-            ],
-            "source": [
-                "#How many workIds?\n",
-                "summary=df3.groupby('id').count()[['anonymousId']]\n",
-                "print(summary)\n"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 29,
-            "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/plain": [
-                            "56338"
-                        ]
-                    },
-                    "execution_count": 29,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
-            "source": [
-                "#How many workIds?\n",
-                "summary['anonymousId'].count()"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 31,
-            "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/html": [
-                            "<div>\n",
-                            "<style scoped>\n",
-                            "    .dataframe tbody tr th:only-of-type {\n",
-                            "        vertical-align: middle;\n",
-                            "    }\n",
-                            "\n",
-                            "    .dataframe tbody tr th {\n",
-                            "        vertical-align: top;\n",
-                            "    }\n",
-                            "\n",
-                            "    .dataframe thead th {\n",
-                            "        text-align: right;\n",
-                            "    }\n",
-                            "</style>\n",
-                            "<table border=\"1\" class=\"dataframe\">\n",
-                            "  <thead>\n",
-                            "    <tr style=\"text-align: right;\">\n",
-                            "      <th></th>\n",
-                            "      <th>anonymousId</th>\n",
-                            "    </tr>\n",
-                            "    <tr>\n",
-                            "      <th>id</th>\n",
-                            "      <th></th>\n",
-                            "    </tr>\n",
-                            "  </thead>\n",
-                            "  <tbody>\n",
-                            "    <tr>\n",
-                            "      <td>mbk4nzgb</td>\n",
-                            "      <td>727</td>\n",
-                            "    </tr>\n",
-                            "    <tr>\n",
-                            "      <td>a2qbugvp</td>\n",
-                            "      <td>650</td>\n",
-                            "    </tr>\n",
-                            "    <tr>\n",
-                            "      <td>xreb5xue</td>\n",
-                            "      <td>414</td>\n",
-                            "    </tr>\n",
-                            "    <tr>\n",
-                            "      <td>e5qgc2dj</td>\n",
-                            "      <td>335</td>\n",
-                            "    </tr>\n",
-                            "    <tr>\n",
-                            "      <td>bs83y268</td>\n",
-                            "      <td>301</td>\n",
-                            "    </tr>\n",
-                            "  </tbody>\n",
-                            "</table>\n",
-                            "</div>"
-                        ],
-                        "text/plain": [
-                            "          anonymousId\n",
-                            "id                   \n",
-                            "mbk4nzgb          727\n",
-                            "a2qbugvp          650\n",
-                            "xreb5xue          414\n",
-                            "e5qgc2dj          335\n",
-                            "bs83y268          301"
-                        ]
-                    },
-                    "execution_count": 31,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
-            "source": [
-                "#create index for dataframe\n",
-                "sorted=summary.sort_values(by=['anonymousId'], ascending=False)\n",
-                "sorted.head()"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 32,
-            "metadata": {},
-            "outputs": [],
-            "source": [
-                "#output data to csv to check\n",
-                "\n",
-                "summary.to_csv('selects_by_workId.csv') "
+                "G_tophigh_fv = run_graph_partial(dist_mat_fv)\n",
+                "G_tophigh_fv2 = run_graph_partial(dist_mat_fv2)\n",
+                "G_tophigh_fv20 = run_graph_partial(dist_mat_fv20)\n",
+                "G_tophigh_fv80 = run_graph_partial(dist_mat_fv80)\n",
+                "G_tophigh_fv100 = run_graph_partial(dist_mat_fv100)\n",
+                "G_tophigh_fv500 = run_graph_partial(dist_mat_fv500)\n",
+                "G_tophigh_fv1000 = run_graph_partial(dist_mat_fv1000)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
-            "source": []
+            "source": [
+                "G_tophigh_dict = {\n",
+                "    \"G_tophigh_fv\": G_tophigh_fv,\n",
+                "    \"G_tophigh_fv2\": G_tophigh_fv2,\n",
+                "    \"G_tophigh_fv20\": G_tophigh_fv20,\n",
+                "    \"G_tophigh_fv80\": G_tophigh_fv80,\n",
+                "    \"G_tophigh_fv100\": G_tophigh_fv100,\n",
+                "    \"G_tophigh_fv500\": G_tophigh_fv500,\n",
+                "    \"G_tophigh_fv1000\": G_tophigh_fv1000,\n",
+                "}"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "_ = plot_graph(G_tophigh_fv, figsize=(3, 3))\n",
+                "_ = plot_graph(G_tophigh_fv2, figsize=(3, 3))\n",
+                "_ = plot_graph(G_tophigh_fv20, figsize=(3, 3))\n",
+                "_ = plot_graph(G_tophigh_fv80, figsize=(3, 3))\n",
+                "_ = plot_graph(G_tophigh_fv100, figsize=(3, 3))\n",
+                "_ = plot_graph(G_tophigh_fv500, figsize=(3, 3))\n",
+                "_ = plot_graph(G_tophigh_fv1000, figsize=(3, 3))"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Fully connected graphs\n",
+                "G_full_fv = create_network_graph(dist_mat_fv)\n",
+                "G_full_fv2 = create_network_graph(dist_mat_fv2)\n",
+                "G_full_fv20 = create_network_graph(dist_mat_fv20)\n",
+                "G_full_fv80 = create_network_graph(dist_mat_fv80)\n",
+                "G_full_fv100 = create_network_graph(dist_mat_fv100)\n",
+                "G_full_fv500 = create_network_graph(dist_mat_fv500)\n",
+                "G_full_fv1000 = create_network_graph(dist_mat_fv1000)"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "G_full_dict = {\n",
+                "    \"G_full_fv\": G_full_fv,\n",
+                "    \"G_full_fv2\": G_full_fv2,\n",
+                "    \"G_full_fv20\": G_full_fv20,\n",
+                "    \"G_full_fv80\": G_full_fv80,\n",
+                "    \"G_full_fv100\": G_full_fv100,\n",
+                "    \"G_full_fv500\": G_full_fv500,\n",
+                "    \"G_full_fv1000\": G_full_fv1000,\n",
+                "}"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "_ = plot_graph(G_full_fv, figsize=(3, 3))\n",
+                "_ = plot_graph(G_full_fv80, figsize=(3, 3))\n",
+                "_ = plot_graph(G_full_fv1000, figsize=(3, 3))"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "### 5. Pick the nodes you are going to go between in the network\n",
+                "\n",
+                "- Furthest apart? High cosine distance = different image features\n",
+                "- Random?"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "high_coords = np.where(dist_mat_fv == np.amax(dist_mat_fv))\n",
+                "print(\n",
+                "    \"Picking the first highest cosine out of {} with the same highest value\".format(\n",
+                "        len(high_coords)\n",
+                "    )\n",
+                ")\n",
+                "node1 = list(zip(high_coords[0], high_coords[1]))[0][0]\n",
+                "node2 = list(zip(high_coords[0], high_coords[1]))[0][1]\n",
+                "print(node1)\n",
+                "print(node2)\n",
+                "print(image_names_dict[node1])\n",
+                "print(image_names_dict[node2])"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "np.random.seed(4)\n",
+                "node1 = np.random.choice(list(image_names_dict))\n",
+                "node2 = np.random.choice(list(image_names_dict))\n",
+                "print(node1)\n",
+                "print(node2)\n",
+                "print(image_names_dict[node1])  # V0040357EL\n",
+                "print(image_names_dict[node2])  # V0020158"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "# Run different pathways using these graphs"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "def run_pathway(\n",
+                "    G_dict,\n",
+                "    pathway_algo,\n",
+                "    node1,\n",
+                "    node2,\n",
+                "    image_names_dict,\n",
+                "    images_dir,\n",
+                "    image_type,\n",
+                "    path_size=None,\n",
+                "    best_path=True,\n",
+                "    best_type=\"sum\",\n",
+                "):\n",
+                "\n",
+                "    G = G_dict[1]\n",
+                "    try:\n",
+                "        if pathway_algo == nx.dijkstra_path:\n",
+                "            node_path = pathway_algo(G, node1, node2, weight=None)\n",
+                "        elif pathway_algo == nx.astar_path:\n",
+                "            node_path = pathway_algo(G, node1, node2, weight=None)\n",
+                "        elif pathway_algo == defined_path:\n",
+                "            G_weights = nx.to_numpy_matrix(G)\n",
+                "            node_path = pathway_algo(\n",
+                "                G, node1, node2, G_weights, path_size, best_path, best_type\n",
+                "            )\n",
+                "\n",
+                "        image_names_path = [image_names_dict[n] for n in node_path]\n",
+                "\n",
+                "        title = \"Graph type is {}.\\nPathway algo is {}.\\nBest type is {}\".format(\n",
+                "            G_dict[0], str(locals()[\"pathway_algo\"]), best_type\n",
+                "        )\n",
+                "\n",
+                "        return (\n",
+                "            image_pathway_plot(images_dir, image_type, image_names_path, title),\n",
+                "            node_path,\n",
+                "        )\n",
+                "    except:\n",
+                "        return print(\"There is no pathway between nodes\"), _"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "## Play with the dijkstra_path pathway"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "pathway_algo = nx.dijkstra_path\n",
+                "\n",
+                "run_pathway_partial = partial(\n",
+                "    run_pathway,\n",
+                "    pathway_algo=pathway_algo,\n",
+                "    node1=node1,\n",
+                "    node2=node2,\n",
+                "    image_names_dict=image_names_dict,\n",
+                "    images_dir=images_dir,\n",
+                "    image_type=image_type,\n",
+                ")"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "for G_top in G_top_dict.items():\n",
+                "    run_pathway_partial(G_top)"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "for G_high in G_high_dict.items():\n",
+                "    run_pathway_partial(G_high)"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "for G_tophigh in G_tophigh_dict.items():\n",
+                "    run_pathway_partial(G_tophigh)"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "Try using my defined path function. In this I can use the fully connected graph too. Note that using the fully connected graph with an undefined number of nodes will just return a direct pathway from the first image to the second."
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "### Play with the A* path"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "pathway_algo = nx.astar_path\n",
+                "\n",
+                "run_astar_pathway_partial = partial(\n",
+                "    run_pathway,\n",
+                "    pathway_algo=pathway_algo,\n",
+                "    node1=node1,\n",
+                "    node2=node2,\n",
+                "    image_names_dict=image_names_dict,\n",
+                "    images_dir=images_dir,\n",
+                "    image_type=image_type,\n",
+                ")"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "for G_top in G_top_dict.items():\n",
+                "    run_astar_pathway_partial(G_top)"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "run_astar_pathway_partial((\"G_full_fv\", G_full_dict[\"G_full_fv\"]))"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "### Play with the defined_path"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "pathway_algo = defined_path\n",
+                "\n",
+                "run_defined_pathway_partial = partial(\n",
+                "    run_pathway,\n",
+                "    pathway_algo=pathway_algo,\n",
+                "    node1=node1,\n",
+                "    node2=node2,\n",
+                "    image_names_dict=image_names_dict,\n",
+                "    images_dir=images_dir,\n",
+                "    image_type=image_type,\n",
+                ")"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "run_defined_pathway_partial(\n",
+                "    (\"G_top_fv\", G_top_dict[\"G_top_fv\"]), path_size=10, best_type=\"sum\"\n",
+                ")"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "run_defined_pathway_partial(\n",
+                "    (\"G_top_fv\", G_top_dict[\"G_top_fv\"]), path_size=10, best_type=\"average\"\n",
+                ")"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "run_defined_pathway_partial(\n",
+                "    (\"G_top_fv\", G_top_dict[\"G_top_fv\"]), path_size=10, best_type=\"variance\"\n",
+                ")"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "for G_top in G_top_dict.items():\n",
+                "    run_defined_pathway_partial(G_top, path_size=9, best_type=\"sum\")\n",
+                "    run_defined_pathway_partial(G_top, path_size=9, best_type=\"variance\")"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "run_defined_pathway_partial(\n",
+                "    (\"G_full_fv\", G_full_dict[\"G_full_fv\"]), path_size=3, best_type=\"variance\"\n",
+                ")"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Takes so long!\n",
+                "# run_defined_pathway_partial(('G_full_fv', G_full_dict['G_full_fv']), path_size=5)"
+            ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Environment (conda_pytorch_p36)",
             "language": "python",
-            "name": "python3"
+            "name": "conda_pytorch_p36"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.4"
+            "version": "3.6.5"
         }
     },
     "nbformat": 4,
-    "nbformat_minor": 4
+    "nbformat_minor": 2
 }
```

### Comparing `weco-datascience-0.1.8/research_notebooks/devise/README.md` & `weco-datascience-0.1.9/notebooks/devise/README.md`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/research_notebooks/devise/notebooks/01 - recreating the original paper with tiny imagenet.ipynb` & `weco-datascience-0.1.9/notebooks/devise/notebooks/01 - recreating the original paper with tiny imagenet.ipynb`

 * *Files 3% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9966478914633485%*

 * *Differences: {"'cells'": '{1: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("white")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\'), (20, \'\\n\'), (21, \'device = '*

 * *            'torch.device("cuda" if torch.cuda.is_available() else "cpu")\')], delete: [19, 4, '*

 * *            '3]}}, 2: {\'source\': [\'base_path = "/mnt/efs/images/tiny-imagenet-200/"\']}, 4: '*

 * *            "{'source': {insert: [(0, 'wv_path = "*

 * *            '"/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec"\\n\'), […]*

```diff
@@ -21,40 +21,42 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('white')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"white\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import os\n",
                 "import io\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
                 "from PIL import Image\n",
                 "from scipy.spatial.distance import cdist\n",
                 "\n",
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader\n",
                 "from torchvision import models, transforms\n",
                 "\n",
                 "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "base_path = '/mnt/efs/images/tiny-imagenet-200/'"
+                "base_path = \"/mnt/efs/images/tiny-imagenet-200/\""
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# wordvectors\n",
@@ -63,19 +65,21 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "wv_path = '/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec'\n",
-                "wv_file = io.open(wv_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
+                "wv_path = \"/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec\"\n",
+                "wv_file = io.open(wv_path, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\")\n",
                 "\n",
-                "fasttext = {line.split()[0]: np.array(line.split()[1:]).astype(np.float)\n",
-                "            for line in tqdm(list(wv_file))}"
+                "fasttext = {\n",
+                "    line.split()[0]: np.array(line.split()[1:]).astype(np.float)\n",
+                "    for line in tqdm(list(wv_file))\n",
+                "}"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -93,46 +97,46 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "clean = lambda x: x.lower().strip().replace(' ', '-').split(',-')"
+                "clean = lambda x: x.lower().strip().replace(\" \", \"-\").split(\",-\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open(base_path + 'wnids.txt') as f:\n",
+                "with open(base_path + \"wnids.txt\") as f:\n",
                 "    wnids = np.array([id.strip() for id in f.readlines()])\n",
                 "\n",
                 "wordnet = {}\n",
-                "with open(base_path + 'words.txt') as f:\n",
+                "with open(base_path + \"words.txt\") as f:\n",
                 "    for line in f.readlines():\n",
-                "        wnid, raw_words = line.split('\\t')\n",
-                "        words = [word for word in clean(raw_words)\n",
-                "                 if word in vocabulary]\n",
-                "        \n",
+                "        wnid, raw_words = line.split(\"\\t\")\n",
+                "        words = [word for word in clean(raw_words) if word in vocabulary]\n",
+                "\n",
                 "        if wnid in wnids and len(words) > 0:\n",
                 "            wordnet[wnid] = words"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "wnid_to_wordvector = {wnid: (np.array([fasttext[word] for word in words])\n",
-                "                             .mean(axis=0))\n",
-                "                      for wnid, words in wordnet.items()}\n",
+                "wnid_to_wordvector = {\n",
+                "    wnid: (np.array([fasttext[word] for word in words]).mean(axis=0))\n",
+                "    for wnid, words in wordnet.items()\n",
+                "}\n",
                 "\n",
                 "wnids = list(wnid_to_wordvector.keys())"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -144,16 +148,16 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "wnid = np.random.choice(wnids)\n",
-                "image_path = base_path + 'train/' + wnid + '/images/' + wnid + '_{}.JPEG'\n",
-                "print(' '.join(wordnet[wnid]))\n",
+                "image_path = base_path + \"train/\" + wnid + \"/images/\" + wnid + \"_{}.JPEG\"\n",
+                "print(\" \".join(wordnet[wnid]))\n",
                 "Image.open(image_path.format(np.random.choice(500)))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -167,21 +171,21 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "df = {}\n",
                 "\n",
                 "for wnid in wnids:\n",
-                "    wnid_path = base_path + 'train/' + wnid + '/images/'\n",
+                "    wnid_path = base_path + \"train/\" + wnid + \"/images/\"\n",
                 "    image_paths = [wnid_path + file_name for file_name in os.listdir(wnid_path)]\n",
                 "    for path in image_paths:\n",
                 "        df[path] = wnid\n",
                 "\n",
                 "df = pd.Series(df).to_frame().reset_index()\n",
-                "df.columns = ['path', 'wnid']"
+                "df.columns = [\"path\", \"wnid\"]"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "Pandas is great for working with this kind of structured data - we can quickly shuffle the dataframe:"
@@ -189,15 +193,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df = df.sample(frac=1).reset_index(drop=True) "
+                "df = df.sample(frac=1).reset_index(drop=True)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "and split it into 80:20 train:test portions. "
@@ -209,15 +213,15 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "split_ratio = 0.8\n",
                 "train_size = int(split_ratio * len(df))\n",
                 "\n",
                 "train_df = df.loc[:train_size]\n",
-                "test_df  = df.loc[train_size:]"
+                "test_df = df.loc[train_size:]"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "n.b. tiny-imagenet already has `train/`, `test/`, and `val/` directories set up which we could have used here instead. However, we're just illustrating the principle in this notebook so the data itself isn't important, and we'll use this kind of split later on when incorporating non-toy data.\n",
@@ -228,23 +232,22 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class ImageDataset(Dataset):\n",
-                "    def __init__(self, dataframe, wnid_to_wordvector,\n",
-                "                 transform=transforms.ToTensor()):\n",
-                "        self.image_paths = dataframe['path'].values\n",
-                "        self.wnids = dataframe['wnid'].values\n",
+                "    def __init__(self, dataframe, wnid_to_wordvector, transform=transforms.ToTensor()):\n",
+                "        self.image_paths = dataframe[\"path\"].values\n",
+                "        self.wnids = dataframe[\"wnid\"].values\n",
                 "        self.wnid_to_wordvector = wnid_to_wordvector\n",
                 "        self.transform = transform\n",
                 "\n",
                 "    def __getitem__(self, index):\n",
-                "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
+                "        image = Image.open(self.image_paths[index]).convert(\"RGB\")\n",
                 "        if self.transform is not None:\n",
                 "            image = self.transform(image)\n",
                 "\n",
                 "        target = torch.Tensor(wnid_to_wordvector[self.wnids[index]])\n",
                 "        return image, target\n",
                 "\n",
                 "    def __len__(self):\n",
@@ -260,22 +263,25 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train_transform = transforms.Compose([transforms.Resize(224),\n",
-                "                                      transforms.RandomHorizontalFlip(),\n",
-                "                                      transforms.RandomRotation(15),\n",
-                "                                      transforms.RandomGrayscale(0.25),\n",
-                "                                      transforms.ToTensor()])\n",
+                "train_transform = transforms.Compose(\n",
+                "    [\n",
+                "        transforms.Resize(224),\n",
+                "        transforms.RandomHorizontalFlip(),\n",
+                "        transforms.RandomRotation(15),\n",
+                "        transforms.RandomGrayscale(0.25),\n",
+                "        transforms.ToTensor(),\n",
+                "    ]\n",
+                ")\n",
                 "\n",
-                "test_transform = transforms.Compose([transforms.Resize(224),\n",
-                "                                     transforms.ToTensor()])"
+                "test_transform = transforms.Compose([transforms.Resize(224), transforms.ToTensor()])"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "Now all we need to do is pass our dataframe, dictionary of word vectors, and the desired image transforms to the `ImageDataset` object to define our data pipeline for training and testing."
@@ -302,22 +308,19 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "batch_size = 128\n",
                 "\n",
-                "train_loader = DataLoader(dataset=train_dataset,\n",
-                "                          batch_size=batch_size,\n",
-                "                          num_workers=5,\n",
-                "                          shuffle=True)\n",
-                "\n",
-                "test_loader = DataLoader(dataset=test_dataset,\n",
-                "                         batch_size=batch_size,\n",
-                "                         num_workers=5)"
+                "train_loader = DataLoader(\n",
+                "    dataset=train_dataset, batch_size=batch_size, num_workers=5, shuffle=True\n",
+                ")\n",
+                "\n",
+                "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=5)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# building the model\n",
@@ -364,18 +367,18 @@
             "outputs": [],
             "source": [
                 "class DeViSE(nn.Module):\n",
                 "    def __init__(self, backbone, target_size=300):\n",
                 "        super(DeViSE, self).__init__()\n",
                 "        self.backbone = backbone\n",
                 "        self.head = nn.Sequential(\n",
-                "            nn.Linear(in_features=(25088), out_features=target_size*2),\n",
+                "            nn.Linear(in_features=(25088), out_features=target_size * 2),\n",
                 "            nn.ReLU(inplace=True),\n",
                 "            nn.Dropout(),\n",
-                "            nn.Linear(in_features=target_size*2, out_features=target_size),\n",
+                "            nn.Linear(in_features=target_size * 2, out_features=target_size),\n",
                 "            nn.ReLU(inplace=True),\n",
                 "            nn.Dropout(),\n",
                 "            nn.Linear(in_features=target_size, out_features=target_size),\n",
                 "        )\n",
                 "\n",
                 "    def forward(self, x):\n",
                 "        x = self.backbone(x)\n",
@@ -407,14 +410,15 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "losses = []\n",
                 "flags = torch.ones(batch_size).cuda()\n",
                 "\n",
+                "\n",
                 "def train(model, train_loader, loss_function, optimiser, n_epochs):\n",
                 "    for epoch in range(n_epochs):\n",
                 "        model.train()\n",
                 "        loop = tqdm(train_loader)\n",
                 "        for images, targets in loop:\n",
                 "            images = images.cuda(non_blocking=True)\n",
                 "            targets = targets.cuda(non_blocking=True)\n",
@@ -422,15 +426,15 @@
                 "            optimiser.zero_grad()\n",
                 "            predictions = model(images)\n",
                 "\n",
                 "            loss = loss_function(predictions, targets, flags)\n",
                 "            loss.backward()\n",
                 "            optimiser.step()\n",
                 "\n",
-                "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
+                "            loop.set_description(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
                 "            loop.set_postfix(loss=loss.item())\n",
                 "            losses.append(loss.item())"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -459,19 +463,21 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train(model=devise_model,\n",
-                "      n_epochs=3,\n",
-                "      train_loader=train_loader,\n",
-                "      loss_function=loss_function,\n",
-                "      optimiser=optimiser)"
+                "train(\n",
+                "    model=devise_model,\n",
+                "    n_epochs=3,\n",
+                "    train_loader=train_loader,\n",
+                "    loss_function=loss_function,\n",
+                "    optimiser=optimiser,\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "When that's done, we can take a look at how the losses are doing."
@@ -480,17 +486,19 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "loss_data = pd.Series(losses).rolling(window=15).mean()\n",
-                "ax = loss_data.plot();\n",
+                "ax = loss_data.plot()\n",
                 "\n",
-                "ax.set_xlim(0,);\n",
+                "ax.set_xlim(\n",
+                "    0,\n",
+                ")\n",
                 "ax.set_ylim(0, 1);"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -517,15 +525,15 @@
                 "\n",
                 "        predictions = devise_model(images)\n",
                 "        loss = loss_function(predictions, targets, flags)\n",
                 "\n",
                 "        preds.append(predictions.cpu().data.numpy())\n",
                 "        test_loss.append(loss.item())\n",
                 "\n",
-                "        test_loop.set_description('Test set')\n",
+                "        test_loop.set_description(\"Test set\")\n",
                 "        test_loop.set_postfix(loss=np.mean(test_loss[-5:]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -546,29 +554,31 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def search(query, n=5):\n",
-                "    image_paths = test_df['path'].values\n",
+                "    image_paths = test_df[\"path\"].values\n",
                 "    distances = cdist(fasttext[query].reshape(1, -1), preds)\n",
                 "    closest_n_paths = image_paths[np.argsort(distances)].squeeze()[:n]\n",
-                "    close_images = [np.array(Image.open(image_path).convert('RGB')) \n",
-                "                    for image_path in closest_n_paths]\n",
+                "    close_images = [\n",
+                "        np.array(Image.open(image_path).convert(\"RGB\"))\n",
+                "        for image_path in closest_n_paths\n",
+                "    ]\n",
                 "    return Image.fromarray(np.concatenate(close_images, axis=1))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('bridge')"
+                "search(\"bridge\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "It works! The network has never seen the word 'bridge', has never been told what a bridge might look like, and has never seen any of the test set's images, but thanks to the combined subtlety of the word vector space which we're embedding our images in and the dexterity with which a neural network can manipulate manifolds like these, the machine has enough knowledge to make a very good guess at what a bridge might be. This has been trained on a tiny, terribly grainy set of data but it's enough to get startlingly good results."
```

### Comparing `weco-datascience-0.1.8/research_notebooks/devise/notebooks/02 - scaling up to imagenet.ipynb` & `weco-datascience-0.1.9/notebooks/devise/notebooks/02 - scaling up to imagenet.ipynb`

 * *Files 4% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9956287551479859%*

 * *Differences: {"'cells'": '{1: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\'), (21, \'\\n\'), (22, \'device = '*

 * *            'torch.device("cuda" if torch.cuda.is_available() else "cpu")\')], delete: [20, 4, '*

 * *            "3]}}, 3: {'source': {insert: [(0, 'wv_path = "*

 * *            '"/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec"\\n\'), (1, \'wv_file = '*

 * *            'io.open(wv_path, "r", encoding="utf-8", newline="\\\ […]*

```diff
@@ -15,16 +15,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import os\n",
                 "import io\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
                 "from PIL import Image\n",
                 "from scipy.spatial.distance import cdist\n",
@@ -32,15 +33,16 @@
                 "\n",
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader\n",
                 "from torchvision import models, transforms\n",
                 "\n",
                 "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# wordvectors"
@@ -48,19 +50,21 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "wv_path = '/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec'\n",
-                "wv_file = io.open(wv_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
+                "wv_path = \"/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec\"\n",
+                "wv_file = io.open(wv_path, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\")\n",
                 "\n",
-                "fasttext = {line.split()[0]: np.array(line.split()[1:]).astype(np.float)\n",
-                "            for line in tqdm(list(wv_file)[1:])}"
+                "fasttext = {\n",
+                "    line.split()[0]: np.array(line.split()[1:]).astype(np.float)\n",
+                "    for line in tqdm(list(wv_file)[1:])\n",
+                "}"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -86,40 +90,42 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "clean = lambda x: x.lower().strip().split(', ')"
+                "clean = lambda x: x.lower().strip().split(\", \")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "mat = loadmat('/home/jupyter/ILSVRC2012_devkit_t12/data/meta.mat')\n",
-                "wnid_to_words = {line[0][1][0]: clean(line[0][2][0]) for line in mat['synsets']}\n",
-                "competition_id_to_wnid = {line[0][0][0][0]: line[0][1][0] for line in mat['synsets']}"
+                "mat = loadmat(\"/home/jupyter/ILSVRC2012_devkit_t12/data/meta.mat\")\n",
+                "wnid_to_words = {line[0][1][0]: clean(line[0][2][0]) for line in mat[\"synsets\"]}\n",
+                "competition_id_to_wnid = {line[0][0][0][0]: line[0][1][0] for line in mat[\"synsets\"]}"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "wnid_to_wordvector = {wnid: (np.array([fasttext[word] \n",
-                "                                       if word in fasttext \n",
-                "                                       else mean_wv \n",
-                "                                       for word in words ])\n",
-                "                             .mean(axis=0))\n",
-                "                      for wnid, words in wnid_to_words.items()}\n",
+                "wnid_to_wordvector = {\n",
+                "    wnid: (\n",
+                "        np.array(\n",
+                "            [fasttext[word] if word in fasttext else mean_wv for word in words]\n",
+                "        ).mean(axis=0)\n",
+                "    )\n",
+                "    for wnid, words in wnid_to_words.items()\n",
+                "}\n",
                 "\n",
                 "wnids = list(wnid_to_wordvector.keys())"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -129,32 +135,32 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "id_path = '/mnt/efs/images/ILSVRC2012_validation_ground_truth.txt'\n",
+                "id_path = \"/mnt/efs/images/ILSVRC2012_validation_ground_truth.txt\"\n",
                 "competition_ids = pd.read_csv(id_path, header=None).values.squeeze()\n",
                 "\n",
-                "image_path = '/mnt/efs/images/ILSVRC2012/'\n",
+                "image_path = \"/mnt/efs/images/ILSVRC2012/\"\n",
                 "image_paths = np.sort([image_path + file_name for file_name in os.listdir(image_path)])"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "index = np.random.choice(50000)\n",
                 "competition_id = competition_ids[index]\n",
                 "wnid = competition_id_to_wnid[competition_id]\n",
                 "\n",
-                "print(' '.join(wnid_to_words[wnid]))\n",
+                "print(\" \".join(wnid_to_words[wnid]))\n",
                 "Image.open(image_paths[index])"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -166,57 +172,57 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "df = dict(zip(image_paths, competition_ids))\n",
                 "\n",
                 "df = pd.Series(df).to_frame().reset_index()\n",
-                "df.columns = ['path', 'wnid']"
+                "df.columns = [\"path\", \"wnid\"]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df = df.sample(frac=1).reset_index(drop=True) "
+                "df = df.sample(frac=1).reset_index(drop=True)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "split_ratio = 0.8\n",
                 "train_size = int(split_ratio * len(df))\n",
                 "\n",
                 "train_df = df.loc[:train_size]\n",
-                "test_df  = df.loc[train_size:]"
+                "test_df = df.loc[train_size:]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class ImageDataset(Dataset):\n",
-                "    def __init__(self, dataframe, \n",
-                "                 competition_id_to_wnid, wnid_to_wordvector,\n",
-                "                 transform):\n",
-                "        self.image_paths = dataframe['path'].values\n",
-                "        self.wnids = dataframe['wnid'].values\n",
+                "    def __init__(\n",
+                "        self, dataframe, competition_id_to_wnid, wnid_to_wordvector, transform\n",
+                "    ):\n",
+                "        self.image_paths = dataframe[\"path\"].values\n",
+                "        self.wnids = dataframe[\"wnid\"].values\n",
                 "        self.competition_id_to_wnid = competition_id_to_wnid\n",
                 "        self.wnid_to_wordvector = wnid_to_wordvector\n",
                 "        self.transform = transform\n",
                 "\n",
                 "    def __getitem__(self, index):\n",
-                "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
+                "        image = Image.open(self.image_paths[index]).convert(\"RGB\")\n",
                 "        if self.transform is not None:\n",
                 "            image = self.transform(image)\n",
                 "\n",
                 "        wnid = competition_id_to_wnid[self.wnids[index]]\n",
                 "        target = torch.Tensor(wnid_to_wordvector[wnid])\n",
                 "        return image, target\n",
                 "\n",
@@ -226,57 +232,63 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train_transform = transforms.Compose([transforms.RandomResizedCrop(224, scale=[0.5, 0.9]),\n",
-                "                                      transforms.RandomHorizontalFlip(),\n",
-                "                                      transforms.RandomRotation(15),\n",
-                "                                      transforms.RandomGrayscale(0.25),\n",
-                "                                      transforms.ToTensor()])\n",
+                "train_transform = transforms.Compose(\n",
+                "    [\n",
+                "        transforms.RandomResizedCrop(224, scale=[0.5, 0.9]),\n",
+                "        transforms.RandomHorizontalFlip(),\n",
+                "        transforms.RandomRotation(15),\n",
+                "        transforms.RandomGrayscale(0.25),\n",
+                "        transforms.ToTensor(),\n",
+                "    ]\n",
+                ")\n",
                 "\n",
-                "test_transform = transforms.Compose([transforms.RandomResizedCrop(224, scale=[0.5, 0.9]),\n",
-                "                                     transforms.ToTensor()])"
+                "test_transform = transforms.Compose(\n",
+                "    [transforms.RandomResizedCrop(224, scale=[0.5, 0.9]), transforms.ToTensor()]\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train_dataset = ImageDataset(dataframe=train_df, \n",
-                "                             competition_id_to_wnid=competition_id_to_wnid, \n",
-                "                             wnid_to_wordvector=wnid_to_wordvector, \n",
-                "                             transform=train_transform)\n",
+                "train_dataset = ImageDataset(\n",
+                "    dataframe=train_df,\n",
+                "    competition_id_to_wnid=competition_id_to_wnid,\n",
+                "    wnid_to_wordvector=wnid_to_wordvector,\n",
+                "    transform=train_transform,\n",
+                ")\n",
                 "\n",
-                "test_dataset = ImageDataset(dataframe=test_df, \n",
-                "                            competition_id_to_wnid=competition_id_to_wnid, \n",
-                "                            wnid_to_wordvector=wnid_to_wordvector, \n",
-                "                            transform=test_transform)"
+                "test_dataset = ImageDataset(\n",
+                "    dataframe=test_df,\n",
+                "    competition_id_to_wnid=competition_id_to_wnid,\n",
+                "    wnid_to_wordvector=wnid_to_wordvector,\n",
+                "    transform=test_transform,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "batch_size = 128\n",
                 "\n",
-                "train_loader = DataLoader(dataset=train_dataset,\n",
-                "                          batch_size=batch_size,\n",
-                "                          num_workers=5,\n",
-                "                          shuffle=True)\n",
-                "\n",
-                "test_loader = DataLoader(dataset=test_dataset,\n",
-                "                         batch_size=batch_size,\n",
-                "                         num_workers=5)"
+                "train_loader = DataLoader(\n",
+                "    dataset=train_dataset, batch_size=batch_size, num_workers=5, shuffle=True\n",
+                ")\n",
+                "\n",
+                "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=5)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# building the model"
@@ -308,18 +320,18 @@
             "outputs": [],
             "source": [
                 "class DeViSE(nn.Module):\n",
                 "    def __init__(self, backbone, target_size=300):\n",
                 "        super(DeViSE, self).__init__()\n",
                 "        self.backbone = backbone\n",
                 "        self.head = nn.Sequential(\n",
-                "            nn.Linear(in_features=(25088), out_features=target_size*2),\n",
+                "            nn.Linear(in_features=(25088), out_features=target_size * 2),\n",
                 "            nn.ReLU(inplace=True),\n",
                 "            nn.Dropout(),\n",
-                "            nn.Linear(in_features=target_size*2, out_features=target_size),\n",
+                "            nn.Linear(in_features=target_size * 2, out_features=target_size),\n",
                 "            nn.ReLU(inplace=True),\n",
                 "            nn.Dropout(),\n",
                 "            nn.Linear(in_features=target_size, out_features=target_size),\n",
                 "        )\n",
                 "\n",
                 "    def forward(self, x):\n",
                 "        x = self.backbone(x)\n",
@@ -349,31 +361,32 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "losses = []\n",
                 "\n",
+                "\n",
                 "def train(model, train_loader, loss_function, optimiser, n_epochs):\n",
                 "    for epoch in range(n_epochs):\n",
                 "        model.train()\n",
                 "        loop = tqdm(train_loader)\n",
                 "        for images, targets in loop:\n",
                 "            images = images.cuda(non_blocking=True)\n",
                 "            targets = targets.cuda(non_blocking=True)\n",
                 "            flags = torch.ones(len(targets)).cuda(non_blocking=True)\n",
-                "            \n",
+                "\n",
                 "            optimiser.zero_grad()\n",
                 "            predictions = model(images)\n",
                 "\n",
                 "            loss = loss_function(predictions, targets, flags)\n",
                 "            loss.backward()\n",
                 "            optimiser.step()\n",
                 "\n",
-                "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
+                "            loop.set_description(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
                 "            loop.set_postfix(loss=loss.item())\n",
                 "            losses.append(loss.item())"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
@@ -388,31 +401,35 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train(model=devise_model,\n",
-                "      n_epochs=3,\n",
-                "      train_loader=train_loader,\n",
-                "      loss_function=loss_function,\n",
-                "      optimiser=optimiser)"
+                "train(\n",
+                "    model=devise_model,\n",
+                "    n_epochs=3,\n",
+                "    train_loader=train_loader,\n",
+                "    loss_function=loss_function,\n",
+                "    optimiser=optimiser,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "loss_data = pd.Series(losses).rolling(window=15).mean()\n",
-                "ax = loss_data.plot();\n",
+                "ax = loss_data.plot()\n",
                 "\n",
-                "ax.set_xlim(0,);\n",
+                "ax.set_xlim(\n",
+                "    0,\n",
+                ")\n",
                 "ax.set_ylim(0, 1);"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -438,15 +455,15 @@
                 "\n",
                 "        predictions = devise_model(images)\n",
                 "        loss = loss_function(predictions, targets, flags)\n",
                 "\n",
                 "        preds.append(predictions.cpu().data.numpy())\n",
                 "        test_loss.append(loss.item())\n",
                 "\n",
-                "        test_loop.set_description('Test set')\n",
+                "        test_loop.set_description(\"Test set\")\n",
                 "        test_loop.set_postfix(loss=np.mean(test_loss[-5:]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -475,29 +492,31 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def search(query, n=5):\n",
-                "    image_paths = test_df['path'].values\n",
+                "    image_paths = test_df[\"path\"].values\n",
                 "    distances = cdist(fasttext[query].reshape(1, -1), preds)\n",
                 "    closest_n_paths = image_paths[np.argsort(distances)].squeeze()[:n]\n",
-                "    close_images = [np.array(Image.open(image_path).resize((224, 224)).convert('RGB')) \n",
-                "                    for image_path in closest_n_paths]\n",
+                "    close_images = [\n",
+                "        np.array(Image.open(image_path).resize((224, 224)).convert(\"RGB\"))\n",
+                "        for image_path in closest_n_paths\n",
+                "    ]\n",
                 "    return Image.fromarray(np.concatenate(close_images, axis=1))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('bridge')"
+                "search(\"bridge\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "Great, that works too. We're now working with much larger, more complex data but the network is still able to make inferences about the interactions between written and visual language."
```

### Comparing `weco-datascience-0.1.8/research_notebooks/devise/notebooks/03 - broadening the scope of our classes.ipynb` & `weco-datascience-0.1.9/notebooks/devise/notebooks/03 - broadening the scope of our classes.ipynb`

 * *Files 3% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9959954807302913%*

 * *Differences: {"'cells'": '{1: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\'), (23, \'\\n\'), (24, \'device = '*

 * *            'torch.device("cuda" if torch.cuda.is_available() else "cpu")\')], delete: [22, 4, '*

 * *            "3]}}, 3: {'source': {insert: [(1, 'wordnet_url = "*

 * *            '"http://files.fast.ai/data/classids.txt"\\n\'), (3, \'for line in '*

 * *            'requests.get(wordnet_url).text.split("\\\\n"):\\n\'), (7, \' […]*

```diff
@@ -14,16 +14,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import os\n",
                 "import io\n",
                 "import requests\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
                 "from PIL import Image\n",
@@ -33,15 +34,16 @@
                 "\n",
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader\n",
                 "from torchvision import models, transforms\n",
                 "\n",
                 "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# get wordnet nouns"
@@ -50,30 +52,31 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "id_to_word = {}\n",
-                "wordnet_url = 'http://files.fast.ai/data/classids.txt'\n",
+                "wordnet_url = \"http://files.fast.ai/data/classids.txt\"\n",
                 "\n",
-                "for line in requests.get(wordnet_url).text.split('\\n'):\n",
+                "for line in requests.get(wordnet_url).text.split(\"\\n\"):\n",
                 "    try:\n",
                 "        id, word = line.split()\n",
                 "        id_to_word[id] = word\n",
-                "    except: pass    "
+                "    except:\n",
+                "        pass"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "wordnet_nouns = [word.lower().replace('_', '-') for word in id_to_word.values()]"
+                "wordnet_nouns = [word.lower().replace(\"_\", \"-\") for word in id_to_word.values()]"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# load word vectors"
@@ -81,19 +84,21 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "wv_path = '/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec'\n",
-                "wv_file = io.open(wv_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
+                "wv_path = \"/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec\"\n",
+                "wv_file = io.open(wv_path, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\")\n",
                 "\n",
-                "word_vectors = {line.split()[0]: np.array(line.split()[1:]).astype(np.float)\n",
-                "                for line in tqdm(list(wv_file))}"
+                "word_vectors = {\n",
+                "    line.split()[0]: np.array(line.split()[1:]).astype(np.float)\n",
+                "    for line in tqdm(list(wv_file))\n",
+                "}"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -115,25 +120,29 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "query = np.random.choice(valid_queries)\n",
-                "base_url = 'https://www.google.com/search?tbm=isch&q='\n",
+                "base_url = \"https://www.google.com/search?tbm=isch&q=\"\n",
                 "\n",
                 "soup = BeautifulSoup(requests.get(base_url + query).content)\n",
-                "urls = [img['src'] for img in soup.findAll('img')]\n",
+                "urls = [img[\"src\"] for img in soup.findAll(\"img\")]\n",
                 "\n",
                 "print(query)\n",
                 "\n",
-                "images = [(Image.open(io.BytesIO(requests.get(url).content))\n",
-                "           .resize((64, 64), resample=Image.BILINEAR)\n",
-                "           .convert('RGB'))\n",
-                "          for url in urls]\n",
+                "images = [\n",
+                "    (\n",
+                "        Image.open(io.BytesIO(requests.get(url).content))\n",
+                "        .resize((64, 64), resample=Image.BILINEAR)\n",
+                "        .convert(\"RGB\")\n",
+                "    )\n",
+                "    for url in urls\n",
+                "]\n",
                 "\n",
                 "Image.fromarray(np.concatenate(images, axis=1))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -144,32 +153,33 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def image_search(query):\n",
-                "    base_url = 'https://www.google.com/search?tbm=isch&q='\n",
+                "    base_url = \"https://www.google.com/search?tbm=isch&q=\"\n",
                 "\n",
                 "    soup = BeautifulSoup(requests.get(base_url + query).content)\n",
-                "    urls = [img['src'] for img in soup.findAll('img')]\n",
-                "    \n",
-                "    images = [Image.open(io.BytesIO(requests.get(url).content)).convert('RGB')\n",
-                "              for url in urls]\n",
-                "    \n",
+                "    urls = [img[\"src\"] for img in soup.findAll(\"img\")]\n",
+                "\n",
+                "    images = [\n",
+                "        Image.open(io.BytesIO(requests.get(url).content)).convert(\"RGB\") for url in urls\n",
+                "    ]\n",
+                "\n",
                 "    return images"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "images = [i.resize((224,224)) for i in image_search('dog')]\n",
+                "images = [i.resize((224, 224)) for i in image_search(\"dog\")]\n",
                 "Image.fromarray(np.concatenate(images, axis=1))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -179,20 +189,20 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "save_path = '/mnt/efs/images/google_scraping/'\n",
+                "save_path = \"/mnt/efs/images/google_scraping/\"\n",
                 "\n",
                 "for query in tqdm(np.random.choice(valid_queries, 2000)):\n",
                 "    images = image_search(query)\n",
                 "    for i, image in enumerate(images):\n",
-                "        image.save(save_path + '{}_{}.jpg'.format(query, i))"
+                "        image.save(save_path + \"{}_{}.jpg\".format(query, i))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "from here onwards, the process is much the same as before. We'll define our data loading processes, build a simple model with a pre-trained feature-extracting backbone and train it until the loss bottoms out. Then we'll evaluate how well it has generalised against a pre-defined test set and run some test queries using out-of-vocabulary words.\n",
@@ -205,58 +215,57 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "df = {}\n",
                 "\n",
                 "for file_name in os.listdir(save_path):\n",
-                "    df[save_path + file_name] = file_name.split('_')[0]\n",
+                "    df[save_path + file_name] = file_name.split(\"_\")[0]\n",
                 "\n",
                 "df = pd.Series(df).to_frame().reset_index()\n",
-                "df.columns = ['path', 'word']"
+                "df.columns = [\"path\", \"word\"]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df = df.sample(frac=1).reset_index(drop=True) "
+                "df = df.sample(frac=1).reset_index(drop=True)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "split_ratio = 0.8\n",
                 "train_size = int(split_ratio * len(df))\n",
                 "\n",
                 "train_df = df.loc[:train_size]\n",
-                "test_df  = df.loc[train_size:]"
+                "test_df = df.loc[train_size:]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class ImageDataset(Dataset):\n",
-                "    def __init__(self, dataframe, word_vectors,\n",
-                "                 transform=transforms.ToTensor()):\n",
-                "        self.image_paths = dataframe['path'].values\n",
-                "        self.words = dataframe['word'].values\n",
+                "    def __init__(self, dataframe, word_vectors, transform=transforms.ToTensor()):\n",
+                "        self.image_paths = dataframe[\"path\"].values\n",
+                "        self.words = dataframe[\"word\"].values\n",
                 "        self.word_vectors = word_vectors\n",
                 "        self.transform = transform\n",
                 "\n",
                 "    def __getitem__(self, index):\n",
-                "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
+                "        image = Image.open(self.image_paths[index]).convert(\"RGB\")\n",
                 "        if self.transform is not None:\n",
                 "            image = self.transform(image)\n",
                 "\n",
                 "        target = torch.Tensor(word_vectors[self.words[index]])\n",
                 "        return image, target\n",
                 "\n",
                 "    def __len__(self):\n",
@@ -265,22 +274,27 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train_transform = transforms.Compose([transforms.RandomResizedCrop(224, scale=[0.6, 0.9]),\n",
-                "                                      transforms.RandomHorizontalFlip(),\n",
-                "                                      transforms.RandomRotation(15),\n",
-                "                                      transforms.RandomGrayscale(0.25),\n",
-                "                                      transforms.ToTensor()])\n",
-                "\n",
-                "test_transform = transforms.Compose([transforms.RandomResizedCrop(224, scale=[0.6, 0.9]),\n",
-                "                                     transforms.ToTensor()])"
+                "train_transform = transforms.Compose(\n",
+                "    [\n",
+                "        transforms.RandomResizedCrop(224, scale=[0.6, 0.9]),\n",
+                "        transforms.RandomHorizontalFlip(),\n",
+                "        transforms.RandomRotation(15),\n",
+                "        transforms.RandomGrayscale(0.25),\n",
+                "        transforms.ToTensor(),\n",
+                "    ]\n",
+                ")\n",
+                "\n",
+                "test_transform = transforms.Compose(\n",
+                "    [transforms.RandomResizedCrop(224, scale=[0.6, 0.9]), transforms.ToTensor()]\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -293,22 +307,19 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "batch_size = 128\n",
                 "\n",
-                "train_loader = DataLoader(dataset=train_dataset,\n",
-                "                          batch_size=batch_size,\n",
-                "                          num_workers=5,\n",
-                "                          shuffle=True)\n",
-                "\n",
-                "test_loader = DataLoader(dataset=test_dataset,\n",
-                "                         batch_size=batch_size,\n",
-                "                         num_workers=5)"
+                "train_loader = DataLoader(\n",
+                "    dataset=train_dataset, batch_size=batch_size, num_workers=5, shuffle=True\n",
+                ")\n",
+                "\n",
+                "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=5)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# building the model"
@@ -340,18 +351,18 @@
             "outputs": [],
             "source": [
                 "class DeViSE(nn.Module):\n",
                 "    def __init__(self, backbone, target_size=300):\n",
                 "        super(DeViSE, self).__init__()\n",
                 "        self.backbone = backbone\n",
                 "        self.head = nn.Sequential(\n",
-                "            nn.Linear(in_features=(25088), out_features=target_size*2),\n",
+                "            nn.Linear(in_features=(25088), out_features=target_size * 2),\n",
                 "            nn.ReLU(inplace=True),\n",
                 "            nn.Dropout(),\n",
-                "            nn.Linear(in_features=target_size*2, out_features=target_size),\n",
+                "            nn.Linear(in_features=target_size * 2, out_features=target_size),\n",
                 "            nn.ReLU(inplace=True),\n",
                 "            nn.Dropout(),\n",
                 "            nn.Linear(in_features=target_size, out_features=target_size),\n",
                 "        )\n",
                 "\n",
                 "    def forward(self, x):\n",
                 "        x = self.backbone(x)\n",
@@ -381,14 +392,15 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "losses = []\n",
                 "\n",
+                "\n",
                 "def train(model, train_loader, loss_function, optimiser, n_epochs):\n",
                 "    for epoch in range(n_epochs):\n",
                 "        model.train()\n",
                 "        loop = tqdm(train_loader)\n",
                 "        for images, targets in loop:\n",
                 "            images = images.cuda(non_blocking=True)\n",
                 "            targets = targets.cuda(non_blocking=True)\n",
@@ -397,15 +409,15 @@
                 "            optimiser.zero_grad()\n",
                 "            predictions = model(images)\n",
                 "\n",
                 "            loss = loss_function(predictions, targets, flags)\n",
                 "            loss.backward()\n",
                 "            optimiser.step()\n",
                 "\n",
-                "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
+                "            loop.set_description(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
                 "            loop.set_postfix(loss=loss.item())\n",
                 "            losses.append(loss.item())"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
@@ -420,31 +432,35 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train(model=devise_model,\n",
-                "      n_epochs=3,\n",
-                "      train_loader=train_loader,\n",
-                "      loss_function=loss_function,\n",
-                "      optimiser=optimiser)"
+                "train(\n",
+                "    model=devise_model,\n",
+                "    n_epochs=3,\n",
+                "    train_loader=train_loader,\n",
+                "    loss_function=loss_function,\n",
+                "    optimiser=optimiser,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "loss_data = pd.Series(losses).rolling(window=15).mean()\n",
-                "ax = loss_data.plot();\n",
+                "ax = loss_data.plot()\n",
                 "\n",
-                "ax.set_xlim(0,);\n",
+                "ax.set_xlim(\n",
+                "    0,\n",
+                ")\n",
                 "ax.set_ylim(0, 1);"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -463,22 +479,22 @@
                 "devise_model.eval()\n",
                 "with torch.no_grad():\n",
                 "    test_loop = tqdm(test_loader)\n",
                 "    for images, targets in test_loop:\n",
                 "        images = images.cuda(non_blocking=True)\n",
                 "        targets = targets.cuda(non_blocking=True)\n",
                 "        flags = torch.ones(len(targets)).cuda(non_blocking=True)\n",
-                "        \n",
+                "\n",
                 "        predictions = devise_model(images)\n",
                 "        loss = loss_function(predictions, targets, flags)\n",
                 "\n",
                 "        preds.append(predictions.cpu().data.numpy())\n",
                 "        test_loss.append(loss.item())\n",
                 "\n",
-                "        test_loop.set_description('Test set')\n",
+                "        test_loop.set_description(\"Test set\")\n",
                 "        test_loop.set_postfix(loss=np.mean(test_loss[-5:]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -507,29 +523,31 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def search(query, n=5):\n",
-                "    image_paths = test_df['path'].values\n",
+                "    image_paths = test_df[\"path\"].values\n",
                 "    distances = cdist(word_vectors[query].reshape(1, -1), preds)\n",
                 "    closest_n_paths = image_paths[np.argsort(distances)].squeeze()[:n]\n",
-                "    close_images = [np.array(Image.open(image_path).convert('RGB').resize((224,224)))\n",
-                "                    for image_path in closest_n_paths]\n",
+                "    close_images = [\n",
+                "        np.array(Image.open(image_path).convert(\"RGB\").resize((224, 224)))\n",
+                "        for image_path in closest_n_paths\n",
+                "    ]\n",
                 "    return Image.fromarray(np.concatenate(close_images, axis=1))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('bridge')"
+                "search(\"bridge\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "again, this works! We're getting somewhere now, and making significant changes to the established theory set out in the original DeViSE paper."
```

### Comparing `weco-datascience-0.1.8/research_notebooks/devise/notebooks/04 - testing on wellcome images.ipynb` & `weco-datascience-0.1.9/notebooks/devise/notebooks/04 - testing on wellcome images.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9959411754911756%*

 * *Differences: {"'cells'": '{1: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\'), (23, \'\\n\'), (24, \'device = '*

 * *            'torch.device("cuda" if torch.cuda.is_available() else "cpu")\')], delete: [22, 4, '*

 * *            "3]}}, 3: {'source': {insert: [(0, 'wv_path = "*

 * *            '"/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec"\\n\'), (1, \'wv_file = '*

 * *            'io.open(wv_path, "r", encoding="utf-8", newline="\\\ […]*

```diff
@@ -13,16 +13,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import os\n",
                 "import io\n",
                 "import requests\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
                 "from PIL import Image\n",
@@ -32,15 +33,16 @@
                 "\n",
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader\n",
                 "from torchvision import models, transforms\n",
                 "\n",
                 "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# load word vectors\n",
@@ -49,19 +51,21 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "wv_path = '/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec'\n",
-                "wv_file = io.open(wv_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
+                "wv_path = \"/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec\"\n",
+                "wv_file = io.open(wv_path, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\")\n",
                 "\n",
-                "word_vectors = {line.split()[0]: np.array(line.split()[1:]).astype(np.float)\n",
-                "                for line in tqdm(list(wv_file))}"
+                "word_vectors = {\n",
+                "    line.split()[0]: np.array(line.split()[1:]).astype(np.float)\n",
+                "    for line in tqdm(list(wv_file))\n",
+                "}"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# model\n",
@@ -94,18 +98,18 @@
             "outputs": [],
             "source": [
                 "class DeViSE(nn.Module):\n",
                 "    def __init__(self, backbone, target_size=300):\n",
                 "        super(DeViSE, self).__init__()\n",
                 "        self.backbone = backbone\n",
                 "        self.head = nn.Sequential(\n",
-                "            nn.Linear(in_features=(25088), out_features=target_size*2),\n",
+                "            nn.Linear(in_features=(25088), out_features=target_size * 2),\n",
                 "            nn.ReLU(inplace=True),\n",
                 "            nn.Dropout(),\n",
-                "            nn.Linear(in_features=target_size*2, out_features=target_size),\n",
+                "            nn.Linear(in_features=target_size * 2, out_features=target_size),\n",
                 "            nn.ReLU(inplace=True),\n",
                 "            nn.Dropout(),\n",
                 "            nn.Linear(in_features=target_size, out_features=target_size),\n",
                 "        )\n",
                 "\n",
                 "    def forward(self, x):\n",
                 "        x = self.backbone(x)\n",
@@ -126,15 +130,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "devise_model.load_state_dict(torch.load('/mnt/efs/models/devise-google-2018-10-03.pt'))"
+                "devise_model.load_state_dict(torch.load(\"/mnt/efs/models/devise-google-2018-10-03.pt\"))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# wellcome images dataset and dataloader\n",
@@ -145,61 +149,62 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "df = {}\n",
                 "\n",
-                "for subdir in os.listdir('/mnt/efs/images/wellcome_images/'):\n",
-                "    subdir_path = '/mnt/efs/images/wellcome_images/{}/'.format(subdir)\n",
+                "for subdir in os.listdir(\"/mnt/efs/images/wellcome_images/\"):\n",
+                "    subdir_path = \"/mnt/efs/images/wellcome_images/{}/\".format(subdir)\n",
                 "    for file_name in os.listdir(subdir_path):\n",
                 "        df[subdir_path + file_name] = subdir\n",
                 "\n",
                 "df = pd.Series(df).to_frame().reset_index()\n",
-                "df.columns = ['path', 'word']"
+                "df.columns = [\"path\", \"word\"]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df = df.sample(frac=1).reset_index(drop=True) "
+                "df = df.sample(frac=1).reset_index(drop=True)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class ImageDataset(Dataset):\n",
                 "    def __init__(self, dataframe, transform=transforms.ToTensor()):\n",
-                "        self.image_paths = dataframe['path'].values\n",
+                "        self.image_paths = dataframe[\"path\"].values\n",
                 "        self.transform = transform\n",
                 "\n",
                 "    def __getitem__(self, index):\n",
-                "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
+                "        image = Image.open(self.image_paths[index]).convert(\"RGB\")\n",
                 "        if self.transform is not None:\n",
                 "            image = self.transform(image)\n",
                 "        return image\n",
                 "\n",
                 "    def __len__(self):\n",
                 "        return len(self.image_paths)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "transform = transforms.Compose([transforms.RandomResizedCrop(224, scale=[0.6, 0.9]),\n",
-                "                                transforms.ToTensor()])"
+                "transform = transforms.Compose(\n",
+                "    [transforms.RandomResizedCrop(224, scale=[0.6, 0.9]), transforms.ToTensor()]\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -209,17 +214,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "test_loader = DataLoader(dataset=dataset,\n",
-                "                         batch_size=128,\n",
-                "                         num_workers=5)"
+                "test_loader = DataLoader(dataset=dataset, batch_size=128, num_workers=5)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# make predictions for wellcome images"
@@ -236,16 +239,16 @@
                 "devise_model.eval()\n",
                 "with torch.no_grad():\n",
                 "    test_loop = tqdm(test_loader)\n",
                 "    for images in test_loop:\n",
                 "        images = images.cuda(non_blocking=True)\n",
                 "        predictions = devise_model(images)\n",
                 "        preds.append(predictions.cpu().data.numpy())\n",
-                "        \n",
-                "        test_loop.set_description('Test set')"
+                "\n",
+                "        test_loop.set_description(\"Test set\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -273,29 +276,31 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def search(query, n=5):\n",
-                "    image_paths = df['path'].values\n",
+                "    image_paths = df[\"path\"].values\n",
                 "    distances = cdist(word_vectors[query].reshape(1, -1), preds)\n",
                 "    closest_n_paths = image_paths[np.argsort(distances)].squeeze()[:n]\n",
-                "    close_images = [np.array(Image.open(image_path).convert('RGB').resize((224,224)))\n",
-                "                    for image_path in closest_n_paths]\n",
+                "    close_images = [\n",
+                "        np.array(Image.open(image_path).convert(\"RGB\").resize((224, 224)))\n",
+                "        for image_path in closest_n_paths\n",
+                "    ]\n",
                 "    return Image.fromarray(np.concatenate(close_images, axis=1))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('sad')"
+                "search(\"sad\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "That works reasonably well, but the differences in the style of the datasets is clear. We'll need to address this later."
```

### Comparing `weco-datascience-0.1.8/research_notebooks/devise/notebooks/05 - sentence embeddings with infersent.ipynb` & `weco-datascience-0.1.9/notebooks/devise/notebooks/05 - sentence embeddings with infersent.ipynb`

 * *Files 4% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9973032407407407%*

 * *Differences: {"'cells'": '{1: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\'), (10, \'import numpy as np\\n\'), '*

 * *            '(21, \'nltk.download("punkt")\\n\'), (22, \'device = torch.device("cuda" if '*

 * *            'torch.cuda.is_available() else "cpu")\')], delete: [21, 20, 9, 4, 3]}}, 4: '*

 * *            '{\'source\': {insert: [(0, \'MODEL_PATH = "/mnt/efs/models/infersent2.pkl"\\n\'), (2, '*

 * *            '\'params_model = { […]*

```diff
@@ -14,33 +14,34 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import os\n",
                 "import json\n",
                 "import nltk\n",
-                "import numpy as np \n",
+                "import numpy as np\n",
                 "import pandas as pd\n",
                 "from PIL import Image\n",
                 "from scipy.spatial.distance import cdist\n",
                 "from tqdm import tqdm_notebook as tqdm\n",
                 "\n",
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader\n",
                 "from torchvision import models, transforms\n",
                 "\n",
-                "nltk.download('punkt')\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "nltk.download(\"punkt\")\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# load InferSent model\n",
@@ -58,34 +59,36 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "MODEL_PATH =  '/mnt/efs/models/infersent2.pkl'\n",
+                "MODEL_PATH = \"/mnt/efs/models/infersent2.pkl\"\n",
                 "\n",
-                "params_model = {'bsize': 1024, \n",
-                "                'word_emb_dim': 300, \n",
-                "                'enc_lstm_dim': 2048,\n",
-                "                'pool_type': 'max', \n",
-                "                'dpout_model': 0.0, \n",
-                "                'version': 2}\n",
+                "params_model = {\n",
+                "    \"bsize\": 1024,\n",
+                "    \"word_emb_dim\": 300,\n",
+                "    \"enc_lstm_dim\": 2048,\n",
+                "    \"pool_type\": \"max\",\n",
+                "    \"dpout_model\": 0.0,\n",
+                "    \"version\": 2,\n",
+                "}\n",
                 "\n",
                 "infersent_model = InferSent(params_model)\n",
                 "infersent_model.load_state_dict(torch.load(MODEL_PATH))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "W2V_PATH = '/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec'\n",
+                "W2V_PATH = \"/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec\"\n",
                 "infersent_model.set_w2v_path(W2V_PATH)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -113,18 +116,18 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open('/mnt/efs/images/coco/annotations/captions_val2014.json') as f:\n",
+                "with open(\"/mnt/efs/images/coco/annotations/captions_val2014.json\") as f:\n",
                 "    meta = json.load(f)\n",
-                "    \n",
-                "captions = pd.DataFrame(meta['annotations']).set_index('image_id')['caption'].values"
+                "\n",
+                "captions = pd.DataFrame(meta[\"annotations\"]).set_index(\"image_id\")[\"caption\"].values"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# embed captions with infersent"
@@ -154,15 +157,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "distances = cdist(embedding, embeddings, 'cosine').squeeze()\n",
+                "distances = cdist(embedding, embeddings, \"cosine\").squeeze()\n",
                 "closest_captions = captions[np.argsort(distances)]\n",
                 "closest_captions[:10]"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -180,16 +183,16 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "single_word_embedding = infersent_model.encode(['doctor'])\n",
-                "distances = cdist(single_word_embedding, embeddings, 'cosine').squeeze()\n",
+                "single_word_embedding = infersent_model.encode([\"doctor\"])\n",
+                "distances = cdist(single_word_embedding, embeddings, \"cosine\").squeeze()\n",
                 "closest_captions = captions[np.argsort(distances)]\n",
                 "closest_captions[:10]"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
```

### Comparing `weco-datascience-0.1.8/research_notebooks/devise/notebooks/06 - devise against sentence embeddings.ipynb` & `weco-datascience-0.1.9/notebooks/devise/notebooks/06 - devise against sentence embeddings.ipynb`

 * *Files 4% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9963560291339043%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\'), (10, \'import numpy as np\\n\'), '*

 * *            '(21, \'nltk.download("punkt")\\n\'), (22, \'device = torch.device("cuda" if '*

 * *            'torch.cuda.is_available() else "cpu")\')], delete: [21, 20, 9, 4, 3]}}, 2: '*

 * *            "{'source': {insert: [(0, 'with "*

 * *            'open("/mnt/efs/images/coco/annotations/captions_val2014.json") as f:\\n\' […]*

```diff
@@ -5,33 +5,34 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import os\n",
                 "import json\n",
                 "import nltk\n",
-                "import numpy as np \n",
+                "import numpy as np\n",
                 "import pandas as pd\n",
                 "from PIL import Image\n",
                 "from scipy.spatial.distance import cdist\n",
                 "from tqdm import tqdm_notebook as tqdm\n",
                 "\n",
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader\n",
                 "from torchvision import models, transforms\n",
                 "\n",
-                "nltk.download('punkt')\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "nltk.download(\"punkt\")\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# load coco images and captions"
@@ -39,31 +40,32 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open('/mnt/efs/images/coco/annotations/captions_val2014.json') as f:\n",
+                "with open(\"/mnt/efs/images/coco/annotations/captions_val2014.json\") as f:\n",
                 "    meta = json.load(f)\n",
                 "\n",
-                "df = (pd.merge(pd.DataFrame(meta['images']).set_index('id'),\n",
-                "               pd.DataFrame(meta['annotations']).set_index('image_id'), \n",
-                "               left_index=True, right_index=True)\n",
-                "      .reset_index()\n",
-                "      [['caption', 'file_name']]\n",
-                "     )\n",
-                "\n",
-                "df['file_name'] = '/mnt/efs/images/coco/val2014/' + df['file_name']\n",
-                "\n",
-                "df['caption'] = (df['caption']\n",
-                "                 .apply(lambda x: ''.join([c for c in x if c.isalpha() or c.isspace()]))\n",
-                "                 .apply(str.lower)\n",
-                "                 .apply(lambda x: ' '.join(x.split()))\n",
-                "                )"
+                "df = pd.merge(\n",
+                "    pd.DataFrame(meta[\"images\"]).set_index(\"id\"),\n",
+                "    pd.DataFrame(meta[\"annotations\"]).set_index(\"image_id\"),\n",
+                "    left_index=True,\n",
+                "    right_index=True,\n",
+                ").reset_index()[[\"caption\", \"file_name\"]]\n",
+                "\n",
+                "df[\"file_name\"] = \"/mnt/efs/images/coco/val2014/\" + df[\"file_name\"]\n",
+                "\n",
+                "df[\"caption\"] = (\n",
+                "    df[\"caption\"]\n",
+                "    .apply(lambda x: \"\".join([c for c in x if c.isalpha() or c.isspace()]))\n",
+                "    .apply(str.lower)\n",
+                "    .apply(lambda x: \" \".join(x.split()))\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# train test splits"
@@ -75,15 +77,15 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "split_ratio = 0.8\n",
                 "train_size = int(split_ratio * len(df))\n",
                 "\n",
                 "train_df = df.loc[:train_size]\n",
-                "test_df  = df.loc[train_size:]\n",
+                "test_df = df.loc[train_size:]\n",
                 "len(train_df), len(test_df)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -101,34 +103,36 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "MODEL_PATH =  '/mnt/efs/models/infersent2.pkl'\n",
+                "MODEL_PATH = \"/mnt/efs/models/infersent2.pkl\"\n",
                 "\n",
-                "params_model = {'bsize': 1024, \n",
-                "                'word_emb_dim': 300, \n",
-                "                'enc_lstm_dim': 2048,\n",
-                "                'pool_type': 'max', \n",
-                "                'dpout_model': 0.0, \n",
-                "                'version': 2}\n",
+                "params_model = {\n",
+                "    \"bsize\": 1024,\n",
+                "    \"word_emb_dim\": 300,\n",
+                "    \"enc_lstm_dim\": 2048,\n",
+                "    \"pool_type\": \"max\",\n",
+                "    \"dpout_model\": 0.0,\n",
+                "    \"version\": 2,\n",
+                "}\n",
                 "\n",
                 "infersent_model = InferSent(params_model)\n",
                 "infersent_model.load_state_dict(torch.load(MODEL_PATH))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "W2V_PATH = '/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec'\n",
+                "W2V_PATH = \"/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec\"\n",
                 "infersent_model.set_w2v_path(W2V_PATH)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -155,16 +159,16 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train_embeddings = infersent_model.encode(train_df['caption'].values, tokenize=True)\n",
-                "test_embeddings = infersent_model.encode(test_df['caption'].values, tokenize=True)\n",
+                "train_embeddings = infersent_model.encode(train_df[\"caption\"].values, tokenize=True)\n",
+                "test_embeddings = infersent_model.encode(test_df[\"caption\"].values, tokenize=True)\n",
                 "\n",
                 "len(train_embeddings), len(test_embeddings)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -177,24 +181,23 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class CaptionsDataset(Dataset):\n",
-                "    def __init__(self, path_df, caption_embeddings, \n",
-                "                 transform=transforms.ToTensor()):\n",
+                "    def __init__(self, path_df, caption_embeddings, transform=transforms.ToTensor()):\n",
                 "        self.ids = path_df.index.values\n",
-                "        self.image_paths = path_df['file_name'].values\n",
-                "        self.titles = path_df['caption'].values\n",
+                "        self.image_paths = path_df[\"file_name\"].values\n",
+                "        self.titles = path_df[\"caption\"].values\n",
                 "        self.caption_embeddings = caption_embeddings\n",
                 "        self.transform = transform\n",
                 "\n",
                 "    def __getitem__(self, index):\n",
-                "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
+                "        image = Image.open(self.image_paths[index]).convert(\"RGB\")\n",
                 "        if self.transform is not None:\n",
                 "            image = self.transform(image)\n",
                 "\n",
                 "        target = self.caption_embeddings[index]\n",
                 "        return image, target\n",
                 "\n",
                 "    def __len__(self):\n",
@@ -203,18 +206,22 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "transform = transforms.Compose([transforms.RandomResizedCrop(224, scale=[0.5, 0.9]),\n",
-                "                                transforms.RandomHorizontalFlip(),\n",
-                "                                transforms.RandomGrayscale(0.25),\n",
-                "                                transforms.ToTensor()])"
+                "transform = transforms.Compose(\n",
+                "    [\n",
+                "        transforms.RandomResizedCrop(224, scale=[0.5, 0.9]),\n",
+                "        transforms.RandomHorizontalFlip(),\n",
+                "        transforms.RandomGrayscale(0.25),\n",
+                "        transforms.ToTensor(),\n",
+                "    ]\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -243,22 +250,19 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "batch_size = 128\n",
                 "\n",
-                "train_loader = DataLoader(dataset=train_dataset, \n",
-                "                          batch_size=batch_size,\n",
-                "                          shuffle=True,\n",
-                "                          num_workers=5)\n",
-                "\n",
-                "test_loader = DataLoader(dataset=test_dataset, \n",
-                "                         batch_size=batch_size,\n",
-                "                         num_workers=5)"
+                "train_loader = DataLoader(\n",
+                "    dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=5\n",
+                ")\n",
+                "\n",
+                "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=5)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# create DeViSE model"
@@ -290,18 +294,18 @@
             "outputs": [],
             "source": [
                 "class DeViSE(nn.Module):\n",
                 "    def __init__(self, backbone, target_size=300):\n",
                 "        super(DeViSE, self).__init__()\n",
                 "        self.backbone = backbone\n",
                 "        self.head = nn.Sequential(\n",
-                "            nn.Linear(in_features=(25088), out_features=target_size*2),\n",
+                "            nn.Linear(in_features=(25088), out_features=target_size * 2),\n",
                 "            nn.ReLU(inplace=True),\n",
                 "            nn.Dropout(),\n",
-                "            nn.Linear(in_features=target_size*2, out_features=target_size),\n",
+                "            nn.Linear(in_features=target_size * 2, out_features=target_size),\n",
                 "            nn.ReLU(inplace=True),\n",
                 "            nn.Dropout(),\n",
                 "            nn.Linear(in_features=target_size, out_features=target_size),\n",
                 "        )\n",
                 "\n",
                 "    def forward(self, x):\n",
                 "        x = self.backbone(x)\n",
@@ -331,38 +335,48 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "losses = []\n",
                 "\n",
-                "def train(model, train_loader, n_epochs, loss_function, \n",
-                "          additional_metric, optimiser, device=device):\n",
-                "    '''\n",
+                "\n",
+                "def train(\n",
+                "    model,\n",
+                "    train_loader,\n",
+                "    n_epochs,\n",
+                "    loss_function,\n",
+                "    additional_metric,\n",
+                "    optimiser,\n",
+                "    device=device,\n",
+                "):\n",
+                "    \"\"\"\n",
                 "    do some training\n",
-                "    '''\n",
+                "    \"\"\"\n",
                 "    model.train()\n",
                 "    for epoch in range(n_epochs):\n",
                 "        loop = tqdm(train_loader)\n",
                 "        for data, target in loop:\n",
-                "            data, target, flags = (data.cuda(non_blocking=True), \n",
-                "                                   target.cuda(non_blocking=True), \n",
-                "                                   torch.ones(len(target)).cuda(non_blocking=True))\n",
+                "            data, target, flags = (\n",
+                "                data.cuda(non_blocking=True),\n",
+                "                target.cuda(non_blocking=True),\n",
+                "                torch.ones(len(target)).cuda(non_blocking=True),\n",
+                "            )\n",
                 "\n",
                 "            optimiser.zero_grad()\n",
                 "            prediction = model(data)\n",
                 "\n",
                 "            loss = loss_function(prediction, target, flags)\n",
                 "            mean_sq_error = additional_metric(prediction, target)\n",
                 "            losses.append([loss.item(), mean_sq_error.item()])\n",
                 "\n",
                 "            loss.backward()\n",
                 "            optimiser.step()\n",
                 "\n",
-                "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
+                "            loop.set_description(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
                 "            loop.set_postfix(loss=loss.item(), mse=mean_sq_error.item())"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -378,35 +392,41 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train(model=devise_model,\n",
-                "      train_loader=train_loader,\n",
-                "      loss_function=loss_function,\n",
-                "      additional_metric=mse, \n",
-                "      optimiser=optimiser,\n",
-                "      n_epochs=3)"
+                "train(\n",
+                "    model=devise_model,\n",
+                "    train_loader=train_loader,\n",
+                "    loss_function=loss_function,\n",
+                "    additional_metric=mse,\n",
+                "    optimiser=optimiser,\n",
+                "    n_epochs=3,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "loss_data = pd.DataFrame(losses).rolling(window=15).mean()\n",
-                "loss_data.columns = ['cosine loss', 'mse']\n",
-                "ax = loss_data.plot(subplots=True);\n",
+                "loss_data.columns = [\"cosine loss\", \"mse\"]\n",
+                "ax = loss_data.plot(subplots=True)\n",
                 "\n",
-                "ax[0].set_xlim(0,);\n",
-                "ax[0].set_ylim(0.3, 0.6);\n",
-                "ax[1].set_ylim(0,);"
+                "ax[0].set_xlim(\n",
+                "    0,\n",
+                ")\n",
+                "ax[0].set_ylim(0.3, 0.6)\n",
+                "ax[1].set_ylim(\n",
+                "    0,\n",
+                ");"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# evaluate on test set"
@@ -420,25 +440,27 @@
             "source": [
                 "preds = []\n",
                 "test_loss = []\n",
                 "\n",
                 "with torch.no_grad():\n",
                 "    test_loop = tqdm(test_loader)\n",
                 "    for data, target in test_loop:\n",
-                "        data, target, flags = (data.cuda(non_blocking=True),\n",
-                "                               target.cuda(non_blocking=True),\n",
-                "                               torch.ones(len(target)).cuda(non_blocking=True))\n",
+                "        data, target, flags = (\n",
+                "            data.cuda(non_blocking=True),\n",
+                "            target.cuda(non_blocking=True),\n",
+                "            torch.ones(len(target)).cuda(non_blocking=True),\n",
+                "        )\n",
                 "\n",
                 "        prediction = devise_model.eval()(data)\n",
                 "        loss = loss_function(prediction, target, flags)\n",
                 "\n",
                 "        preds.append(prediction.cpu().data.numpy())\n",
                 "        test_loss.append(loss.item())\n",
                 "\n",
-                "        test_loop.set_description('Test set')\n",
+                "        test_loop.set_description(\"Test set\")\n",
                 "        test_loop.set_postfix(loss=loss.item())"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -460,35 +482,41 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def search(query):\n",
                 "    query_embedding = infersent_model.encode([query], tokenize=True)\n",
                 "\n",
-                "    distances = cdist(query_embedding, preds, 'cosine').squeeze()\n",
-                "    nearby_image_paths = test_df['file_name'].values[np.argsort(distances)][:20]\n",
-                "    nearby_images = [np.array((Image.open(path)\n",
-                "                               .convert('RGB')\n",
-                "                               .resize((224, 224), Image.BILINEAR)))\n",
-                "                     for path in nearby_image_paths]\n",
-                "\n",
-                "    return Image.fromarray(np.concatenate([np.concatenate(nearby_images[:5], axis=1),\n",
-                "                                           np.concatenate(nearby_images[5:10], axis=1),\n",
-                "                                           np.concatenate(nearby_images[10:15], axis=1),\n",
-                "                                           np.concatenate(nearby_images[15:20], axis=1)],\n",
-                "                                          axis=0))"
+                "    distances = cdist(query_embedding, preds, \"cosine\").squeeze()\n",
+                "    nearby_image_paths = test_df[\"file_name\"].values[np.argsort(distances)][:20]\n",
+                "    nearby_images = [\n",
+                "        np.array((Image.open(path).convert(\"RGB\").resize((224, 224), Image.BILINEAR)))\n",
+                "        for path in nearby_image_paths\n",
+                "    ]\n",
+                "\n",
+                "    return Image.fromarray(\n",
+                "        np.concatenate(\n",
+                "            [\n",
+                "                np.concatenate(nearby_images[:5], axis=1),\n",
+                "                np.concatenate(nearby_images[5:10], axis=1),\n",
+                "                np.concatenate(nearby_images[10:15], axis=1),\n",
+                "                np.concatenate(nearby_images[15:20], axis=1),\n",
+                "            ],\n",
+                "            axis=0,\n",
+                "        )\n",
+                "    )"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('a man playing tennis')"
+                "search(\"a man playing tennis\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
```

### Comparing `weco-datascience-0.1.8/research_notebooks/devise/notebooks/07 - sentence embeddings via multi-task learning.ipynb` & `weco-datascience-0.1.9/notebooks/devise/notebooks/07 - sentence embeddings via multi-task learning.ipynb`

 * *Files 8% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9957327144296454%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 14)\\n\'), (15, \'import numpy as np\\n\'), '*

 * *            '(21, \'\\n\'), (33, \'nlp = spacy.load("en")\\n\'), (34, '*

 * *            '\'nltk.download("punkt")\\n\'), (35, \'device = torch.device("cuda" if '*

 * *            'torch.cuda.is_available() else "cpu")\')], delete: [33, 32, 31, 14, 4, 3]}}, 2: '*

 * *            "{'source': {insert: [(0, 'texts = [\\n'), (1, ' […]*

```diff
@@ -5,45 +5,47 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (20, 14)\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 14)\n",
                 "\n",
                 "import requests\n",
                 "import string\n",
                 "\n",
                 "import os\n",
                 "import json\n",
                 "import nltk\n",
                 "import spacy\n",
                 "import itertools\n",
-                "import numpy as np \n",
+                "import numpy as np\n",
                 "import pandas as pd\n",
                 "from PIL import Image\n",
                 "from scipy.spatial.distance import cdist\n",
                 "from tqdm import tqdm_notebook as tqdm\n",
                 "from tqdm import tqdm as tqdm_\n",
+                "\n",
                 "tqdm_.pandas()\n",
                 "\n",
                 "import io\n",
                 "from nltk.tokenize import word_tokenize\n",
                 "from sklearn.preprocessing import LabelEncoder\n",
                 "\n",
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader\n",
                 "from torchvision import models, transforms\n",
                 "\n",
-                "nlp = spacy.load('en')\n",
-                "nltk.download('punkt')\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "nlp = spacy.load(\"en\")\n",
+                "nltk.download(\"punkt\")\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# very simple character level RNN\n",
@@ -52,18 +54,20 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "texts = [requests.get('http://www.gutenberg.org/files/{}/{}.txt'.format(i, i)).text\n",
-                "         for i in np.random.choice(np.arange(start=1000, stop=1200), 10)]\n",
+                "texts = [\n",
+                "    requests.get(\"http://www.gutenberg.org/files/{}/{}.txt\".format(i, i)).text\n",
+                "    for i in np.random.choice(np.arange(start=1000, stop=1200), 10)\n",
+                "]\n",
                 "\n",
-                "text = '\\n'.join(texts)\n",
+                "text = \"\\n\".join(texts)\n",
                 "len(text)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -85,15 +89,15 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def random_chunk(text, chunk_length=100):\n",
                 "    start_index = np.random.randint(0, len(text) - chunk_length - 1)\n",
                 "    end_index = start_index + chunk_length\n",
-                "    input_text = text[start_index : end_index]\n",
+                "    input_text = text[start_index:end_index]\n",
                 "    target_character = text[end_index]\n",
                 "    return list(input_text), target_character"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -108,18 +112,18 @@
             "outputs": [],
             "source": [
                 "class ChunkDataset(Dataset):\n",
                 "    def __init__(self, text, label_encoder, length):\n",
                 "        self.text = text\n",
                 "        self.label_encoder = label_encoder\n",
                 "        self.length = length\n",
-                "        \n",
+                "\n",
                 "    def __getitem__(self, index):\n",
                 "        input_text, target_character = random_chunk(self.text)\n",
-                "        \n",
+                "\n",
                 "        input_indexes = self.label_encoder.transform(input_text)\n",
                 "        target_index = self.label_encoder.transform([target_character])\n",
                 "        return input_indexes, target_index\n",
                 "\n",
                 "    def __len__(self):\n",
                 "        return self.length"
             ]
@@ -138,21 +142,17 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "batch_size = 200\n",
                 "\n",
-                "train_loader = DataLoader(dataset=train_dataset, \n",
-                "                          batch_size=batch_size, \n",
-                "                          num_workers=5)\n",
-                "\n",
-                "test_loader = DataLoader(dataset=test_dataset, \n",
-                "                         batch_size=batch_size, \n",
-                "                         num_workers=5)"
+                "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=5)\n",
+                "\n",
+                "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=5)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### model"
@@ -166,129 +166,139 @@
             "source": [
                 "class SequenceEncoder(nn.Module):\n",
                 "    def __init__(self, num_embeddings, embedding_size, hidden_size, label_encoder):\n",
                 "        super(SequenceEncoder, self).__init__()\n",
                 "        self.label_encoder = label_encoder\n",
                 "        self.embedding = nn.Embedding(num_embeddings, embedding_size)\n",
                 "        self.lstm = nn.LSTM(embedding_size, hidden_size)\n",
-                "        self.decoder = nn.Sequential(nn.Dropout(0.2),\n",
-                "                                     nn.Linear(hidden_size, num_embeddings))\n",
+                "        self.decoder = nn.Sequential(\n",
+                "            nn.Dropout(0.2), nn.Linear(hidden_size, num_embeddings)\n",
+                "        )\n",
                 "\n",
                 "    def forward(self, indexes):\n",
                 "        embedded = self.embedding(indexes)\n",
                 "        output, hidden = self.lstm(embedded)\n",
                 "        return self.decoder(output[:, -1])\n",
-                "    \n",
+                "\n",
                 "    def predict_next_character(self, indexes):\n",
                 "        preds = self.forward(indexes)[-1]\n",
                 "        guess_index = preds.argmax()[0]\n",
                 "        next_character = self.label_encoder.inverse_transform([guess_index])\n",
                 "        return next_character[0]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "losses = []\n",
+                "\n",
+                "\n",
                 "def train(model, train_loader, loss_function, optimiser, n_epochs):\n",
                 "    for epoch in range(n_epochs):\n",
                 "        model.train()\n",
                 "        loop = tqdm(train_loader)\n",
                 "        for inputs, targets in loop:\n",
                 "            inputs = inputs.cuda(non_blocking=True)\n",
                 "            targets = targets.cuda(non_blocking=True).view(-1)\n",
                 "\n",
                 "            optimiser.zero_grad()\n",
                 "            preds = model(inputs)\n",
                 "\n",
                 "            loss = loss_function(preds, targets)\n",
                 "            loss.backward()\n",
                 "            optimiser.step()\n",
-                "            \n",
+                "\n",
                 "            losses.append(loss.item())\n",
-                "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
+                "            loop.set_description(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
                 "            loop.set_postfix(loss=np.mean(losses[-20:]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "torch.backends.cudnn.benchmark = True\n",
                 "\n",
-                "model = SequenceEncoder(num_embeddings=len(all_characters), \n",
-                "                        embedding_size=128,\n",
-                "                        hidden_size=256,\n",
-                "                        label_encoder=label_encoder\n",
-                "                       ).to(device)\n",
+                "model = SequenceEncoder(\n",
+                "    num_embeddings=len(all_characters),\n",
+                "    embedding_size=128,\n",
+                "    hidden_size=256,\n",
+                "    label_encoder=label_encoder,\n",
+                ").to(device)\n",
                 "\n",
                 "trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
                 "optimiser = optim.Adam(trainable_parameters, lr=0.001)\n",
                 "loss_function = nn.CrossEntropyLoss()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train(model=model,\n",
-                "      train_loader=train_loader,\n",
-                "      loss_function=loss_function,\n",
-                "      optimiser=optimiser,\n",
-                "      n_epochs=10)"
+                "train(\n",
+                "    model=model,\n",
+                "    train_loader=train_loader,\n",
+                "    loss_function=loss_function,\n",
+                "    optimiser=optimiser,\n",
+                "    n_epochs=10,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "loss_data = pd.Series(losses).rolling(window=15).mean()\n",
-                "ax = loss_data.plot();\n",
+                "ax = loss_data.plot()\n",
                 "ax.set_ylim(0, 5);"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "text_chunk, target_character = random_chunk(text)\n",
-                "indexes = label_encoder.transform(text_chunk).reshape(-1,1)\n",
+                "indexes = label_encoder.transform(text_chunk).reshape(-1, 1)\n",
                 "indexes = torch.Tensor(indexes).long().cuda()\n",
                 "\n",
                 "x = model(indexes)[-1]\n",
                 "\n",
                 "\n",
-                "print(''.join(text_chunk))\n",
-                "print('''\n",
+                "print(\"\".join(text_chunk))\n",
+                "print(\n",
+                "    \"\"\"\n",
                 "------------------------------\n",
                 "predicted character:\\t{}\n",
                 "actual character:\\t{}\n",
-                "      '''.format(label_encoder.inverse_transform([x.argmax()])[0],\n",
-                "                 target_character))"
+                "      \"\"\".format(\n",
+                "        label_encoder.inverse_transform([x.argmax()])[0], target_character\n",
+                "    )\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "text_chunk, target_character = random_chunk(text)\n",
                 "\n",
+                "\n",
                 "def predict(text_chunk):\n",
                 "    indexes = label_encoder.transform(text_chunk)\n",
                 "    indexes = torch.Tensor(indexes).long().cuda().unsqueeze(0)\n",
                 "    return model.predict_next_character(indexes)"
             ]
         },
         {
@@ -305,15 +315,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "''.join(text_chunk)"
+                "\"\".join(text_chunk)"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
             "display_name": "Python 3",
             "language": "python",
```

### Comparing `weco-datascience-0.1.8/research_notebooks/devise/notebooks/08 - sentence embeddings via NLI from scratch.ipynb` & `weco-datascience-0.1.9/notebooks/devise/notebooks/08 - sentence embeddings via NLI from scratch.ipynb`

 * *Files 3% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.995373580727402%*

 * *Differences: {"'cells'": '{1: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 14)\\n\'), (12, \'import numpy as np\\n\'), '*

 * *            '(18, \'\\n\'), (30, \'nlp = spacy.load("en")\\n\'), (31, '*

 * *            '\'nltk.download("punkt")\\n\'), (32, \'device = torch.device("cuda" if '*

 * *            'torch.cuda.is_available() else "cpu")\')], delete: [30, 29, 28, 11, 4, 3]}}, 3: '*

 * *            "{'source': ['multinli = pd.read_json(\\n', '    […]*

```diff
@@ -14,42 +14,44 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (20, 14)\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 14)\n",
                 "\n",
                 "import os\n",
                 "import json\n",
                 "import nltk\n",
                 "import spacy\n",
                 "import itertools\n",
-                "import numpy as np \n",
+                "import numpy as np\n",
                 "import pandas as pd\n",
                 "from PIL import Image\n",
                 "from scipy.spatial.distance import cdist\n",
                 "from tqdm import tqdm_notebook as tqdm\n",
                 "from tqdm import tqdm as tqdm_\n",
+                "\n",
                 "tqdm_.pandas()\n",
                 "\n",
                 "import io\n",
                 "from nltk.tokenize import word_tokenize\n",
                 "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
                 "\n",
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader, random_split\n",
                 "from torchvision import models, transforms\n",
                 "\n",
-                "nlp = spacy.load('en')\n",
-                "nltk.download('punkt')\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "nlp = spacy.load(\"en\")\n",
+                "nltk.download(\"punkt\")\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# assemble SNLI, MultiNLI and COCO dataframes\n",
@@ -58,47 +60,47 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "multinli = (pd.read_json('/mnt/efs/nlp/natural_language_inference/multinli_1.0/multinli_1.0_train.jsonl', \n",
-                "                         lines=True)\n",
-                "            [['gold_label', 'sentence1', 'sentence2']]\n",
-                "           )"
+                "multinli = pd.read_json(\n",
+                "    \"/mnt/efs/nlp/natural_language_inference/multinli_1.0/multinli_1.0_train.jsonl\",\n",
+                "    lines=True,\n",
+                ")[[\"gold_label\", \"sentence1\", \"sentence2\"]]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "snli = (pd.read_json('/mnt/efs/nlp/natural_language_inference/snli_1.0/snli_1.0_train.jsonl', \n",
-                "                     lines=True)\n",
-                "        [['gold_label', 'sentence1', 'sentence2']]\n",
-                "       )"
+                "snli = pd.read_json(\n",
+                "    \"/mnt/efs/nlp/natural_language_inference/snli_1.0/snli_1.0_train.jsonl\", lines=True\n",
+                ")[[\"gold_label\", \"sentence1\", \"sentence2\"]]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open('/mnt/efs/nlp/natural_language_inference/coco2014/captions_train2014.json') as f:\n",
-                "    df = pd.DataFrame(json.load(f)['annotations'])\n",
+                "with open(\n",
+                "    \"/mnt/efs/nlp/natural_language_inference/coco2014/captions_train2014.json\"\n",
+                ") as f:\n",
+                "    df = pd.DataFrame(json.load(f)[\"annotations\"])\n",
                 "\n",
                 "coco, i = {}, 0\n",
-                "for image_id in tqdm(df['image_id'].unique()):\n",
-                "    captions = df[df['image_id'] == image_id]['caption'].values\n",
+                "for image_id in tqdm(df[\"image_id\"].unique()):\n",
+                "    captions = df[df[\"image_id\"] == image_id][\"caption\"].values\n",
                 "    for s1, s2 in list(itertools.combinations(captions, 2)):\n",
-                "        coco[i] = {'gold_label': 'entailment',\n",
-                "                   'sentence1': s1, 'sentence2': s2}\n",
+                "        coco[i] = {\"gold_label\": \"entailment\", \"sentence1\": s1, \"sentence2\": s2}\n",
                 "        i += 1\n",
                 "\n",
                 "coco = pd.DataFrame(coco).T\n",
                 "del df"
             ]
         },
         {
@@ -113,19 +115,17 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "sentences = (pd.concat([multinli, snli, coco])\n",
-                "             .fillna('')\n",
-                "             ['sentence1']\n",
-                "             .sample(20000)\n",
-                "             .values)\n",
+                "sentences = (\n",
+                "    pd.concat([multinli, snli, coco]).fillna(\"\")[\"sentence1\"].sample(20000).values\n",
+                ")\n",
                 "\n",
                 "subjects = {}\n",
                 "i = 0"
             ]
         },
         {
             "cell_type": "markdown",
@@ -138,18 +138,20 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "for sentence in tqdm(sentences):\n",
                 "    for word in nlp(sentence):\n",
-                "        if word.pos_ == 'NOUN':\n",
-                "            subjects[i] = {'sentence1': word.text,\n",
-                "                           'sentence2': sentence,\n",
-                "                           'gold_label': 'entailment'}\n",
+                "        if word.pos_ == \"NOUN\":\n",
+                "            subjects[i] = {\n",
+                "                \"sentence1\": word.text,\n",
+                "                \"sentence2\": sentence,\n",
+                "                \"gold_label\": \"entailment\",\n",
+                "            }\n",
                 "            i += 1"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -161,21 +163,22 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "for sentence in tqdm(sentences):\n",
                 "    words = nlp(sentence)\n",
                 "    for i in range(len(words) - 1):\n",
-                "        word_1, word_2 = words[i:i+2]\n",
-                "        if ((word_1.pos_ == 'ADJ') & (word_2.pos_ == 'NOUN')):            \n",
-                "            subjects[i] = {'sentence1': word.text,\n",
-                "                           'sentence2': sentence,\n",
-                "                           'gold_label': 'entailment'}\n",
-                "            i += 1\n",
-                "            "
+                "        word_1, word_2 = words[i : i + 2]\n",
+                "        if (word_1.pos_ == \"ADJ\") & (word_2.pos_ == \"NOUN\"):\n",
+                "            subjects[i] = {\n",
+                "                \"sentence1\": word.text,\n",
+                "                \"sentence2\": sentence,\n",
+                "                \"gold_label\": \"entailment\",\n",
+                "            }\n",
+                "            i += 1"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "We'll now transform that dictionary into a dataframe so that it can be combined with the ones we loaded in before."
@@ -200,16 +203,16 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df = pd.concat([multinli, snli, coco, subjects]).fillna('')\n",
-                "df = df.drop(df[df['gold_label'] == '-'].index)\n",
+                "df = pd.concat([multinli, snli, coco, subjects]).fillna(\"\")\n",
+                "df = df.drop(df[df[\"gold_label\"] == \"-\"].index)\n",
                 "df.reset_index(inplace=True, drop=True)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -221,36 +224,37 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "wv_path = '/mnt/efs/nlp/word_vectors/fasttext/wiki-news-300d-1M.vec'\n",
-                "wv_file = io.open(wv_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
+                "wv_path = \"/mnt/efs/nlp/word_vectors/fasttext/wiki-news-300d-1M.vec\"\n",
+                "wv_file = io.open(wv_path, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\")\n",
                 "\n",
-                "fasttext = {line.split()[0]: np.array(line.split()[1:])\n",
-                "            for line in tqdm(list(wv_file))}\n",
+                "fasttext = {line.split()[0]: np.array(line.split()[1:]) for line in tqdm(list(wv_file))}\n",
                 "\n",
                 "pad_value, start_value, end_value = 0.25, 0.5, 0.75\n",
-                "fasttext['<p>'] = np.full(shape=(300,), fill_value=pad_value)\n",
-                "fasttext['<s>'] = np.full(shape=(300,), fill_value=start_value)\n",
-                "fasttext['</s>'] = np.full(shape=(300,), fill_value=end_value)"
+                "fasttext[\"<p>\"] = np.full(shape=(300,), fill_value=pad_value)\n",
+                "fasttext[\"<s>\"] = np.full(shape=(300,), fill_value=start_value)\n",
+                "fasttext[\"</s>\"] = np.full(shape=(300,), fill_value=end_value)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def preprocess(sentence):\n",
-                "    index_list = ([word_to_index['<s>']] + \n",
-                "                  [word_to_index[w] for w in word_tokenize(sentence) if w in fasttext] + \n",
-                "                  [word_to_index['</s>']])\n",
+                "    index_list = (\n",
+                "        [word_to_index[\"<s>\"]]\n",
+                "        + [word_to_index[w] for w in word_tokenize(sentence) if w in fasttext]\n",
+                "        + [word_to_index[\"</s>\"]]\n",
+                "    )\n",
                 "    return index_list"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -273,36 +277,36 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df['sentence1'] = df['sentence1'].apply(str.lower)\n",
-                "df['sentence2'] = df['sentence2'].apply(str.lower)"
+                "df[\"sentence1\"] = df[\"sentence1\"].apply(str.lower)\n",
+                "df[\"sentence2\"] = df[\"sentence2\"].apply(str.lower)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df['sentence1'] = df['sentence1'].progress_apply(preprocess)\n",
-                "df['sentence2'] = df['sentence2'].progress_apply(preprocess)"
+                "df[\"sentence1\"] = df[\"sentence1\"].progress_apply(preprocess)\n",
+                "df[\"sentence2\"] = df[\"sentence2\"].progress_apply(preprocess)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "le = LabelEncoder()\n",
-                "df['gold_label'] = le.fit_transform(df['gold_label'].values)"
+                "df[\"gold_label\"] = le.fit_transform(df[\"gold_label\"].values)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# dataset and dataloader"
@@ -312,17 +316,17 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class NLIDataset(Dataset):\n",
                 "    def __init__(self, dataframe):\n",
-                "        self.sentence1s = dataframe['sentence1'].values\n",
-                "        self.sentence2s = dataframe['sentence2'].values\n",
-                "        self.labels = dataframe['gold_label'].values\n",
+                "        self.sentence1s = dataframe[\"sentence1\"].values\n",
+                "        self.sentence2s = dataframe[\"sentence2\"].values\n",
+                "        self.labels = dataframe[\"gold_label\"].values\n",
                 "\n",
                 "    def __getitem__(self, index):\n",
                 "        s1 = self.sentence1s[index]\n",
                 "        s2 = self.sentence2s[index]\n",
                 "        label = self.labels[index]\n",
                 "        return s1, s2, label\n",
                 "\n",
@@ -359,15 +363,15 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "train_df = shuffled_df.loc[:train_size]\n",
-                "test_df  = shuffled_df.loc[train_size:]"
+                "test_df = shuffled_df.loc[train_size:]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -385,18 +389,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "class_weights = (train_df['gold_label']\n",
-                "                 .value_counts(normalize=True)\n",
-                "                 .sort_index()\n",
-                "                 .values)\n",
+                "class_weights = train_df[\"gold_label\"].value_counts(normalize=True).sort_index().values\n",
                 "\n",
                 "class_weights = torch.Tensor(class_weights).cuda()"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -409,62 +410,65 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def sentence_to_indexes(sentence):\n",
                 "    tokenised = word_tokenize(sentence)\n",
-                "    indexes = [word_to_index[word] \n",
-                "               for word in tokenised \n",
-                "               if word in word_to_index]\n",
+                "    indexes = [word_to_index[word] for word in tokenised if word in word_to_index]\n",
                 "    return indexes\n",
                 "\n",
+                "\n",
                 "def pad_sequence(sentences, pad_length=None):\n",
                 "    if pad_length is None:\n",
                 "        pad_length = max([len(sent) for sent in sentences])\n",
                 "\n",
-                "    padded = np.full((len(sentences), pad_length), word_to_index['<p>'])\n",
+                "    padded = np.full((len(sentences), pad_length), word_to_index[\"<p>\"])\n",
                 "    for i, sentence in enumerate(sentences):\n",
-                "        padded[i][pad_length - len(sentence):] = sentence\n",
+                "        padded[i][pad_length - len(sentence) :] = sentence\n",
                 "    return padded\n",
                 "\n",
                 "\n",
                 "def custom_collate_fn(batch):\n",
                 "    s1, s2, labels = zip(*batch)\n",
-                "    \n",
+                "\n",
                 "    batch_size = len(labels)\n",
                 "    seq_length = max([len(s) for s in (s1 + s2)])\n",
                 "\n",
                 "    padded_s1 = pad_sequence(s1, pad_length=seq_length)\n",
                 "    padded_s2 = pad_sequence(s2, pad_length=seq_length)\n",
-                "    \n",
+                "\n",
                 "    wv_s1 = np.stack([[index_to_wordvec[i] for i in seq] for seq in padded_s1])\n",
                 "    wv_s2 = np.stack([[index_to_wordvec[i] for i in seq] for seq in padded_s2])\n",
-                "    \n",
+                "\n",
                 "    return wv_s1, wv_s2, labels"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "batch_size = 64\n",
                 "\n",
-                "train_loader = DataLoader(dataset=train_dataset,\n",
-                "                          batch_size=batch_size,\n",
-                "                          num_workers=5,\n",
-                "                          shuffle=True,\n",
-                "                          collate_fn=custom_collate_fn)\n",
-                "\n",
-                "test_loader = DataLoader(dataset=test_dataset,\n",
-                "                         batch_size=batch_size,\n",
-                "                         num_workers=5,\n",
-                "                         collate_fn=custom_collate_fn)"
+                "train_loader = DataLoader(\n",
+                "    dataset=train_dataset,\n",
+                "    batch_size=batch_size,\n",
+                "    num_workers=5,\n",
+                "    shuffle=True,\n",
+                "    collate_fn=custom_collate_fn,\n",
+                ")\n",
+                "\n",
+                "test_loader = DataLoader(\n",
+                "    dataset=test_dataset,\n",
+                "    batch_size=batch_size,\n",
+                "    num_workers=5,\n",
+                "    collate_fn=custom_collate_fn,\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# build models\n",
@@ -475,39 +479,42 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "hidden_size = 2048\n",
                 "\n",
+                "\n",
                 "class SentenceEncoder(nn.Module):\n",
-                "    def __init__(self, ):\n",
+                "    def __init__(\n",
+                "        self,\n",
+                "    ):\n",
                 "        super(SentenceEncoder, self).__init__()\n",
-                "        self.enc_lstm = nn.LSTM(input_size=300, \n",
-                "                                hidden_size=hidden_size, \n",
-                "                                num_layers=1,\n",
-                "                                bidirectional=True)\n",
-                "        \n",
+                "        self.enc_lstm = nn.LSTM(\n",
+                "            input_size=300, hidden_size=hidden_size, num_layers=1, bidirectional=True\n",
+                "        )\n",
+                "\n",
                 "    def forward(self, wv_batch):\n",
                 "        embedded, _ = self.enc_lstm(wv_batch)\n",
-                "        max_pooled = torch.max(embedded, 1)[0] \n",
+                "        max_pooled = torch.max(embedded, 1)[0]\n",
                 "        return max_pooled\n",
                 "\n",
                 "\n",
                 "class NLINet(nn.Module):\n",
                 "    def __init__(self, index_to_wordvec):\n",
                 "        super(NLINet, self).__init__()\n",
                 "        self.index_to_wordvec = index_to_wordvec\n",
                 "        self.encoder = SentenceEncoder()\n",
-                "        self.classifier = nn.Sequential(nn.Dropout(0.2),\n",
-                "                                        nn.Linear(hidden_size*8, 128),\n",
-                "                                        nn.ReLU(),\n",
-                "                                        nn.Dropout(0.2),\n",
-                "                                        nn.Linear(128, 3),\n",
-                "                                       )\n",
+                "        self.classifier = nn.Sequential(\n",
+                "            nn.Dropout(0.2),\n",
+                "            nn.Linear(hidden_size * 8, 128),\n",
+                "            nn.ReLU(),\n",
+                "            nn.Dropout(0.2),\n",
+                "            nn.Linear(128, 3),\n",
+                "        )\n",
                 "\n",
                 "    def forward(self, s1, s2):\n",
                 "        u, v = self.encoder(s1), self.encoder(s2)\n",
                 "        features = torch.cat((u, v, torch.abs(u - v), u * v), 1)\n",
                 "        return self.classifier(features)\n",
                 "\n",
                 "    def encode(self, sentences):\n",
@@ -553,67 +560,71 @@
                 "            loss = loss_function(preds, target)\n",
                 "            loss.backward()\n",
                 "            optimiser.step()\n",
                 "\n",
                 "            n_correct = target.eq(preds.max(1)[1]).cpu().sum()\n",
                 "            accuracy = (n_correct / batch_size) * 100\n",
                 "\n",
-                "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
+                "            loop.set_description(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
                 "            loop.set_postfix(loss=loss.item(), acc=accuracy.item())\n",
                 "            losses.append([loss.item(), accuracy.item()])"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "torch.backends.cudnn.benchmark = True\n",
                 "\n",
                 "model = NLINet(index_to_wordvec).to(device)\n",
-                "model.load_state_dict(torch.load('/mnt/efs/models/nlinet-2018-10-08.pt'))\n",
+                "model.load_state_dict(torch.load(\"/mnt/efs/models/nlinet-2018-10-08.pt\"))\n",
                 "\n",
                 "trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
                 "optimiser = optim.Adam(trainable_parameters, lr=0.001)\n",
                 "loss_function = nn.CrossEntropyLoss(weight=class_weights)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train(model=model,\n",
-                "      train_loader=train_loader,\n",
-                "      loss_function=loss_function,\n",
-                "      optimiser=optimiser,\n",
-                "      n_epochs=3)"
+                "train(\n",
+                "    model=model,\n",
+                "    train_loader=train_loader,\n",
+                "    loss_function=loss_function,\n",
+                "    optimiser=optimiser,\n",
+                "    n_epochs=3,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "loss_data = pd.Series(losses).rolling(window=50).mean()\n",
-                "ax = loss_data.plot();\n",
-                "ax.set_xlim(0,);\n",
+                "ax = loss_data.plot()\n",
+                "ax.set_xlim(\n",
+                "    0,\n",
+                ")\n",
                 "ax.set_ylim(0, 1.1);"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "torch.save(model.state_dict(), '/mnt/efs/models/nlinet-2018-10-08.pt')"
+                "torch.save(model.state_dict(), \"/mnt/efs/models/nlinet-2018-10-08.pt\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# evaluate\n",
@@ -623,48 +634,45 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def embed(sentence):\n",
-                "    indexes = ([word_to_index['<s>']] + \n",
-                "               sentence_to_indexes(sentence) +\n",
-                "               [word_to_index['</s>']])\n",
+                "    indexes = (\n",
+                "        [word_to_index[\"<s>\"]] + sentence_to_indexes(sentence) + [word_to_index[\"</s>\"]]\n",
+                "    )\n",
                 "    wvs = np.stack([index_to_wordvec[i] for i in indexes])\n",
                 "    embedding = model.encoder(torch.Tensor([wvs]).cuda()).cpu().data.numpy()\n",
                 "    return embedding.squeeze()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "sentences = (pd.concat([multinli, snli, coco])\n",
-                "             .fillna('')\n",
-                "             ['sentence1']\n",
-                "             .sample(20)\n",
-                "             .values)\n",
+                "sentences = pd.concat([multinli, snli, coco]).fillna(\"\")[\"sentence1\"].sample(20).values\n",
                 "\n",
                 "embeddings = [embed(sentence) for sentence in sentences]\n",
                 "\n",
                 "for i, sentence in enumerate(sentences):\n",
                 "    print(i, sentence)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "from scipy.spatial.distance import cdist\n",
-                "distance_matrix = cdist(embeddings, embeddings, metric='cosine')\n",
+                "\n",
+                "distance_matrix = cdist(embeddings, embeddings, metric=\"cosine\")\n",
                 "sns.heatmap(distance_matrix);"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -713,37 +721,37 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "meta = pd.read_json('/mnt/efs/other/works.json', lines=True)\n",
-                "meta.index = meta['identifiers'].apply(lambda x: x[0]['value']).rename()"
+                "meta = pd.read_json(\"/mnt/efs/other/works.json\", lines=True)\n",
+                "meta.index = meta[\"identifiers\"].apply(lambda x: x[0][\"value\"]).rename()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "titles = meta['title'].values\n",
+                "titles = meta[\"title\"].values\n",
                 "title_embeddings = np.array([embed(sentence) for sentence in tqdm(titles)])"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "query_sentence = 'table'\n",
+                "query_sentence = \"table\"\n",
                 "query_embedding = embed(query_sentence).reshape(1, -1)\n",
-                "distances = cdist(query_embedding, title_embeddings, metric='cosine')\n",
+                "distances = cdist(query_embedding, title_embeddings, metric=\"cosine\")\n",
                 "print(query_sentence)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -765,19 +773,21 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "import pickle\n",
                 "\n",
                 "sentence_encoder = model.encoder\n",
-                "torch.save(sentence_encoder.state_dict(), '/mnt/efs/models/sentence-encoder-2018-10-08.pt')\n",
-                "\n",
-                "np.save('/mnt/efs/models/index_to_wordvec.npy', index_to_wordvec)\n",
-                "pickle.dump(word_to_index, open('/mnt/efs/models/word_to_index.pkl', 'wb'))\n",
-                "pickle.dump(index_to_word, open('/mnt/efs/models/index_to_word.pkl', 'wb'))"
+                "torch.save(\n",
+                "    sentence_encoder.state_dict(), \"/mnt/efs/models/sentence-encoder-2018-10-08.pt\"\n",
+                ")\n",
+                "\n",
+                "np.save(\"/mnt/efs/models/index_to_wordvec.npy\", index_to_wordvec)\n",
+                "pickle.dump(word_to_index, open(\"/mnt/efs/models/word_to_index.pkl\", \"wb\"))\n",
+                "pickle.dump(index_to_word, open(\"/mnt/efs/models/index_to_word.pkl\", \"wb\"))"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
             "display_name": "Python 3",
             "language": "python",
```

### Comparing `weco-datascience-0.1.8/research_notebooks/devise/notebooks/09 - devise against custom sentence embedding.ipynb` & `weco-datascience-0.1.9/notebooks/devise/notebooks/09 - devise against custom sentence embedding.ipynb`

 * *Files 4% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9933951007496514%*

 * *Differences: {"'cells'": '{1: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 14)\\n\'), (12, \'import numpy as np\\n\'), '*

 * *            '(18, \'\\n\'), (32, \'nlp = spacy.load("en")\\n\'), (33, \'\\n\'), (34, '*

 * *            '\'nltk.download("punkt")\\n\'), (35, \'device = torch.device("cuda" if '*

 * *            'torch.cuda.is_available() else "cpu")\')], delete: [32, 31, 29, 11, 4, 3]}}, 3: '*

 * *            "{'source': ['index_to_wordvec =  […]*

```diff
@@ -13,44 +13,47 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (20, 14)\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 14)\n",
                 "\n",
                 "import os\n",
                 "import json\n",
                 "import nltk\n",
                 "import pickle\n",
                 "import itertools\n",
-                "import numpy as np \n",
+                "import numpy as np\n",
                 "import pandas as pd\n",
                 "from PIL import Image\n",
                 "from scipy.spatial.distance import cdist\n",
                 "from tqdm import tqdm_notebook as tqdm\n",
                 "from tqdm import tqdm as tqdm_\n",
+                "\n",
                 "tqdm_.pandas()\n",
                 "\n",
                 "import io\n",
                 "from nltk.tokenize import word_tokenize\n",
                 "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
                 "\n",
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader, random_split\n",
                 "from torchvision import models, transforms\n",
                 "\n",
                 "import spacy\n",
-                "nlp = spacy.load('en')\n",
                 "\n",
-                "nltk.download('punkt')\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "nlp = spacy.load(\"en\")\n",
+                "\n",
+                "nltk.download(\"punkt\")\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# load data\n",
@@ -59,17 +62,17 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "index_to_wordvec = np.load('/mnt/efs/models/index_to_wordvec.npy')\n",
-                "word_to_index = pickle.load(open('/mnt/efs/models/word_to_index.pkl', 'rb'))\n",
-                "index_to_word = pickle.load(open('/mnt/efs/models/index_to_word.pkl', 'rb'))"
+                "index_to_wordvec = np.load(\"/mnt/efs/models/index_to_wordvec.npy\")\n",
+                "word_to_index = pickle.load(open(\"/mnt/efs/models/word_to_index.pkl\", \"rb\"))\n",
+                "index_to_word = pickle.load(open(\"/mnt/efs/models/index_to_word.pkl\", \"rb\"))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# utils\n",
@@ -81,32 +84,31 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def sentence_to_indexes(sentence):\n",
                 "    sentence = sentence.lower()\n",
                 "    tokenised = word_tokenize(sentence)\n",
-                "    indexes = [word_to_index[word] \n",
-                "               for word in tokenised \n",
-                "               if word in word_to_index]\n",
+                "    indexes = [word_to_index[word] for word in tokenised if word in word_to_index]\n",
                 "    return indexes\n",
                 "\n",
                 "\n",
                 "def embed(sentence):\n",
-                "    indexes = ([word_to_index['<s>']] + \n",
-                "               sentence_to_indexes(sentence) +\n",
-                "               [word_to_index['</s>']])\n",
+                "    indexes = (\n",
+                "        [word_to_index[\"<s>\"]] + sentence_to_indexes(sentence) + [word_to_index[\"</s>\"]]\n",
+                "    )\n",
                 "    wvs = np.stack([index_to_wordvec[i] for i in indexes])\n",
                 "    embedding = model(torch.Tensor([wvs]).cuda()).cpu().data.numpy()\n",
                 "    return embedding.squeeze()\n",
                 "\n",
+                "\n",
                 "def embed_paragraph(paragraph):\n",
                 "    sentences = nltk.sent_tokenize(paragraph)\n",
                 "    if len(sentences) == 0:\n",
-                "        embeddings = embed('.')\n",
+                "        embeddings = embed(\".\")\n",
                 "    else:\n",
                 "        embeddings = [embed(sentence) for sentence in sentences]\n",
                 "    return np.array(embeddings).max(axis=0)"
             ]
         },
         {
             "cell_type": "markdown",
@@ -120,39 +122,40 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "hidden_size = 2048\n",
                 "\n",
+                "\n",
                 "class SentenceEncoder(nn.Module):\n",
                 "    def __init__(self):\n",
                 "        super(SentenceEncoder, self).__init__()\n",
-                "        self.enc_lstm = nn.LSTM(input_size=300, \n",
-                "                                hidden_size=hidden_size, \n",
-                "                                num_layers=1,\n",
-                "                                bidirectional=True)\n",
-                "        \n",
+                "        self.enc_lstm = nn.LSTM(\n",
+                "            input_size=300, hidden_size=hidden_size, num_layers=1, bidirectional=True\n",
+                "        )\n",
+                "\n",
                 "    def forward(self, wv_batch):\n",
                 "        embedded, _ = self.enc_lstm(wv_batch)\n",
-                "        max_pooled = torch.max(embedded, 1)[0] \n",
+                "        max_pooled = torch.max(embedded, 1)[0]\n",
                 "        return max_pooled\n",
                 "\n",
                 "\n",
                 "class NLINet(nn.Module):\n",
                 "    def __init__(self, index_to_wordvec):\n",
                 "        super(NLINet, self).__init__()\n",
                 "        self.index_to_wordvec = index_to_wordvec\n",
                 "        self.encoder = SentenceEncoder()\n",
-                "        self.classifier = nn.Sequential(nn.Dropout(0.2),\n",
-                "                                        nn.Linear(hidden_size*8, 128),\n",
-                "                                        nn.ReLU(),\n",
-                "                                        nn.Dropout(0.2),\n",
-                "                                        nn.Linear(128, 3),\n",
-                "                                       )\n",
+                "        self.classifier = nn.Sequential(\n",
+                "            nn.Dropout(0.2),\n",
+                "            nn.Linear(hidden_size * 8, 128),\n",
+                "            nn.ReLU(),\n",
+                "            nn.Dropout(0.2),\n",
+                "            nn.Linear(128, 3),\n",
+                "        )\n",
                 "\n",
                 "    def forward(self, s1, s2):\n",
                 "        u, v = self.encoder(s1), self.encoder(s2)\n",
                 "        features = torch.cat((u, v, torch.abs(u - v), u * v), 1)\n",
                 "        return self.classifier(features)\n",
                 "\n",
                 "    def encode(self, sentences):\n",
@@ -165,15 +168,15 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "model = SentenceEncoder().to(device)\n",
                 "\n",
-                "model_path = '/mnt/efs/models/sentence-encoder-2018-10-08.pt'\n",
+                "model_path = \"/mnt/efs/models/sentence-encoder-2018-10-08.pt\"\n",
                 "model.load_state_dict(torch.load(model_path))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -183,72 +186,77 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "wellcome_image_path = '/mnt/efs/images/wellcome_images/'\n",
+                "wellcome_image_path = \"/mnt/efs/images/wellcome_images/\"\n",
                 "\n",
-                "wellcome_image_paths = [wellcome_image_path + subdir + '/' + wellcome_image_id\n",
-                "                        for subdir in os.listdir(wellcome_image_path)\n",
-                "                        for wellcome_image_id in os.listdir(wellcome_image_path+subdir)]\n",
-                "\n",
-                "wellcome_image_ids = [path.split('/')[-1].split('.')[0] for path in wellcome_image_paths]\n",
+                "wellcome_image_paths = [\n",
+                "    wellcome_image_path + subdir + \"/\" + wellcome_image_id\n",
+                "    for subdir in os.listdir(wellcome_image_path)\n",
+                "    for wellcome_image_id in os.listdir(wellcome_image_path + subdir)\n",
+                "]\n",
+                "\n",
+                "wellcome_image_ids = [\n",
+                "    path.split(\"/\")[-1].split(\".\")[0] for path in wellcome_image_paths\n",
+                "]\n",
                 "\n",
                 "wellcome_path_series = pd.Series(dict(zip(wellcome_image_ids, wellcome_image_paths)))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "meta = pd.read_json('/mnt/efs/other/works.json', lines=True)\n",
-                "meta.index = meta['identifiers'].apply(lambda x: x[0]['value']).rename()\n",
-                "wellcome_title_series = meta['title'].fillna('')"
+                "meta = pd.read_json(\"/mnt/efs/other/works.json\", lines=True)\n",
+                "meta.index = meta[\"identifiers\"].apply(lambda x: x[0][\"value\"]).rename()\n",
+                "wellcome_title_series = meta[\"title\"].fillna(\"\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "wellcome_df = pd.concat([wellcome_path_series, wellcome_title_series], axis=1)\n",
-                "wellcome_df.columns = ['file_name', 'caption']\n",
+                "wellcome_df.columns = [\"file_name\", \"caption\"]\n",
                 "\n",
                 "wellcome_df = wellcome_df.dropna()\n",
-                "wellcome_df['caption'] = wellcome_df['caption']"
+                "wellcome_df[\"caption\"] = wellcome_df[\"caption\"]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open('/mnt/efs/images/coco/annotations/captions_val2014.json') as f:\n",
+                "with open(\"/mnt/efs/images/coco/annotations/captions_val2014.json\") as f:\n",
                 "    meta = json.load(f)\n",
                 "\n",
-                "coco_df = (pd.merge(pd.DataFrame(meta['images']).set_index('id'),\n",
-                "                    pd.DataFrame(meta['annotations']).set_index('image_id'), \n",
-                "                    left_index=True, right_index=True)\n",
-                "           .reset_index()\n",
-                "           [['caption', 'file_name']]\n",
-                "          )\n",
-                "\n",
-                "coco_df['file_name'] = '/mnt/efs/images/coco/val2014/' + coco_df['file_name']\n",
-                "\n",
-                "coco_df['caption'] = (coco_df['caption']\n",
-                "                      .apply(lambda x: ''.join([c for c in x if c.isalpha() or c.isspace()]))\n",
-                "                      .apply(str.lower)\n",
-                "                      .apply(lambda x: ' '.join(x.split()))\n",
-                "                     )"
+                "coco_df = pd.merge(\n",
+                "    pd.DataFrame(meta[\"images\"]).set_index(\"id\"),\n",
+                "    pd.DataFrame(meta[\"annotations\"]).set_index(\"image_id\"),\n",
+                "    left_index=True,\n",
+                "    right_index=True,\n",
+                ").reset_index()[[\"caption\", \"file_name\"]]\n",
+                "\n",
+                "coco_df[\"file_name\"] = \"/mnt/efs/images/coco/val2014/\" + coco_df[\"file_name\"]\n",
+                "\n",
+                "coco_df[\"caption\"] = (\n",
+                "    coco_df[\"caption\"]\n",
+                "    .apply(lambda x: \"\".join([c for c in x if c.isalpha() or c.isspace()]))\n",
+                "    .apply(str.lower)\n",
+                "    .apply(lambda x: \" \".join(x.split()))\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -269,18 +277,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "source_data = (pd.concat([wellcome_df, coco_df])\n",
-                "               .fillna('')\n",
-                "               .sample(50000)\n",
-                "               .values)"
+                "source_data = pd.concat([wellcome_df, coco_df]).fillna(\"\").sample(50000).values"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "We extract the nouns from the sequence (using spacy's POS tagger) and add them to a dictionary, paired with their original image path. We'll also grab any adjective-noun pairs while we're there."
@@ -292,26 +297,27 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "subjects, i = {}, 0\n",
                 "\n",
                 "for caption, path in tqdm(source_data):\n",
                 "    words = nlp(caption)\n",
-                "    \n",
+                "\n",
                 "    for word in words:\n",
-                "        if word.pos_ == 'NOUN':\n",
-                "            subjects[i] = {'caption': word.text,\n",
-                "                           'file_name': path}\n",
+                "        if word.pos_ == \"NOUN\":\n",
+                "            subjects[i] = {\"caption\": word.text, \"file_name\": path}\n",
                 "            i += 1\n",
-                "    \n",
+                "\n",
                 "    for i in range(len(words) - 1):\n",
-                "        word_1, word_2 = words[i:i+2]\n",
-                "        if ((word_1.pos_ == 'ADJ') & (word_2.pos_ == 'NOUN')):\n",
-                "            subjects[i] = {'caption': ' '.join([word_1.text, word_2.text]),\n",
-                "                           'file_name': path}\n",
+                "        word_1, word_2 = words[i : i + 2]\n",
+                "        if (word_1.pos_ == \"ADJ\") & (word_2.pos_ == \"NOUN\"):\n",
+                "            subjects[i] = {\n",
+                "                \"caption\": \" \".join([word_1.text, word_2.text]),\n",
+                "                \"file_name\": path,\n",
+                "            }\n",
                 "            i += 1"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -361,16 +367,16 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train_embeddings = np.array([embed(caption) for caption in tqdm(train_df['caption'])])\n",
-                "test_embeddings = np.array([embed(caption) for caption in tqdm(test_df['caption'])])"
+                "train_embeddings = np.array([embed(caption) for caption in tqdm(train_df[\"caption\"])])\n",
+                "test_embeddings = np.array([embed(caption) for caption in tqdm(test_df[\"caption\"])])"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# datasets and dataloaders\n",
@@ -380,23 +386,22 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class CaptionsDataset(Dataset):\n",
-                "    def __init__(self, path_df, caption_embeddings, \n",
-                "                 transform=transforms.ToTensor()):\n",
+                "    def __init__(self, path_df, caption_embeddings, transform=transforms.ToTensor()):\n",
                 "        self.ids = path_df.index.values\n",
-                "        self.image_paths = path_df['file_name'].values\n",
+                "        self.image_paths = path_df[\"file_name\"].values\n",
                 "        self.caption_embeddings = caption_embeddings\n",
                 "        self.transform = transform\n",
                 "\n",
                 "    def __getitem__(self, index):\n",
-                "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
+                "        image = Image.open(self.image_paths[index]).convert(\"RGB\")\n",
                 "        if self.transform is not None:\n",
                 "            image = self.transform(image)\n",
                 "\n",
                 "        target = self.caption_embeddings[index]\n",
                 "        return image, target\n",
                 "\n",
                 "    def __len__(self):\n",
@@ -405,21 +410,26 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train_transform = transforms.Compose([transforms.RandomResizedCrop(224, scale=[0.65, 0.9]),\n",
-                "                                      transforms.RandomHorizontalFlip(),\n",
-                "                                      transforms.RandomGrayscale(0.35),\n",
-                "                                      transforms.ToTensor()])\n",
-                "\n",
-                "test_transform = transforms.Compose([transforms.RandomResizedCrop(224, scale=[0.65, 0.9]),\n",
-                "                                     transforms.ToTensor()])"
+                "train_transform = transforms.Compose(\n",
+                "    [\n",
+                "        transforms.RandomResizedCrop(224, scale=[0.65, 0.9]),\n",
+                "        transforms.RandomHorizontalFlip(),\n",
+                "        transforms.RandomGrayscale(0.35),\n",
+                "        transforms.ToTensor(),\n",
+                "    ]\n",
+                ")\n",
+                "\n",
+                "test_transform = transforms.Compose(\n",
+                "    [transforms.RandomResizedCrop(224, scale=[0.65, 0.9]), transforms.ToTensor()]\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -439,22 +449,19 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "batch_size = 64\n",
                 "\n",
-                "train_loader = DataLoader(dataset=train_dataset, \n",
-                "                          batch_size=batch_size,\n",
-                "                          shuffle=True,\n",
-                "                          num_workers=5)\n",
-                "\n",
-                "test_loader = DataLoader(dataset=test_dataset, \n",
-                "                         batch_size=batch_size,\n",
-                "                         num_workers=5)"
+                "train_loader = DataLoader(\n",
+                "    dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=5\n",
+                ")\n",
+                "\n",
+                "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=5)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# create DeViSE model\n",
@@ -476,22 +483,23 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class DeViSE(nn.Module):\n",
-                "    '''\n",
+                "    \"\"\"\n",
                 "    learn to map images into sentence space\n",
-                "    '''\n",
+                "    \"\"\"\n",
+                "\n",
                 "    def __init__(self, backbone, target_size):\n",
                 "        super(DeViSE, self).__init__()\n",
                 "        self.backbone = backbone\n",
                 "        self.head = nn.Sequential(\n",
-                "            nn.Linear(in_features=512*7*7, out_features=target_size),\n",
+                "            nn.Linear(in_features=512 * 7 * 7, out_features=target_size),\n",
                 "            nn.ReLU(inplace=True),\n",
                 "            nn.Dropout(),\n",
                 "            nn.Linear(in_features=target_size, out_features=target_size),\n",
                 "        )\n",
                 "\n",
                 "    def forward(self, x):\n",
                 "        x = self.backbone(x)\n",
@@ -504,15 +512,15 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "devise_model = DeViSE(backbone, target_size=4096).to(device)\n",
-                "devise_model_path = '/mnt/efs/models/devise-2018-10-09.pt'\n",
+                "devise_model_path = \"/mnt/efs/models/devise-2018-10-09.pt\"\n",
                 "devise_model.load_state_dict(torch.load(devise_model_path))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -524,35 +532,45 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "losses = []\n",
                 "\n",
-                "def train(model, train_loader, n_epochs, loss_function, \n",
-                "          additional_metric, optimiser, device=device):\n",
+                "\n",
+                "def train(\n",
+                "    model,\n",
+                "    train_loader,\n",
+                "    n_epochs,\n",
+                "    loss_function,\n",
+                "    additional_metric,\n",
+                "    optimiser,\n",
+                "    device=device,\n",
+                "):\n",
                 "    for epoch in range(n_epochs):\n",
                 "        model.train()\n",
                 "        loop = tqdm(train_loader)\n",
                 "        for data, target in loop:\n",
-                "            data, target, flags = (data.cuda(non_blocking=True), \n",
-                "                                   target.cuda(non_blocking=True), \n",
-                "                                   torch.ones(len(target)).cuda(non_blocking=True))\n",
+                "            data, target, flags = (\n",
+                "                data.cuda(non_blocking=True),\n",
+                "                target.cuda(non_blocking=True),\n",
+                "                torch.ones(len(target)).cuda(non_blocking=True),\n",
+                "            )\n",
                 "\n",
                 "            optimiser.zero_grad()\n",
                 "            prediction = model(data)\n",
                 "\n",
                 "            loss = loss_function(prediction, target, flags)\n",
                 "            mean_sq_error = additional_metric(prediction, target)\n",
                 "            losses.append([loss.item(), mean_sq_error.item()])\n",
                 "\n",
                 "            loss.backward()\n",
                 "            optimiser.step()\n",
                 "\n",
-                "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
+                "            loop.set_description(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
                 "            loop.set_postfix(loss=loss.item(), mse=mean_sq_error.item())"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -574,35 +592,41 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train(model=devise_model,\n",
-                "      train_loader=train_loader,\n",
-                "      loss_function=loss_function,\n",
-                "      additional_metric=mse, \n",
-                "      optimiser=optimiser,\n",
-                "      n_epochs=3)"
+                "train(\n",
+                "    model=devise_model,\n",
+                "    train_loader=train_loader,\n",
+                "    loss_function=loss_function,\n",
+                "    additional_metric=mse,\n",
+                "    optimiser=optimiser,\n",
+                "    n_epochs=3,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "loss_data = pd.DataFrame(losses).rolling(window=15).mean()\n",
-                "loss_data.columns = ['cosine loss', 'mse']\n",
-                "ax = loss_data.plot(subplots=True);\n",
+                "loss_data.columns = [\"cosine loss\", \"mse\"]\n",
+                "ax = loss_data.plot(subplots=True)\n",
                 "\n",
-                "ax[0].set_xlim(0,);\n",
-                "ax[0].set_ylim(0, 0.6);\n",
-                "ax[1].set_ylim(0,);"
+                "ax[0].set_xlim(\n",
+                "    0,\n",
+                ")\n",
+                "ax[0].set_ylim(0, 0.6)\n",
+                "ax[1].set_ylim(\n",
+                "    0,\n",
+                ");"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# predict\n",
@@ -617,25 +641,27 @@
             "source": [
                 "preds = []\n",
                 "test_loss = []\n",
                 "\n",
                 "with torch.no_grad():\n",
                 "    test_loop = tqdm(test_loader)\n",
                 "    for data, target in test_loop:\n",
-                "        data, target, flags = (data.cuda(),\n",
-                "                               target.cuda(),\n",
-                "                               torch.ones(len(target)).cuda())\n",
+                "        data, target, flags = (\n",
+                "            data.cuda(),\n",
+                "            target.cuda(),\n",
+                "            torch.ones(len(target)).cuda(),\n",
+                "        )\n",
                 "\n",
                 "        prediction = devise_model.eval()(data)\n",
                 "        loss = loss_function(prediction, target, flags)\n",
                 "\n",
                 "        preds.append(prediction.cpu().data.numpy())\n",
                 "        test_loss.append(loss.item())\n",
                 "\n",
-                "        test_loop.set_description('Test set')\n",
+                "        test_loop.set_description(\"Test set\")\n",
                 "        test_loop.set_postfix(loss=loss.item())"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -676,215 +702,221 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def search(query):\n",
                 "    query_embedding = embed(query).reshape(-1, 4096)\n",
                 "\n",
-                "    distances = cdist(query_embedding, embeddings, 'cosine').squeeze()\n",
-                "    nearby_image_paths = test_df['file_name'].values[np.argsort(distances)][:20]\n",
-                "    nearby_images = [np.array((Image.open(path)\n",
-                "                               .convert('RGB')\n",
-                "                               .resize((224, 224), Image.BILINEAR)))\n",
-                "                     for path in nearby_image_paths]\n",
-                "\n",
-                "    return Image.fromarray(np.concatenate([np.concatenate(nearby_images[:5], axis=1),\n",
-                "                                           np.concatenate(nearby_images[5:10], axis=1),\n",
-                "                                           np.concatenate(nearby_images[10:15], axis=1),\n",
-                "                                           np.concatenate(nearby_images[15:20], axis=1)],\n",
-                "                                          axis=0))"
+                "    distances = cdist(query_embedding, embeddings, \"cosine\").squeeze()\n",
+                "    nearby_image_paths = test_df[\"file_name\"].values[np.argsort(distances)][:20]\n",
+                "    nearby_images = [\n",
+                "        np.array((Image.open(path).convert(\"RGB\").resize((224, 224), Image.BILINEAR)))\n",
+                "        for path in nearby_image_paths\n",
+                "    ]\n",
+                "\n",
+                "    return Image.fromarray(\n",
+                "        np.concatenate(\n",
+                "            [\n",
+                "                np.concatenate(nearby_images[:5], axis=1),\n",
+                "                np.concatenate(nearby_images[5:10], axis=1),\n",
+                "                np.concatenate(nearby_images[10:15], axis=1),\n",
+                "                np.concatenate(nearby_images[15:20], axis=1),\n",
+                "            ],\n",
+                "            axis=0,\n",
+                "        )\n",
+                "    )"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('simulations of protein structure')"
+                "search(\"simulations of protein structure\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('text written in hindi')"
+                "search(\"text written in hindi\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('photograph of stone pillars in a church')"
+                "search(\"photograph of stone pillars in a church\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('portrait of a man')"
+                "search(\"portrait of a man\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('portrait of a woman')"
+                "search(\"portrait of a woman\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('mri scan of a brain')"
+                "search(\"mri scan of a brain\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('pretty drawings of plants and flowers')"
+                "search(\"pretty drawings of plants and flowers\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('really horrible , disgusting drawings of burns and skin diseases')"
+                "search(\"really horrible , disgusting drawings of burns and skin diseases\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('surgical instruments')"
+                "search(\"surgical instruments\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('astronomical charts of the moons')"
+                "search(\"astronomical charts of the moons\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('a cat preparing for surgery')"
+                "search(\"a cat preparing for surgery\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('dancing skeletons')"
+                "search(\"dancing skeletons\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('giraffe')"
+                "search(\"giraffe\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('a man dancing')"
+                "search(\"a man dancing\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('a collection of blood cells')"
+                "search(\"a collection of blood cells\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('a waterfall')"
+                "search(\"a waterfall\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('anatomical details of the tendons in hands and fingers')"
+                "search(\"anatomical details of the tendons in hands and fingers\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('buddhist man sitting with folded legs')"
+                "search(\"buddhist man sitting with folded legs\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('AIDS posters')"
+                "search(\"AIDS posters\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "search('fractured bone')"
+                "search(\"fractured bone\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "torch.save(devise_model.state_dict(), '/mnt/efs/models/devise-2018-10-09.pt')"
+                "torch.save(devise_model.state_dict(), \"/mnt/efs/models/devise-2018-10-09.pt\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# save devise'd embeddings\n",
@@ -893,43 +925,47 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "all_caption_embeddings = np.array([embed(caption) for caption in tqdm(wellcome_df['caption'].values)])"
+                "all_caption_embeddings = np.array(\n",
+                "    [embed(caption) for caption in tqdm(wellcome_df[\"caption\"].values)]\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "full_dataset = CaptionsDataset(wellcome_df, all_caption_embeddings, transform=test_transform)\n",
-                "full_loader = DataLoader(dataset=full_dataset, \n",
-                "                         batch_size=batch_size,\n",
-                "                         num_workers=5)"
+                "full_dataset = CaptionsDataset(\n",
+                "    wellcome_df, all_caption_embeddings, transform=test_transform\n",
+                ")\n",
+                "full_loader = DataLoader(dataset=full_dataset, batch_size=batch_size, num_workers=5)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "embeddings = []\n",
                 "\n",
                 "with torch.no_grad():\n",
                 "    loop = tqdm(full_loader)\n",
                 "    for data, target in loop:\n",
-                "        data, target, flags = (data.cuda(),\n",
-                "                               target.cuda(),\n",
-                "                               torch.ones(len(target)).cuda())\n",
+                "        data, target, flags = (\n",
+                "            data.cuda(),\n",
+                "            target.cuda(),\n",
+                "            torch.ones(len(target)).cuda(),\n",
+                "        )\n",
                 "\n",
                 "        embedding = devise_model.eval()(data)\n",
                 "        embeddings.append(embedding.cpu().data.numpy())"
             ]
         },
         {
             "cell_type": "code",
@@ -942,25 +978,27 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "path_to_id = lambda x: x.split('/')[-1].split('.')[0]"
+                "path_to_id = lambda x: x.split(\"/\")[-1].split(\".\")[0]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "np.save('/mnt/efs/models/embeddings.npy', embeddings)\n",
-                "np.save('/mnt/efs/models/image_ids.npy', wellcome_df['file_name'].apply(path_to_id).values)"
+                "np.save(\"/mnt/efs/models/embeddings.npy\", embeddings)\n",
+                "np.save(\n",
+                "    \"/mnt/efs/models/image_ids.npy\", wellcome_df[\"file_name\"].apply(path_to_id).values\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
```

### Comparing `weco-datascience-0.1.8/research_notebooks/devise/notebooks/10 - inference only demo.ipynb` & `weco-datascience-0.1.9/notebooks/devise/notebooks/10 - inference only demo.ipynb`

 * *Files 3% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9959510528349271%*

 * *Differences: {"'cells'": "{1: {'source': {insert: [(4, 'import numpy as np\\n'), (9, "*

 * *            '\'nltk.download("punkt")\\n\'), (10, \'\\n\'), (11, \'device = torch.device("cuda" if '*

 * *            'torch.cuda.is_available() else "cpu")\')], delete: [10, 8, 4]}}, 3: {\'source\': '*

 * *            "{insert: [(0, 'index_to_wordvec = "*

 * *            'np.load("/mnt/efs/models/index_to_wordvec.npy")\\n\'), (1, \'word_to_index = '*

 * *            'pickle.load(open("/mnt/efs/models/word_to_index.pkl", "rb"))\\n\'), (3, \'path_to_id '*

 * * […]*

```diff
@@ -14,21 +14,22 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "import torch\n",
                 "import pickle\n",
                 "import nmslib\n",
                 "import urllib\n",
-                "import numpy as np \n",
+                "import numpy as np\n",
                 "\n",
                 "import nltk\n",
                 "from nltk.tokenize import word_tokenize, sent_tokenize\n",
-                "nltk.download('punkt')\n",
                 "\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "nltk.download(\"punkt\")\n",
+                "\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# load data\n",
@@ -37,20 +38,19 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "index_to_wordvec = np.load('/mnt/efs/models/index_to_wordvec.npy')\n",
-                "word_to_index = pickle.load(open('/mnt/efs/models/word_to_index.pkl', 'rb'))\n",
+                "index_to_wordvec = np.load(\"/mnt/efs/models/index_to_wordvec.npy\")\n",
+                "word_to_index = pickle.load(open(\"/mnt/efs/models/word_to_index.pkl\", \"rb\"))\n",
                 "\n",
-                "path_to_id = lambda x: x.split('/')[-1].split('.')[0]\n",
-                "image_ids = np.array(list(map(path_to_id, \n",
-                "                              np.load('/mnt/efs/models/image_ids.npy'))))"
+                "path_to_id = lambda x: x.split(\"/\")[-1].split(\".\")[0]\n",
+                "image_ids = np.array(list(map(path_to_id, np.load(\"/mnt/efs/models/image_ids.npy\"))))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# load devise'd embeddings for all images\n",
@@ -59,15 +59,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "embeddings = np.load('/mnt/efs/models/embeddings.npy').reshape(-1, 4096)"
+                "embeddings = np.load(\"/mnt/efs/models/embeddings.npy\").reshape(-1, 4096)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# utils\n",
@@ -78,27 +78,27 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def sentence_to_indexes(sentence):\n",
                 "    tokenised = word_tokenize(sentence)\n",
-                "    indexes = [word_to_index[word] \n",
-                "               for word in tokenised \n",
-                "               if word in word_to_index]\n",
+                "    indexes = [word_to_index[word] for word in tokenised if word in word_to_index]\n",
                 "    return indexes\n",
                 "\n",
+                "\n",
                 "def embed(sentence):\n",
-                "    indexes = ([word_to_index['<s>']] + \n",
-                "               sentence_to_indexes(sentence) +\n",
-                "               [word_to_index['</s>']])\n",
+                "    indexes = (\n",
+                "        [word_to_index[\"<s>\"]] + sentence_to_indexes(sentence) + [word_to_index[\"</s>\"]]\n",
+                "    )\n",
                 "    wvs = np.stack([index_to_wordvec[i] for i in indexes])\n",
-                "    embedding = model(torch.Tensor([wvs]).cuda()).cpu().data.numpy()    \n",
+                "    embedding = model(torch.Tensor([wvs]).cuda()).cpu().data.numpy()\n",
                 "    return embedding.squeeze()\n",
                 "\n",
+                "\n",
                 "def embed_paragraph(paragraph):\n",
                 "    sentences = sent_tokenize(paragraph)\n",
                 "    if len(sentences) == 0:\n",
                 "        return None\n",
                 "    else:\n",
                 "        embeddings = [embed(sentence) for sentence in sentences]\n",
                 "        return np.array(embeddings).max(axis=0)"
@@ -116,36 +116,36 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "hidden_size = 2048\n",
                 "\n",
+                "\n",
                 "class SentenceEncoder(nn.Module):\n",
                 "    def __init__(self):\n",
                 "        super(SentenceEncoder, self).__init__()\n",
-                "        self.enc_lstm = nn.LSTM(input_size=300, \n",
-                "                                hidden_size=hidden_size, \n",
-                "                                num_layers=1,\n",
-                "                                bidirectional=True)\n",
-                "        \n",
+                "        self.enc_lstm = nn.LSTM(\n",
+                "            input_size=300, hidden_size=hidden_size, num_layers=1, bidirectional=True\n",
+                "        )\n",
+                "\n",
                 "    def forward(self, wv_batch):\n",
                 "        embedded, _ = self.enc_lstm(wv_batch)\n",
-                "        max_pooled = torch.max(embedded, 1)[0] \n",
+                "        max_pooled = torch.max(embedded, 1)[0]\n",
                 "        return max_pooled"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "model = SentenceEncoder().to(device)\n",
-                "model_path = '/mnt/efs/models/sentence-encoder-2018-10-08.pt'\n",
+                "model_path = \"/mnt/efs/models/sentence-encoder-2018-10-08.pt\"\n",
                 "\n",
                 "model.load_state_dict(torch.load(model_path))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -158,17 +158,17 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "index = nmslib.init(method='hnsw', space='cosinesimil')\n",
+                "index = nmslib.init(method=\"hnsw\", space=\"cosinesimil\")\n",
                 "index.addDataPointBatch(embeddings)\n",
-                "index.createIndex({'post': 2}, print_progress=True)"
+                "index.createIndex({\"post\": 2}, print_progress=True)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# search\n",
@@ -188,17 +188,17 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "results = search('mri brain scan')\n",
-                "base_url = 'https://wellcomecollection.org/works?query='\n",
-                "url_query = urllib.parse.quote_plus(' '.join(results))\n",
+                "results = search(\"mri brain scan\")\n",
+                "base_url = \"https://wellcomecollection.org/works?query=\"\n",
+                "url_query = urllib.parse.quote_plus(\" \".join(results))\n",
                 "print(base_url + url_query)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
```

### Comparing `weco-datascience-0.1.8/research_notebooks/elastic_lsh/README.md` & `weco-datascience-0.1.9/notebooks/elastic_lsh/README.md`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/research_notebooks/elastic_lsh/notebooks/01 - feature hashing.ipynb` & `weco-datascience-0.1.9/notebooks/elastic_lsh/notebooks/01 - feature hashing.ipynb`

 * *Files 3% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9972541711420388%*

 * *Differences: {"'cells'": '{2: {\'source\': [\'feature_vector_dir = "/Users/pimh/Desktop/feature_vectors/"\']}, '*

 * *            "3: {'source': {insert: [(0, 'feature_vector_ids = np.random.choice(\\n'), (1, '    "*

 * *            "os.listdir(feature_vector_dir), 10_000, replace=False\\n'), (2, ')\\n')], delete: "*

 * *            "[0]}}, 4: {'source': {insert: [(1, '    os.path.join(feature_vector_dir, id) for id "*

 * *            "in feature_vector_ids\\n')], delete: [2, 1]}}, 5: {'source': {insert: [(5, '\\n')], "*

 * *            "delete […]*

```diff
@@ -28,51 +28,52 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "feature_vector_dir = '/Users/pimh/Desktop/feature_vectors/'"
+                "feature_vector_dir = \"/Users/pimh/Desktop/feature_vectors/\""
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "feature_vector_ids = np.random.choice(os.listdir(feature_vector_dir), 10_000, replace=False)\n",
+                "feature_vector_ids = np.random.choice(\n",
+                "    os.listdir(feature_vector_dir), 10_000, replace=False\n",
+                ")\n",
                 "# feature_vector_ids = os.listdir(feature_vector_dir)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "feature_vector_paths = [\n",
-                "    os.path.join(feature_vector_dir, id) \n",
-                "    for id in feature_vector_ids\n",
+                "    os.path.join(feature_vector_dir, id) for id in feature_vector_ids\n",
                 "]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "feature_vectors = []\n",
                 "for path in feature_vector_paths:\n",
                 "    with open(path) as f:\n",
                 "        feature_vector = np.fromfile(f, dtype=np.float32)\n",
                 "        feature_vectors.append(feature_vector)\n",
-                "    \n",
+                "\n",
                 "feature_vectors = np.stack(feature_vectors)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -91,15 +92,17 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def get_image(query_id):\n",
-                "    base_url = 'https://iiif.wellcomecollection.org/image/{}.jpg/full/,300/0/default.jpg'\n",
+                "    base_url = (\n",
+                "        \"https://iiif.wellcomecollection.org/image/{}.jpg/full/,300/0/default.jpg\"\n",
+                "    )\n",
                 "    response = requests.get(base_url.format(query_id))\n",
                 "    image = Image.open(BytesIO(response.content))\n",
                 "    return image"
             ]
         },
         {
             "cell_type": "code",
@@ -132,15 +135,15 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "for i, feature_group in enumerate(tqdm(feature_groups)):\n",
                 "    clustering_alg = MeanShift(n_clusters=32).fit(feature_group)\n",
-                "    with open(f'models/kmeans_{i}.pkl', 'wb') as f:\n",
+                "    with open(f\"models/kmeans_{i}.pkl\", \"wb\") as f:\n",
                 "        pickle.dump(clustering_alg, f)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -152,39 +155,38 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "feature_vector_ids = os.listdir(feature_vector_dir)\n",
                 "\n",
                 "feature_vector_paths = [\n",
-                "    os.path.join(feature_vector_dir, id) \n",
-                "    for id in feature_vector_ids\n",
+                "    os.path.join(feature_vector_dir, id) for id in feature_vector_ids\n",
                 "]\n",
                 "\n",
                 "feature_vectors = []\n",
                 "for path in feature_vector_paths:\n",
                 "    with open(path) as f:\n",
                 "        feature_vector = np.fromfile(f, dtype=np.float32)\n",
                 "        feature_vectors.append(feature_vector)\n",
-                "    \n",
+                "\n",
                 "feature_vectors = np.stack(feature_vectors)\n",
                 "feature_vectors.shape"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "clusters = []\n",
                 "feature_groups = np.split(feature_vectors, indices_or_sections=256, axis=1)\n",
                 "\n",
                 "for i, feature_group in enumerate(tqdm(feature_groups)):\n",
-                "    with open(f'models/kmeans_{i}.pkl', 'rb') as f:\n",
+                "    with open(f\"models/kmeans_{i}.pkl\", \"rb\") as f:\n",
                 "        kmeans = pickle.load(f)\n",
                 "\n",
                 "    labels = kmeans.predict(feature_group)\n",
                 "    clusters.append(labels)"
             ]
         },
         {
@@ -206,21 +208,21 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def listify_for_es(cluster_array):\n",
-                "    return [f'{i}-{val}' for i, val in enumerate(cluster_array)]\n",
+                "    return [f\"{i}-{val}\" for i, val in enumerate(cluster_array)]\n",
                 "\n",
                 "\n",
                 "def get_es_client():\n",
-                "    username = ''\n",
-                "    password = ''\n",
-                "    url = ''\n",
+                "    username = \"\"\n",
+                "    password = \"\"\n",
+                "    url = \"\"\n",
                 "    return Elasticsearch(url, http_auth=(username, password))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -231,15 +233,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "index_name = 'image-similarity-256-32-agg'\n",
+                "index_name = \"image-similarity-256-32-agg\"\n",
                 "# es.indices.delete(index=index_name)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -251,21 +253,21 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "actions = [\n",
-                "  {\n",
-                "    \"_index\": index_name,\n",
-                "    \"_type\": \"feature_vector\",\n",
-                "    \"_id\": feature_vector_id,\n",
-                "    \"_source\": {\"feature_vector\": listify_for_es(cluster_array)\n",
-                "  }}\n",
-                "for feature_vector_id, cluster_array in tqdm(zip(feature_vector_ids, clusters))\n",
+                "    {\n",
+                "        \"_index\": index_name,\n",
+                "        \"_type\": \"feature_vector\",\n",
+                "        \"_id\": feature_vector_id,\n",
+                "        \"_source\": {\"feature_vector\": listify_for_es(cluster_array)},\n",
+                "    }\n",
+                "    for feature_vector_id, cluster_array in tqdm(zip(feature_vector_ids, clusters))\n",
                 "]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -285,39 +287,35 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def stack_images(images):\n",
                 "    return Image.fromarray(\n",
-                "        np.concatenate([\n",
-                "            np.array(image) for image in images\n",
-                "        ], axis=1)\n",
+                "        np.concatenate([np.array(image) for image in images], axis=1)\n",
                 "    )\n",
                 "\n",
+                "\n",
                 "def get_neighbour_images(query_id, index_name, n=10):\n",
                 "    res = es.search(\n",
                 "        index=index_name,\n",
                 "        size=n,\n",
                 "        body={\n",
                 "            \"query\": {\n",
                 "                \"more_like_this\": {\n",
                 "                    \"fields\": [\"feature_vector.keyword\"],\n",
-                "                    \"like\": [{\n",
-                "                        \"_index\": index_name,\n",
-                "                        \"_id\": query_id\n",
-                "                    }],\n",
+                "                    \"like\": [{\"_index\": index_name, \"_id\": query_id}],\n",
                 "                    \"min_term_freq\": 1,\n",
                 "                }\n",
                 "            }\n",
-                "        }\n",
+                "        },\n",
                 "    )\n",
                 "\n",
-                "    neighbour_ids = [hit['_id'] for hit in res['hits']['hits']]\n",
-                "    print(res['hits']['total']['value'])\n",
+                "    neighbour_ids = [hit[\"_id\"] for hit in res[\"hits\"][\"hits\"]]\n",
+                "    print(res[\"hits\"][\"total\"][\"value\"])\n",
                 "    print(neighbour_ids)\n",
                 "    neighbour_images = [get_image(id) for id in neighbour_ids]\n",
                 "    return stack_images(neighbour_images)"
             ]
         },
         {
             "cell_type": "code",
@@ -359,15 +357,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open('data/exact_nearest_neighbour.pkl', 'rb') as f:\n",
+                "with open(\"data/exact_nearest_neighbour.pkl\", \"rb\") as f:\n",
                 "    exact_nearest_neighbour_dict = pickle.load(f)\n",
                 "\n",
                 "query_ids = np.array(list(exact_nearest_neighbour_dict.keys()))"
             ]
         },
         {
             "cell_type": "code",
@@ -377,45 +375,42 @@
             "source": [
                 "def calculate_badness(preds, targets):\n",
                 "    total_badness = 0\n",
                 "    shared_hashes = list(set(preds) & set(targets))\n",
                 "    for work_id in shared_hashes:\n",
                 "        pred = np.where(preds == work_id)[0][0]\n",
                 "        target = np.where(targets == work_id)[0][0]\n",
-                "        badness = abs(pred-target) / math.log(target+2)\n",
+                "        badness = abs(pred - target) / math.log(target + 2)\n",
                 "        total_badness += badness\n",
                 "\n",
                 "    return total_badness / len(shared_hashes)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def get_neighbour_ids(query_id):\n",
                 "    res = es.search(\n",
-                "        index='image-similarity',\n",
+                "        index=\"image-similarity\",\n",
                 "        size=1000,\n",
                 "        body={\n",
                 "            \"query\": {\n",
                 "                \"more_like_this\": {\n",
                 "                    \"fields\": [\"feature_vector.keyword\"],\n",
-                "                    \"like\": [{\n",
-                "                        \"_index\": \"image-similarity-256-256\",\n",
-                "                        \"_id\": query_id\n",
-                "                    }],\n",
+                "                    \"like\": [{\"_index\": \"image-similarity-256-256\", \"_id\": query_id}],\n",
                 "                    \"min_term_freq\": 1,\n",
                 "                }\n",
                 "            }\n",
-                "        }\n",
+                "        },\n",
                 "    )\n",
                 "\n",
-                "    neighbour_ids = [hit['_id'] for hit in res['hits']['hits']]\n",
+                "    neighbour_ids = [hit[\"_id\"] for hit in res[\"hits\"][\"hits\"]]\n",
                 "    return neighbour_ids"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -424,15 +419,15 @@
                 "badnesses = {}\n",
                 "\n",
                 "for i, query_id in enumerate(query_ids):\n",
                 "    preds = np.array(get_neighbour_ids(query_id))[:100]\n",
                 "    targets = np.array(exact_nearest_neighbour_dict[query_id])[:100]\n",
                 "    badness = calculate_badness(preds, targets)\n",
                 "    badnesses[query_id] = badness\n",
-                "    print(i, '\\t', badness)"
+                "    print(i, \"\\t\", badness)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
```

### Comparing `weco-datascience-0.1.8/research_notebooks/elastic_lsh/notebooks/02 - palette hashing.ipynb` & `weco-datascience-0.1.9/notebooks/elastic_lsh/notebooks/02 - palette hashing.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9968570402298851%*

 * *Differences: {"'cells'": '{2: {\'source\': [\'feature_vector_dir = "/Users/pimh/Desktop/palette_vectors/"\']}, '*

 * *            "3: {'source': {insert: [(0, 'with open(os.path.join(feature_vector_dir, "*

 * *            '"image_ids.npy"), "rb") as f:\\n\')], delete: [0]}}, 4: {\'source\': {insert: [(0, '*

 * *            '\'with open(os.path.join(feature_vector_dir, "palette_embeddings.npy"), "rb") as '*

 * *            "f:\\n')], delete: [0]}}, 8: {'source': {insert: [(1, '    base_url = (\\n'), (2, "*

 * *            "'        "*

 * *            […]*

```diff
@@ -27,34 +27,34 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "feature_vector_dir = '/Users/pimh/Desktop/palette_vectors/'"
+                "feature_vector_dir = \"/Users/pimh/Desktop/palette_vectors/\""
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open(os.path.join(feature_vector_dir, 'image_ids.npy'), 'rb') as f:\n",
+                "with open(os.path.join(feature_vector_dir, \"image_ids.npy\"), \"rb\") as f:\n",
                 "    feature_vector_ids = np.load(f)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open(os.path.join(feature_vector_dir, 'palette_embeddings.npy'), 'rb') as f:\n",
+                "with open(os.path.join(feature_vector_dir, \"palette_embeddings.npy\"), \"rb\") as f:\n",
                 "    feature_vectors = np.load(f)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -82,15 +82,17 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def get_image(query_id):\n",
-                "    base_url = 'https://iiif.wellcomecollection.org/image/{}.jpg/full/,300/0/default.jpg'\n",
+                "    base_url = (\n",
+                "        \"https://iiif.wellcomecollection.org/image/{}.jpg/full/,300/0/default.jpg\"\n",
+                "    )\n",
                 "    response = requests.get(base_url.format(query_id))\n",
                 "    image = Image.open(BytesIO(response.content))\n",
                 "    return image"
             ]
         },
         {
             "cell_type": "code",
@@ -179,15 +181,15 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def listify_for_es(cluster_array):\n",
-                "    return [f'{i}-{val}' for i, val in enumerate(cluster_array)]"
+                "    return [f\"{i}-{val}\" for i, val in enumerate(cluster_array)]"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# send data to elasticsearch"
@@ -196,53 +198,53 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def get_es_client():\n",
-                "    username = ''\n",
-                "    password = ''\n",
-                "    url = ''\n",
+                "    username = \"\"\n",
+                "    password = \"\"\n",
+                "    url = \"\"\n",
                 "    return Elasticsearch(url, http_auth=(username, password))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "es = get_es_client()\n",
-                "es.indices.delete(index='palette-similarity')"
+                "es.indices.delete(index=\"palette-similarity\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "es.indices.create(index='palette-similarity')"
+                "es.indices.create(index=\"palette-similarity\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "actions = [\n",
-                "  {\n",
-                "    \"_index\": \"palette-similarity\",\n",
-                "    \"_type\": \"feature_vector\",\n",
-                "    \"_id\": feature_vector_id,\n",
-                "    \"_source\": {\"feature_vector\": listify_for_es(cluster_array)\n",
-                "  }}\n",
-                "for feature_vector_id, cluster_array in tqdm(zip(feature_vector_ids, clusters))\n",
+                "    {\n",
+                "        \"_index\": \"palette-similarity\",\n",
+                "        \"_type\": \"feature_vector\",\n",
+                "        \"_id\": feature_vector_id,\n",
+                "        \"_source\": {\"feature_vector\": listify_for_es(cluster_array)},\n",
+                "    }\n",
+                "    for feature_vector_id, cluster_array in tqdm(zip(feature_vector_ids, clusters))\n",
                 "]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -262,39 +264,35 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def stack_images(images):\n",
                 "    return Image.fromarray(\n",
-                "        np.concatenate([\n",
-                "            np.array(image) for image in images\n",
-                "        ], axis=1)\n",
+                "        np.concatenate([np.array(image) for image in images], axis=1)\n",
                 "    )\n",
                 "\n",
+                "\n",
                 "def get_neighbour_images(query_id, n=10):\n",
                 "    res = es.search(\n",
-                "        index='palette-similarity',\n",
+                "        index=\"palette-similarity\",\n",
                 "        size=n,\n",
                 "        body={\n",
                 "            \"query\": {\n",
                 "                \"more_like_this\": {\n",
                 "                    \"fields\": [\"feature_vector.keyword\"],\n",
-                "                    \"like\": [{\n",
-                "                        \"_index\": \"palette-similarity\",\n",
-                "                        \"_id\": query_id\n",
-                "                    }],\n",
+                "                    \"like\": [{\"_index\": \"palette-similarity\", \"_id\": query_id}],\n",
                 "                    \"min_term_freq\": 1,\n",
                 "                }\n",
                 "            }\n",
-                "        }\n",
+                "        },\n",
                 "    )\n",
                 "\n",
-                "    neighbour_ids = [hit['_id'] for hit in res['hits']['hits']]\n",
-                "    print(res['hits']['total']['value'])\n",
+                "    neighbour_ids = [hit[\"_id\"] for hit in res[\"hits\"][\"hits\"]]\n",
+                "    print(res[\"hits\"][\"total\"][\"value\"])\n",
                 "    neighbour_images = [get_image(id) for id in neighbour_ids]\n",
                 "    return stack_images(neighbour_images)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
```

### Comparing `weco-datascience-0.1.8/research_notebooks/elastic_lsh/notebooks/03 - direct cosine similarity calculation.ipynb` & `weco-datascience-0.1.9/notebooks/elastic_lsh/notebooks/03 - direct cosine similarity calculation.ipynb`

 * *Files 3% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9963800619885635%*

 * *Differences: {"'cells'": '{2: {\'source\': [\'feature_vector_dir = "/Users/pimh/Desktop/feature_vectors/"\']}, '*

 * *            "4: {'source': {insert: [(1, '    os.path.join(feature_vector_dir, id) for id in "*

 * *            "feature_vector_ids\\n')], delete: [2, 1]}}, 5: {'source': {insert: [(5, '\\n')], "*

 * *            "delete: [5]}}, 8: {'source': {insert: [(1, '    base_url = (\\n'), (2, '        "*

 * *            '"https://iiif.wellcomecollection.org/image/{}.jpg/full/,300/0/default.jpg"\\n\'), (3, '*

 * *            "'    )\\n')] […]*

```diff
@@ -29,15 +29,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "feature_vector_dir = '/Users/pimh/Desktop/feature_vectors/'"
+                "feature_vector_dir = \"/Users/pimh/Desktop/feature_vectors/\""
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -49,31 +49,30 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "feature_vector_paths = [\n",
-                "    os.path.join(feature_vector_dir, id) \n",
-                "    for id in feature_vector_ids\n",
+                "    os.path.join(feature_vector_dir, id) for id in feature_vector_ids\n",
                 "]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "feature_vectors = []\n",
                 "for path in feature_vector_paths:\n",
                 "    with open(path) as f:\n",
                 "        feature_vector = np.fromfile(f, dtype=np.float32)\n",
                 "        feature_vectors.append(feature_vector)\n",
-                "    \n",
+                "\n",
                 "feature_vectors = np.stack(feature_vectors)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -94,15 +93,17 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def get_image(query_id):\n",
-                "    base_url = 'https://iiif.wellcomecollection.org/image/{}.jpg/full/,300/0/default.jpg'\n",
+                "    base_url = (\n",
+                "        \"https://iiif.wellcomecollection.org/image/{}.jpg/full/,300/0/default.jpg\"\n",
+                "    )\n",
                 "    response = requests.get(base_url.format(query_id))\n",
                 "    image = Image.open(BytesIO(response.content))\n",
                 "    return image"
             ]
         },
         {
             "cell_type": "code",
@@ -155,81 +156,76 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def get_es_client():\n",
-                "    username = ''\n",
-                "    password = ''\n",
-                "    url = ''\n",
+                "    username = \"\"\n",
+                "    password = \"\"\n",
+                "    url = \"\"\n",
                 "    return Elasticsearch(url, http_auth=(username, password))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "es = get_es_client()\n",
-                "es.indices.delete(index='dense-vectors')"
+                "es.indices.delete(index=\"dense-vectors\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "es.indices.create(\n",
-                "    index='dense-vectors',\n",
+                "    index=\"dense-vectors\",\n",
                 "    body={\n",
                 "        \"mappings\": {\n",
-                "            \"properties\": {\n",
-                "                \"feature_vector\": { \n",
-                "                    \"type\": \"dense_vector\",\n",
-                "                    \"dims\": 256\n",
-                "                }\n",
-                "            }\n",
+                "            \"properties\": {\"feature_vector\": {\"type\": \"dense_vector\", \"dims\": 256}}\n",
                 "        }\n",
-                "    }\n",
+                "    },\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "from pprint import pprint\n",
                 "\n",
-                "pprint(es.indices.get_field_mapping(\n",
-                "    index='dense-vectors',\n",
-                "    fields=['feature_vector']\n",
-                "))"
+                "pprint(es.indices.get_field_mapping(index=\"dense-vectors\", fields=[\"feature_vector\"]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "actions = [\n",
-                "  {\n",
-                "    \"_index\": \"dense-vectors\",\n",
-                "    \"_type\": \"feature_vector\",\n",
-                "    \"_id\": feature_vector_id,\n",
-                "    \"_source\": {\n",
-                "        \"feature_vector\": feature_vector.tolist(),\n",
-                "        \"another_field\": \"some text\"\n",
-                "  }}\n",
-                "for feature_vector_id, feature_vector in tqdm(zip(feature_vector_ids, reduced_dim_feature_vectors))\n",
+                "    {\n",
+                "        \"_index\": \"dense-vectors\",\n",
+                "        \"_type\": \"feature_vector\",\n",
+                "        \"_id\": feature_vector_id,\n",
+                "        \"_source\": {\n",
+                "            \"feature_vector\": feature_vector.tolist(),\n",
+                "            \"another_field\": \"some text\",\n",
+                "        },\n",
+                "    }\n",
+                "    for feature_vector_id, feature_vector in tqdm(\n",
+                "        zip(feature_vector_ids, reduced_dim_feature_vectors)\n",
+                "    )\n",
                 "]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
```

### Comparing `weco-datascience-0.1.8/research_notebooks/elastic_lsh/notebooks/04 - finding correlated groups of features.ipynb` & `weco-datascience-0.1.9/notebooks/elastic_lsh/notebooks/04 - finding correlated groups of features.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9979331140350878%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\')], delete: [4, 3]}}, 3: {\'source\': '*

 * *            '[\'feature_vector_dir = "/Users/pimh/Desktop/feature_vectors/"\']}, 5: {\'source\': '*

 * *            "{insert: [(1, '    os.path.join(feature_vector_dir, id) for id in "*

 * *            "feature_vector_ids\\n')], delete: [2, 1]}}, 6: {'source': {insert: [(5, '\\n')], "*

 * *            'delete: [5]}}, 17: {\'sou […]*

```diff
@@ -5,16 +5,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)"
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -42,15 +43,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "feature_vector_dir = '/Users/pimh/Desktop/feature_vectors/'"
+                "feature_vector_dir = \"/Users/pimh/Desktop/feature_vectors/\""
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -62,31 +63,30 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "feature_vector_paths = [\n",
-                "    os.path.join(feature_vector_dir, id) \n",
-                "    for id in feature_vector_ids\n",
+                "    os.path.join(feature_vector_dir, id) for id in feature_vector_ids\n",
                 "]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "feature_vectors = []\n",
                 "for path in feature_vector_paths:\n",
                 "    with open(path) as f:\n",
                 "        feature_vector = np.fromfile(f, dtype=np.float32)\n",
                 "        feature_vectors.append(feature_vector)\n",
-                "    \n",
+                "\n",
                 "feature_vectors = np.stack(feature_vectors)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -179,15 +179,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open('data/column_labels.npy', 'wb') as f:\n",
+                "with open(\"data/column_labels.npy\", \"wb\") as f:\n",
                 "    np.save(f, clusters.labels_)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
```

### Comparing `weco-datascience-0.1.8/research_notebooks/elastic_lsh/notebooks/05 - feature hashing with grouped features.ipynb` & `weco-datascience-0.1.9/notebooks/elastic_lsh/notebooks/05 - feature hashing with grouped features.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.997106396627566%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\')], delete: [4, 3]}}, 3: {\'source\': '*

 * *            '[\'feature_vector_dir = "/Users/pimh/Desktop/feature_vectors/"\']}, 5: {\'source\': '*

 * *            "{insert: [(1, '    os.path.join(feature_vector_dir, id) for id in "*

 * *            "feature_vector_ids\\n')], delete: [2, 1]}}, 6: {'source': {insert: [(5, '\\n')], "*

 * *            'delete: [5]}}, 9: {\'sour […]*

```diff
@@ -5,16 +5,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)"
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -43,15 +44,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "feature_vector_dir = '/Users/pimh/Desktop/feature_vectors/'"
+                "feature_vector_dir = \"/Users/pimh/Desktop/feature_vectors/\""
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -63,31 +64,30 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "feature_vector_paths = [\n",
-                "    os.path.join(feature_vector_dir, id) \n",
-                "    for id in feature_vector_ids\n",
+                "    os.path.join(feature_vector_dir, id) for id in feature_vector_ids\n",
                 "]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "feature_vectors = []\n",
                 "for path in feature_vector_paths:\n",
                 "    with open(path) as f:\n",
                 "        feature_vector = np.fromfile(f, dtype=np.float32)\n",
                 "        feature_vectors.append(feature_vector)\n",
-                "    \n",
+                "\n",
                 "feature_vectors = np.stack(feature_vectors)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -107,15 +107,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open('data/column_labels.npy', 'rb') as f:\n",
+                "with open(\"data/column_labels.npy\", \"rb\") as f:\n",
                 "    column_labels = np.load(f)"
             ]
         },
         {
             "cell_type": "markdown",
             "execution_count": null,
             "metadata": {},
@@ -132,16 +132,16 @@
             "source": [
                 "clusters = []\n",
                 "\n",
                 "for i in tqdm(np.unique(column_labels)):\n",
                 "    feature_group = feature_vectors[:, column_labels == 1]\n",
                 "    kmeans = KMeans(n_clusters=32).fit(feature_group)\n",
                 "    clusters.append(kmeans.labels_)\n",
-                "    \n",
-                "    with open(f'models/kmeans_{i}.pkl', 'wb') as f:\n",
+                "\n",
+                "    with open(f\"models/kmeans_{i}.pkl\", \"wb\") as f:\n",
                 "        pickle.dump(kmeans, f)"
             ]
         },
         {
             "cell_type": "markdown",
             "execution_count": null,
             "metadata": {},
@@ -155,24 +155,23 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "feature_vector_ids = os.listdir(feature_vector_dir)\n",
                 "\n",
                 "feature_vector_paths = [\n",
-                "    os.path.join(feature_vector_dir, id) \n",
-                "    for id in feature_vector_ids\n",
+                "    os.path.join(feature_vector_dir, id) for id in feature_vector_ids\n",
                 "]\n",
                 "\n",
                 "feature_vectors = []\n",
                 "for path in feature_vector_paths:\n",
                 "    with open(path) as f:\n",
                 "        feature_vector = np.fromfile(f, dtype=np.float32)\n",
                 "        feature_vectors.append(feature_vector)\n",
-                "    \n",
+                "\n",
                 "feature_vectors = np.stack(feature_vectors)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -186,15 +185,15 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "clusters = []\n",
                 "\n",
                 "for i in tqdm(np.unique(column_labels)):\n",
-                "    with open(f'models/kmeans_{i}.pkl', 'rb') as f:\n",
+                "    with open(f\"models/kmeans_{i}.pkl\", \"rb\") as f:\n",
                 "        kmeans = pickle.load(f)\n",
                 "\n",
                 "    feature_group = feature_vectors[:, column_labels == 1]\n",
                 "    labels = kmeans.predict(feature_group)\n",
                 "    clusters.append(labels)"
             ]
         },
@@ -228,37 +227,37 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def listify_for_es(cluster_array):\n",
-                "    return [f'{i}-{val}' for i, val in enumerate(cluster_array)]"
+                "    return [f\"{i}-{val}\" for i, val in enumerate(cluster_array)]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def get_es_client():\n",
-                "    username = ''\n",
-                "    password = ''\n",
-                "    url = ''\n",
+                "    username = \"\"\n",
+                "    password = \"\"\n",
+                "    url = \"\"\n",
                 "    return Elasticsearch(url, http_auth=(username, password))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "index_name = 'image-similarity-256-32'\n",
+                "index_name = \"image-similarity-256-32\"\n",
                 "\n",
                 "es = get_es_client()\n",
                 "es.indices.delete(index=index_name)"
             ]
         },
         {
             "cell_type": "code",
@@ -272,21 +271,21 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "actions = [\n",
-                "  {\n",
-                "    \"_index\": index_name,\n",
-                "    \"_type\": \"feature_vector\",\n",
-                "    \"_id\": feature_vector_id,\n",
-                "    \"_source\": {\"feature_vector\": listify_for_es(cluster_array)\n",
-                "  }}\n",
-                "for feature_vector_id, cluster_array in tqdm(zip(feature_vector_ids, clusters))\n",
+                "    {\n",
+                "        \"_index\": index_name,\n",
+                "        \"_type\": \"feature_vector\",\n",
+                "        \"_id\": feature_vector_id,\n",
+                "        \"_source\": {\"feature_vector\": listify_for_es(cluster_array)},\n",
+                "    }\n",
+                "    for feature_vector_id, cluster_array in tqdm(zip(feature_vector_ids, clusters))\n",
                 "]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -307,46 +306,45 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def get_image(query_id):\n",
-                "    base_url = 'https://iiif.wellcomecollection.org/image/{}.jpg/full/,300/0/default.jpg'\n",
+                "    base_url = (\n",
+                "        \"https://iiif.wellcomecollection.org/image/{}.jpg/full/,300/0/default.jpg\"\n",
+                "    )\n",
                 "    response = requests.get(base_url.format(query_id))\n",
                 "    image = Image.open(BytesIO(response.content))\n",
                 "    return image\n",
                 "\n",
+                "\n",
                 "def stack_images(images):\n",
                 "    return Image.fromarray(\n",
-                "        np.concatenate([\n",
-                "            np.array(image) for image in images\n",
-                "        ], axis=1)\n",
+                "        np.concatenate([np.array(image) for image in images], axis=1)\n",
                 "    )\n",
                 "\n",
+                "\n",
                 "def get_neighbour_images(query_id, index_name, n=10):\n",
                 "    res = es.search(\n",
                 "        index=index_name,\n",
                 "        size=n,\n",
                 "        body={\n",
                 "            \"query\": {\n",
                 "                \"more_like_this\": {\n",
                 "                    \"fields\": [\"feature_vector.keyword\"],\n",
-                "                    \"like\": [{\n",
-                "                        \"_index\": index_name,\n",
-                "                        \"_id\": query_id\n",
-                "                    }],\n",
+                "                    \"like\": [{\"_index\": index_name, \"_id\": query_id}],\n",
                 "                    \"min_term_freq\": 1,\n",
                 "                }\n",
                 "            }\n",
-                "        }\n",
+                "        },\n",
                 "    )\n",
                 "\n",
-                "    neighbour_ids = [hit['_id'] for hit in res['hits']['hits']]\n",
-                "    print(res['hits']['total']['value'])\n",
+                "    neighbour_ids = [hit[\"_id\"] for hit in res[\"hits\"][\"hits\"]]\n",
+                "    print(res[\"hits\"][\"total\"][\"value\"])\n",
                 "    neighbour_images = [get_image(id) for id in neighbour_ids]\n",
                 "    return stack_images(neighbour_images)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
```

### Comparing `weco-datascience-0.1.8/research_notebooks/elastic_lsh/notebooks/06 - exact evaluation set.ipynb` & `weco-datascience-0.1.9/notebooks/elastic_lsh/notebooks/06 - exact evaluation set.ipynb`

 * *Files 3% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9975572127525253%*

 * *Differences: {"'cells'": '{3: {\'source\': [\'feature_vector_dir = "/Users/pimh/Desktop/feature_vectors/"\']}, '*

 * *            "5: {'source': {insert: [(1, '    os.path.join(feature_vector_dir, id) for id in "*

 * *            "feature_vector_ids\\n')], delete: [2, 1]}}, 6: {'source': {insert: [(5, '\\n')], "*

 * *            "delete: [5]}}, 9: {'source': {insert: [(1, '    base_url = (\\n'), (2, '        "*

 * *            '"https://iiif.wellcomecollection.org/image/{}.jpg/full/,300/0/default.jpg"\\n\'), (3, '*

 * *            "'    )\\n'), […]*

```diff
@@ -39,15 +39,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "feature_vector_dir = '/Users/pimh/Desktop/feature_vectors/'"
+                "feature_vector_dir = \"/Users/pimh/Desktop/feature_vectors/\""
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -59,31 +59,30 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "feature_vector_paths = [\n",
-                "    os.path.join(feature_vector_dir, id) \n",
-                "    for id in feature_vector_ids\n",
+                "    os.path.join(feature_vector_dir, id) for id in feature_vector_ids\n",
                 "]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "feature_vectors = []\n",
                 "for path in feature_vector_paths:\n",
                 "    with open(path) as f:\n",
                 "        feature_vector = np.fromfile(f, dtype=np.float32)\n",
                 "        feature_vectors.append(feature_vector)\n",
-                "    \n",
+                "\n",
                 "feature_vectors = np.stack(feature_vectors)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -104,41 +103,43 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def get_image(query_id):\n",
-                "    base_url = 'https://iiif.wellcomecollection.org/image/{}.jpg/full/,300/0/default.jpg'\n",
+                "    base_url = (\n",
+                "        \"https://iiif.wellcomecollection.org/image/{}.jpg/full/,300/0/default.jpg\"\n",
+                "    )\n",
                 "    response = requests.get(base_url.format(query_id))\n",
                 "    image = Image.open(BytesIO(response.content))\n",
                 "    return image\n",
                 "\n",
+                "\n",
                 "def stack_images(images):\n",
                 "    return Image.fromarray(\n",
-                "        np.concatenate([\n",
-                "            np.array(image) for image in images\n",
-                "        ], axis=1)\n",
+                "        np.concatenate([np.array(image) for image in images], axis=1)\n",
                 "    )\n",
                 "\n",
+                "\n",
                 "def images_from_ids(ids, n=10):\n",
                 "    neighbour_images = [get_image(id) for id in ids[:n]]\n",
                 "    return stack_images(neighbour_images)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def nearest_neighbours(query_id):\n",
                 "    query_index = np.where(np.array(feature_vector_ids) == query_id)[0][0]\n",
                 "    query = feature_vectors[query_index].reshape(1, -1)\n",
-                "    distances = cdist(query, feature_vectors, 'cosine')\n",
+                "    distances = cdist(query, feature_vectors, \"cosine\")\n",
                 "    ordered_indexes = np.argsort(distances)[0]\n",
                 "    ordered_ids = [feature_vector_ids[index] for index in ordered_indexes]\n",
                 "\n",
                 "    return ordered_ids"
             ]
         },
         {
@@ -180,15 +181,16 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "import pickle\n",
-                "with open('exact_nearest_neighbour.pkl', 'wb') as f:\n",
+                "\n",
+                "with open(\"exact_nearest_neighbour.pkl\", \"wb\") as f:\n",
                 "    pickle.dump(exact_nearest_neighbour_dict, f)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
```

### Comparing `weco-datascience-0.1.8/research_notebooks/image_pathways/Image pathways project.md` & `weco-datascience-0.1.9/notebooks/image_pathways/README.md`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/1. Preprocess_images.ipynb` & `weco-datascience-0.1.9/notebooks/image_pathways/notebooks/1. Preprocess_images.ipynb`

 * *Files 7% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9955484984872398%*

 * *Differences: {"'cells'": '{3: {\'source\': [\'bucket_name = "wellcomecollection-miro-images-public"\']}, 4: '*

 * *            '{\'source\': {insert: [(0, \'sts = boto3.client("sts")\\n\'), (2, \'    '*

 * *            'RoleArn="arn:aws:iam::760097843905:role/calm-assumable_read_role",\\n\'), (3, \'    '*

 * *            'RoleSessionName="AssumeRoleSession1",\\n\'), (5, \'credentials = '*

 * *            'assumed_role_object["Credentials"]\')], delete: [5, 3, 2, 0]}}, 5: {\'source\': '*

 * *            '{insert: [(0, \'s3_fetch = boto3.resource […]*

```diff
@@ -44,77 +44,71 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "bucket_name = 'wellcomecollection-miro-images-public'"
+                "bucket_name = \"wellcomecollection-miro-images-public\""
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "sts = boto3.client('sts')\n",
+                "sts = boto3.client(\"sts\")\n",
                 "assumed_role_object = sts.assume_role(\n",
-                "    RoleArn='arn:aws:iam::760097843905:role/calm-assumable_read_role',\n",
-                "    RoleSessionName='AssumeRoleSession1'\n",
+                "    RoleArn=\"arn:aws:iam::760097843905:role/calm-assumable_read_role\",\n",
+                "    RoleSessionName=\"AssumeRoleSession1\",\n",
                 ")\n",
-                "credentials = assumed_role_object['Credentials']"
+                "credentials = assumed_role_object[\"Credentials\"]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "s3_fetch = boto3.resource('s3',\n",
-                "    aws_access_key_id=credentials['AccessKeyId'],\n",
-                "    aws_secret_access_key=credentials['SecretAccessKey'],\n",
-                "    aws_session_token=credentials['SessionToken']\n",
+                "s3_fetch = boto3.resource(\n",
+                "    \"s3\",\n",
+                "    aws_access_key_id=credentials[\"AccessKeyId\"],\n",
+                "    aws_secret_access_key=credentials[\"SecretAccessKey\"],\n",
+                "    aws_session_token=credentials[\"SessionToken\"],\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "bucket = s3_fetch.Bucket(bucket_name)\n",
-                "bucket_info = bucket.meta.client.list_objects(\n",
-                "    Bucket=bucket.name,\n",
-                "    Delimiter='/'\n",
-                "    )"
+                "bucket_info = bucket.meta.client.list_objects(Bucket=bucket.name, Delimiter=\"/\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# Get all folder names.\n",
-                "folder_names = [\n",
-                "    f['Prefix'] for f in bucket_info.get('CommonPrefixes')\n",
-                "    ]\n",
-                "print(\"{} image folders\".format(len(folder_names))) # 219\n",
+                "folder_names = [f[\"Prefix\"] for f in bucket_info.get(\"CommonPrefixes\")]\n",
+                "print(\"{} image folders\".format(len(folder_names)))  # 219\n",
                 "\n",
                 "# Get all file dirs from all folders. Takes a minute or so\n",
                 "print(\"Getting all file dir names for all images...\")\n",
                 "file_dirs = []\n",
                 "for folder_name in tqdm(folder_names):\n",
-                "    file_dirs.extend(\n",
-                "        [s.key for s in bucket.objects.filter(Prefix=folder_name)]\n",
-                "    )\n",
-                "print(\"{} image files\".format(len(file_dirs))) # 120589"
+                "    file_dirs.extend([s.key for s in bucket.objects.filter(Prefix=folder_name)])\n",
+                "print(\"{} image files\".format(len(file_dirs)))  # 120589"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "##  2. Preprocess images"
@@ -123,23 +117,20 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def get_image(file_dir, bucket_name):\n",
-                "    \n",
-                "    obj = s3_fetch.Object(\n",
-                "        bucket_name,\n",
-                "        file_dir\n",
-                "        )\n",
-                "    im = Image.open(BytesIO(obj.get()['Body'].read()))\n",
+                "\n",
+                "    obj = s3_fetch.Object(bucket_name, file_dir)\n",
+                "    im = Image.open(BytesIO(obj.get()[\"Body\"].read()))\n",
                 "    im.thumbnail((224, 224))\n",
                 "    if im.mode != \"RGB\":\n",
-                "        im = im.convert('RGB')\n",
+                "        im = im.convert(\"RGB\")\n",
                 "\n",
                 "    return im"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
@@ -148,15 +139,15 @@
             "source": [
                 "# https://chrisalbon.com/python/data_wrangling/break_list_into_chunks_of_equal_size/\n",
                 "# Create a function called \"chunks\" with two arguments, l and n:\n",
                 "def chunks(l, n):\n",
                 "    # For item i in a range that is a length of l,\n",
                 "    for i in range(0, len(l), n):\n",
                 "        # Create an index range for l of n items:\n",
-                "        yield l[i:i+n]"
+                "        yield l[i : i + n]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -168,20 +159,26 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "filename_prefix = \"processed_images_batch_\"\n",
-                "filenames = os.listdir('../data/')\n",
+                "filenames = os.listdir(\"../data/\")\n",
                 "batch_numbers_completed = [\n",
-                "    int(os.path.splitext(filename)[0].replace(filename_prefix, \"\")) for filename in filenames if filename_prefix in filename\n",
-                "    ]\n",
-                "\n",
-                "print(\"{} batches completed out of {}\".format(len(batch_numbers_completed),round(len(file_dirs)/batch_size)))"
+                "    int(os.path.splitext(filename)[0].replace(filename_prefix, \"\"))\n",
+                "    for filename in filenames\n",
+                "    if filename_prefix in filename\n",
+                "]\n",
+                "\n",
+                "print(\n",
+                "    \"{} batches completed out of {}\".format(\n",
+                "        len(batch_numbers_completed), round(len(file_dirs) / batch_size)\n",
+                "    )\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -189,22 +186,25 @@
                 "# It takes a long time and sometimes there are errors, so I will process and save in batches\n",
                 "\n",
                 "for i, batch in tqdm(enumerate(batches)):\n",
                 "    if not i in batch_numbers_completed:\n",
                 "        print(i)\n",
                 "        try:\n",
                 "            batch_images = {\n",
-                "                os.path.splitext(\n",
-                "                    os.path.basename(file_dir)\n",
-                "                )[0]: get_image(file_dir, bucket_name) for file_dir in batch\n",
+                "                os.path.splitext(os.path.basename(file_dir))[0]: get_image(\n",
+                "                    file_dir, bucket_name\n",
+                "                )\n",
+                "                for file_dir in batch\n",
                 "            }\n",
-                "            with open('../data/processed_images_batch_{}.pkl'.format(i), 'wb') as handle:\n",
+                "            with open(\n",
+                "                \"../data/processed_images_batch_{}.pkl\".format(i), \"wb\"\n",
+                "            ) as handle:\n",
                 "                pickle.dump(batch_images, handle)\n",
                 "        except:\n",
-                "            print(\"Issue with batch {}\".format(i))\n"
+                "            print(\"Issue with batch {}\".format(i))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### (Optional) how big are the images files?"
@@ -212,16 +212,16 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "file_dir = '../data/'\n",
-                "file_name_start = 'processed_images_batch'\n",
+                "file_dir = \"../data/\"\n",
+                "file_name_start = \"processed_images_batch\"\n",
                 "\n",
                 "image_file_names = os.listdir(file_dir)\n",
                 "image_file_names = [file for file in image_file_names if file_name_start in file]"
             ]
         },
         {
             "cell_type": "code",
@@ -237,15 +237,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "plt.hist(list(sizes.values()), bins = 100)\n",
+                "plt.hist(list(sizes.values()), bins=100)\n",
                 "plt.show()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -256,15 +256,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "sum(list(sizes.values())) # 34GB"
+                "sum(list(sizes.values()))  # 34GB"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# 3. The batch pickles are too big to load and keep in memory\n",
@@ -279,33 +279,35 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "file_dir = '../data/'\n",
-                "file_name_start = 'processed_images_batch'\n",
+                "file_dir = \"../data/\"\n",
+                "file_name_start = \"processed_images_batch\"\n",
                 "\n",
                 "data_file_names = os.listdir(file_dir)\n",
-                "batch_image_file_names = [file for file in data_file_names if (file_name_start in file and '.pkl' in file)]"
+                "batch_image_file_names = [\n",
+                "    file for file in data_file_names if (file_name_start in file and \".pkl\" in file)\n",
+                "]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "for image_file_name in tqdm(image_file_names):\n",
-                "    with open(file_dir + image_file_name, 'rb') as handle:\n",
+                "    with open(file_dir + image_file_name, \"rb\") as handle:\n",
                 "        image_batch = pickle.load(handle)\n",
                 "        for image_name, image in image_batch.items():\n",
-                "            if not image_name+\".png\" in data_file_names:\n",
-                "                image.save(file_dir + image_name + \".png\") "
+                "            if not image_name + \".png\" in data_file_names:\n",
+                "                image.save(file_dir + image_name + \".png\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "## 4. Get the images for the files in the 4 batches that didn't work in step 2\n",
@@ -317,15 +319,19 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "incomplete_batch_numbers = [i for i in range(0,round(len(file_dirs)/batch_size)) if i not in batch_numbers_completed]\n",
+                "incomplete_batch_numbers = [\n",
+                "    i\n",
+                "    for i in range(0, round(len(file_dirs) / batch_size))\n",
+                "    if i not in batch_numbers_completed\n",
+                "]\n",
                 "incomplete_batch_numbers"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -354,17 +360,15 @@
             "outputs": [],
             "source": [
                 "bad_images = []\n",
                 "good_images = []\n",
                 "for image_dir in tqdm(bad_batch_image_file_names):\n",
                 "    try:\n",
                 "        image = get_image(image_dir, bucket_name)\n",
-                "        image_name = os.path.splitext(\n",
-                "            os.path.basename(image_dir)\n",
-                "            )[0]\n",
+                "        image_name = os.path.splitext(os.path.basename(image_dir))[0]\n",
                 "        image.save(file_dir + image_name + \".png\")\n",
                 "        good_images.append(image_dir)\n",
                 "    except:\n",
                 "        bad_images.append(image_dir)"
             ]
         },
         {
```

### Comparing `weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/2. Get_feature_vectors.ipynb` & `weco-datascience-0.1.9/notebooks/image_pathways/notebooks/2. Get_feature_vectors.ipynb`

 * *Files 3% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9973726003934338%*

 * *Differences: {"'cells'": '{4: {\'source\': {insert: [(1, \'images_dir = "../data/"\\n\'), (2, \'image_type = '*

 * *            '".png"\\n\')], delete: [2, 1]}}, 5: {\'source\': {insert: [(3, \'s3 = '*

 * *            'boto3.resource("s3")\\n\'), (4, \'my_bucket = '*

 * *            's3.Bucket("miro-images-feature-vectors")\\n\'), (7, \'    '*

 * *            "os.path.basename(file.key)\\n'), (8, '    for file in "*

 * *            "my_bucket.objects.filter(Prefix=feat_vect_file_dir)\\n'), (9, ']')], delete: [8, 7, "*

 * *            "4, 3]}}, 11: { […]*

```diff
@@ -63,16 +63,16 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# Get all the png image names from the data folder\n",
-                "images_dir = '../data/'\n",
-                "image_type = '.png'\n",
+                "images_dir = \"../data/\"\n",
+                "image_type = \".png\"\n",
                 "\n",
                 "image_names = os.listdir(images_dir)\n",
                 "image_names = [os.path.splitext(file)[0] for file in image_names if image_type in file]\n",
                 "len(image_names)"
             ]
         },
         {
@@ -80,20 +80,21 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# Remove the image files which already have feature vectors found\n",
                 "feat_vect_file_dir = \"feature_vectors\"\n",
                 "\n",
-                "s3 = boto3.resource('s3')\n",
-                "my_bucket = s3.Bucket('miro-images-feature-vectors')\n",
+                "s3 = boto3.resource(\"s3\")\n",
+                "my_bucket = s3.Bucket(\"miro-images-feature-vectors\")\n",
                 "\n",
                 "images_run = [\n",
-                "    os.path.basename(file.key) for file in my_bucket.objects.filter(Prefix=feat_vect_file_dir)\n",
-                "    ]"
+                "    os.path.basename(file.key)\n",
+                "    for file in my_bucket.objects.filter(Prefix=feat_vect_file_dir)\n",
+                "]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -141,56 +142,56 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class imagesDataset(Dataset):\n",
                 "    def __init__(\n",
-                "            self, image_names, images_dir, image_type,\n",
-                "            transforms=transforms.ToTensor()\n",
-                "        ):\n",
-                "        \n",
+                "        self, image_names, images_dir, image_type, transforms=transforms.ToTensor()\n",
+                "    ):\n",
+                "\n",
                 "        self.transforms = transforms\n",
                 "        self.image_names = image_names\n",
                 "        self.images_dir = images_dir\n",
                 "        self.image_type = image_type\n",
-                "        self.index_to_id = {\n",
-                "            index: id for index, id in enumerate(self.image_names) \n",
-                "        }\n",
-                "    \n",
+                "        self.index_to_id = {index: id for index, id in enumerate(self.image_names)}\n",
+                "\n",
                 "    def __len__(self):\n",
                 "        return len(self.image_names)\n",
-                "    \n",
+                "\n",
                 "    def __getitem__(self, index):\n",
                 "        image_id = self.index_to_id[index]\n",
-                "        \n",
-                "        im = Image.open(\n",
-                "            self.images_dir + image_id + self.image_type\n",
-                "            )\n",
-                "        \n",
+                "\n",
+                "        im = Image.open(self.images_dir + image_id + self.image_type)\n",
+                "\n",
                 "        img = self.transforms(im)\n",
-                "        \n",
+                "\n",
                 "        image_name = image_id\n",
-                "                                      \n",
-                "        return image_name, img\n",
-                "    "
+                "\n",
+                "        return image_name, img"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# Create all the images transforms\n",
-                "min_img_size = 224, 224  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
-                "transform_pipeline = transforms.Compose([transforms.Resize(min_img_size),\n",
-                "                                         transforms.ToTensor(),\n",
-                "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
-                "                                                              std=[0.229, 0.224, 0.225])])\n",
+                "min_img_size = (\n",
+                "    224,\n",
+                "    224,\n",
+                ")  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
+                "transform_pipeline = transforms.Compose(\n",
+                "    [\n",
+                "        transforms.Resize(min_img_size),\n",
+                "        transforms.ToTensor(),\n",
+                "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
+                "    ]\n",
+                ")\n",
                 "\n",
                 "# Remove the last layer from the model, so that the output will be a feature vector\n",
                 "vgg16_short = vgg16\n",
                 "vgg16_short.classifier = vgg16.classifier[:4]"
             ]
         },
         {
@@ -200,18 +201,18 @@
             "outputs": [],
             "source": [
                 "our_dataloader = DataLoader(\n",
                 "    dataset=imagesDataset(\n",
                 "        image_names=image_names,\n",
                 "        images_dir=images_dir,\n",
                 "        image_type=image_type,\n",
-                "        transforms=transform_pipeline\n",
+                "        transforms=transform_pipeline,\n",
                 "    ),\n",
                 "    batch_size=32,\n",
-                "    shuffle=True\n",
+                "    shuffle=True,\n",
                 ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -220,25 +221,24 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "s3 = boto3.client('s3')\n",
+                "s3 = boto3.client(\"s3\")\n",
                 "for image_names, images in tqdm(our_dataloader):\n",
                 "    images = images.to(device)\n",
                 "    feature_vectors = vgg16_short(images)\n",
-                "    feature_vectors = feature_vectors.to('cpu')\n",
+                "    feature_vectors = feature_vectors.to(\"cpu\")\n",
                 "    for image_name, feature_vector in zip(image_names, feature_vectors):\n",
                 "        s3.put_object(\n",
                 "            Bucket=\"miro-images-feature-vectors\",\n",
                 "            Key=\"feature_vectors/\" + image_name,\n",
-                "            Body=feature_vector.detach().numpy().tobytes()\n",
-                "            \n",
+                "            Body=feature_vector.detach().numpy().tobytes(),\n",
                 "        )"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -252,31 +252,31 @@
             "outputs": [],
             "source": [
                 "our_dataloader_test = DataLoader(\n",
                 "    dataset=imagesDataset(\n",
                 "        image_names=images_run[0:2],\n",
                 "        images_dir=images_dir,\n",
                 "        image_type=image_type,\n",
-                "        transforms=transform_pipeline\n",
+                "        transforms=transform_pipeline,\n",
                 "    ),\n",
                 "    batch_size=32,\n",
-                "    shuffle=True\n",
+                "    shuffle=True,\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "for image_names, images in tqdm(our_dataloader_test):\n",
                 "    images = images.to(device)\n",
                 "    feature_vectors = vgg16_short(images)\n",
-                "    feature_vectors = feature_vectors.to('cpu')\n",
+                "    feature_vectors = feature_vectors.to(\"cpu\")\n",
                 "    for image_name, feature_vector in zip(image_names, feature_vectors):\n",
                 "        print(image_name)\n",
                 "        print(feature_vector[0:10])\n",
                 "        print(feature_vector.detach().numpy().tobytes()[0:100])"
             ]
         },
         {
@@ -301,41 +301,41 @@
             "outputs": [],
             "source": [
                 "# https://alexwlchan.net/2017/07/listing-s3-keys/\n",
                 "def get_all_s3_keys(bucket):\n",
                 "    \"\"\"Get a list of all keys in an S3 bucket.\"\"\"\n",
                 "    keys = []\n",
                 "\n",
-                "    kwargs = {'Bucket': bucket}\n",
+                "    kwargs = {\"Bucket\": bucket}\n",
                 "    while True:\n",
                 "        resp = s3.list_objects_v2(**kwargs)\n",
-                "        for obj in resp['Contents']:\n",
-                "            keys.append(obj['Key'])\n",
+                "        for obj in resp[\"Contents\"]:\n",
+                "            keys.append(obj[\"Key\"])\n",
                 "\n",
                 "        try:\n",
-                "            kwargs['ContinuationToken'] = resp['NextContinuationToken']\n",
+                "            kwargs[\"ContinuationToken\"] = resp[\"NextContinuationToken\"]\n",
                 "        except KeyError:\n",
                 "            break\n",
                 "\n",
                 "    return keys"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "s3 = boto3.client('s3')\n",
-                "bucket_name = 'miro-images-feature-vectors'\n",
+                "s3 = boto3.client(\"s3\")\n",
+                "bucket_name = \"miro-images-feature-vectors\"\n",
                 "\n",
                 "keys = get_all_s3_keys(bucket_name)\n",
                 "\n",
                 "folder_name = \"feature_vectors\"\n",
-                "keys = [k for k in keys if k.split(\"/\")[0]==folder_name]\n",
+                "keys = [k for k in keys if k.split(\"/\")[0] == folder_name]\n",
                 "\n",
                 "len(keys)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
@@ -358,23 +358,18 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "feature_vectors = {}\n",
                 "for key in tqdm(keys):\n",
-                "    obj = s3.get_object(\n",
-                "            Bucket=bucket_name,\n",
-                "            Key=key\n",
-                "        )\n",
-                "    read_obj = obj['Body'].read()\n",
-                "\n",
-                "    feature_vectors[key] = np.frombuffer(\n",
-                "                    read_obj, dtype=np.float\n",
-                "                )"
+                "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
+                "    read_obj = obj[\"Body\"].read()\n",
+                "\n",
+                "    feature_vectors[key] = np.frombuffer(read_obj, dtype=np.float)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -414,44 +409,44 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "scaler = StandardScaler()\n",
                 "data_rescaled = scaler.fit_transform(feature_vectors_list)\n",
                 "corner_x = 100\n",
                 "\n",
-                "#Fitting the PCA algorithm with our Data\n",
-                "pca = PCA().fit(data_rescaled) # (n_samples, n_features)\n",
+                "# Fitting the PCA algorithm with our Data\n",
+                "pca = PCA().fit(data_rescaled)  # (n_samples, n_features)\n",
                 "variance_vals = np.cumsum(pca.explained_variance_ratio_)\n",
-                "#Plotting the Cumulative Summation of the Explained Variance\n",
+                "# Plotting the Cumulative Summation of the Explained Variance\n",
                 "plt.figure()\n",
-                "plt.plot([0,1000], [1,1], 'r--')\n",
+                "plt.plot([0, 1000], [1, 1], \"r--\")\n",
                 "plt.plot(variance_vals)\n",
-                "plt.plot(corner_x, variance_vals[corner_x],'x')\n",
-                "plt.xlabel('Number of Components')\n",
-                "plt.ylabel('Variance (%)') #for each component\n",
-                "plt.title('Feature Vectors Explained Variance')\n",
-                "plt.xlim(0,1000)\n",
+                "plt.plot(corner_x, variance_vals[corner_x], \"x\")\n",
+                "plt.xlabel(\"Number of Components\")\n",
+                "plt.ylabel(\"Variance (%)\")  # for each component\n",
+                "plt.title(\"Feature Vectors Explained Variance\")\n",
+                "plt.xlim(0, 1000)\n",
                 "plt.show()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "fig= plt.figure()\n",
-                "plt.plot([0,1000], [1,1], 'r--')\n",
+                "fig = plt.figure()\n",
+                "plt.plot([0, 1000], [1, 1], \"r--\")\n",
                 "plt.plot(variance_vals)\n",
-                "plt.plot(corner_x, variance_vals[corner_x],'x')\n",
-                "plt.xlabel('Number of Components')\n",
-                "plt.ylabel('Variance (%)') #for each component\n",
-                "plt.title('Feature Vectors Explained Variance')\n",
-                "plt.xlim(0,1000)\n",
-                "plt.savefig('../feat_vec_var.png')\n",
+                "plt.plot(corner_x, variance_vals[corner_x], \"x\")\n",
+                "plt.xlabel(\"Number of Components\")\n",
+                "plt.ylabel(\"Variance (%)\")  # for each component\n",
+                "plt.title(\"Feature Vectors Explained Variance\")\n",
+                "plt.xlim(0, 1000)\n",
+                "plt.savefig(\"../feat_vec_var.png\")\n",
                 "plt.close(fig)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -471,21 +466,21 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "s3 = boto3.client('s3')\n",
+                "s3 = boto3.client(\"s3\")\n",
                 "for i, transformed_data in tqdm(enumerate(transformed_feature_vectors)):\n",
                 "    image_name = os.path.basename(feature_vectors_names[i])\n",
                 "    s3.put_object(\n",
                 "        Bucket=\"miro-images-feature-vectors\",\n",
                 "        Key=\"reduced_feature_vectors_{}_dims/{}\".format(corner_x, image_name),\n",
-                "        Body=bytes(transformed_data)\n",
+                "        Body=bytes(transformed_data),\n",
                 "    )"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
```

### Comparing `weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/3. Graph_pathways.ipynb` & `weco-datascience-0.1.9/notebooks/image_pathways/notebooks/3. Graph_pathways.ipynb`

 * *Files 4% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.997777469644467%*

 * *Differences: {"'cells'": "{2: {'source': {insert: [(3, 'import ast\\n')], delete: [3]}}, 4: {'source': {insert: "*

 * *            "[(17, '    reorder_images,\\n'), (18, ')')], delete: [18, 17]}}, 5: {'source': "*

 * *            '{insert: [(1, \'images_dir = "data/"\\n\'), (2, \'image_type = ".png"\\n\')], delete: '*

 * *            "[2, 1]}}, 7: {'source': {insert: [(1, 'np.random.seed(0)  # For dev\\n'), (2, "*

 * *            "'image_name_list = np.random.choice(image_names, n_sample, replace=False)')], delete: "*

 * *            "[2, 1]}}, […]*

```diff
@@ -26,15 +26,15 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "from tqdm import tqdm\n",
                 "import os\n",
                 "from io import BytesIO\n",
-                "import ast \n",
+                "import ast\n",
                 "import numpy as np\n",
                 "import pickle\n",
                 "\n",
                 "from PIL import Image\n",
                 "import torch\n",
                 "import boto3\n",
                 "from scipy.spatial.distance import cdist\n",
@@ -73,27 +73,27 @@
                 "    plot_graph,\n",
                 "    create_network_graph,\n",
                 "    visualise_clusters,\n",
                 "    reduce_data,\n",
                 "    get_random_node_path,\n",
                 "    image_pathway_plot,\n",
                 "    visualize_scatter_with_images,\n",
-                "    reorder_images\n",
-                "    )"
+                "    reorder_images,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# Get all the png image names from the data folder\n",
-                "images_dir = 'data/'\n",
-                "image_type = '.png'\n",
+                "images_dir = \"data/\"\n",
+                "image_type = \".png\"\n",
                 "\n",
                 "image_names = os.listdir(images_dir)\n",
                 "image_names = [os.path.splitext(file)[0] for file in image_names if image_type in file]\n",
                 "len(image_names)"
             ]
         },
         {
@@ -107,16 +107,16 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "n_sample = 2000\n",
-                "np.random.seed(0) # For dev\n",
-                "image_name_list = np.random.choice(image_names, n_sample, replace = False)"
+                "np.random.seed(0)  # For dev\n",
+                "image_name_list = np.random.choice(image_names, n_sample, replace=False)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "## Import feature vectors as they are (>4000 dimensions)"
@@ -124,28 +124,30 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "bucket_name = 'miro-images-feature-vectors'\n",
+                "bucket_name = \"miro-images-feature-vectors\"\n",
                 "folder_name = \"feature_vectors\"\n",
                 "\n",
                 "bucket_name = bucket_name\n",
-                "s3 = boto3.client('s3')"
+                "s3 = boto3.client(\"s3\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "feature_vectors, _ = import_feature_vectors(s3, bucket_name, folder_name, image_name_list)\n",
+                "feature_vectors, _ = import_feature_vectors(\n",
+                "    s3, bucket_name, folder_name, image_name_list\n",
+                ")\n",
                 "\n",
                 "# Remove the name of this image from the list if no feature vector was found for it\n",
                 "image_name_list = [x for x in image_name_list if x in list(feature_vectors.keys())]"
             ]
         },
         {
             "cell_type": "code",
@@ -177,29 +179,23 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "percentile_var = 0.2\n",
                 "p = np.percentile(dist_mat, percentile_var)\n",
                 "print(\n",
-                "    \"{}% of the data (number of neighbours\".format(\n",
-                "        percentile_var\n",
-                "        ),\n",
-                "    \"for each node will be about {})\".format(\n",
-                "        len(dist_mat)*(percentile_var/100)\n",
-                "        ),\n",
-                "    \"has a cosine distance below {}\".format(\n",
-                "        round(p,2)\n",
-                "        )\n",
-                "     )\n",
-                "fig = plt.figure(figsize=(10,5))\n",
-                "plt.hist(dist_mat.flatten(), bins = 30)\n",
-                "plt.xlabel('Cosine distance')\n",
-                "plt.ylabel('Frequency')\n",
-                "plt.savefig('cosine_dists.png')\n",
+                "    \"{}% of the data (number of neighbours\".format(percentile_var),\n",
+                "    \"for each node will be about {})\".format(len(dist_mat) * (percentile_var / 100)),\n",
+                "    \"has a cosine distance below {}\".format(round(p, 2)),\n",
+                ")\n",
+                "fig = plt.figure(figsize=(10, 5))\n",
+                "plt.hist(dist_mat.flatten(), bins=30)\n",
+                "plt.xlabel(\"Cosine distance\")\n",
+                "plt.ylabel(\"Frequency\")\n",
+                "plt.savefig(\"cosine_dists.png\")\n",
                 "plt.close(fig)\n",
                 "plt.show()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
@@ -284,27 +280,27 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "figsize = (5,5)\n",
-                "_ = plot_graph(G_top, figsize=figsize) \n",
+                "figsize = (5, 5)\n",
+                "_ = plot_graph(G_top, figsize=figsize)\n",
                 "_ = plot_graph(G_threshold, figsize=figsize)\n",
                 "_ = plot_graph(G_top_threshold, figsize=figsize)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_names_dict = {k:v for k,v in enumerate(image_name_list)}"
+                "image_names_dict = {k: v for k, v in enumerate(image_name_list)}"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -359,18 +355,18 @@
             "outputs": [],
             "source": [
                 "visualize_scatter_with_images(\n",
                 "    x_data,\n",
                 "    image_name_list=image_name_list,\n",
                 "    images_dir=images_dir,\n",
                 "    image_type=image_type,\n",
-                "    figsize=(20,20),\n",
+                "    figsize=(20, 20),\n",
                 "    image_zoom=0.15,\n",
-                "    pathway=node_path\n",
-                "    )\n"
+                "    pathway=node_path,\n",
+                ")"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
             "display_name": "Environment (conda_pytorch_p36)",
             "language": "python",
```

### Comparing `weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/4. Graph_pathways_experiments.ipynb` & `weco-datascience-0.1.9/notebooks/wikipedia/notebooks/pretrained bert.ipynb`

 * *Files 22% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9438957975882518%*

 * *Differences: {"'cells'": "{0: {'source': {insert: [(0, '%matplotlib inline\\n'), (1, 'import matplotlib.pyplot "*

 * *            "as plt\\n'), (2, 'import seaborn as sns\\n'), (3, '\\n'), (4, "*

 * *            '\'sns.set_style("whitegrid")\\n\'), (5, \'plt.rcParams["figure.figsize"] = (20, '*

 * *            "20)\\n'), (6, '\\n'), (8, 'import re\\n'), (10, 'import pandas as pd\\n'), (11, 'from "*

 * *            "bs4 import BeautifulSoup\\n'), (12, 'from tqdm import tqdm_notebook as tqdm\\n'), "*

 * *            "(13, 'from IPython.core.displ […]*

```diff
@@ -1,765 +1,696 @@
 {
     "cells": [
         {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "In this notebook we:\n",
-                "- Download the feature vectors/reduced dim feature vectors from S3 (7 options)\n",
-                "- Get the distance matrices for each of these\n",
-                "- Pick the nodes you are going to go between in the network\n",
-                "- Build the graphs using 3 types of neighbour definitions (top neighbours, or neighbours close defined by a threshold, a mixture of these or a fully connected graph)\n",
-                "- Run different pathways (dijkstra path, the a* path or my defined path) using these graphs\n",
-                "\n",
-                "The outcome of this notebook points to using the __raw feature vectors, and with a network where each node is connected to its top 3 neighbours__."
-            ]
-        },
-        {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "%load_ext autoreload\n",
-                "%autoreload 2"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": null,
-            "metadata": {},
-            "outputs": [],
-            "source": [
-                "from tqdm import tqdm\n",
+                "%matplotlib inline\n",
+                "import matplotlib.pyplot as plt\n",
+                "import seaborn as sns\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
+                "\n",
                 "import os\n",
-                "from io import BytesIO\n",
-                "import ast \n",
+                "import re\n",
                 "import numpy as np\n",
-                "import pickle\n",
-                "from itertools import compress\n",
-                "from collections import Counter\n",
-                "import operator\n",
-                "from functools import partial\n",
+                "import pandas as pd\n",
+                "from bs4 import BeautifulSoup\n",
+                "from tqdm import tqdm_notebook as tqdm\n",
+                "from IPython.core.display import display, HTML\n",
                 "\n",
-                "from PIL import Image\n",
                 "import torch\n",
-                "import boto3\n",
-                "from scipy.spatial.distance import cdist\n",
-                "import networkx as nx\n",
-                "import matplotlib.pyplot as plt\n",
-                "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
-                "from itertools import combinations\n",
-                "import umap.umap_ as umap"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": null,
-            "metadata": {},
-            "outputs": [],
-            "source": [
-                "cd .."
+                "from torch import nn, optim\n",
+                "from torch.nn.utils.rnn import pad_sequence\n",
+                "from torch.utils.data import Dataset, DataLoader\n",
+                "from sklearn.model_selection import train_test_split\n",
+                "\n",
+                "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForTokenClassification\n",
+                "\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
+                "torch.cuda.empty_cache()"
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": null,
+            "cell_type": "markdown",
             "metadata": {},
-            "outputs": [],
             "source": [
-                "from src.network_functions import (\n",
-                "    import_feature_vectors,\n",
-                "    get_all_s3_keys,\n",
-                "    get_distances,\n",
-                "    image_pathway_plot,\n",
-                "    get_top_neighbours,\n",
-                "    get_high_neighbours,\n",
-                "    get_top_high_neighbours,\n",
-                "    create_network_graph,\n",
-                "    plot_graph,\n",
-                "    defined_path\n",
-                "    )"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": null,
-            "metadata": {},
-            "outputs": [],
-            "source": [
-                "images_dir = 'data/'\n",
-                "image_type = '.png'"
+                "so, we know that bert works and will produce some beautiful embeddings which we can fine tune. now we need to put together the training data for the embeddings, using the bert tokeniser\n",
+                "\n",
+                "# data"
             ]
         },
         {
-            "cell_type": "markdown",
+            "cell_type": "raw",
             "metadata": {},
             "source": [
-                "### 1. Get the names of the ~5000 feature vectors which I found different dimensionality reductions\n",
+                "base_path = '/mnt/efs/wikipedia/dumps/text/'\n",
+                "paths = np.random.choice(os.listdir(base_path), size=10)\n",
+                "\n",
+                "all_text = ''\n",
+                "for path in paths:\n",
+                "    filenames = os.listdir(base_path + path)\n",
+                "    for filename in tqdm(filenames):\n",
+                "        with open(base_path + path + '/' + filename, 'rb') as f:\n",
+                "            all_text += f.read().decode('latin1')\n",
+                "\n",
+                "pattern = r'(?:<doc.+>)((.|\\s|\\S)*?)(?:<\\/doc>)'\n",
+                "articles = [\n",
+                "    '\\n\\n'.join(article[0].split('\\n\\n')[1:])\n",
+                "    for article in re.findall(pattern, all_text)\n",
+                "]\n",
                 "\n",
-                "Pick a sample if you want to make it quicker"
+                "#articles = np.random.choice(articles, size=1000)"
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": null,
+            "cell_type": "raw",
             "metadata": {},
-            "outputs": [],
             "source": [
-                "bucket_name = 'miro-images-feature-vectors'\n",
-                "bucket_name = bucket_name\n",
-                "s3 = boto3.client('s3')"
+                "len(articles)"
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": null,
+            "cell_type": "markdown",
             "metadata": {},
-            "outputs": [],
             "source": [
-                "keys = get_all_s3_keys(bucket_name, s3)"
+                "### cleaning pipeline"
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": null,
+            "cell_type": "raw",
             "metadata": {},
-            "outputs": [],
             "source": [
-                "folder_name = \"reduced_feature_vectors_100_dims\"\n",
+                "def label_linkable_tokens(sentence, tokenizer, label_all=True):\n",
+                "    parsed_html = BeautifulSoup(sentence, 'html.parser')\n",
+                "\n",
+                "    link_text = [link.text for link in parsed_html.find_all('a')]\n",
+                "    tokenised_links = [tokenizer.tokenize(link) for link in link_text]\n",
+                "    tokenised_text = tokenizer.tokenize(parsed_html.text)\n",
+                "    target_sequence = np.zeros(len(tokenised_text))\n",
+                "\n",
+                "    for link in tokenised_links:\n",
+                "        start_positions = kmp(tokenised_text, link)\n",
+                "        if label_all:            \n",
+                "            for pos in start_positions:\n",
+                "                target_sequence[pos : pos + len(link)] = 1\n",
+                "        elif label_all == False and len(start_positions) > 0:\n",
+                "            pos = start_positions[0]\n",
+                "            target_sequence[pos : pos + len(link)] = 1\n",
+                "        else: \n",
+                "            pass\n",
+                "\n",
+                "    token_sequence = tokenizer.convert_tokens_to_ids(tokenised_text)\n",
+                "    return token_sequence, target_sequence\n",
+                "\n",
+                "\n",
+                "def kmp(sequence, sub):\n",
+                "    \"\"\"         \n",
+                "    Knuth\u2013Morris\u2013Pratt algorithm, returning the starting position\n",
+                "    of a specified subsequence within another, larger sequence.\n",
+                "    Usually used for string matching.\n",
+                "    \"\"\"\n",
+                "    partial = [0]\n",
+                "    for i in range(1, len(sub)):\n",
+                "        j = partial[i - 1]\n",
+                "        while j > 0 and sub[j] != sub[i]:\n",
+                "            j = partial[j - 1]\n",
+                "        partial.append(j + 1 if sub[j] == sub[i] else j)\n",
                 "\n",
-                "image_names = [os.path.split(k)[1] for k in keys if folder_name in k]"
+                "    positions, j = [], 0\n",
+                "    for i in range(len(sequence)):\n",
+                "        while j > 0 and sequence[i] != sub[j]:\n",
+                "            j = partial[j - 1]\n",
+                "        if sequence[i] == sub[j]: j += 1\n",
+                "        if j == len(sub): \n",
+                "            positions.append(i - (j - 1))\n",
+                "            j = 0\n",
+                "\n",
+                "    return positions"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "n_sample = 1000\n",
-                "np.random.seed(0) # For dev\n",
-                "image_names = np.random.choice(image_names, n_sample, replace = False)"
+                "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", do_lower_case=False)"
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": null,
+            "cell_type": "raw",
             "metadata": {},
-            "outputs": [],
             "source": [
-                "len(image_names)"
+                "token_sequences, target_sequences = [], []\n",
+                "\n",
+                "for i, article in enumerate(tqdm(articles)):\n",
+                "    if i % 1000 == 0: print(i)\n",
+                "    try:\n",
+                "        tokenized_sequence, target_sequence = label_linkable_tokens(article, tokenizer)        \n",
+                "        token_sequences.append(tokenized_sequence)\n",
+                "        target_sequences.append(target_sequence)\n",
+                "    except:\n",
+                "        pass"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### 2. Download the feature vectors/reduced dim feature vectors from S3"
+                "# save and load"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "feature_vectors, _ = import_feature_vectors(s3, bucket_name, \"feature_vectors\", image_names)\n",
-                "feature_vectors_2_dims, _ = import_feature_vectors(s3, bucket_name, \"reduced_feature_vectors_2_dims\", image_names)\n",
-                "feature_vectors_20_dims, _ = import_feature_vectors(s3, bucket_name, \"reduced_feature_vectors_20_dims\", image_names)\n",
-                "feature_vectors_80_dims, _ = import_feature_vectors(s3, bucket_name, \"reduced_feature_vectors_80_dims\", image_names)\n",
-                "feature_vectors_100_dims, _ = import_feature_vectors(s3, bucket_name, \"reduced_feature_vectors_100_dims\", image_names)\n",
-                "feature_vectors_500_dims, _ = import_feature_vectors(s3, bucket_name, \"reduced_feature_vectors_500_dims\", image_names)\n",
-                "feature_vectors_1000_dims, _ = import_feature_vectors(s3, bucket_name, \"reduced_feature_vectors_1000_dims\", image_names)"
+                "import pickle"
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": null,
+            "cell_type": "raw",
             "metadata": {},
-            "outputs": [],
             "source": [
-                "# Remove the name of this image from the list if no feature vector data was found for it\n",
-                "image_names = [x for x in image_names if x in list(feature_vectors.keys())]\n",
-                "image_names = [x for x in image_names if x in list(feature_vectors_100_dims.keys())]\n",
-                "len(image_names)"
+                "pickle.dump(token_sequences, open('/mnt/efs/wikipedia/token_sequences.pkl', 'wb'))\n",
+                "pickle.dump(target_sequences, open('/mnt/efs/wikipedia/target_sequences.pkl', 'wb'))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_names_dict = {k:v for k,v in enumerate(image_names)}"
+                "token_sequences = pickle.load(open(\"/mnt/efs/wikipedia/token_sequences.pkl\", \"rb\"))\n",
+                "target_sequences = pickle.load(open(\"/mnt/efs/wikipedia/target_sequences.pkl\", \"rb\"))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### 3. Get the distance matrices"
+                "# cont"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "dist_mat_fv = get_distances(feature_vectors)\n",
-                "dist_mat_fv2 = get_distances(feature_vectors_2_dims)\n",
-                "dist_mat_fv20 = get_distances(feature_vectors_20_dims)\n",
-                "dist_mat_fv80 = get_distances(feature_vectors_80_dims)\n",
-                "dist_mat_fv100 = get_distances(feature_vectors_100_dims)\n",
-                "dist_mat_fv500 = get_distances(feature_vectors_500_dims)\n",
-                "dist_mat_fv1000 = get_distances(feature_vectors_1000_dims)"
+                "i = np.random.randint(len(token_sequences))\n",
+                "output_html = \"\"\n",
+                "\n",
+                "tokens = tokenizer.convert_ids_to_tokens(token_sequences[i])\n",
+                "targets = target_sequences[i]\n",
+                "for token, target in zip(tokens, targets):\n",
+                "    if target == 1:\n",
+                "        output_html += f\"<b>{token}</b> \"\n",
+                "    else:\n",
+                "        output_html += token + \" \"\n",
+                "\n",
+                "display(HTML(output_html))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### 4. To save running time, build the graphs, then mess with pathway algos.\n",
-                "\n",
-                "I build four types of graphs using the parameters (when applicable):\n",
-                "- number_neighbours = 3\n",
-                "- dist_threshold = 0.35\n",
-                "\n",
-                "Types of graphs:\n",
-                "1. Using the top n neighbours : each node is connected to its n closest neighbours\n",
-                "2. Using all connections < threshold distance : each node is connected to all it's closest neighbours, defined by a threshold\n",
-                "3. Using all connections < threshold distance or top n : each node is connected to all it's closest neighbours, defined by a threshold, and if there are no 'close' neighbours, then the top n\n",
-                "4. Fully connected graph : every node is connected to each other\n"
+                "# dataset and dataloader"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "def run_graph(dist_mat, neighbour_function,\n",
-                "              number_neighbours=None, dist_threshold=None):\n",
-                "    \n",
-                "    if neighbour_function == get_top_neighbours:\n",
-                "        dist_mat_neighbours = neighbour_function(dist_mat=dist_mat, n=number_neighbours)\n",
-                "    elif neighbour_function == get_high_neighbours:\n",
-                "        dist_mat_neighbours = neighbour_function(dist_mat=dist_mat, dist_threshold=dist_threshold)\n",
-                "    elif neighbour_function == get_top_high_neighbours:\n",
-                "        dist_mat_neighbours = neighbour_function(dist_mat=dist_mat, n=number_neighbours, dist_threshold=dist_threshold)\n",
+                "class SequenceDataset(Dataset):\n",
+                "    def __init__(self, token_sequences, target_sequences):\n",
+                "        where_big_enough = np.where([len(seq) > 10 for seq in token_sequences])\n",
+                "        self.token_sequences = np.array(token_sequences)[where_big_enough]\n",
+                "        self.target_sequences = np.array(target_sequences)[where_big_enough]\n",
+                "        self.lim = 512\n",
+                "\n",
+                "    def __getitem__(self, index):\n",
+                "        token_sequence = self.token_sequences[index]\n",
+                "        target_sequence = self.target_sequences[index]\n",
                 "\n",
-                "    G = create_network_graph(dist_mat_neighbours)\n",
-                "    \n",
-                "    return G"
+                "        # if the sequence is too long for the model to handle,\n",
+                "        # grab a random chunk of acceptable length instead\n",
+                "        if len(token_sequence) > self.lim:\n",
+                "            start_ix = len(token_sequence) - np.random.choice(self.lim)\n",
+                "            token_sequence = token_sequence[start_ix : start_ix + self.lim]\n",
+                "            target_sequence = target_sequence[start_ix : start_ix + self.lim]\n",
+                "\n",
+                "        tokens = torch.LongTensor(token_sequence)\n",
+                "        targets = torch.LongTensor(target_sequence)\n",
+                "        return tokens, targets\n",
+                "\n",
+                "    def __len__(self):\n",
+                "        return len(self.token_sequences)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "neighbour_function = get_top_neighbours\n",
-                "number_neighbours = 3\n",
+                "def collate_fn(batch):\n",
+                "    token_sequences, target_sequences = zip(*batch)\n",
+                "    seq_lens = torch.LongTensor([len(seq) for seq in token_sequences])\n",
+                "    sorted_lens, sort_indicies = seq_lens.sort(dim=0, descending=True)\n",
+                "\n",
+                "    sorted_tokens = [token_sequences[i] for i in sort_indicies]\n",
+                "    sorted_targets = [target_sequences[i] for i in sort_indicies]\n",
+                "\n",
+                "    padded_tokens = pad_sequence(\n",
+                "        sequences=sorted_tokens, padding_value=0, batch_first=True\n",
+                "    )\n",
                 "\n",
-                "run_graph_partial = partial(\n",
-                "    run_graph,\n",
-                "    neighbour_function=neighbour_function,\n",
-                "    number_neighbours=number_neighbours\n",
+                "    padded_targets = pad_sequence(\n",
+                "        sequences=sorted_targets, padding_value=0, batch_first=True\n",
                 "    )\n",
                 "\n",
-                "G_top_fv = run_graph_partial(dist_mat_fv)\n",
-                "G_top_fv2 = run_graph_partial(dist_mat_fv2)\n",
-                "G_top_fv20 = run_graph_partial(dist_mat_fv20)\n",
-                "G_top_fv80 = run_graph_partial(dist_mat_fv80)\n",
-                "G_top_fv100 = run_graph_partial(dist_mat_fv100)\n",
-                "G_top_fv500 = run_graph_partial(dist_mat_fv500)\n",
-                "G_top_fv1000 = run_graph_partial(dist_mat_fv1000)"
+                "    tokens = torch.LongTensor(padded_tokens)\n",
+                "    targets = torch.LongTensor(padded_targets)\n",
+                "    return tokens, targets"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "G_top_dict = {\n",
-                "    'G_top_fv':G_top_fv,\n",
-                "    'G_top_fv2':G_top_fv2,\n",
-                "    'G_top_fv20':G_top_fv20,\n",
-                "    'G_top_fv80':G_top_fv80,\n",
-                "    'G_top_fv100':G_top_fv100,\n",
-                "    'G_top_fv500':G_top_fv500,\n",
-                "    'G_top_fv1000':G_top_fv1000,\n",
-                "}"
+                "train_tokens, test_tokens, train_targets, test_targets = train_test_split(\n",
+                "    token_sequences, target_sequences, test_size=0.20, random_state=42\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "_ = plot_graph(G_top_fv, figsize=(3,3))\n",
-                "_ = plot_graph(G_top_fv2, figsize=(3,3))\n",
-                "_ = plot_graph(G_top_fv20, figsize=(3,3))\n",
-                "_ = plot_graph(G_top_fv80, figsize=(3,3))\n",
-                "_ = plot_graph(G_top_fv100, figsize=(3,3))\n",
-                "_ = plot_graph(G_top_fv500, figsize=(3,3))\n",
-                "_ = plot_graph(G_top_fv1000, figsize=(3,3))"
+                "train_dataset = SequenceDataset(train_tokens, train_targets)\n",
+                "\n",
+                "train_loader = DataLoader(\n",
+                "    dataset=train_dataset,\n",
+                "    batch_size=32,\n",
+                "    num_workers=5,\n",
+                "    shuffle=True,\n",
+                "    collate_fn=collate_fn,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "neighbour_function = get_high_neighbours\n",
-                "dist_threshold = 0.8\n",
-                "\n",
-                "run_graph_partial = partial(\n",
-                "    run_graph,\n",
-                "    neighbour_function=neighbour_function,\n",
-                "    dist_threshold=dist_threshold\n",
-                "    )\n",
+                "test_dataset = SequenceDataset(test_tokens, test_targets)\n",
                 "\n",
-                "G_high_fv = run_graph_partial(dist_mat_fv)\n",
-                "G_high_fv2 = run_graph_partial(dist_mat_fv2)\n",
-                "G_high_fv20 = run_graph_partial(dist_mat_fv20)\n",
-                "G_high_fv80 = run_graph_partial(dist_mat_fv80)\n",
-                "G_high_fv100 = run_graph_partial(dist_mat_fv100)\n",
-                "G_high_fv500 = run_graph_partial(dist_mat_fv500)\n",
-                "G_high_fv1000 = run_graph_partial(dist_mat_fv1000)"
+                "test_loader = DataLoader(\n",
+                "    dataset=test_dataset,\n",
+                "    batch_size=1,\n",
+                "    num_workers=5,\n",
+                "    shuffle=True,\n",
+                "    collate_fn=collate_fn,\n",
+                ")"
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": null,
+            "cell_type": "markdown",
             "metadata": {},
-            "outputs": [],
             "source": [
-                "G_high_dict = {\n",
-                "    'G_high_fv':G_high_fv,\n",
-                "    'G_high_fv2':G_high_fv2,\n",
-                "    'G_high_fv20':G_high_fv20,\n",
-                "    'G_high_fv80':G_high_fv80,\n",
-                "    'G_high_fv100':G_high_fv100,\n",
-                "    'G_high_fv500':G_high_fv500,\n",
-                "    'G_high_fv1000':G_high_fv1000,\n",
-                "}"
+                "# model"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "_ = plot_graph(G_high_fv, figsize=(3,3))\n",
-                "_ = plot_graph(G_high_fv2, figsize=(3,3))\n",
-                "_ = plot_graph(G_high_fv20, figsize=(3,3))\n",
-                "_ = plot_graph(G_high_fv80, figsize=(3,3))\n",
-                "_ = plot_graph(G_high_fv100, figsize=(3,3))\n",
-                "_ = plot_graph(G_high_fv500, figsize=(3,3))\n",
-                "_ = plot_graph(G_high_fv1000, figsize=(3,3))"
+                "stacked = np.hstack(train_targets)\n",
+                "a, b = len(stacked) - stacked.sum(), stacked.sum()\n",
+                "class_weights = torch.Tensor([b, a]) / (b + a)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "neighbour_function = get_top_high_neighbours\n",
-                "dist_threshold = 0.8\n",
-                "number_neighbours = 3\n",
-                "\n",
-                "run_graph_partial = partial(\n",
-                "    run_graph,\n",
-                "    neighbour_function=neighbour_function,\n",
-                "    number_neighbours=number_neighbours,\n",
-                "    dist_threshold=dist_threshold\n",
-                "    )\n",
+                "class LinkLabeller(nn.Module):\n",
+                "    def __init__(self):\n",
+                "        super(LinkLabeller, self).__init__()\n",
+                "        self.backbone = BertModel.from_pretrained(\"bert-base-cased\")\n",
+                "        self.head = nn.Sequential(\n",
+                "            nn.Dropout(0.3),\n",
+                "            nn.Linear(768, 128),\n",
+                "            nn.ReLU(),\n",
+                "            nn.Dropout(0.3),\n",
+                "            nn.Linear(128, 16),\n",
+                "            nn.ReLU(),\n",
+                "            nn.Dropout(0.3),\n",
+                "            nn.Linear(16, 2),\n",
+                "        )\n",
                 "\n",
-                "G_tophigh_fv = run_graph_partial(dist_mat_fv)\n",
-                "G_tophigh_fv2 = run_graph_partial(dist_mat_fv2)\n",
-                "G_tophigh_fv20 = run_graph_partial(dist_mat_fv20)\n",
-                "G_tophigh_fv80 = run_graph_partial(dist_mat_fv80)\n",
-                "G_tophigh_fv100 = run_graph_partial(dist_mat_fv100)\n",
-                "G_tophigh_fv500 = run_graph_partial(dist_mat_fv500)\n",
-                "G_tophigh_fv1000 = run_graph_partial(dist_mat_fv1000)\n"
+                "    def forward(self, token_sequences):\n",
+                "        segments = torch.zeros_like(token_sequences)\n",
+                "        x, _ = self.backbone(token_sequences, segments)\n",
+                "        return self.head(x[-1])"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "G_tophigh_dict = {\n",
-                "    'G_tophigh_fv':G_tophigh_fv,\n",
-                "    'G_tophigh_fv2':G_tophigh_fv2,\n",
-                "    'G_tophigh_fv20':G_tophigh_fv20,\n",
-                "    'G_tophigh_fv80':G_tophigh_fv80,\n",
-                "    'G_tophigh_fv100':G_tophigh_fv100,\n",
-                "    'G_tophigh_fv500':G_tophigh_fv500,\n",
-                "    'G_tophigh_fv1000':G_tophigh_fv1000,\n",
-                "}"
+                "model = LinkLabeller().to(device)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "_ = plot_graph(G_tophigh_fv, figsize=(3,3))\n",
-                "_ = plot_graph(G_tophigh_fv2, figsize=(3,3))\n",
-                "_ = plot_graph(G_tophigh_fv20, figsize=(3,3))\n",
-                "_ = plot_graph(G_tophigh_fv80, figsize=(3,3))\n",
-                "_ = plot_graph(G_tophigh_fv100, figsize=(3,3))\n",
-                "_ = plot_graph(G_tophigh_fv500, figsize=(3,3))\n",
-                "_ = plot_graph(G_tophigh_fv1000, figsize=(3,3))\n"
+                "loss_function = nn.CrossEntropyLoss(weight=class_weights.to(device))"
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": null,
+            "cell_type": "markdown",
             "metadata": {},
-            "outputs": [],
             "source": [
-                "# Fully connected graphs\n",
-                "G_full_fv = create_network_graph(dist_mat_fv)\n",
-                "G_full_fv2 = create_network_graph(dist_mat_fv2)\n",
-                "G_full_fv20 = create_network_graph(dist_mat_fv20)\n",
-                "G_full_fv80 = create_network_graph(dist_mat_fv80)\n",
-                "G_full_fv100 = create_network_graph(dist_mat_fv100)\n",
-                "G_full_fv500 = create_network_graph(dist_mat_fv500)\n",
-                "G_full_fv1000 = create_network_graph(dist_mat_fv1000)"
+                "# train"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "G_full_dict = {\n",
-                "    'G_full_fv':G_full_fv,\n",
-                "    'G_full_fv2':G_full_fv2,\n",
-                "    'G_full_fv20':G_full_fv20,\n",
-                "    'G_full_fv80':G_full_fv80,\n",
-                "    'G_full_fv100':G_full_fv100,\n",
-                "    'G_full_fv500':G_full_fv500,\n",
-                "    'G_full_fv1000':G_full_fv1000,\n",
-                "}"
+                "losses = []\n",
+                "torch.backends.cudnn.benchmark = True\n",
+                "\n",
+                "for param in list(model.backbone.children())[0].parameters():\n",
+                "    param.requires_grad = False\n",
+                "\n",
+                "for module in list(list(list(model.backbone.children())[1].children())[0].children())[\n",
+                "    :10\n",
+                "]:\n",
+                "    for param in module.parameters():\n",
+                "        param.requires_grad = False\n",
+                "\n",
+                "trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
+                "optimiser = optim.Adam(trainable_parameters, lr=0.0001)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "_ = plot_graph(G_full_fv, figsize=(3,3))\n",
-                "_ = plot_graph(G_full_fv80, figsize=(3,3))\n",
-                "_ = plot_graph(G_full_fv1000, figsize=(3,3))"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "### 5. Pick the nodes you are going to go between in the network\n",
+                "def train(model, train_loader, loss_function, optimiser, n_epochs):\n",
+                "    model.train()\n",
+                "    for epoch in range(n_epochs):\n",
+                "        loop = tqdm(train_loader)\n",
+                "        for tokens, targets in loop:\n",
+                "            tokens = tokens.cuda(non_blocking=True)\n",
+                "            targets = targets.cuda(non_blocking=True)\n",
+                "            segments = torch.zeros_like(tokens)\n",
+                "\n",
+                "            optimiser.zero_grad()\n",
+                "            preds = model(tokens)\n",
+                "            loss = loss_function(preds.permute(0, 2, 1), targets)\n",
+                "            loss.backward()\n",
+                "            optimiser.step()\n",
                 "\n",
-                "- Furthest apart? High cosine distance = different image features\n",
-                "- Random?"
+                "            losses.append(loss.item())\n",
+                "            loop.set_description(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
+                "            loop.set_postfix(loss=np.mean(losses[-100:]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "high_coords = np.where(dist_mat_fv == np.amax(dist_mat_fv))\n",
-                "print(\"Picking the first highest cosine out of {} with the same highest value\".format(len(high_coords)))\n",
-                "node1 = list(zip(high_coords[0], high_coords[1]))[0][0]\n",
-                "node2 = list(zip(high_coords[0], high_coords[1]))[0][1]\n",
-                "print(node1)\n",
-                "print(node2)\n",
-                "print(image_names_dict[node1])\n",
-                "print(image_names_dict[node2])"
+                "train(model, train_loader, loss_function, optimiser, n_epochs=3)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "np.random.seed(4)\n",
-                "node1 = np.random.choice(list(image_names_dict))\n",
-                "node2 = np.random.choice(list(image_names_dict))\n",
-                "print(node1)\n",
-                "print(node2)\n",
-                "print(image_names_dict[node1]) # V0040357EL\n",
-                "print(image_names_dict[node2]) # V0020158"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "# Run different pathways using these graphs"
+                "loss_data = pd.Series(losses).rolling(window=100).mean()\n",
+                "ax = loss_data.plot()\n",
+                "ax.set_ylim(0);"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "def run_pathway(G_dict, pathway_algo, node1, node2,\n",
-                "                image_names_dict, images_dir, image_type,\n",
-                "               path_size=None, best_path=True, best_type=\"sum\"):\n",
-                "    \n",
-                "    G = G_dict[1]\n",
-                "    try:\n",
-                "        if pathway_algo == nx.dijkstra_path:\n",
-                "            node_path = pathway_algo(G, node1, node2, weight=None)\n",
-                "        elif pathway_algo == nx.astar_path:\n",
-                "            node_path = pathway_algo(G, node1, node2, weight=None)\n",
-                "        elif pathway_algo == defined_path:\n",
-                "            G_weights = nx.to_numpy_matrix(G)\n",
-                "            node_path = pathway_algo(G, node1, node2, G_weights, path_size, best_path, best_type)\n",
-                "\n",
-                "        image_names_path = [image_names_dict[n] for n in node_path]\n",
-                "        \n",
-                "        title = \"Graph type is {}.\\nPathway algo is {}.\\nBest type is {}\".format(\n",
-                "            G_dict[0],\n",
-                "            str(locals()['pathway_algo']),\n",
-                "            best_type\n",
-                "            )\n",
-                "\n",
-                "        return image_pathway_plot(images_dir, image_type, image_names_path, title), node_path\n",
-                "    except:\n",
-                "        return print(\"There is no pathway between nodes\"), _"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "## Play with the dijkstra_path pathway"
+                "def clean(output_string):\n",
+                "    return (\n",
+                "        output_string.replace(\"</b> <b>\", \" \")\n",
+                "        .replace(\"<b>##\", \"<b>\")\n",
+                "        .replace(\" ##\", \"\")\n",
+                "    )"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "pathway_algo = nx.dijkstra_path\n",
+                "def format_output(tokens, targets, preds):\n",
+                "    target_string, pred_string = \"\", \"\"\n",
                 "\n",
-                "run_pathway_partial = partial(\n",
-                "    run_pathway,\n",
-                "    pathway_algo=pathway_algo,\n",
-                "    node1=node1,\n",
-                "    node2=node2,\n",
-                "    image_names_dict=image_names_dict,\n",
-                "    images_dir=images_dir,\n",
-                "    image_type=image_type\n",
-                "    )"
+                "    for token_id, target, pred in zip(tokens, targets, preds):\n",
+                "        token = tokenizer.convert_ids_to_tokens([token_id.item()])[0]\n",
+                "\n",
+                "        if target == 1:\n",
+                "            target_string += \"<b>\" + token + \"</b> \"\n",
+                "        else:\n",
+                "            target_string += token + \" \"\n",
+                "\n",
+                "        if pred == 1:\n",
+                "            pred_string += \"<b>\" + token + \"</b> \"\n",
+                "        else:\n",
+                "            pred_string += token + \" \"\n",
+                "\n",
+                "    output_string = (\n",
+                "        \"PRED:<br>\"\n",
+                "        + clean(pred_string)\n",
+                "        + \"<br><br>TARG:<br>\"\n",
+                "        + clean(target_string)\n",
+                "        + \"<br><br>--------<br><br>\"\n",
+                "    )\n",
+                "\n",
+                "    return output_string"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "for G_top in G_top_dict.items():\n",
-                "    run_pathway_partial(G_top)"
+                "output = \"\"\n",
+                "\n",
+                "with torch.no_grad():\n",
+                "    for i, (tokens, targets) in enumerate(test_loader):\n",
+                "        if i < 10:\n",
+                "            tokens = tokens  # .cuda()\n",
+                "            targets = targets  # .cuda()\n",
+                "            segments = torch.zeros_like(tokens)\n",
+                "\n",
+                "            preds = model(tokens)\n",
+                "            preds = nn.LogSoftmax(dim=1)(preds[0]).argmax(dim=1)\n",
+                "\n",
+                "            output += format_output(tokens[0], targets[0], preds)\n",
+                "        else:\n",
+                "            break\n",
+                "\n",
+                "display(HTML(output))"
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": null,
+            "cell_type": "markdown",
             "metadata": {},
-            "outputs": [],
             "source": [
-                "for G_high in G_high_dict.items():\n",
-                "    run_pathway_partial(G_high)"
+                "# save model"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "for G_tophigh in G_tophigh_dict.items():\n",
-                "    run_pathway_partial(G_tophigh)"
+                "PATH = \"/mnt/efs/models/20190222_bert_link_labeller.pt\""
             ]
         },
         {
-            "cell_type": "markdown",
+            "cell_type": "raw",
             "metadata": {},
             "source": [
-                "Try using my defined path function. In this I can use the fully connected graph too. Note that using the fully connected graph with an undefined number of nodes will just return a direct pathway from the first image to the second."
+                "torch.save(model.state_dict(), PATH)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### Play with the A* path"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": null,
-            "metadata": {},
-            "outputs": [],
-            "source": [
-                "pathway_algo = nx.astar_path\n",
-                "\n",
-                "run_astar_pathway_partial = partial(\n",
-                "    run_pathway,\n",
-                "    pathway_algo=pathway_algo,\n",
-                "    node1=node1,\n",
-                "    node2=node2,\n",
-                "    image_names_dict=image_names_dict,\n",
-                "    images_dir=images_dir,\n",
-                "    image_type=image_type\n",
-                "    )"
+                "# load model for use on cpu"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "for G_top in G_top_dict.items():\n",
-                "    run_astar_pathway_partial(G_top)"
+                "model = LinkLabeller()\n",
+                "model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "run_astar_pathway_partial(('G_full_fv', G_full_dict['G_full_fv']))"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "### Play with the defined_path"
+                "model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "pathway_algo = defined_path\n",
+                "tokens, targets = next(iter(test_loader))\n",
                 "\n",
-                "run_defined_pathway_partial = partial(\n",
-                "    run_pathway,\n",
-                "    pathway_algo=pathway_algo,\n",
-                "    node1=node1,\n",
-                "    node2=node2,\n",
-                "    image_names_dict=image_names_dict,\n",
-                "    images_dir=images_dir,\n",
-                "    image_type=image_type\n",
-                "    )"
+                "preds = model(tokens)\n",
+                "preds = nn.LogSoftmax(dim=1)(preds[0]).argmax(dim=1)\n",
+                "output_html = format_output(tokens[0], targets[0], preds)\n",
+                "display(HTML(output_html))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "run_defined_pathway_partial(('G_top_fv', G_top_dict['G_top_fv']), path_size=10, best_type=\"sum\")"
+                "preds = model(tokens)\n",
+                "preds = nn.LogSoftmax(dim=1)(preds[0]).argmax(dim=1)\n",
+                "output_html = format_output(tokens[0], targets[0], preds)\n",
+                "display(HTML(output_html))"
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": null,
+            "cell_type": "markdown",
             "metadata": {},
-            "outputs": [],
             "source": [
-                "run_defined_pathway_partial(('G_top_fv', G_top_dict['G_top_fv']), path_size=10, best_type=\"average\")"
+                "# try with new text from wellcome domain"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "run_defined_pathway_partial(('G_top_fv', G_top_dict['G_top_fv']), path_size=10, best_type=\"variance\")"
+                "text = \"\"\"\n",
+                "Last week I attended a colloquium in Berlin, Das Erbe der Berliner Sexualwissenschaft: Eine Fachtagung sexualwissenschaftlicher Archive, commemorating the 80th anniversary of destruction of Magnus Hirschfeld\u2018s Institut f\u00fcr Sexualwissenschaft by the Nazis on 6 May 1933.\n",
+                "\n",
+                "I had been asked to talk about the material we hold in the Wellcome Library relating to Hirschfeld and his legacy and the impact of continental sexual science on British sexologists. There is a small amount of material specifically relating to Hirschfeld in Archives and Manuscripts: like Havelock Ellis, he was a respondent to Dr Josef Strasser\u2019s questionnaire on his career decisions, c. 1930, and his 3-page letter to Strasser and a pamphlet can be found in MS.7042.\n",
+                "\n",
+                "There is also a group of photographs of the World League for Sexual Reform (founded by Hirschfeld) Congress in Brno, 1932 among the archives of the Family Planning Association. Charlotte Wolff worked with Hirschfeld in her younger days in Berlin, and her papers among the archives of the British Psychological Society include her research files for her 1986 biography of him, the first to be published in English. The Library also holds copies of several of his works.\n",
+                "\n",
+                "I was also able to mention that we hold the papers of Hirschfeld\u2019s important precursor, Richard von Krafft-Ebing, as well as some material on Havelock Ellis, and important early printed works of sexology, including the first edition of Krafft-Ebing\u2019s Psychopathia Sexualis and the German, and first English, editions of Ellis and J A Symond\u2019s Sexual Inversion (the latter is very rare since Symonds\u2019 executor bought up the entire edition to protect the family from scandal and distress). There is also a significant amount in A&M and the Library more generally pertaining to Hirschfeld\u2019s leading British disciple, the Australian doctor Norman Haire.\n",
+                "\n",
+                "\"\"\"\n",
+                "\n",
+                "text_tokens = tokenizer.tokenize(text)\n",
+                "tokens = torch.LongTensor([tokenizer.convert_tokens_to_ids(text_tokens)])\n",
+                "\n",
+                "preds_continuous = model(tokens)\n",
+                "preds = nn.LogSoftmax(dim=1)(preds_continuous[0]).argmax(dim=1)\n",
+                "output_html = format_output(tokens[0], torch.zeros_like(tokens[0]), preds)\n",
+                "display(HTML(output_html))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "for G_top in G_top_dict.items():\n",
-                "    run_defined_pathway_partial(G_top, path_size = 9, best_type=\"sum\")\n",
-                "    run_defined_pathway_partial(G_top, path_size = 9, best_type=\"variance\")"
+                "preds_continuous.detach().numpy().sum(axis=2)[0]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "run_defined_pathway_partial(('G_full_fv', G_full_dict['G_full_fv']), path_size=3, best_type=\"variance\")"
+                "pd.Series(preds_continuous.detach().numpy().sum(axis=2)[0]).plot.bar()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
-            "source": [
-                "# Takes so long!\n",
-                "# run_defined_pathway_partial(('G_full_fv', G_full_dict['G_full_fv']), path_size=5)"
-            ]
+            "source": []
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Environment (conda_pytorch_p36)",
+            "display_name": "Python [conda env:pytorch_p36]",
             "language": "python",
-            "name": "conda_pytorch_p36"
+            "name": "conda-env-pytorch_p36-py"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.6.5"
+            "version": "3.6.4"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/5. Graph_pathways_focused.ipynb` & `weco-datascience-0.1.9/notebooks/image_pathways/notebooks/5. Graph_pathways_focused.ipynb`

 * *Files 5% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9960907389711737%*

 * *Differences: {"'cells'": "{2: {'source': {insert: [(3, 'import ast\\n')], delete: [3]}}, 4: {'source': {insert: "*

 * *            "[(10, '    create_network_graph,\\n'), (11, ')')], delete: [11, 10]}}, 5: {'source': "*

 * *            '[\'images_dir = "data/"\\n\', \'image_type = ".png"\']}, 7: {\'source\': {insert: '*

 * *            '[(0, \'bucket_name = "miro-images-feature-vectors"\\n\'), (2, \'s3 = '*

 * *            'boto3.client("s3")\')], delete: [2, 0]}}, 10: {\'source\': {insert: [(1, '*

 * *            "'np.random.seed(0)  # For dev […]*

```diff
@@ -26,15 +26,15 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "from tqdm import tqdm\n",
                 "import os\n",
                 "from io import BytesIO\n",
-                "import ast \n",
+                "import ast\n",
                 "import numpy as np\n",
                 "import pickle\n",
                 "from itertools import compress\n",
                 "from collections import Counter\n",
                 "import operator\n",
                 "from functools import partial\n",
                 "\n",
@@ -70,26 +70,26 @@
                 "    get_distances,\n",
                 "    get_all_s3_keys,\n",
                 "    image_pathway_plot,\n",
                 "    plot_graph,\n",
                 "    defined_path,\n",
                 "    reduce_data_nd,\n",
                 "    visualize_scatter_with_images,\n",
-                "    create_network_graph\n",
-                "    )"
+                "    create_network_graph,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "images_dir = 'data/'\n",
-                "image_type = '.png'"
+                "images_dir = \"data/\"\n",
+                "image_type = \".png\""
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### 1. Get the names of the feature vectors I found\n",
@@ -99,17 +99,17 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "bucket_name = 'miro-images-feature-vectors'\n",
+                "bucket_name = \"miro-images-feature-vectors\"\n",
                 "bucket_name = bucket_name\n",
-                "s3 = boto3.client('s3')"
+                "s3 = boto3.client(\"s3\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -130,16 +130,16 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "n_sample = 10000\n",
-                "np.random.seed(0) # For dev\n",
-                "image_names = np.random.choice(image_names, n_sample, replace = False)"
+                "np.random.seed(0)  # For dev\n",
+                "image_names = np.random.choice(image_names, n_sample, replace=False)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -156,15 +156,17 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "feature_vectors, _ = import_feature_vectors(s3, bucket_name, \"feature_vectors\", image_names)"
+                "feature_vectors, _ = import_feature_vectors(\n",
+                "    s3, bucket_name, \"feature_vectors\", image_names\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -175,15 +177,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_names_dict = {k:v for k,v in enumerate(image_names)}"
+                "image_names_dict = {k: v for k, v in enumerate(image_names)}"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### 3. Make graph"
@@ -219,15 +221,19 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "high_coords = np.where(dist_mat == np.amax(dist_mat))\n",
-                "print(\"Picking the first highest cosine out of {} with the same highest value\".format(len(high_coords)))\n",
+                "print(\n",
+                "    \"Picking the first highest cosine out of {} with the same highest value\".format(\n",
+                "        len(high_coords)\n",
+                "    )\n",
+                ")\n",
                 "node1 = list(zip(high_coords[0], high_coords[1]))[0][0]\n",
                 "node2 = list(zip(high_coords[0], high_coords[1]))[0][1]\n",
                 "print(node1)\n",
                 "print(node2)\n",
                 "print(image_names_dict[node1])\n",
                 "print(image_names_dict[node2])"
             ]
@@ -244,35 +250,48 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "def run_pathway(G, pathway_algo, node1, node2,\n",
-                "                image_names_dict, images_dir, image_type,\n",
-                "               path_size=None, best_path=True, best_type=\"sum\"):\n",
-                "    \n",
+                "def run_pathway(\n",
+                "    G,\n",
+                "    pathway_algo,\n",
+                "    node1,\n",
+                "    node2,\n",
+                "    image_names_dict,\n",
+                "    images_dir,\n",
+                "    image_type,\n",
+                "    path_size=None,\n",
+                "    best_path=True,\n",
+                "    best_type=\"sum\",\n",
+                "):\n",
+                "\n",
                 "    try:\n",
                 "        if pathway_algo == nx.dijkstra_path:\n",
                 "            node_path = pathway_algo(G, node1, node2, weight=None)\n",
                 "        elif pathway_algo == nx.astar_path:\n",
                 "            node_path = pathway_algo(G, node1, node2, weight=None)\n",
                 "        elif pathway_algo == defined_path:\n",
                 "            G_weights = nx.to_numpy_matrix(G)\n",
-                "            node_path = pathway_algo(G, node1, node2, G_weights, path_size, best_path, best_type)\n",
+                "            node_path = pathway_algo(\n",
+                "                G, node1, node2, G_weights, path_size, best_path, best_type\n",
+                "            )\n",
                 "\n",
                 "        image_names_path = [image_names_dict[n] for n in node_path]\n",
-                "        \n",
+                "\n",
                 "        title = \"Pathway algo is {}.\\nBest type is {}\".format(\n",
-                "            str(locals()['pathway_algo']),\n",
-                "            best_type\n",
-                "            )\n",
+                "            str(locals()[\"pathway_algo\"]), best_type\n",
+                "        )\n",
                 "\n",
-                "        return image_pathway_plot(images_dir, image_type, image_names_path, title), node_path\n",
+                "        return (\n",
+                "            image_pathway_plot(images_dir, image_type, image_names_path, title),\n",
+                "            node_path,\n",
+                "        )\n",
                 "    except:\n",
                 "        return print(\"There is no pathway between nodes\"), _"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
@@ -282,37 +301,37 @@
                 "run_defined_pathway_partial = partial(\n",
                 "    run_pathway,\n",
                 "    pathway_algo=defined_path,\n",
                 "    node1=node1,\n",
                 "    node2=node2,\n",
                 "    image_names_dict=image_names_dict,\n",
                 "    images_dir=images_dir,\n",
-                "    image_type=image_type\n",
-                "    )\n",
+                "    image_type=image_type,\n",
+                ")\n",
                 "\n",
                 "run_dijk_pathway_partial = partial(\n",
                 "    run_pathway,\n",
                 "    pathway_algo=nx.dijkstra_path,\n",
                 "    node1=node1,\n",
                 "    node2=node2,\n",
                 "    image_names_dict=image_names_dict,\n",
                 "    images_dir=images_dir,\n",
-                "    image_type=image_type\n",
-                "    )"
+                "    image_type=image_type,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "(_,node_path_dijk) = run_dijk_pathway_partial(G)\n",
-                "(_,node_path_sum) = run_defined_pathway_partial(G, path_size=12, best_type=\"sum\")\n",
-                "(_,node_path_var) = run_defined_pathway_partial(G, path_size=12, best_type=\"variance\")"
+                "(_, node_path_dijk) = run_dijk_pathway_partial(G)\n",
+                "(_, node_path_sum) = run_defined_pathway_partial(G, path_size=12, best_type=\"sum\")\n",
+                "(_, node_path_var) = run_defined_pathway_partial(G, path_size=12, best_type=\"variance\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -322,17 +341,17 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "pos = plot_graph(G, figsize=(8,8), node_list = node_path_dijk)\n",
-                "_ = plot_graph(G, figsize=(8,8), node_list = node_path_sum, pos=pos)\n",
-                "_ = plot_graph(G, figsize=(8,8), node_list = node_path_var, pos=pos)"
+                "pos = plot_graph(G, figsize=(8, 8), node_list=node_path_dijk)\n",
+                "_ = plot_graph(G, figsize=(8, 8), node_list=node_path_sum, pos=pos)\n",
+                "_ = plot_graph(G, figsize=(8, 8), node_list=node_path_var, pos=pos)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -347,52 +366,52 @@
             "outputs": [],
             "source": [
                 "visualize_scatter_with_images(\n",
                 "    x_data,\n",
                 "    image_name_list=image_names,\n",
                 "    images_dir=images_dir,\n",
                 "    image_type=image_type,\n",
-                "    figsize=(20,20),\n",
+                "    figsize=(20, 20),\n",
                 "    image_zoom=0.1,\n",
-                "    pathway=[image_names_dict[n] for n in node_path_dijk]\n",
-                "    )"
+                "    pathway=[image_names_dict[n] for n in node_path_dijk],\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "visualize_scatter_with_images(\n",
                 "    x_data,\n",
                 "    image_name_list=image_names,\n",
                 "    images_dir=images_dir,\n",
                 "    image_type=image_type,\n",
-                "    figsize=(20,20),\n",
+                "    figsize=(20, 20),\n",
                 "    image_zoom=0.1,\n",
-                "    pathway=[image_names_dict[n] for n in node_path_sum]\n",
-                "    )"
+                "    pathway=[image_names_dict[n] for n in node_path_sum],\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "visualize_scatter_with_images(\n",
                 "    x_data,\n",
                 "    image_name_list=image_names,\n",
                 "    images_dir=images_dir,\n",
                 "    image_type=image_type,\n",
-                "    figsize=(20,20),\n",
+                "    figsize=(20, 20),\n",
                 "    image_zoom=0.1,\n",
-                "    pathway=[image_names_dict[n] for n in node_path_var]\n",
-                "    )"
+                "    pathway=[image_names_dict[n] for n in node_path_var],\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
```

### Comparing `weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/6. Spaced_pathway.ipynb` & `weco-datascience-0.1.9/notebooks/image_pathways/notebooks/6. Spaced_pathway.ipynb`

 * *Files 4% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9962550237486298%*

 * *Differences: {"'cells'": "{2: {'source': {insert: [(3, 'import ast\\n')], delete: [3]}}, 4: {'source': {insert: "*

 * *            "[(4, '    fv_spaced_pathway_nD,\\n'), (5, ')')], delete: [5, 4]}}, 5: {'source': "*

 * *            '{insert: [(1, \'images_dir = "data/"\\n\'), (2, \'image_type = ".png"\\n\')], delete: '*

 * *            "[2, 1]}}, 7: {'source': {insert: [(1, 'np.random.seed(0)  # For dev\\n'), (2, "*

 * *            "'image_name_list = np.random.choice(image_names, n_sample, replace=False)')], delete: "*

 * *            "[2, 1]} […]*

```diff
@@ -25,15 +25,15 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "from tqdm import tqdm\n",
                 "import os\n",
                 "from io import BytesIO\n",
-                "import ast \n",
+                "import ast\n",
                 "import numpy as np\n",
                 "import pickle\n",
                 "from itertools import compress\n",
                 "from collections import Counter\n",
                 "import operator\n",
                 "\n",
                 "from PIL import Image\n",
@@ -62,27 +62,27 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "from src.network_functions import (\n",
                 "    import_feature_vectors,\n",
                 "    image_pathway_plot,\n",
                 "    reduce_data_nd,\n",
-                "    fv_spaced_pathway_nD\n",
-                "    )"
+                "    fv_spaced_pathway_nD,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# Get all the png image names from the data folder\n",
-                "images_dir = 'data/'\n",
-                "image_type = '.png'\n",
+                "images_dir = \"data/\"\n",
+                "image_type = \".png\"\n",
                 "\n",
                 "image_names = os.listdir(images_dir)\n",
                 "image_names = [os.path.splitext(file)[0] for file in image_names if image_type in file]\n",
                 "len(image_names)"
             ]
         },
         {
@@ -95,16 +95,16 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "n_sample = 1000\n",
-                "np.random.seed(0) # For dev\n",
-                "image_name_list = np.random.choice(image_names, n_sample, replace = False)"
+                "np.random.seed(0)  # For dev\n",
+                "image_name_list = np.random.choice(image_names, n_sample, replace=False)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### 1. Get feature vectors as they are (>4000 dimensions)"
@@ -112,30 +112,32 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "bucket_name = 'miro-images-feature-vectors'\n",
+                "bucket_name = \"miro-images-feature-vectors\"\n",
                 "folder_name = \"feature_vectors\"\n",
                 "n = 3 # This is what X degrees of separation uses 15, but perhaps this is too much, should it be a fraction of the n_sample?\n",
-                "dist_threshold = 0.35 \n",
+                "dist_threshold = 0.35\n",
                 "\n",
                 "bucket_name = bucket_name\n",
-                "s3 = boto3.client('s3')"
+                "s3 = boto3.client(\"s3\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "feature_vectors, _ = import_feature_vectors(s3, bucket_name, folder_name, image_name_list)\n",
+                "feature_vectors, _ = import_feature_vectors(\n",
+                "    s3, bucket_name, folder_name, image_name_list\n",
+                ")\n",
                 "\n",
                 "# Remove the name of this image from the list if no feature vector was found for it\n",
                 "image_name_list = [x for x in image_name_list if x in list(feature_vectors.keys())]"
             ]
         },
         {
             "cell_type": "code",
@@ -148,15 +150,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_names_dict = {k:v for k,v in enumerate(image_name_list)}"
+                "image_names_dict = {k: v for k, v in enumerate(image_name_list)}"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### 2. Experimenting with the umap.UMAP parameters\n",
@@ -170,24 +172,30 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "for n_neighbors in (2, 50, 200):\n",
                 "    for min_dist in (0.1, 0.5, 0.9):\n",
                 "        for n_components in (2, 3):\n",
-                "            x_data = reduce_data_nd(feature_vectors, n_components, n_neighbors, min_dist)\n",
+                "            x_data = reduce_data_nd(\n",
+                "                feature_vectors, n_components, n_neighbors, min_dist\n",
+                "            )\n",
                 "\n",
                 "            node1 = 100\n",
                 "            node2 = 30\n",
                 "            n_nodes = 6\n",
                 "\n",
                 "            node_path = fv_spaced_pathway_nD(x_data, node1, node2, n_nodes)\n",
                 "            image_pathway_plot(\n",
-                "                images_dir, image_type, node_path,\n",
-                "                title=\"{} components, {} min dist, {} neighbors\".format(n_components, min_dist, n_neighbors)\n",
+                "                images_dir,\n",
+                "                image_type,\n",
+                "                node_path,\n",
+                "                title=\"{} components, {} min dist, {} neighbors\".format(\n",
+                "                    n_components, min_dist, n_neighbors\n",
+                "                ),\n",
                 "            )"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -196,24 +204,28 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "feature_vectors_trans = {k:list(v) for k,v in feature_vectors.items()}\n",
+                "feature_vectors_trans = {k: list(v) for k, v in feature_vectors.items()}\n",
                 "\n",
                 "node1 = 100\n",
                 "node2 = 30\n",
                 "n_nodes = 6\n",
                 "\n",
                 "node_path = fv_spaced_pathway_nD(feature_vectors_trans, node1, node2, n_nodes)\n",
                 "image_pathway_plot(\n",
-                "    images_dir, image_type, node_path,\n",
-                "    title=\"{} components, {} min dist, {} neighbors\".format(n_components, min_dist, n_neighbors)\n",
+                "    images_dir,\n",
+                "    image_type,\n",
+                "    node_path,\n",
+                "    title=\"{} components, {} min dist, {} neighbors\".format(\n",
+                "        n_components, min_dist, n_neighbors\n",
+                "    ),\n",
                 ")"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
             "display_name": "Environment (conda_pytorch_p36)",
```

### Comparing `weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/7. Save_all_FV_pathways.ipynb` & `weco-datascience-0.1.9/notebooks/image_pathways/notebooks/7. Save_all_FV_pathways.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9967775372775373%*

 * *Differences: {"'cells'": "{2: {'source': {insert: [(3, 'import ast\\n')], delete: [3]}}, 4: {'source': ['from "*

 * *            "src.network_functions import import_feature_vectors']}, 5: {'source': {insert: [(1, "*

 * *            '\'images_dir = "data/"\\n\'), (2, \'image_type = ".png"\\n\')], delete: [2, 1]}}, 7: '*

 * *            '{\'source\': {insert: [(0, \'bucket_name = "miro-images-feature-vectors"\\n\'), (3, '*

 * *            '\'dist_threshold = 0.35\\n\'), (6, \'s3 = boto3.client("s3")\')], delete: [6, 3, '*

 * *            "0]}},  […]*

```diff
@@ -24,15 +24,15 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "from tqdm import tqdm\n",
                 "import os\n",
                 "from io import BytesIO\n",
-                "import ast \n",
+                "import ast\n",
                 "import numpy as np\n",
                 "import pickle\n",
                 "from itertools import compress\n",
                 "from collections import Counter\n",
                 "import operator\n",
                 "import datetime\n",
                 "\n",
@@ -58,28 +58,26 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "from src.network_functions import (\n",
-                "    import_feature_vectors\n",
-                "    )"
+                "from src.network_functions import import_feature_vectors"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# Get all the png image names from the data folder\n",
-                "images_dir = 'data/'\n",
-                "image_type = '.png'\n",
+                "images_dir = \"data/\"\n",
+                "image_type = \".png\"\n",
                 "\n",
                 "image_names = os.listdir(images_dir)\n",
                 "image_names = [os.path.splitext(file)[0] for file in image_names if image_type in file]\n",
                 "len(image_names)"
             ]
         },
         {
@@ -91,21 +89,21 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "bucket_name = 'miro-images-feature-vectors'\n",
+                "bucket_name = \"miro-images-feature-vectors\"\n",
                 "folder_name = \"feature_vectors\"\n",
                 "n = 3 # This is what X degrees of separation uses 15, but perhaps this is too much, should it be a fraction of the n_sample?\n",
-                "dist_threshold = 0.35 \n",
+                "dist_threshold = 0.35\n",
                 "\n",
                 "bucket_name = bucket_name\n",
-                "s3 = boto3.client('s3')"
+                "s3 = boto3.client(\"s3\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -146,26 +144,30 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# Save\n",
                 "now = datetime.datetime.now()\n",
                 "date = now.strftime(\"%Y%m%d\")\n",
-                "np.save('data/{}_feature_vectors_ids'.format(date), np.array(list(feature_vectors.keys())))\n",
-                "np.save('data/{}_feature_vectors'.format(date), np.array(list(feature_vectors.values())))"
+                "np.save(\n",
+                "    \"data/{}_feature_vectors_ids\".format(date), np.array(list(feature_vectors.keys()))\n",
+                ")\n",
+                "np.save(\n",
+                "    \"data/{}_feature_vectors\".format(date), np.array(list(feature_vectors.values()))\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "print('data/{}_feature_vectors_ids'.format(date))\n",
-                "print('data/{}_feature_vectors'.format(date))"
+                "print(\"data/{}_feature_vectors_ids\".format(date))\n",
+                "print(\"data/{}_feature_vectors\".format(date))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
```

### Comparing `weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/8. Use_all_FV.ipynb` & `weco-datascience-0.1.9/notebooks/image_pathways/notebooks/8. Use_all_FV.ipynb`

 * *Files 14% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9961337498721101%*

 * *Differences: {"'cells'": "{4: {'source': {insert: [(5, '    visualize_scatter_with_images,\\n')], delete: "*

 * *            '[5]}}, 8: {\'source\': [\'images_dir = "data/images/"\\n\', \'image_type = '*

 * *            '".png"\']}, 10: {\'source\': {insert: [(4, \'for i in tqdm(range(0, 10)):\\n\'), (7, '*

 * *            "'    node_path, path_dists = get_pathway(\\n'), (8, '        feature_vectors_ids, "*

 * *            "feature_vectors, id_1, id_2, n_nodes, sample_size\\n'), (9, '    )\\n'), (12, "*

 * *            "'\\n')], delete: [10, 7, […]*

```diff
@@ -52,15 +52,15 @@
             "outputs": [],
             "source": [
                 "from src.network_functions import (\n",
                 "    get_pathway,\n",
                 "    image_pathway_plot,\n",
                 "    image_pathway_scaled_plot,\n",
                 "    reduce_data_nd,\n",
-                "    visualize_scatter_with_images\n",
+                "    visualize_scatter_with_images,\n",
                 ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -88,16 +88,16 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "images_dir = 'data/images/'\n",
-                "image_type = '.png'"
+                "images_dir = \"data/images/\"\n",
+                "image_type = \".png\""
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "## Look at some random pathways using all the images"
@@ -109,21 +109,23 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "nodes_used = []\n",
                 "all_path_dists = []\n",
                 "n_nodes = 10\n",
                 "sample_size = None\n",
-                "for i in tqdm(range(0,10)):\n",
+                "for i in tqdm(range(0, 10)):\n",
                 "    id_1 = np.random.choice(feature_vectors_ids)\n",
                 "    id_2 = np.random.choice(feature_vectors_ids)\n",
-                "    node_path, path_dists = get_pathway(feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size)\n",
+                "    node_path, path_dists = get_pathway(\n",
+                "        feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size\n",
+                "    )\n",
                 "    print(node_path)\n",
                 "    image_pathway_plot(images_dir, image_type, node_path)\n",
-                "    \n",
+                "\n",
                 "    nodes_used.append([id_1, id_2])\n",
                 "    all_path_dists.extend(path_dists)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
@@ -139,21 +141,23 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "nodes_used = []\n",
                 "all_path_dists = []\n",
                 "n_nodes = 10\n",
                 "sample_size = None\n",
-                "for i in tqdm(range(0,10)):\n",
+                "for i in tqdm(range(0, 10)):\n",
                 "    id_1 = np.random.choice(feature_vectors_ids)\n",
                 "    id_2 = np.random.choice(feature_vectors_ids)\n",
-                "    node_path, path_dists = get_pathway(feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size)\n",
+                "    node_path, path_dists = get_pathway(\n",
+                "        feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size\n",
+                "    )\n",
                 "    print(node_path)\n",
                 "    image_pathway_plot(images_dir, image_type, node_path)\n",
-                "    \n",
+                "\n",
                 "    nodes_used.append([id_1, id_2])\n",
                 "    all_path_dists.extend(path_dists)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
@@ -182,106 +186,118 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "id_1 = 'B0006448'\n",
-                "id_2 = 'V0021276'\n",
+                "id_1 = \"B0006448\"\n",
+                "id_2 = \"V0021276\"\n",
                 "\n",
                 "n_nodes = 8\n",
                 "sample_size = None\n",
                 "\n",
-                "node_path, path_dists = get_pathway(feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size)\n",
+                "node_path, path_dists = get_pathway(\n",
+                "    feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size\n",
+                ")\n",
                 "print(node_path)\n",
                 "image_pathway_scaled_plot(images_dir, image_type, node_path)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "id_1 = 'B0006448'\n",
-                "id_2 = 'V0021276'\n",
+                "id_1 = \"B0006448\"\n",
+                "id_2 = \"V0021276\"\n",
                 "\n",
                 "n_nodes = 8\n",
                 "sample_size = None\n",
                 "\n",
-                "node_path, path_dists = get_pathway(feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size)\n",
+                "node_path, path_dists = get_pathway(\n",
+                "    feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size\n",
+                ")\n",
                 "print(node_path)\n",
                 "image_pathway_plot(images_dir, image_type, node_path)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "id_1 = 'B0008895'\n",
-                "id_2 = 'M0010374'#'V0005248'#'V0006023'#'V0001893'\n",
+                "id_1 = \"B0008895\"\n",
+                "id_2 = \"M0010374\"  #'V0005248'#'V0006023'#'V0001893'\n",
                 "\n",
                 "n_nodes = 9\n",
                 "sample_size = None\n",
                 "\n",
-                "node_path, path_dists = get_pathway(feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size)\n",
+                "node_path, path_dists = get_pathway(\n",
+                "    feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size\n",
+                ")\n",
                 "print(node_path)\n",
                 "image_pathway_plot(images_dir, image_type, node_path)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "id_1 = 'V0046313'\n",
-                "id_2 = 'L0061460'\n",
+                "id_1 = \"V0046313\"\n",
+                "id_2 = \"L0061460\"\n",
                 "\n",
                 "n_nodes = 9\n",
                 "sample_size = None\n",
                 "\n",
-                "node_path, path_dists = get_pathway(feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size)\n",
+                "node_path, path_dists = get_pathway(\n",
+                "    feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size\n",
+                ")\n",
                 "print(node_path)\n",
                 "image_pathway_plot(images_dir, image_type, node_path)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "id_1 = 'V0001893'\n",
-                "id_2 = 'V0047369EL'\n",
+                "id_1 = \"V0001893\"\n",
+                "id_2 = \"V0047369EL\"\n",
                 "\n",
                 "n_nodes = 9\n",
                 "sample_size = None\n",
                 "\n",
-                "node_path, path_dists = get_pathway(feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size)\n",
+                "node_path, path_dists = get_pathway(\n",
+                "    feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size\n",
+                ")\n",
                 "print(node_path)\n",
                 "image_pathway_plot(images_dir, image_type, node_path)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "id_1 = 'A0000001'\n",
-                "id_2 = 'B0006893'\n",
+                "id_1 = \"A0000001\"\n",
+                "id_2 = \"B0006893\"\n",
                 "\n",
                 "n_nodes = 8\n",
                 "sample_size = None\n",
                 "\n",
-                "node_path, path_dists = get_pathway(feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size)\n",
+                "node_path, path_dists = get_pathway(\n",
+                "    feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size\n",
+                ")\n",
                 "image_pathway_plot(images_dir, image_type, node_path)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -292,21 +308,23 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "id_1 = 'B0002621'\n",
-                "id_2 = 'V0010033'\n",
+                "id_1 = \"B0002621\"\n",
+                "id_2 = \"V0010033\"\n",
                 "\n",
                 "n_nodes = 10\n",
                 "sample_size = None\n",
                 "\n",
-                "node_path, path_dists = get_pathway(feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size)\n",
+                "node_path, path_dists = get_pathway(\n",
+                "    feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size\n",
+                ")\n",
                 "image_pathway_plot(images_dir, image_type, node_path)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -317,21 +335,23 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "id_1 = 'A0000785'\n",
-                "id_2 = 'V0040933'\n",
+                "id_1 = \"A0000785\"\n",
+                "id_2 = \"V0040933\"\n",
                 "\n",
                 "n_nodes = 8\n",
                 "sample_size = None\n",
                 "\n",
-                "node_path, path_dists = get_pathway(feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size)\n",
+                "node_path, path_dists = get_pathway(\n",
+                "    feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size\n",
+                ")\n",
                 "image_pathway_plot(images_dir, image_type, node_path)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -367,21 +387,23 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "id_1 = 'V0044783'\n",
-                "id_2 = 'V0046793'\n",
+                "id_1 = \"V0044783\"\n",
+                "id_2 = \"V0046793\"\n",
                 "\n",
                 "n_nodes = 8\n",
                 "sample_size = None\n",
                 "\n",
-                "node_path, path_dists = get_pathway(feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size)\n",
+                "node_path, path_dists = get_pathway(\n",
+                "    feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size\n",
+                ")\n",
                 "print(node_path)\n",
                 "image_pathway_plot(images_dir, image_type, node_path)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -404,19 +426,15 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "np.random.seed(0)\n",
-                "rand_args = np.random.choice(\n",
-                "                range(0, len(feature_vectors)),\n",
-                "                sample_size,\n",
-                "                replace = False\n",
-                "            )\n",
+                "rand_args = np.random.choice(range(0, len(feature_vectors)), sample_size, replace=False)\n",
                 "feature_vectors_sample = feature_vectors[rand_args]\n",
                 "feature_vectors_ids_sample = feature_vectors_ids[rand_args]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
@@ -437,26 +455,26 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "plt.hist(all_dists, bins = 50)\n",
+                "plt.hist(all_dists, bins=50)\n",
                 "plt.show()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# Get pairs with distances from an equally spaced dist, and plot in order of most similar to least\n",
-                "ideal_dists = np.linspace(min(all_dists), max(all_dists), num = 10)\n",
+                "ideal_dists = np.linspace(min(all_dists), max(all_dists), num=10)\n",
                 "# ideal_dists = np.linspace(40, 50, num = 10)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -468,20 +486,22 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "for ideal_dist in ideal_dists:\n",
-                "    pairs = feature_vectors_ids_sample[np.argwhere((dists<(ideal_dist+0.1)) & (dists>(ideal_dist-0.1)))]\n",
-                "    index = np.random.choice(pairs.shape[0])  \n",
+                "    pairs = feature_vectors_ids_sample[\n",
+                "        np.argwhere((dists < (ideal_dist + 0.1)) & (dists > (ideal_dist - 0.1)))\n",
+                "    ]\n",
+                "    index = np.random.choice(pairs.shape[0])\n",
                 "    pair = pairs[index]\n",
                 "    print(pair)\n",
                 "\n",
-                "    fig = plt.figure(figsize=(20,10))\n",
+                "    fig = plt.figure(figsize=(20, 10))\n",
                 "    columns = len(pair)\n",
                 "    for i, image_name in enumerate(pair):\n",
                 "        image = Image.open(images_dir + image_name + image_type)\n",
                 "        ax = plt.subplot(2, columns, i + 1)\n",
                 "        ax.set_axis_off()\n",
                 "        plt.imshow(image)\n",
                 "        image.close()"
@@ -497,27 +517,27 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# Remove duplicate [a, b] and [b, a] should be the same\n",
-                "v_different_pairs = feature_vectors_ids_sample[np.argwhere(dists>373)]\n",
-                "v_different_pairs = np.unique(np.sort(v_different_pairs), axis = 0)\n",
+                "v_different_pairs = feature_vectors_ids_sample[np.argwhere(dists > 373)]\n",
+                "v_different_pairs = np.unique(np.sort(v_different_pairs), axis=0)\n",
                 "v_different_pairs.shape"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "for figs in v_different_pairs:\n",
-                "    fig = plt.figure(figsize=(20,10))\n",
+                "    fig = plt.figure(figsize=(20, 10))\n",
                 "    columns = len(figs)\n",
                 "    for i, image_name in enumerate(figs):\n",
                 "        image = Image.open(images_dir + image_name + image_type)\n",
                 "        ax = plt.subplot(2, columns, i + 1)\n",
                 "        ax.set_axis_off()\n",
                 "        plt.imshow(image)\n",
                 "        image.close()"
@@ -525,16 +545,16 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "v_similar_pairs = feature_vectors_ids_sample[np.argwhere((dists<13) & (dists!=0))]\n",
-                "v_similar_pairs = np.unique(np.sort(v_similar_pairs), axis = 0)\n",
+                "v_similar_pairs = feature_vectors_ids_sample[np.argwhere((dists < 13) & (dists != 0))]\n",
+                "v_similar_pairs = np.unique(np.sort(v_similar_pairs), axis=0)\n",
                 "v_similar_pairs.shape"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -546,15 +566,15 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "for figs in v_similar_pairs:\n",
-                "    fig = plt.figure(figsize=(5,2))\n",
+                "    fig = plt.figure(figsize=(5, 2))\n",
                 "    columns = len(figs)\n",
                 "    for i, image_name in enumerate(figs):\n",
                 "        image = Image.open(images_dir + image_name + image_type)\n",
                 "        ax = plt.subplot(2, columns, i + 1)\n",
                 "        ax.set_axis_off()\n",
                 "        plt.imshow(image)\n",
                 "        image.close()"
@@ -562,16 +582,18 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "quite_similar_pairs = feature_vectors_ids_sample[np.argwhere((dists<35) & (dists>34.99))]\n",
-                "quite_similar_pairs = np.unique(np.sort(quite_similar_pairs), axis = 0)\n",
+                "quite_similar_pairs = feature_vectors_ids_sample[\n",
+                "    np.argwhere((dists < 35) & (dists > 34.99))\n",
+                "]\n",
+                "quite_similar_pairs = np.unique(np.sort(quite_similar_pairs), axis=0)\n",
                 "quite_similar_pairs.shape"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -583,15 +605,15 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "for figs in quite_similar_pairs:\n",
-                "    fig = plt.figure(figsize=(20,10))\n",
+                "    fig = plt.figure(figsize=(20, 10))\n",
                 "    columns = len(figs)\n",
                 "    for i, image_name in enumerate(figs):\n",
                 "        image = Image.open(images_dir + image_name + image_type)\n",
                 "        ax = plt.subplot(2, columns, i + 1)\n",
                 "        ax.set_axis_off()\n",
                 "        plt.imshow(image)\n",
                 "        image.close()"
@@ -606,30 +628,32 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "np.random.choice(feature_vectors_ids_sample,10)"
+                "np.random.choice(feature_vectors_ids_sample, 10)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "id_1 = 'V0001893'\n",
-                "id_2 = 'V0047369EL'\n",
+                "id_1 = \"V0001893\"\n",
+                "id_2 = \"V0047369EL\"\n",
                 "\n",
                 "n_nodes = 10\n",
                 "sample_size = None\n",
                 "\n",
-                "node_path, path_dists = get_pathway(feature_vectors_ids_sample, feature_vectors_sample, id_1, id_2, n_nodes, sample_size)\n",
+                "node_path, path_dists = get_pathway(\n",
+                "    feature_vectors_ids_sample, feature_vectors_sample, id_1, id_2, n_nodes, sample_size\n",
+                ")\n",
                 "image_pathway_plot(images_dir, image_type, node_path)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -640,15 +664,17 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "feature_vectors_sample_trans = {k:list(v) for k,v in zip(feature_vectors_ids_sample,feature_vectors_sample)}\n"
+                "feature_vectors_sample_trans = {\n",
+                "    k: list(v) for k, v in zip(feature_vectors_ids_sample, feature_vectors_sample)\n",
+                "}"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -663,34 +689,34 @@
             "outputs": [],
             "source": [
                 "visualize_scatter_with_images(\n",
                 "    x_data,\n",
                 "    image_name_list=feature_vectors_ids_sample,\n",
                 "    images_dir=images_dir,\n",
                 "    image_type=image_type,\n",
-                "    figsize=(30,20),\n",
+                "    figsize=(30, 20),\n",
                 "    image_zoom=0.1,\n",
-                "    pathway=node_path\n",
-                "    )"
+                "    pathway=node_path,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "visualize_scatter_with_images(\n",
                 "    x_data,\n",
                 "    image_name_list=feature_vectors_ids_sample,\n",
                 "    images_dir=images_dir,\n",
                 "    image_type=image_type,\n",
-                "    figsize=(30,20),\n",
-                "    image_zoom=0.13\n",
-                "    )"
+                "    figsize=(30, 20),\n",
+                "    image_zoom=0.13,\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "## Add extra images to the sample if there are specific images you want"
@@ -699,47 +725,70 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def add_new_images(\n",
-                "    id_1, id_2,\n",
-                "    feature_vectors_ids, feature_vectors,\n",
-                "    feature_vectors_ids_sample, feature_vectors_sample,\n",
-                "    n_nodes):\n",
-                "    \n",
+                "    id_1,\n",
+                "    id_2,\n",
+                "    feature_vectors_ids,\n",
+                "    feature_vectors,\n",
+                "    feature_vectors_ids_sample,\n",
+                "    feature_vectors_sample,\n",
+                "    n_nodes,\n",
+                "):\n",
+                "\n",
                 "    sample_size = None\n",
                 "\n",
-                "    node_path, path_dists = get_pathway(feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size)\n",
+                "    node_path, path_dists = get_pathway(\n",
+                "        feature_vectors_ids, feature_vectors, id_1, id_2, n_nodes, sample_size\n",
+                "    )\n",
                 "\n",
                 "    node_path_extra = [f for f in node_path if f not in feature_vectors_ids_sample]\n",
-                "    \n",
-                "    node_path_extra_index = [i for i, fv_id in enumerate(feature_vectors_ids) if fv_id in node_path_extra]\n",
-                "    \n",
-                "    feature_vectors_ids_sample_extra = np.concatenate((feature_vectors_ids_sample, np.array(feature_vectors_ids[node_path_extra_index])))\n",
-                "    feature_vectors_sample_extra = np.concatenate((feature_vectors_sample, feature_vectors[node_path_extra_index]))\n",
-                "    \n",
-                "    feature_vectors_sample_extra_trans = {k:list(v) for k,v in zip(feature_vectors_ids_sample_extra, feature_vectors_sample_extra)}\n",
+                "\n",
+                "    node_path_extra_index = [\n",
+                "        i for i, fv_id in enumerate(feature_vectors_ids) if fv_id in node_path_extra\n",
+                "    ]\n",
+                "\n",
+                "    feature_vectors_ids_sample_extra = np.concatenate(\n",
+                "        (\n",
+                "            feature_vectors_ids_sample,\n",
+                "            np.array(feature_vectors_ids[node_path_extra_index]),\n",
+                "        )\n",
+                "    )\n",
+                "    feature_vectors_sample_extra = np.concatenate(\n",
+                "        (feature_vectors_sample, feature_vectors[node_path_extra_index])\n",
+                "    )\n",
+                "\n",
+                "    feature_vectors_sample_extra_trans = {\n",
+                "        k: list(v)\n",
+                "        for k, v in zip(feature_vectors_ids_sample_extra, feature_vectors_sample_extra)\n",
+                "    }\n",
                 "\n",
                 "    x_data = reduce_data_nd(feature_vectors_sample_extra_trans)\n",
-                "    \n",
+                "\n",
                 "    return node_path, x_data, feature_vectors_ids_sample_extra"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "node_path, x_data, feature_vectors_ids_sample_extra =add_new_images(\n",
-                "    'V0044783', 'V0046793',\n",
-                "    feature_vectors_ids, feature_vectors,\n",
-                "    feature_vectors_ids_sample, feature_vectors_sample, 8)"
+                "node_path, x_data, feature_vectors_ids_sample_extra = add_new_images(\n",
+                "    \"V0044783\",\n",
+                "    \"V0046793\",\n",
+                "    feature_vectors_ids,\n",
+                "    feature_vectors,\n",
+                "    feature_vectors_ids_sample,\n",
+                "    feature_vectors_sample,\n",
+                "    8,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -754,30 +803,35 @@
             "outputs": [],
             "source": [
                 "visualize_scatter_with_images(\n",
                 "    x_data,\n",
                 "    image_name_list=feature_vectors_ids_sample_extra,\n",
                 "    images_dir=images_dir,\n",
                 "    image_type=image_type,\n",
-                "    figsize=(30,20),\n",
+                "    figsize=(30, 20),\n",
                 "    image_zoom=0.1,\n",
-                "    pathway=node_path\n",
-                "    )"
+                "    pathway=node_path,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "node_path, x_data, feature_vectors_ids_sample_extra =add_new_images(\n",
-                "    'B0008895', 'M0010374',\n",
-                "    feature_vectors_ids, feature_vectors,\n",
-                "    feature_vectors_ids_sample, feature_vectors_sample, 9)\n"
+                "node_path, x_data, feature_vectors_ids_sample_extra = add_new_images(\n",
+                "    \"B0008895\",\n",
+                "    \"M0010374\",\n",
+                "    feature_vectors_ids,\n",
+                "    feature_vectors,\n",
+                "    feature_vectors_ids_sample,\n",
+                "    feature_vectors_sample,\n",
+                "    9,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -793,18 +847,18 @@
             "outputs": [],
             "source": [
                 "visualize_scatter_with_images(\n",
                 "    x_data,\n",
                 "    image_name_list=feature_vectors_ids_sample_extra,\n",
                 "    images_dir=images_dir,\n",
                 "    image_type=image_type,\n",
-                "    figsize=(30,20),\n",
+                "    figsize=(30, 20),\n",
                 "    image_zoom=0.1,\n",
-                "    pathway=node_path\n",
-                "    )"
+                "    pathway=node_path,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
```

### Comparing `weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/medium_blog_images.ipynb` & `weco-datascience-0.1.9/notebooks/image_pathways/notebooks/medium_blog_images.ipynb`

 * *Files 19% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9928445667084089%*

 * *Differences: {"'cells'": "{2: {'source': {insert: [(0, 'LABELS_URL = "*

 * *            '"https://s3.amazonaws.com/outcome-blog/imagenet/labels.json"\\n\')], delete: [0]}}, '*

 * *            '4: {\'source\': [\'bucket_name = "wellcomecollection-miro-images-public"\']}, 5: '*

 * *            '{\'source\': {insert: [(0, \'sts = boto3.client("sts")\\n\'), (2, \'    '*

 * *            'RoleArn="arn:aws:iam::760097843905:role/calm-assumable_read_role",\\n\'), (3, \'    '*

 * *            'RoleSessionName="AssumeRoleSession1",\\n\'), (5, \'credentia […]*

```diff
@@ -39,15 +39,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "LABELS_URL = 'https://s3.amazonaws.com/outcome-blog/imagenet/labels.json'\n",
+                "LABELS_URL = \"https://s3.amazonaws.com/outcome-blog/imagenet/labels.json\"\n",
                 "\n",
                 "# Let's get our class labels for this model.\n",
                 "response = requests.get(LABELS_URL)  # Make an HTTP GET request and store the response.\n",
                 "labels = {int(key): value for key, value in response.json().items()}"
             ]
         },
         {
@@ -59,77 +59,71 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "bucket_name = 'wellcomecollection-miro-images-public'"
+                "bucket_name = \"wellcomecollection-miro-images-public\""
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "sts = boto3.client('sts')\n",
+                "sts = boto3.client(\"sts\")\n",
                 "assumed_role_object = sts.assume_role(\n",
-                "    RoleArn='arn:aws:iam::760097843905:role/calm-assumable_read_role',\n",
-                "    RoleSessionName='AssumeRoleSession1'\n",
+                "    RoleArn=\"arn:aws:iam::760097843905:role/calm-assumable_read_role\",\n",
+                "    RoleSessionName=\"AssumeRoleSession1\",\n",
                 ")\n",
-                "credentials = assumed_role_object['Credentials']"
+                "credentials = assumed_role_object[\"Credentials\"]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "s3_fetch = boto3.resource('s3',\n",
-                "    aws_access_key_id=credentials['AccessKeyId'],\n",
-                "    aws_secret_access_key=credentials['SecretAccessKey'],\n",
-                "    aws_session_token=credentials['SessionToken']\n",
+                "s3_fetch = boto3.resource(\n",
+                "    \"s3\",\n",
+                "    aws_access_key_id=credentials[\"AccessKeyId\"],\n",
+                "    aws_secret_access_key=credentials[\"SecretAccessKey\"],\n",
+                "    aws_session_token=credentials[\"SessionToken\"],\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "bucket = s3_fetch.Bucket(bucket_name)\n",
-                "bucket_info = bucket.meta.client.list_objects(\n",
-                "    Bucket=bucket.name,\n",
-                "    Delimiter='/'\n",
-                "    )"
+                "bucket_info = bucket.meta.client.list_objects(Bucket=bucket.name, Delimiter=\"/\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# Get all folder names.\n",
-                "folder_names = [\n",
-                "    f['Prefix'] for f in bucket_info.get('CommonPrefixes')\n",
-                "    ]\n",
-                "print(\"{} image folders\".format(len(folder_names))) # 219\n",
+                "folder_names = [f[\"Prefix\"] for f in bucket_info.get(\"CommonPrefixes\")]\n",
+                "print(\"{} image folders\".format(len(folder_names)))  # 219\n",
                 "\n",
                 "# Get all file dirs from all folders. Takes a minute or so\n",
                 "print(\"Getting all file dir names for all images...\")\n",
                 "file_dirs = []\n",
                 "for folder_name in tqdm(folder_names):\n",
-                "    file_dirs.extend(\n",
-                "        [s.key for s in bucket.objects.filter(Prefix=folder_name)]\n",
-                "    )\n",
-                "print(\"{} image files\".format(len(file_dirs))) # 120589"
+                "    file_dirs.extend([s.key for s in bucket.objects.filter(Prefix=folder_name)])\n",
+                "print(\"{} image files\".format(len(file_dirs)))  # 120589"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### Get one image, or an high res image path"
@@ -137,272 +131,321 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_name = \"V0001893\" # ['V0001893', 'V0047369EL']\n",
+                "image_name = \"V0001893\"  # ['V0001893', 'V0047369EL']\n",
                 "file_dir = [f for f in file_dirs if image_name in f][0]\n",
                 "\n",
-                "obj = s3_fetch.Object(\n",
-                "        bucket_name,\n",
-                "        file_dir\n",
-                "        )\n",
-                "im = Image.open(BytesIO(obj.get()['Body'].read()))\n",
+                "obj = s3_fetch.Object(bucket_name, file_dir)\n",
+                "im = Image.open(BytesIO(obj.get()[\"Body\"].read()))\n",
                 "file_name = os.path.splitext(os.path.basename(file_dir))[0]\n",
                 "im\n",
                 "im.save(\"../medium_blog_images/{}.png\".format(file_name))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_names = [os.path.splitext(os.path.basename(f))[0] for f in np.random.choice(file_dirs, 11)]+[\"A0000001\"]"
+                "image_names = [\n",
+                "    os.path.splitext(os.path.basename(f))[0] for f in np.random.choice(file_dirs, 11)\n",
+                "] + [\"A0000001\"]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "image_names = [\n",
-                "    'L0061160',\n",
-                "     'L0038847',\n",
-                "     'B0006893',\n",
-                "     'V0010192EL',\n",
-                "     'V0025035',\n",
-                "     'L0052856',\n",
-                "     'V0050358',\n",
-                "     'L0008713',\n",
-                "     'V0007884EL',\n",
-                "     'M0012095',\n",
-                "     'V0010104',\n",
-                "     'A0000001']"
+                "    \"L0061160\",\n",
+                "    \"L0038847\",\n",
+                "    \"B0006893\",\n",
+                "    \"V0010192EL\",\n",
+                "    \"V0025035\",\n",
+                "    \"L0052856\",\n",
+                "    \"V0050358\",\n",
+                "    \"L0008713\",\n",
+                "    \"V0007884EL\",\n",
+                "    \"M0012095\",\n",
+                "    \"V0010104\",\n",
+                "    \"A0000001\",\n",
+                "]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "images = {}\n",
-                "plot_images_sizes =[]\n",
+                "plot_images_sizes = []\n",
                 "for image_name in tqdm(image_names):\n",
                 "    file = [f for f in file_dirs if image_name in f][0]\n",
                 "    obj = s3_fetch.Object(bucket_name, file)\n",
-                "    im = Image.open(BytesIO(obj.get()['Body'].read()))  \n",
+                "    im = Image.open(BytesIO(obj.get()[\"Body\"].read()))\n",
                 "    if im.mode != \"RGB\":\n",
-                "        im = im.convert('RGB')\n",
+                "        im = im.convert(\"RGB\")\n",
                 "    im.thumbnail((224, 224), resample=Image.BICUBIC)\n",
                 "    plot_images_sizes.append(im.size)\n",
                 "    images[image_name] = im"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# Multi-row image\n",
                 "\n",
-                "fig = plt.figure(figsize=(20,10))\n",
+                "fig = plt.figure(figsize=(20, 10))\n",
                 "columns = 6\n",
                 "for i, (image_name, im) in enumerate(images.items()):\n",
-                "    ax = plt.subplot(np.ceil(len(image_names)/columns), columns, i + 1)\n",
-                "    #plt.title(image_name)\n",
+                "    ax = plt.subplot(np.ceil(len(image_names) / columns), columns, i + 1)\n",
+                "    # plt.title(image_name)\n",
                 "    ax.set_axis_off()\n",
                 "    plt.imshow(im)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def print_path(image_names, columns=len(image_names)):\n",
-                "    \n",
+                "\n",
                 "    images = {}\n",
-                "    plot_images_sizes =[]\n",
+                "    plot_images_sizes = []\n",
                 "    for image_name in tqdm(image_names):\n",
                 "        file = [f for f in file_dirs if image_name in f][0]\n",
                 "        obj = s3_fetch.Object(bucket_name, file)\n",
-                "        im = Image.open(BytesIO(obj.get()['Body'].read()))  \n",
+                "        im = Image.open(BytesIO(obj.get()[\"Body\"].read()))\n",
                 "        if im.mode != \"RGB\":\n",
-                "            im = im.convert('RGB')\n",
+                "            im = im.convert(\"RGB\")\n",
                 "        im.thumbnail((224, 224), resample=Image.BICUBIC)\n",
                 "        plot_images_sizes.append(im.size)\n",
                 "        images[image_name] = im\n",
-                "    \n",
+                "\n",
                 "    max_y = max([c[1] for c in plot_images_sizes])\n",
-                "    rescale_x = [c[0]*max_y/c[1] for c in plot_images_sizes]\n",
+                "    rescale_x = [c[0] * max_y / c[1] for c in plot_images_sizes]\n",
                 "    columns = len(image_names)\n",
-                "    fig = plt.figure(figsize=(20,30))\n",
-                "    gs = gridspec.GridSpec(1, columns, width_ratios=rescale_x) \n",
+                "    fig = plt.figure(figsize=(20, 30))\n",
+                "    gs = gridspec.GridSpec(1, columns, width_ratios=rescale_x)\n",
                 "\n",
                 "    for i, (image_name, im) in enumerate(images.items()):\n",
                 "        ax = plt.subplot(gs[i])\n",
                 "        ax.set_axis_off()\n",
                 "        plt.imshow(im)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_names = ['B0008895', 'N0021591', 'B0007199', 'A0001358', 'V0007108', 'V0036001', 'V0037737', 'V0026902EL', 'M0010374']\n",
+                "image_names = [\n",
+                "    \"B0008895\",\n",
+                "    \"N0021591\",\n",
+                "    \"B0007199\",\n",
+                "    \"A0001358\",\n",
+                "    \"V0007108\",\n",
+                "    \"V0036001\",\n",
+                "    \"V0037737\",\n",
+                "    \"V0026902EL\",\n",
+                "    \"M0010374\",\n",
+                "]\n",
                 "print_path(image_names)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_names = ['V0001893', 'V0047369EL']\n",
+                "image_names = [\"V0001893\", \"V0047369EL\"]\n",
                 "print_path(image_names, 6)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_names = ['B0000663','V0014173']\n",
+                "image_names = [\"B0000663\", \"V0014173\"]\n",
                 "print_path(image_names, 6)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_names = ['L0078444', 'L0078481']\n",
+                "image_names = [\"L0078444\", \"L0078481\"]\n",
                 "print_path(image_names, 6)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_names = ['V0003760', 'V0006594']\n",
+                "image_names = [\"V0003760\", \"V0006594\"]\n",
                 "print_path(image_names, 6)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# One row image\n",
-                "image_names = ['A0000001',\n",
-                " 'A0000002',\n",
-                " 'A0000003',\n",
-                " 'A0001260',\n",
-                " 'B0007248',\n",
-                " 'B0004589',\n",
-                " 'B0004848',\n",
-                " 'B0006893']\n",
-                "print_path(image_names)\n"
+                "image_names = [\n",
+                "    \"A0000001\",\n",
+                "    \"A0000002\",\n",
+                "    \"A0000003\",\n",
+                "    \"A0001260\",\n",
+                "    \"B0007248\",\n",
+                "    \"B0004589\",\n",
+                "    \"B0004848\",\n",
+                "    \"B0006893\",\n",
+                "]\n",
+                "print_path(image_names)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_names = ['A0000785',\n",
-                " 'B0001152',\n",
-                " 'A0000318',\n",
-                " 'V0007884EL',\n",
-                " 'L0027241',\n",
-                " 'V0013859',\n",
-                " 'V0013040',\n",
-                " 'V0040933']\n",
+                "image_names = [\n",
+                "    \"A0000785\",\n",
+                "    \"B0001152\",\n",
+                "    \"A0000318\",\n",
+                "    \"V0007884EL\",\n",
+                "    \"L0027241\",\n",
+                "    \"V0013859\",\n",
+                "    \"V0013040\",\n",
+                "    \"V0040933\",\n",
+                "]\n",
                 "print_path(image_names)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_names = ['V0044783',\n",
-                " 'V0022904ER',\n",
-                " 'V0021741',\n",
-                " 'V0021867',\n",
-                " 'V0021857',\n",
-                " 'V0023111',\n",
-                " 'V0023117']\n",
+                "image_names = [\n",
+                "    \"V0044783\",\n",
+                "    \"V0022904ER\",\n",
+                "    \"V0021741\",\n",
+                "    \"V0021867\",\n",
+                "    \"V0021857\",\n",
+                "    \"V0023111\",\n",
+                "    \"V0023117\",\n",
+                "]\n",
                 "print_path(image_names)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_names = ['V0044783', 'V0022904ER', 'V0021741', 'A0000113', 'B0004207', 'V0043888', 'V0023376', 'V0046793']\n",
+                "image_names = [\n",
+                "    \"V0044783\",\n",
+                "    \"V0022904ER\",\n",
+                "    \"V0021741\",\n",
+                "    \"A0000113\",\n",
+                "    \"B0004207\",\n",
+                "    \"V0043888\",\n",
+                "    \"V0023376\",\n",
+                "    \"V0046793\",\n",
+                "]\n",
                 "print_path(image_names)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_names = ['V0001893',\n",
-                "#  'V0003665',\n",
-                " 'V0026311',\n",
-                " 'V0031656',\n",
-                " 'V0007101ER',\n",
-                " 'L0027175',\n",
-                " 'V0042795EL',\n",
-                " 'V0044410',\n",
-                " 'V0042799EL',\n",
-                " 'V0047369EL']\n",
+                "image_names = [\n",
+                "    \"V0001893\",\n",
+                "    #  'V0003665',\n",
+                "    \"V0026311\",\n",
+                "    \"V0031656\",\n",
+                "    \"V0007101ER\",\n",
+                "    \"L0027175\",\n",
+                "    \"V0042795EL\",\n",
+                "    \"V0044410\",\n",
+                "    \"V0042799EL\",\n",
+                "    \"V0047369EL\",\n",
+                "]\n",
                 "print_path(image_names)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_names = ['L0032287', 'M0012716', 'V0049671', 'M0006130', 'L0040595', 'L0056834', 'V0030245', 'V0029003', 'L0034782', 'A0000632']\n",
+                "image_names = [\n",
+                "    \"L0032287\",\n",
+                "    \"M0012716\",\n",
+                "    \"V0049671\",\n",
+                "    \"M0006130\",\n",
+                "    \"L0040595\",\n",
+                "    \"L0056834\",\n",
+                "    \"V0030245\",\n",
+                "    \"V0029003\",\n",
+                "    \"L0034782\",\n",
+                "    \"A0000632\",\n",
+                "]\n",
                 "print_path(image_names)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_names = ['L0045857', 'L0045858', 'L0045856', 'L0045886', 'V0005269', 'V0032946ER', 'V0033137EL', 'V0035635ER', 'V0035703', 'V0035629ER']\n",
+                "image_names = [\n",
+                "    \"L0045857\",\n",
+                "    \"L0045858\",\n",
+                "    \"L0045856\",\n",
+                "    \"L0045886\",\n",
+                "    \"V0005269\",\n",
+                "    \"V0032946ER\",\n",
+                "    \"V0033137EL\",\n",
+                "    \"V0035635ER\",\n",
+                "    \"V0035703\",\n",
+                "    \"V0035629ER\",\n",
+                "]\n",
                 "print_path(image_names)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -418,19 +461,24 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "min_img_size = 224  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
-                "transform_pipeline = transforms.Compose([transforms.Resize(min_img_size),\n",
-                "                                         transforms.ToTensor(),\n",
-                "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
-                "                                                              std=[0.229, 0.224, 0.225])])"
+                "min_img_size = (\n",
+                "    224  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
+                ")\n",
+                "transform_pipeline = transforms.Compose(\n",
+                "    [\n",
+                "        transforms.Resize(min_img_size),\n",
+                "        transforms.ToTensor(),\n",
+                "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
+                "    ]\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -442,49 +490,46 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def predict_image(transform_pipeline, im, model, labels):\n",
-                "    \n",
+                "\n",
                 "    img = transform_pipeline(im)\n",
                 "    img = img.unsqueeze(0)\n",
-                "    \n",
+                "\n",
                 "    # Now let's get a prediction!\n",
                 "    prediction = model(img)  # Returns a Tensor of shape (batch, num class labels)\n",
                 "    return labels[prediction.data.numpy().argmax()]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "image_name = \"B0008895\"\n",
                 "file_dir = [f for f in file_dirs if image_name in f][0]\n",
                 "\n",
-                "obj = s3_fetch.Object(\n",
-                "        bucket_name,\n",
-                "        file_dir\n",
-                "        )\n",
-                "im = Image.open(BytesIO(obj.get()['Body'].read()))\n",
+                "obj = s3_fetch.Object(bucket_name, file_dir)\n",
+                "im = Image.open(BytesIO(obj.get()[\"Body\"].read()))\n",
                 "if im.mode != \"RGB\":\n",
-                "    im = im.convert('RGB')"
+                "    im = im.convert(\"RGB\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "print(predict_image(transform_pipeline, im, vgg16, labels))\n",
-                "im.resize((200,200), resample= Image.BILINEAR)"
+                "im.resize((200, 200), resample=Image.BILINEAR)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -504,41 +549,44 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# Create all the images transforms\n",
-                "min_img_size_fv = 224, 224  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
-                "transform_pipeline_fv = transforms.Compose([transforms.Resize(min_img_size_fv),\n",
-                "                                         transforms.ToTensor(),\n",
-                "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
-                "                                                              std=[0.229, 0.224, 0.225])])\n",
+                "min_img_size_fv = (\n",
+                "    224,\n",
+                "    224,\n",
+                ")  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
+                "transform_pipeline_fv = transforms.Compose(\n",
+                "    [\n",
+                "        transforms.Resize(min_img_size_fv),\n",
+                "        transforms.ToTensor(),\n",
+                "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
+                "    ]\n",
+                ")\n",
                 "# Remove the last layer from the model, so that the output will be a feature vector\n",
                 "vgg16_short = vgg16\n",
                 "vgg16_short.classifier = vgg16.classifier[:4]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "image_name = \"M0010374\"\n",
                 "file_dir = [f for f in file_dirs if image_name in f][0]\n",
                 "\n",
-                "obj = s3_fetch.Object(\n",
-                "        bucket_name,\n",
-                "        file_dir\n",
-                "        )\n",
-                "im = Image.open(BytesIO(obj.get()['Body'].read()))\n",
+                "obj = s3_fetch.Object(bucket_name, file_dir)\n",
+                "im = Image.open(BytesIO(obj.get()[\"Body\"].read()))\n",
                 "if im.mode != \"RGB\":\n",
-                "    im = im.convert('RGB')\n",
-                "    \n",
+                "    im = im.convert(\"RGB\")\n",
+                "\n",
                 "img = transform_pipeline(im)\n",
                 "img = img.unsqueeze(0)\n",
                 "vgg16_short(img)"
             ]
         },
         {
             "cell_type": "code",
```

### Comparing `weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/old_Make_network_testing.ipynb` & `weco-datascience-0.1.9/notebooks/image_pathways/notebooks/old_Make_network_testing.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9964923808964506%*

 * *Differences: {"'cells'": "{0: {'source': {insert: [(3, 'import ast\\n')], delete: [3]}}, 2: {'source': {insert: "*

 * *            '[(5, \'    kwargs = {"Bucket": bucket}\\n\'), (8, \'        for obj in '*

 * *            'resp["Contents"]:\\n\'), (9, \'            keys.append(obj["Key"])\\n\'), (12, '*

 * *            '\'            kwargs["ContinuationToken"] = resp["NextContinuationToken"]\\n\')], '*

 * *            "delete: [12, 9, 8, 5]}}, 3: {'source': {insert: [(0, 'bucket_name = "*

 * *            '"miro-images-feature-vectors"\\n\'), ( […]*

```diff
@@ -5,15 +5,15 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "from tqdm import tqdm\n",
                 "import os\n",
                 "from io import BytesIO\n",
-                "import ast \n",
+                "import ast\n",
                 "import numpy as np\n",
                 "import pickle\n",
                 "\n",
                 "import torch\n",
                 "import boto3\n",
                 "from scipy.spatial.distance import cdist\n",
                 "import networkx as nx\n",
@@ -37,36 +37,36 @@
             "outputs": [],
             "source": [
                 "# https://alexwlchan.net/2017/07/listing-s3-keys/\n",
                 "def get_all_s3_keys(bucket):\n",
                 "    \"\"\"Get a list of all keys in an S3 bucket.\"\"\"\n",
                 "    keys = []\n",
                 "\n",
-                "    kwargs = {'Bucket': bucket}\n",
+                "    kwargs = {\"Bucket\": bucket}\n",
                 "    while True:\n",
                 "        resp = s3.list_objects_v2(**kwargs)\n",
-                "        for obj in resp['Contents']:\n",
-                "            keys.append(obj['Key'])\n",
+                "        for obj in resp[\"Contents\"]:\n",
+                "            keys.append(obj[\"Key\"])\n",
                 "\n",
                 "        try:\n",
-                "            kwargs['ContinuationToken'] = resp['NextContinuationToken']\n",
+                "            kwargs[\"ContinuationToken\"] = resp[\"NextContinuationToken\"]\n",
                 "        except KeyError:\n",
                 "            break\n",
                 "\n",
                 "    return keys"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "bucket_name = 'miro-images-feature-vectors'\n",
-                "s3 = boto3.client('s3')\n",
+                "bucket_name = \"miro-images-feature-vectors\"\n",
+                "s3 = boto3.client(\"s3\")\n",
                 "keys = get_all_s3_keys(bucket_name)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -78,15 +78,15 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "folder_name = \"reduced_feature_vectors_20_dims\"\n",
-                "keys = [k for k in keys if k.split(\"/\")[0]==folder_name]"
+                "keys = [k for k in keys if k.split(\"/\")[0] == folder_name]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -98,23 +98,18 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "feature_vectors = {}\n",
                 "for key in tqdm(keys):\n",
-                "    obj = s3.get_object(\n",
-                "            Bucket=bucket_name,\n",
-                "            Key=key\n",
-                "        )\n",
-                "    read_obj = obj['Body'].read()\n",
-                "    \n",
-                "    feature_vectors[key] = np.frombuffer(\n",
-                "                            read_obj, dtype=np.float\n",
-                "                            )"
+                "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
+                "    read_obj = obj[\"Body\"].read()\n",
+                "\n",
+                "    feature_vectors[key] = np.frombuffer(read_obj, dtype=np.float)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### 2. Get the distances between feature vectors"
@@ -131,15 +126,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "[v for f in feature_vectors_list for v in f if 'nan' in str(v)]"
+                "[v for f in feature_vectors_list for v in f if \"nan\" in str(v)]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -159,30 +154,30 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "dist_mat_top = np.zeros_like(dist_mat)\n",
-                "dist_mat_top[:]=None"
+                "dist_mat_top[:] = None"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "n = 3\n",
                 "\n",
                 "# Find the top n neighbours for each image\n",
                 "\n",
                 "for i, _ in tqdm(enumerate(keys)):\n",
                 "    arr = dist_mat[i].argsort()\n",
-                "    top_args = arr[arr!=i]\n",
+                "    top_args = arr[arr != i]\n",
                 "    dist_mat_top[i][top_args[0:n]] = dist_mat[i][top_args[0:n]]\n",
                 "    for j in top_args[0:n]:\n",
                 "        dist_mat_top[j][i] = dist_mat[j][i]"
             ]
         },
         {
             "cell_type": "markdown",
@@ -193,17 +188,17 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open('../data/processed_images_sample.pkl', 'rb') as handle:\n",
+                "with open(\"../data/processed_images_sample.pkl\", \"rb\") as handle:\n",
                 "    images_original = pickle.load(handle)\n",
-                "    \n",
+                "\n",
                 "# Put in the same order as the feature vectors\n",
                 "images = []\n",
                 "for key in feature_vectors.keys():\n",
                 "    image_key = os.path.basename(key)\n",
                 "    images.append(images_original[image_key])"
             ]
         },
@@ -226,68 +221,68 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def inv_rel_norm(value, min_val, max_val):\n",
-                "    value = (value - min_val)/(max_val - min_val)\n",
-                "    value = 1/(value+1e-8)\n",
+                "    value = (value - min_val) / (max_val - min_val)\n",
+                "    value = 1 / (value + 1e-8)\n",
                 "    return value"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def create_graph(dist_mat_top):\n",
-                "    \n",
+                "\n",
                 "    min_val = np.nanmin(dist_mat_top)\n",
                 "    max_val = np.nanmax(dist_mat_top)\n",
-                "    \n",
-                "    nodes = list(range(0,len(dist_mat_top[0])))\n",
+                "\n",
+                "    nodes = list(range(0, len(dist_mat_top[0])))\n",
                 "\n",
                 "    G = nx.Graph()\n",
                 "    G.add_nodes_from(nodes)\n",
                 "\n",
                 "    # Put the weights in as the distances\n",
                 "    # only inc nodes if they are in the closest related neighbours\n",
                 "    for start, end in list(combinations(nodes, 2)):\n",
                 "        if ~np.isnan(dist_mat_top[start, end]):\n",
-                "            # Since in the plot a higher weight makes the nodes closer, \n",
+                "            # Since in the plot a higher weight makes the nodes closer,\n",
                 "            # but a higher value in the distance matrix means the images are further away,\n",
                 "            # we need to inverse the weight (so higher = closer)\n",
                 "            G.add_edge(\n",
                 "                start,\n",
                 "                end,\n",
-                "                weight=inv_rel_norm(dist_mat_top[start, end], min_val, max_val)\n",
+                "                weight=inv_rel_norm(dist_mat_top[start, end], min_val, max_val),\n",
                 "            )\n",
                 "    return G"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def plot_graph(G, image_names=None):\n",
-                "    \n",
+                "\n",
                 "    pos = nx.spring_layout(G)\n",
                 "\n",
-                "    plt.figure(3,figsize=(10,10)) \n",
+                "    plt.figure(3, figsize=(10, 10))\n",
                 "    nx.draw(G, pos, node_size=10)\n",
                 "    for p in pos:  # raise text positions\n",
                 "        pos[p][1] += 0.06\n",
                 "    if image_names:\n",
-                "        image_names_dict = {k:str(k)+\" \"+v for k,v in enumerate(image_names)}\n",
+                "        image_names_dict = {k: str(k) + \" \" + v for k, v in enumerate(image_names)}\n",
                 "        nx.draw_networkx_labels(G, pos, labels=image_names_dict)\n",
-                "    plt.show()\n"
+                "    plt.show()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -325,47 +320,44 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# from https://www.kaggle.com/gaborvecsei/plants-t-sne\n",
-                "def visualize_scatter_with_images(X_2d_data, images, figsize=(45,45), image_zoom=1):\n",
+                "def visualize_scatter_with_images(X_2d_data, images, figsize=(45, 45), image_zoom=1):\n",
                 "    fig, ax = plt.subplots(figsize=figsize)\n",
                 "    artists = []\n",
                 "    for xy, i in zip(X_2d_data, images):\n",
                 "        x0, y0 = xy\n",
                 "        img = OffsetImage(i, zoom=image_zoom)\n",
-                "        ab = AnnotationBbox(img, (x0, y0), xycoords='data', frameon=False)\n",
+                "        ab = AnnotationBbox(img, (x0, y0), xycoords=\"data\", frameon=False)\n",
                 "        artists.append(ax.add_artist(ab))\n",
                 "    ax.update_datalim(X_2d_data)\n",
                 "    ax.autoscale()\n",
-                "    plt.axis('off')\n",
+                "    plt.axis(\"off\")\n",
                 "    plt.show()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "x_data = [[a, b] for (a,b) in zip(embedding_fv[:, 0], embedding_fv[:, 1])]"
+                "x_data = [[a, b] for (a, b) in zip(embedding_fv[:, 0], embedding_fv[:, 1])]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "visualize_scatter_with_images(\n",
-                "    x_data,\n",
-                "    images = images,\n",
-                "    image_zoom=0.3)"
+                "visualize_scatter_with_images(x_data, images=images, image_zoom=0.3)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### 5a. Pick 2 images and look at the route between them"
@@ -373,15 +365,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_names_dict = {k:v for k,v in enumerate(images)}"
+                "image_names_dict = {k: v for k, v in enumerate(images)}"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -390,15 +382,15 @@
                 "node2 = np.random.choice(list(image_names_dict))\n",
                 "\n",
                 "node_path = nx.dijkstra_path(G, node1, node2, weight=None)\n",
                 "print(node_path)\n",
                 "\n",
                 "show_images = [images[i] for i in node_path]\n",
                 "\n",
-                "fig = plt.figure(figsize=(20,10))\n",
+                "fig = plt.figure(figsize=(20, 10))\n",
                 "columns = len(show_images)\n",
                 "for i, image in enumerate(show_images):\n",
                 "    ax = plt.subplot(len(show_images) / columns + 1, columns, i + 1)\n",
                 "    ax.set_axis_off()\n",
                 "    plt.imshow(image)"
             ]
         },
@@ -453,24 +445,23 @@
                 "node_path_b = nx.dijkstra_path(G, node2, node3, weight=None)\n",
                 "node_path_c = nx.dijkstra_path(G, node3, node1, weight=None)\n",
                 "node_path_3 = node_path_a[:-1] + node_path_b[:-1] + node_path_c\n",
                 "print(node_path_3)\n",
                 "\n",
                 "show_images = [images[i] for i in node_path_3]\n",
                 "\n",
-                "fig = plt.figure(figsize=(20,10))\n",
+                "fig = plt.figure(figsize=(20, 10))\n",
                 "columns = len(show_images)\n",
                 "for i, image in enumerate(show_images):\n",
                 "    ax = plt.subplot(len(show_images) / columns + 1, columns, i + 1)\n",
-                "    \n",
+                "\n",
                 "    if node_path_3[i] in [node1, node2, node3]:\n",
-                "        ax.set(title='NODE')\n",
-                "    ax.set_axis_off()    \n",
-                "    plt.imshow(image)\n",
-                "    "
+                "        ax.set(title=\"NODE\")\n",
+                "    ax.set_axis_off()\n",
+                "    plt.imshow(image)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### 6. Plot route on graph"
@@ -480,57 +471,47 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# from https://www.kaggle.com/gaborvecsei/plants-t-sne\n",
                 "def visualize_scatter_pathway_with_images(\n",
-                "        X_2d_data,\n",
-                "        pathway,\n",
-                "        images,\n",
-                "        figsize=(45,45),\n",
-                "        image_zoom=1\n",
-                "    ):\n",
+                "    X_2d_data, pathway, images, figsize=(45, 45), image_zoom=1\n",
+                "):\n",
                 "    fig, ax = plt.subplots(figsize=figsize)\n",
-                "    \n",
+                "\n",
                 "    x_path = [x_data[c][0] for c in node_path]\n",
                 "    y_path = [x_data[c][1] for c in node_path]\n",
                 "\n",
                 "    artists = []\n",
                 "    for num, (xy, i) in enumerate(zip(X_2d_data, images)):\n",
                 "        x0, y0 = xy\n",
                 "        if num in pathway:\n",
-                "            img = OffsetImage(i, zoom=image_zoom*2, alpha = 0.8)\n",
+                "            img = OffsetImage(i, zoom=image_zoom * 2, alpha=0.8)\n",
                 "        else:\n",
-                "            img = OffsetImage(i, zoom=image_zoom,alpha = 0.2)\n",
-                "        ab = AnnotationBbox(img, (x0, y0), xycoords='data', frameon=False)\n",
+                "            img = OffsetImage(i, zoom=image_zoom, alpha=0.2)\n",
+                "        ab = AnnotationBbox(img, (x0, y0), xycoords=\"data\", frameon=False)\n",
                 "        artists.append(ax.add_artist(ab))\n",
                 "    ax.update_datalim(X_2d_data)\n",
                 "    ax.autoscale()\n",
-                "    plt.plot(x_path, y_path, 'ro-', linewidth=5)\n",
-                "    plt.axis('off')\n",
-                "    \n",
-                "    plt.show()\n",
-                "    \n",
-                "    "
+                "    plt.plot(x_path, y_path, \"ro-\", linewidth=5)\n",
+                "    plt.axis(\"off\")\n",
+                "\n",
+                "    plt.show()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "visualize_scatter_pathway_with_images(\n",
-                "    x_data,\n",
-                "    node_path,\n",
-                "    images = images,\n",
-                "    figsize=(30,30),\n",
-                "    image_zoom=0.3\n",
-                "    )"
+                "    x_data, node_path, images=images, figsize=(30, 30), image_zoom=0.3\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
```

### Comparing `weco-datascience-0.1.8/research_notebooks/image_pathways/notebooks/old_basic_flow_s3sample.ipynb` & `weco-datascience-0.1.9/notebooks/image_pathways/notebooks/old_basic_flow_s3sample.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9955482704445414%*

 * *Differences: {"'cells'": "{0: {'source': {insert: [(13, 'from itertools import combinations\\n')], delete: "*

 * *            "[13]}}, 2: {'source': {insert: [(0, 'LABELS_URL = "*

 * *            '"https://s3.amazonaws.com/outcome-blog/imagenet/labels.json"\\n\')], delete: [0]}}, '*

 * *            '4: {\'source\': [\'bucket_name = "wellcomecollection-miro-images-public"\']}, 5: '*

 * *            '{\'source\': {insert: [(0, \'sts = boto3.client("sts")\\n\'), (2, \'    '*

 * *            'RoleArn="arn:aws:iam::760097843905:role/calm-assumable_r […]*

```diff
@@ -15,15 +15,15 @@
                 "import torchvision.transforms as transforms\n",
                 "import requests\n",
                 "import os\n",
                 "import networkx as nx\n",
                 "from sklearn.metrics.pairwise import euclidean_distances\n",
                 "import matplotlib.pyplot as plt\n",
                 "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
-                "from itertools import combinations \n",
+                "from itertools import combinations\n",
                 "from scipy.spatial.distance import cdist\n",
                 "\n",
                 "import time\n",
                 "\n",
                 "from tqdm import tqdm\n",
                 "\n",
                 "import umap.umap_ as umap"
@@ -41,15 +41,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "LABELS_URL = 'https://s3.amazonaws.com/outcome-blog/imagenet/labels.json'\n",
+                "LABELS_URL = \"https://s3.amazonaws.com/outcome-blog/imagenet/labels.json\"\n",
                 "\n",
                 "# Let's get our class labels for this model.\n",
                 "response = requests.get(LABELS_URL)  # Make an HTTP GET request and store the response.\n",
                 "labels = {int(key): value for key, value in response.json().items()}"
             ]
         },
         {
@@ -61,98 +61,92 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "bucket_name = 'wellcomecollection-miro-images-public'"
+                "bucket_name = \"wellcomecollection-miro-images-public\""
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "sts = boto3.client('sts')\n",
+                "sts = boto3.client(\"sts\")\n",
                 "assumed_role_object = sts.assume_role(\n",
-                "    RoleArn='arn:aws:iam::760097843905:role/calm-assumable_read_role',\n",
-                "    RoleSessionName='AssumeRoleSession1'\n",
+                "    RoleArn=\"arn:aws:iam::760097843905:role/calm-assumable_read_role\",\n",
+                "    RoleSessionName=\"AssumeRoleSession1\",\n",
                 ")\n",
-                "credentials = assumed_role_object['Credentials']"
+                "credentials = assumed_role_object[\"Credentials\"]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "s3 = boto3.resource('s3',\n",
-                "    aws_access_key_id=credentials['AccessKeyId'],\n",
-                "    aws_secret_access_key=credentials['SecretAccessKey'],\n",
-                "    aws_session_token=credentials['SessionToken']\n",
+                "s3 = boto3.resource(\n",
+                "    \"s3\",\n",
+                "    aws_access_key_id=credentials[\"AccessKeyId\"],\n",
+                "    aws_secret_access_key=credentials[\"SecretAccessKey\"],\n",
+                "    aws_session_token=credentials[\"SessionToken\"],\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "bucket = s3.Bucket(bucket_name)\n",
-                "bucket_info = bucket.meta.client.list_objects(\n",
-                "    Bucket=bucket.name,\n",
-                "    Delimiter='/'\n",
-                "    )"
+                "bucket_info = bucket.meta.client.list_objects(Bucket=bucket.name, Delimiter=\"/\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# Get all folder names.\n",
-                "folder_names = [\n",
-                "    f['Prefix'] for f in bucket_info.get('CommonPrefixes')\n",
-                "    ]\n",
-                "print(\"{} image folders\".format(len(folder_names))) # 219\n",
+                "folder_names = [f[\"Prefix\"] for f in bucket_info.get(\"CommonPrefixes\")]\n",
+                "print(\"{} image folders\".format(len(folder_names)))  # 219\n",
                 "\n",
                 "# Get all file dirs from all folders. Takes a minute or so\n",
                 "print(\"Getting all file dir names for all images...\")\n",
                 "file_dir = []\n",
                 "for folder_name in tqdm(folder_names):\n",
-                "    file_dir.extend(\n",
-                "        [s.key for s in bucket.objects.filter(Prefix=folder_name)]\n",
-                "    )\n",
-                "print(\"{} image files\".format(len(file_dir))) # 120589"
+                "    file_dir.extend([s.key for s in bucket.objects.filter(Prefix=folder_name)])\n",
+                "print(\"{} image files\".format(len(file_dir)))  # 120589"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# Pick n random image directories and store them\n",
                 "n = 1000\n",
-                "np.random.seed(seed=0) # Just for dev\n",
+                "np.random.seed(seed=0)  # Just for dev\n",
                 "random_file_dir = np.random.choice(file_dir, n, replace=False)\n",
                 "\n",
                 "print(\"Storing {} random images...\".format(n))\n",
                 "images = []\n",
                 "for file in tqdm(random_file_dir):\n",
                 "    obj = s3.Object(bucket_name, file)\n",
-                "    im = Image.open(BytesIO(obj.get()['Body'].read()))\n",
+                "    im = Image.open(BytesIO(obj.get()[\"Body\"].read()))\n",
                 "    im.thumbnail((750, 750))\n",
                 "    if im.mode != \"RGB\":\n",
-                "        im = im.convert('RGB')\n",
+                "        im = im.convert(\"RGB\")\n",
                 "    images.append(im)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -161,19 +155,24 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "min_img_size = 224  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
-                "transform_pipeline = transforms.Compose([transforms.Resize(min_img_size),\n",
-                "                                         transforms.ToTensor(),\n",
-                "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
-                "                                                              std=[0.229, 0.224, 0.225])])"
+                "min_img_size = (\n",
+                "    224  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
+                ")\n",
+                "transform_pipeline = transforms.Compose(\n",
+                "    [\n",
+                "        transforms.Resize(min_img_size),\n",
+                "        transforms.ToTensor(),\n",
+                "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
+                "    ]\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -186,34 +185,36 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def predict_image(transform_pipeline, im, model, labels):\n",
-                "    \n",
+                "\n",
                 "    img = transform_pipeline(im)\n",
                 "    img = img.unsqueeze(0)\n",
-                "    \n",
+                "\n",
                 "    # Now let's get a prediciton!\n",
                 "    prediction = model(img)  # Returns a Tensor of shape (batch, num class labels)\n",
-                "    prediction = prediction.data.numpy().argmax()  # Our prediction will be the index of the class label with the largest value.\n",
+                "    prediction = (\n",
+                "        prediction.data.numpy().argmax()\n",
+                "    )  # Our prediction will be the index of the class label with the largest value.\n",
                 "    print(prediction)\n",
                 "    return labels[prediction]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "im = images[5]\n",
                 "print(predict_image(transform_pipeline, im, vgg16, labels))\n",
-                "im.resize((200,200), resample= Image.BILINEAR)"
+                "im.resize((200, 200), resample=Image.BILINEAR)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "## 3. Extract feature vectors from images"
@@ -221,19 +222,24 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "min_img_size = 224  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
-                "transform_pipeline = transforms.Compose([transforms.Resize(min_img_size),\n",
-                "                                         transforms.ToTensor(),\n",
-                "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
-                "                                                              std=[0.229, 0.224, 0.225])])"
+                "min_img_size = (\n",
+                "    224  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
+                ")\n",
+                "transform_pipeline = transforms.Compose(\n",
+                "    [\n",
+                "        transforms.Resize(min_img_size),\n",
+                "        transforms.ToTensor(),\n",
+                "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
+                "    ]\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -276,30 +282,30 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "dist_mat_top = np.zeros_like(dist_mat)\n",
-                "dist_mat_top[:]=None"
+                "dist_mat_top[:] = None"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "n = 3\n",
                 "\n",
                 "# Find the top n neighbours for each image\n",
                 "\n",
                 "for i, _ in tqdm(enumerate(images)):\n",
                 "    arr = dist_mat[i].argsort()\n",
-                "    top_args = arr[arr!=i]\n",
+                "    top_args = arr[arr != i]\n",
                 "    dist_mat_top[i][top_args[0:n]] = dist_mat[i][top_args[0:n]]\n",
                 "    for j in top_args[0:n]:\n",
                 "        dist_mat_top[j][i] = dist_mat[j][i]"
             ]
         },
         {
             "cell_type": "markdown",
@@ -311,68 +317,68 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def inv_rel_norm(value, min_val, max_val):\n",
-                "    value = (value - min_val)/(max_val - min_val)\n",
-                "    value = 1/(value+1e-8)\n",
+                "    value = (value - min_val) / (max_val - min_val)\n",
+                "    value = 1 / (value + 1e-8)\n",
                 "    return value"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def create_graph(dist_mat_top):\n",
-                "    \n",
+                "\n",
                 "    min_val = np.nanmin(dist_mat_top)\n",
                 "    max_val = np.nanmax(dist_mat_top)\n",
-                "    \n",
-                "    nodes = list(range(0,len(dist_mat_top[0])))\n",
+                "\n",
+                "    nodes = list(range(0, len(dist_mat_top[0])))\n",
                 "\n",
                 "    G = nx.Graph()\n",
                 "    G.add_nodes_from(nodes)\n",
                 "\n",
                 "    # Put the weights in as the distances\n",
                 "    # only inc nodes if they are in the closest related neighbours\n",
                 "    for start, end in list(combinations(nodes, 2)):\n",
                 "        if ~np.isnan(dist_mat_top[start, end]):\n",
-                "            # Since in the plot a higher weight makes the nodes closer, \n",
+                "            # Since in the plot a higher weight makes the nodes closer,\n",
                 "            # but a higher value in the distance matrix means the images are further away,\n",
                 "            # we need to inverse the weight (so higher = closer)\n",
                 "            G.add_edge(\n",
                 "                start,\n",
                 "                end,\n",
-                "                weight=inv_rel_norm(dist_mat_top[start, end], min_val, max_val)\n",
+                "                weight=inv_rel_norm(dist_mat_top[start, end], min_val, max_val),\n",
                 "            )\n",
                 "    return G"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def plot_graph(G, image_names=None):\n",
-                "    \n",
+                "\n",
                 "    pos = nx.spring_layout(G)\n",
                 "\n",
-                "    plt.figure(3,figsize=(10,10)) \n",
+                "    plt.figure(3, figsize=(10, 10))\n",
                 "    nx.draw(G, pos, node_size=10)\n",
                 "    for p in pos:  # raise text positions\n",
                 "        pos[p][1] += 0.06\n",
                 "    if image_names:\n",
-                "        image_names_dict = {k:str(k)+\" \"+v for k,v in enumerate(image_names)}\n",
+                "        image_names_dict = {k: str(k) + \" \" + v for k, v in enumerate(image_names)}\n",
                 "        nx.draw_networkx_labels(G, pos, labels=image_names_dict)\n",
-                "    plt.show()\n"
+                "    plt.show()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -410,47 +416,44 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# from https://www.kaggle.com/gaborvecsei/plants-t-sne\n",
-                "def visualize_scatter_with_images(X_2d_data, images, figsize=(45,45), image_zoom=1):\n",
+                "def visualize_scatter_with_images(X_2d_data, images, figsize=(45, 45), image_zoom=1):\n",
                 "    fig, ax = plt.subplots(figsize=figsize)\n",
                 "    artists = []\n",
                 "    for xy, i in zip(X_2d_data, images):\n",
                 "        x0, y0 = xy\n",
                 "        img = OffsetImage(i, zoom=image_zoom)\n",
-                "        ab = AnnotationBbox(img, (x0, y0), xycoords='data', frameon=False)\n",
+                "        ab = AnnotationBbox(img, (x0, y0), xycoords=\"data\", frameon=False)\n",
                 "        artists.append(ax.add_artist(ab))\n",
                 "    ax.update_datalim(X_2d_data)\n",
                 "    ax.autoscale()\n",
-                "    plt.axis('off')\n",
+                "    plt.axis(\"off\")\n",
                 "    plt.show()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "x_data = [[a, b] for (a,b) in zip(embedding_fv[:, 0], embedding_fv[:, 1])]"
+                "x_data = [[a, b] for (a, b) in zip(embedding_fv[:, 0], embedding_fv[:, 1])]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "visualize_scatter_with_images(\n",
-                "    x_data,\n",
-                "    images = images,\n",
-                "    image_zoom=0.1)"
+                "visualize_scatter_with_images(x_data, images=images, image_zoom=0.1)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "## Get a list of the biggest differences between 2 images"
@@ -467,15 +470,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "np.nanargmax(dist_mat_top, axis = 0)"
+                "np.nanargmax(dist_mat_top, axis=0)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "## 7. Pick 2 images and look at the route between them"
@@ -483,36 +486,36 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_names_dict = {k:v for k,v in enumerate(random_file_dir)}"
+                "image_names_dict = {k: v for k, v in enumerate(random_file_dir)}"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "node1 = np.random.choice(list(image_names_dict))\n",
                 "node2 = np.random.choice(list(image_names_dict))\n",
                 "\n",
                 "# nice path:\n",
-                "# node1 = 6 \n",
+                "# node1 = 6\n",
                 "# node2 = 146\n",
                 "\n",
                 "node_path = nx.dijkstra_path(G, node1, node2, weight=None)\n",
                 "print(node_path)\n",
                 "\n",
                 "show_images = [images[i] for i in node_path]\n",
                 "\n",
-                "fig = plt.figure(figsize=(20,10))\n",
+                "fig = plt.figure(figsize=(20, 10))\n",
                 "columns = len(show_images)\n",
                 "for i, image in enumerate(show_images):\n",
                 "    ax = plt.subplot(len(show_images) / columns + 1, columns, i + 1)\n",
                 "    ax.set_axis_off()\n",
                 "    plt.imshow(image)"
             ]
         },
```

### Comparing `weco-datascience-0.1.8/research_notebooks/image_pathways/src/network_functions.py` & `weco-datascience-0.1.9/notebooks/image_pathways/src/network_functions.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/research_notebooks/palette/notebooks/00 - RGB, LAB, and human colour perception.ipynb` & `weco-datascience-0.1.9/notebooks/palette/notebooks/00 - RGB, LAB, and human colour perception.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9978857080419581%*

 * *Differences: {"'cells'": "{1: {'source': {insert: [(1, '    return (\\n'), (2, '        sum(\\n'), (3, "*

 * *            "'            [\\n'), (4, '                (channel_2 - channel_1) ** 2\\n'), (5, "*

 * *            "'                for channel_1, channel_2 in zip(colour_1, colour_2)\\n'), (6, "*

 * *            "'            ]\\n'), (7, '        )\\n'), (8, '        ** 0.5\\n'), (9, '    )')], "*

 * *            "delete: [3, 2, 1]}}, 3: {'source': {insert: [(4, '    return (2 * (r_1 - r_2) ** 2 + "*

 * *            "4 * (g_1 - g_2) **  […]*

```diff
@@ -16,17 +16,23 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def colour_distance_1(colour_1, colour_2):\n",
-                "    return sum([(channel_2 - channel_1) ** 2 \n",
-                "                for channel_1, channel_2 in \n",
-                "                zip(colour_1, colour_2)]) ** 0.5"
+                "    return (\n",
+                "        sum(\n",
+                "            [\n",
+                "                (channel_2 - channel_1) ** 2\n",
+                "                for channel_1, channel_2 in zip(colour_1, colour_2)\n",
+                "            ]\n",
+                "        )\n",
+                "        ** 0.5\n",
+                "    )"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "The red, green, and blue channels available to us in RGB space are ideally suited for representing colour on pixelated screens. However, our goal is to represent the _ perceptual differences_ between colours, and RGB isn't ideal for this. It's now [pretty well established](https://en.wikipedia.org/wiki/Color_difference) that euclidean distances in RGB space are a bad representation of the distances that our eyes see.\n",
@@ -43,17 +49,15 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "def colour_distance_2(colour_1, colour_2):\n",
                 "    r_1, g_1, b_1 = colour_1\n",
                 "    r_2, g_2, b_2 = colour_2\n",
                 "\n",
-                "    return (2 * (r_1 - r_2) ** 2 +\n",
-                "            4 * (g_1 - g_2) ** 2 +\n",
-                "            3 * (b_1 - b_2) ** 2) ** 0.5"
+                "    return (2 * (r_1 - r_2) ** 2 + 4 * (g_1 - g_2) ** 2 + 3 * (b_1 - b_2) ** 2) ** 0.5"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "We can improve further by adding some extra weirdness to the red and blue channels\n",
@@ -68,22 +72,21 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def colour_distance_3(colour_1, colour_2):\n",
                 "    r_1, g_1, b_1 = colour_1\n",
                 "    r_2, g_2, b_2 = colour_2\n",
-                "    \n",
+                "\n",
                 "    d_r_sq = (r_1 - r_2) ** 2\n",
                 "    d_g_sq = (g_1 - g_2) ** 2\n",
                 "    d_b_sq = (b_1 - b_2) ** 2\n",
                 "    mean_r = (r_1 + r_2) / 2\n",
-                "    \n",
-                "    d_c_sq = (2 * d_r_sq + 4 * d_g_sq + 3 * d_b_sq +\n",
-                "              (mean_r * (d_r_sq - d_b_sq) / 256))\n",
+                "\n",
+                "    d_c_sq = 2 * d_r_sq + 4 * d_g_sq + 3 * d_b_sq + (mean_r * (d_r_sq - d_b_sq) / 256)\n",
                 "\n",
                 "    return d_c_sq ** 0.5"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
```

### Comparing `weco-datascience-0.1.8/research_notebooks/palette/notebooks/01 - obtaining palettes.ipynb` & `weco-datascience-0.1.9/notebooks/palette/notebooks/01 - obtaining palettes.ipynb`

 * *Files 8% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9975603189993947%*

 * *Differences: {"'cells'": '{3: {\'source\': {insert: [(0, \'path_to_images = "../data/small_images/"\\n\'), (5, '*

 * *            "'    image = Image.fromarray(np.stack((image,) * 3, -1))\\n'), (7, "*

 * *            '\'print(image_id.replace(".jpg", ""))\\n\')], delete: [7, 5, 0]}}, 5: {\'source\': '*

 * *            "{insert: [(1, 'image = image.resize((image_size, image_size), "*

 * *            "resample=Image.BILINEAR)\\n')], delete: [2, 1]}}, 9: {'source': {insert: [(1, "*

 * *            "'cluster = KMeans(n_clusters=n_clusters).fit(lab_ […]*

```diff
@@ -40,22 +40,22 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "path_to_images = '../data/small_images/'\n",
+                "path_to_images = \"../data/small_images/\"\n",
                 "image_id = np.random.choice(os.listdir(path_to_images))\n",
                 "image = Image.open(path_to_images + image_id)\n",
                 "\n",
                 "if len(np.array(image).shape) != 3:\n",
-                "    image = Image.fromarray(np.stack((image,)*3, -1))\n",
+                "    image = Image.fromarray(np.stack((image,) * 3, -1))\n",
                 "\n",
-                "print(image_id.replace('.jpg', ''))\n",
+                "print(image_id.replace(\".jpg\", \"\"))\n",
                 "\n",
                 "image"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -66,16 +66,15 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "image_size = 100\n",
-                "image = image.resize((image_size, image_size), \n",
-                "                     resample=Image.BILINEAR)\n",
+                "image = image.resize((image_size, image_size), resample=Image.BILINEAR)\n",
                 "lab_image = rgb2lab(np.array(image))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -101,15 +100,15 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "n_clusters = 5\n",
-                "cluster = (KMeans(n_clusters=n_clusters).fit(lab_pixels))"
+                "cluster = KMeans(n_clusters=n_clusters).fit(lab_pixels)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "The coordinates of each cluster center are an integral part of [how k-means clustering works](https://en.wikipedia.org/wiki/K-means_clustering), so getting hold of them is also super easy"
@@ -133,19 +132,27 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "palette = (np.hstack([(lab2rgb(np.array(colour.tolist() * image_size * image_size)\n",
-                "                               .reshape(image_size, image_size, 3)) * 255)\n",
-                "                      .astype(np.uint8)\n",
-                "                      for colour in cluster.cluster_centers_])\n",
-                "           .reshape((image_size, image_size * n_clusters, 3)))\n",
+                "palette = np.hstack(\n",
+                "    [\n",
+                "        (\n",
+                "            lab2rgb(\n",
+                "                np.array(colour.tolist() * image_size * image_size).reshape(\n",
+                "                    image_size, image_size, 3\n",
+                "                )\n",
+                "            )\n",
+                "            * 255\n",
+                "        ).astype(np.uint8)\n",
+                "        for colour in cluster.cluster_centers_\n",
+                "    ]\n",
+                ").reshape((image_size, image_size * n_clusters, 3))\n",
                 "\n",
                 "Image.fromarray(palette)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -157,66 +164,72 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def get_palette(image, palette_size=5, image_size=75):\n",
-                "    '''\n",
+                "    \"\"\"\n",
                 "    Return n dominant colours for a given image\n",
-                "    \n",
+                "\n",
                 "    Parameters\n",
                 "    ----------\n",
                 "    image : PIL.Image\n",
                 "        The image for which we want to create a palette of dominant colours\n",
-                "    palette_size : \n",
+                "    palette_size :\n",
                 "        The number of dominant colours to extract\n",
-                "    image_size : \n",
+                "    image_size :\n",
                 "        Images are resized and squared by default to reduce processing time.\n",
-                "        This value sets the side-length of the square. Higher values will \n",
-                "        indrease fidelity,  \n",
-                "    \n",
+                "        This value sets the side-length of the square. Higher values will\n",
+                "        indrease fidelity,\n",
+                "\n",
                 "    Returns\n",
                 "    -------\n",
                 "    palette : np.array\n",
                 "        palette coordinates in LAB space\n",
-                "    '''\n",
+                "    \"\"\"\n",
                 "    image = image.resize((image_size, image_size))\n",
                 "    lab_image = rgb2lab(np.array(image)).reshape(-1, 3)\n",
                 "    clusters = KMeans(n_clusters=palette_size).fit(lab_image)\n",
                 "    return clusters.cluster_centers_\n",
                 "\n",
                 "\n",
                 "def display_palette(palette_colours, image_size=100):\n",
-                "    '''\n",
+                "    \"\"\"\n",
                 "    Return n dominant colours for a given image\n",
-                "    \n",
+                "\n",
                 "    Parameters\n",
                 "    ----------\n",
                 "    palette_colours : np.array\n",
                 "        palette coordinates in LAB space\n",
-                "    image_size : \n",
+                "    image_size :\n",
                 "        The size of each palette colour swatch to be returned\n",
-                "    \n",
+                "\n",
                 "    Returns\n",
                 "    -------\n",
                 "    palette : PIL.Image\n",
                 "        image of square colour swatches\n",
-                "    '''\n",
-                "    palette_size=len(palette_colours)\n",
-                "    \n",
-                "    stretched_colours = [(lab2rgb(np.array(colour * image_size * image_size *5)\n",
-                "                                  .reshape(image_size*5, image_size, 3)) * 255)\n",
-                "                         .astype(np.uint8) \n",
-                "                         for colour in palette_colours]\n",
-                "    \n",
-                "    palette_array = (np.hstack(stretched_colours)\n",
-                "                     .reshape((image_size*5, \n",
-                "                               image_size * palette_size, \n",
-                "                               3)))\n",
+                "    \"\"\"\n",
+                "    palette_size = len(palette_colours)\n",
+                "\n",
+                "    stretched_colours = [\n",
+                "        (\n",
+                "            lab2rgb(\n",
+                "                np.array(colour * image_size * image_size * 5).reshape(\n",
+                "                    image_size * 5, image_size, 3\n",
+                "                )\n",
+                "            )\n",
+                "            * 255\n",
+                "        ).astype(np.uint8)\n",
+                "        for colour in palette_colours\n",
+                "    ]\n",
+                "\n",
+                "    palette_array = np.hstack(stretched_colours).reshape(\n",
+                "        (image_size * 5, image_size * palette_size, 3)\n",
+                "    )\n",
                 "\n",
                 "    return Image.fromarray(palette_array)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -226,19 +239,18 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "rgb_colours = [((lab2rgb(np.array(colour)\n",
-                "                         .reshape(1, 1, 3)) * 255)\n",
-                "                .astype(np.uint8)\n",
-                "                .squeeze())\n",
-                "               for colour in cluster.cluster_centers_]\n",
+                "rgb_colours = [\n",
+                "    ((lab2rgb(np.array(colour).reshape(1, 1, 3)) * 255).astype(np.uint8).squeeze())\n",
+                "    for colour in cluster.cluster_centers_\n",
+                "]\n",
                 "\n",
                 "\n",
                 "for colour in rgb_colours:\n",
                 "    print(colour)"
             ]
         },
         {
@@ -254,14 +266,15 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "def rgb_to_hex(rgb_list):\n",
                 "    r, g, b = [int(round(channel)) for channel in rgb_list]\n",
                 "    return \"#{:02x}{:02x}{:02x}\".format(r, g, b)\n",
                 "\n",
+                "\n",
                 "for colour in rgb_colours:\n",
                 "    print(rgb_to_hex(colour))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
```

### Comparing `weco-datascience-0.1.8/research_notebooks/palette/notebooks/02 - histogram similarity.ipynb` & `weco-datascience-0.1.9/notebooks/palette/notebooks/02 - histogram similarity.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9975756572253283%*

 * *Differences: {"'cells'": '{1: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 15)\\n\')], delete: [4, 3]}}, 3: {\'source\': '*

 * *            '{insert: [(1, \'path_to_images = "../data/small_images/"\\n\'), (3, \'random_ids = '*

 * *            "np.random.choice(os.listdir(path_to_images), n_images, replace=False)\\n'), (7, '    "*

 * *            "try:\\n'), (10, '            image = Image.fromarray(np.stack((image,) * 3, "*

 * *            "-1))\\n') […]*

```diff
@@ -18,16 +18,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (20, 15)\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 15)\n",
                 "\n",
                 "import os\n",
                 "import itertools\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
                 "from PIL import Image\n",
                 "from sklearn.metrics.pairwise import pairwise_distances\n",
@@ -46,30 +47,28 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "n_images = 1000\n",
-                "path_to_images = '../data/small_images/'\n",
+                "path_to_images = \"../data/small_images/\"\n",
                 "\n",
-                "random_ids = np.random.choice(os.listdir(path_to_images), \n",
-                "                              n_images, \n",
-                "                              replace=False)\n",
+                "random_ids = np.random.choice(os.listdir(path_to_images), n_images, replace=False)\n",
                 "\n",
                 "image_dict = {}\n",
                 "for image_id in tqdm(random_ids):\n",
-                "    try: \n",
+                "    try:\n",
                 "        image = Image.open(path_to_images + image_id)\n",
                 "        if len(np.array(image).shape) != 3:\n",
-                "            image = Image.fromarray(np.stack((image,)*3, -1))\n",
+                "            image = Image.fromarray(np.stack((image,) * 3, -1))\n",
                 "        image_dict[image_id] = image\n",
-                "    except: \n",
+                "    except:\n",
                 "        pass\n",
-                "    \n",
+                "\n",
                 "image_ids = list(image_dict.keys())\n",
                 "images = list(image_dict.values())"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
@@ -85,16 +84,16 @@
                 "big_image = np.empty((height, width, 3)).astype(np.uint8)\n",
                 "grid = np.array(list(itertools.product(range(size), range(size))))\n",
                 "sq_images = [image.resize((resolution, resolution)) for image in images]\n",
                 "\n",
                 "for pos, image in zip(grid, sq_images):\n",
                 "    block_t, block_l = pos * resolution\n",
                 "    block_b, block_r = (pos + 1) * resolution\n",
-                "    \n",
-                "    big_image[block_t : block_b, block_l : block_r] = np.array(image)\n",
+                "\n",
+                "    big_image[block_t:block_b, block_l:block_r] = np.array(image)\n",
                 "\n",
                 "Image.fromarray(big_image)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -142,17 +141,15 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "r, g, b = np.split(np.array(hist), 3)\n",
                 "\n",
-                "pd.DataFrame({'r': r, 'g': g, 'b': b}).plot(color=['#333399', \n",
-                "                                                   '#339933', \n",
-                "                                                   '#993333']);"
+                "pd.DataFrame({\"r\": r, \"g\": g, \"b\": b}).plot(color=[\"#333399\", \"#339933\", \"#993333\"]);"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# A naive start\n",
@@ -170,18 +167,19 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "naive_similarity = pd.DataFrame(data=pairwise_distances(histograms, \n",
-                "                                                        metric='cosine'),\n",
-                "                                index=image_ids,\n",
-                "                                columns=image_ids)"
+                "naive_similarity = pd.DataFrame(\n",
+                "    data=pairwise_distances(histograms, metric=\"cosine\"),\n",
+                "    index=image_ids,\n",
+                "    columns=image_ids,\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "we can display the similarity matrix as a heatmap, as follows:"
@@ -248,15 +246,17 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "similar_images = [image_dict[id].resize((300, 300)) for id in most_similar_ids]\n",
-                "Image.fromarray(np.hstack([np.array(image) for image in similar_images]).reshape(300, 1500, 3))"
+                "Image.fromarray(\n",
+                "    np.hstack([np.array(image) for image in similar_images]).reshape(300, 1500, 3)\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "These results are super sketchy... They're not _bad_ exactly, but they're definitely not good. Playing with the results for a while, it's easy to spot where the approach succeeds and where the rough edges appear.\n",
@@ -274,54 +274,54 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def moving_average(arr, w):\n",
-                "    '''\n",
+                "    \"\"\"\n",
                 "    Returns a moving average over a given array\n",
-                "    \n",
+                "\n",
                 "    Parameters\n",
                 "    ----------\n",
                 "    arr : numpy.array\n",
                 "        input array\n",
                 "    n : int\n",
                 "        window size\n",
                 "\n",
                 "    Returns\n",
                 "    -------\n",
                 "    arr : numpy.array\n",
                 "        input array with moving average applied\n",
-                "    '''\n",
+                "    \"\"\"\n",
                 "    cumsum = np.cumsum(arr)\n",
-                "    return (cumsum[w:] - cumsum[:-w])[w - 1:] / w\n",
+                "    return (cumsum[w:] - cumsum[:-w])[w - 1 :] / w\n",
                 "\n",
                 "\n",
                 "def smooth_histogram(hist, w=10):\n",
-                "    '''\n",
-                "    applies a moving average to a image histogram, \n",
+                "    \"\"\"\n",
+                "    applies a moving average to a image histogram,\n",
                 "    retaining separation between the 3 channels\n",
-                "    \n",
+                "\n",
                 "    Parameters\n",
                 "    ----------\n",
                 "    hist : list\n",
                 "        flat input histogram of size=(768,)\n",
                 "    n : int\n",
                 "        window size\n",
                 "\n",
                 "    Returns\n",
                 "    -------\n",
                 "    arr : numpy.array\n",
                 "        input array with moving average applied\n",
-                "    '''\n",
+                "    \"\"\"\n",
                 "    r, g, b = np.split(np.array(hist), 3)\n",
-                "    return np.concatenate([moving_average(r, w), \n",
-                "                           moving_average(g, w), \n",
-                "                           moving_average(b, w)])"
+                "    return np.concatenate(\n",
+                "        [moving_average(r, w), moving_average(g, w), moving_average(b, w)]\n",
+                "    )"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "Note that in `smooth_histogram()` we're splitting the histogram into its three channels before applying the moving average and re-concatenating them into a single output array. We don't want to blur high-intensity reds and low-intensity blues together, so this extra step is necessary.\n",
@@ -340,18 +340,19 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "smooth_similarity = pd.DataFrame(data=pairwise_distances(smooth_histograms, \n",
-                "                                                         metric='cosine'),\n",
-                "                                 index=image_ids,\n",
-                "                                 columns=image_ids)"
+                "smooth_similarity = pd.DataFrame(\n",
+                "    data=pairwise_distances(smooth_histograms, metric=\"cosine\"),\n",
+                "    index=image_ids,\n",
+                "    columns=image_ids,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -372,15 +373,17 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "similar_images = [image_dict[id].resize((300, 300)) for id in most_similar_ids]\n",
-                "Image.fromarray(np.hstack([np.array(image) for image in similar_images]).reshape(300, 1500, 3))"
+                "Image.fromarray(\n",
+                "    np.hstack([np.array(image) for image in similar_images]).reshape(300, 1500, 3)\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "These results are at least as good as those from the raw histograms, which is reassuring. We'll have to wait until the end of the notebook for a direct comparison though.\n",
@@ -418,18 +421,19 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "reduced_similarity = pd.DataFrame(data=pairwise_distances(reduced_histograms, \n",
-                "                                                          metric='cosine'),\n",
-                "                                  index=image_ids,\n",
-                "                                  columns=image_ids)"
+                "reduced_similarity = pd.DataFrame(\n",
+                "    data=pairwise_distances(reduced_histograms, metric=\"cosine\"),\n",
+                "    index=image_ids,\n",
+                "    columns=image_ids,\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "and run a test query"
@@ -450,15 +454,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "most_similar_ids = reduced_similarity[query_id].sort_values().index.values[1:6]\n",
                 "\n",
                 "similar_images = [image_dict[id].resize((300, 300)) for id in most_similar_ids]\n",
-                "Image.fromarray(np.hstack([np.array(image) for image in similar_images]).reshape(300, 1500, 3))"
+                "Image.fromarray(\n",
+                "    np.hstack([np.array(image) for image in similar_images]).reshape(300, 1500, 3)\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# Direct comparison\n",
@@ -486,15 +492,17 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "most_similar_ids = naive_similarity[query_id].sort_values().index.values[1:6]\n",
                 "similar_images = [image_dict[id].resize((300, 300)) for id in most_similar_ids]\n",
-                "Image.fromarray(np.hstack([np.array(image) for image in similar_images]).reshape(300, 1500, 3))"
+                "Image.fromarray(\n",
+                "    np.hstack([np.array(image) for image in similar_images]).reshape(300, 1500, 3)\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### smoothed"
@@ -504,15 +512,17 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "most_similar_ids = smooth_similarity[query_id].sort_values().index.values[1:6]\n",
                 "similar_images = [image_dict[id].resize((300, 300)) for id in most_similar_ids]\n",
-                "Image.fromarray(np.hstack([np.array(image) for image in similar_images]).reshape(300, 1500, 3))"
+                "Image.fromarray(\n",
+                "    np.hstack([np.array(image) for image in similar_images]).reshape(300, 1500, 3)\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### dimensionality reduction"
@@ -522,15 +532,17 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "most_similar_ids = reduced_similarity[query_id].sort_values().index.values[1:6]\n",
                 "similar_images = [image_dict[id].resize((300, 300)) for id in most_similar_ids]\n",
-                "Image.fromarray(np.hstack([np.array(image) for image in similar_images]).reshape(300, 1500, 3))"
+                "Image.fromarray(\n",
+                "    np.hstack([np.array(image) for image in similar_images]).reshape(300, 1500, 3)\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### Conclusion\n",
```

### Comparing `weco-datascience-0.1.8/research_notebooks/palette/notebooks/03 - comparing histograms in 3d.ipynb` & `weco-datascience-0.1.9/notebooks/palette/notebooks/03 - comparing histograms in 3d.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9974236039445017%*

 * *Differences: {"'cells'": '{1: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("white")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\')], delete: [4, 3]}}, 2: {\'source\': '*

 * *            '{insert: [(1, \'path_to_images = "../data/small_images/"\\n\'), (3, \'random_ids = '*

 * *            "np.random.choice(os.listdir(path_to_images), n_images, replace=False)\\n'), (7, '    "*

 * *            "try:\\n'), (10, '            image = Image.fromarray(np.stack((image,) * 3, "*

 * *            "-1))\\n'), (1 […]*

```diff
@@ -19,16 +19,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('white')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"white\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import os\n",
                 "import itertools\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
                 "from PIL import Image\n",
                 "from skimage.color import rgb2lab\n",
@@ -41,28 +42,26 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "n_images = 5000\n",
-                "path_to_images = '../data/small_images/'\n",
+                "path_to_images = \"../data/small_images/\"\n",
                 "\n",
-                "random_ids = np.random.choice(os.listdir(path_to_images), \n",
-                "                              n_images, \n",
-                "                              replace=False)\n",
+                "random_ids = np.random.choice(os.listdir(path_to_images), n_images, replace=False)\n",
                 "\n",
                 "image_dict = {}\n",
                 "for image_id in tqdm(random_ids):\n",
-                "    try: \n",
+                "    try:\n",
                 "        image = Image.open(path_to_images + image_id)\n",
                 "        if len(np.array(image).shape) != 3:\n",
-                "            image = Image.fromarray(np.stack((image,)*3, -1))\n",
+                "            image = Image.fromarray(np.stack((image,) * 3, -1))\n",
                 "        image_dict[image_id] = image\n",
-                "    except: \n",
+                "    except:\n",
                 "        pass\n",
                 "\n",
                 "image_ids = list(image_dict.keys())\n",
                 "images = list(image_dict.values())"
             ]
         },
         {
@@ -76,17 +75,22 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "small_size = 75\n",
                 "\n",
-                "pixel_lists = [(np.array(image.resize((small_size, small_size),\n",
-                "                                      resample=Image.BILINEAR))\n",
-                "                .reshape(-1, 3)) for image in images]\n",
+                "pixel_lists = [\n",
+                "    (\n",
+                "        np.array(\n",
+                "            image.resize((small_size, small_size), resample=Image.BILINEAR)\n",
+                "        ).reshape(-1, 3)\n",
+                "    )\n",
+                "    for image in images\n",
+                "]\n",
                 "\n",
                 "pixel_dict = dict(zip(image_ids, pixel_lists))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -128,16 +132,15 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "embedding = UMAP().fit_transform(bin_counts.T.values)\n",
                 "\n",
-                "plt.scatter(x=embedding[:, 0], \n",
-                "            y=embedding[:, 1]);"
+                "plt.scatter(x=embedding[:, 0], y=embedding[:, 1]);"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### Similarity\n",
@@ -146,18 +149,19 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "similarity = pd.DataFrame(data=pairwise_distances(bin_counts.T, \n",
-                "                                                  metric='cosine'),\n",
-                "                          index=bin_counts.columns,\n",
-                "                          columns=bin_counts.columns)"
+                "similarity = pd.DataFrame(\n",
+                "    data=pairwise_distances(bin_counts.T, metric=\"cosine\"),\n",
+                "    index=bin_counts.columns,\n",
+                "    columns=bin_counts.columns,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -195,23 +199,24 @@
                 "height = int(resolution * size)\n",
                 "width = int(resolution * size)\n",
                 "\n",
                 "big_image = np.empty((height, width, 3)).astype(np.uint8)\n",
                 "grid = np.array(list(itertools.product(range(size), range(size))))\n",
                 "\n",
                 "most_similar_ids = similarity[query_id].sort_values().index.values[1 : n_similar + 1]\n",
-                "similar_images = [image_dict[id].resize((resolution, resolution),\n",
-                "                                        resample=Image.BILINEAR) \n",
-                "                  for id in most_similar_ids]\n",
+                "similar_images = [\n",
+                "    image_dict[id].resize((resolution, resolution), resample=Image.BILINEAR)\n",
+                "    for id in most_similar_ids\n",
+                "]\n",
                 "\n",
                 "for pos, image in zip(grid, similar_images):\n",
                 "    block_t, block_l = pos * resolution\n",
                 "    block_b, block_r = (pos + 1) * resolution\n",
-                "    \n",
-                "    big_image[block_t : block_b, block_l : block_r] = np.array(image)\n",
+                "\n",
+                "    big_image[block_t:block_b, block_l:block_r] = np.array(image)\n",
                 "\n",
                 "Image.fromarray(big_image)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
```

### Comparing `weco-datascience-0.1.8/research_notebooks/palette/notebooks/04 - palette based search.ipynb` & `weco-datascience-0.1.9/notebooks/palette/notebooks/04 - palette based search.ipynb`

 * *Files 4% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9969606162609399%*

 * *Differences: {"'cells'": '{1: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("white")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\')], delete: [4, 3]}}, 3: {\'source\': '*

 * *            '{insert: [(0, \'path_to_images = "../data/small_images/"\\n\')], delete: [0]}}, 6: '*

 * *            '{\'source\': {insert: [(1, \'    """\\n\'), (3, \'\\n\'), (8, \'    palette_size '*

 * *            ":\\n'), (10, '    image_size :\\n'), (12, '        This value sets the side-length of "*

 * *            "the squ […]*

```diff
@@ -21,16 +21,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('white')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"white\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import os\n",
                 "import itertools\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
                 "from PIL import Image\n",
                 "\n",
@@ -67,15 +68,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "path_to_images = '../data/small_images/'\n",
+                "path_to_images = \"../data/small_images/\"\n",
                 "image_1 = Image.open(path_to_images + np.random.choice(os.listdir(path_to_images)))\n",
                 "image_2 = Image.open(path_to_images + np.random.choice(os.listdir(path_to_images)))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
@@ -97,69 +98,76 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def get_palette(image, palette_size=5, image_size=75):\n",
-                "    '''\n",
+                "    \"\"\"\n",
                 "    Return n dominant colours for a given image\n",
-                "    \n",
+                "\n",
                 "    Parameters\n",
                 "    ----------\n",
                 "    image : PIL.Image\n",
                 "        The image for which we want to create a palette of dominant colours\n",
-                "    palette_size : \n",
+                "    palette_size :\n",
                 "        The number of dominant colours to extract\n",
-                "    image_size : \n",
+                "    image_size :\n",
                 "        Images are resized and squared by default to reduce processing time.\n",
-                "        This value sets the side-length of the square. Higher values will \n",
-                "        indrease fidelity,  \n",
-                "    \n",
+                "        This value sets the side-length of the square. Higher values will\n",
+                "        indrease fidelity,\n",
+                "\n",
                 "    Returns\n",
                 "    -------\n",
                 "    palette : np.array\n",
                 "        palette coordinates in LAB space\n",
-                "    '''\n",
+                "    \"\"\"\n",
                 "    image = image.resize((image_size, image_size))\n",
                 "    lab_image = rgb2lab(np.array(image)).reshape(-1, 3)\n",
                 "    clusters = KMeans(n_clusters=palette_size).fit(lab_image)\n",
                 "    return clusters.cluster_centers_\n",
                 "\n",
                 "\n",
                 "def display_palette(palette_colours, image_size=100, big=False):\n",
-                "    '''\n",
+                "    \"\"\"\n",
                 "    Return n dominant colours for a given image\n",
-                "    \n",
+                "\n",
                 "    Parameters\n",
                 "    ----------\n",
                 "    palette_colours : np.array\n",
                 "        palette coordinates in LAB space\n",
-                "    image_size : \n",
+                "    image_size :\n",
                 "        The size of each palette colour swatch to be returned\n",
-                "    \n",
+                "\n",
                 "    Returns\n",
                 "    -------\n",
                 "    palette : PIL.Image\n",
                 "        image of square colour swatches\n",
-                "    '''\n",
-                "    palette_size=len(palette_colours)\n",
-                "    \n",
+                "    \"\"\"\n",
+                "    palette_size = len(palette_colours)\n",
+                "\n",
                 "    scale = 1\n",
-                "    if big: scale = 5\n",
-                "    \n",
-                "    stretched_colours = [(lab2rgb(np.array(colour.tolist() * image_size * image_size * scale)\n",
-                "                                  .reshape(image_size * scale, image_size, 3)) * 255)\n",
-                "                         .astype(np.uint8) \n",
-                "                         for colour in palette_colours]\n",
-                "    \n",
-                "    palette_array = (np.hstack(stretched_colours)\n",
-                "                     .reshape((image_size * scale, \n",
-                "                               image_size * palette_size, \n",
-                "                               3)))\n",
+                "    if big:\n",
+                "        scale = 5\n",
+                "\n",
+                "    stretched_colours = [\n",
+                "        (\n",
+                "            lab2rgb(\n",
+                "                np.array(colour.tolist() * image_size * image_size * scale).reshape(\n",
+                "                    image_size * scale, image_size, 3\n",
+                "                )\n",
+                "            )\n",
+                "            * 255\n",
+                "        ).astype(np.uint8)\n",
+                "        for colour in palette_colours\n",
+                "    ]\n",
+                "\n",
+                "    palette_array = np.hstack(stretched_colours).reshape(\n",
+                "        (image_size * scale, image_size * palette_size, 3)\n",
+                "    )\n",
                 "\n",
                 "    return Image.fromarray(palette_array)\n",
                 "\n",
                 "\n",
                 "def colour_distance(colour_1, colour_2):\n",
                 "    return sum([(a - b) ** 2 for a, b in zip(colour_1, colour_2)]) ** 0.5"
             ]
@@ -194,17 +202,18 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "distances = [[colour_distance(colour_1, colour_2)\n",
-                "              for colour_1 in palette_1] \n",
-                "             for colour_2 in palette_2]\n",
+                "distances = [\n",
+                "    [colour_distance(colour_1, colour_2) for colour_1 in palette_1]\n",
+                "    for colour_2 in palette_2\n",
+                "]\n",
                 "\n",
                 "_, rearrangement = linear_sum_assignment(distances)\n",
                 "palette_1 = [palette_1[i] for i in rearrangement]"
             ]
         },
         {
             "cell_type": "code",
@@ -235,35 +244,33 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "n_images = 1000\n",
-                "path_to_images = '../data/small_images/'\n",
+                "path_to_images = \"../data/small_images/\"\n",
                 "\n",
-                "random_ids = np.random.choice(os.listdir(path_to_images), \n",
-                "                              n_images, \n",
-                "                              replace=False)\n",
+                "random_ids = np.random.choice(os.listdir(path_to_images), n_images, replace=False)\n",
                 "\n",
                 "image_dict = {}\n",
                 "palette_dict = {}\n",
                 "\n",
                 "for image_id in tqdm(random_ids):\n",
-                "    try: \n",
+                "    try:\n",
                 "        image = Image.open(path_to_images + image_id)\n",
-                "        \n",
+                "\n",
                 "        if len(np.array(image).shape) != 3:\n",
-                "            image = Image.fromarray(np.stack((image,)*3, -1))\n",
-                "        \n",
+                "            image = Image.fromarray(np.stack((image,) * 3, -1))\n",
+                "\n",
                 "        image_dict[image_id] = image\n",
                 "        palette_dict[image_id] = get_palette(image)\n",
-                "    except: \n",
+                "    except:\n",
                 "        pass\n",
-                "    \n",
+                "\n",
                 "image_ids = np.sort(list(image_dict.keys()))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -273,40 +280,44 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def palette_distance(palette_1, palette_2):\n",
-                "    distances = [[colour_distance(colour_1, colour_2)\n",
-                "                  for colour_1 in palette_1] \n",
-                "                 for colour_2 in palette_2]\n",
+                "    distances = [\n",
+                "        [colour_distance(colour_1, colour_2) for colour_1 in palette_1]\n",
+                "        for colour_2 in palette_2\n",
+                "    ]\n",
                 "\n",
                 "    _, rearrangement = linear_sum_assignment(distances)\n",
                 "    palette_1 = [palette_1[i] for i in rearrangement]\n",
                 "\n",
-                "    palette_distance = sum([colour_distance(c_1, c_2) \n",
-                "                            for c_1, c_2 in zip(palette_1, palette_2)])\n",
-                "    \n",
+                "    palette_distance = sum(\n",
+                "        [colour_distance(c_1, c_2) for c_1, c_2 in zip(palette_1, palette_2)]\n",
+                "    )\n",
+                "\n",
                 "    return palette_distance"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "distances = {id_1: {id_2: palette_distance(palette_1, palette_2) \n",
-                "              for id_2, palette_2 in palette_dict.items()} \n",
-                "             for id_1, palette_1 in tqdm(palette_dict.items())}\n",
-                "\n",
-                "palette_distances = pd.DataFrame(data=distances,\n",
-                "                                 index=image_ids,\n",
-                "                                 columns=image_ids)"
+                "distances = {\n",
+                "    id_1: {\n",
+                "        id_2: palette_distance(palette_1, palette_2)\n",
+                "        for id_2, palette_2 in palette_dict.items()\n",
+                "    }\n",
+                "    for id_1, palette_1 in tqdm(palette_dict.items())\n",
+                "}\n",
+                "\n",
+                "palette_distances = pd.DataFrame(data=distances, index=image_ids, columns=image_ids)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -334,24 +345,24 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "display_palette(get_palette(Image.open('../data/small_images/' + query_id)))"
+                "display_palette(get_palette(Image.open(\"../data/small_images/\" + query_id)))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "display_palette(get_palette(Image.open('../data/small_images/' + most_similar_palette)))"
+                "display_palette(get_palette(Image.open(\"../data/small_images/\" + most_similar_palette)))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# Palette-based search\n",
@@ -400,35 +411,41 @@
                 "res = 200\n",
                 "n_similar = 16\n",
                 "size = int(n_similar ** 0.5)\n",
                 "\n",
                 "big_image = np.empty((int(res * size), int(res * size), 3)).astype(np.uint8)\n",
                 "grid = np.array(list(itertools.product(range(size), range(size))))\n",
                 "\n",
-                "most_similar_ids = palette_distances[query_id].sort_values().index.values[1 : n_similar+1]\n",
-                "similar_images = [image_dict[image_id].resize((res, res), resample=Image.LANCZOS) \n",
-                "                  for image_id in most_similar_ids]\n",
+                "most_similar_ids = (\n",
+                "    palette_distances[query_id].sort_values().index.values[1 : n_similar + 1]\n",
+                ")\n",
+                "similar_images = [\n",
+                "    image_dict[image_id].resize((res, res), resample=Image.LANCZOS)\n",
+                "    for image_id in most_similar_ids\n",
+                "]\n",
                 "\n",
                 "for pos, image in zip(grid, similar_images):\n",
                 "    block_t, block_l = pos * res\n",
                 "    block_b, block_r = (pos + 1) * res\n",
-                "    \n",
-                "    big_image[block_t : block_b, block_l : block_r] = np.array(image)\n",
+                "\n",
+                "    big_image[block_t:block_b, block_l:block_r] = np.array(image)\n",
                 "\n",
                 "Image.fromarray(big_image)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "print('https://wellcomecollection.org/works?query=' + \n",
-                "      '+'.join([id.strip('.jpg') for id in most_similar_ids]))"
+                "print(\n",
+                "    \"https://wellcomecollection.org/works?query=\"\n",
+                "    + \"+\".join([id.strip(\".jpg\") for id in most_similar_ids])\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "I think this process produces the nicest set of results by quite a long way (though as always, it's hard to know whether my own perception is universal). The fact that we're only comparing the dominant or distinct colours in the image, without weight or bias between them, means that the features picked out by human eyes seem well matched by the machine."
```

### Comparing `weco-datascience-0.1.8/research_notebooks/palette/notebooks/05 - vectorised palette distance.ipynb` & `weco-datascience-0.1.9/notebooks/palette/notebooks/05 - vectorised palette distance.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9973430178032482%*

 * *Differences: {"'cells'": '{1: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("white")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\')], delete: [4, 3]}}, 3: {\'source\': '*

 * *            '{insert: [(1, \'path_to_images = "../data/small_images/"\\n\'), (3, \'random_ids = '*

 * *            "np.random.choice(os.listdir(path_to_images), n_images, replace=False)\\n')], delete: "*

 * *            "[5, 4, 3, 1]}}, 5: {'source': {insert: [(1, '    palette_size = "*

 * *            "len(palette_colours)\\n') […]*

```diff
@@ -16,16 +16,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('white')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"white\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import os\n",
                 "import itertools\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
                 "from PIL import Image\n",
                 "\n",
@@ -48,19 +49,17 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "n_images = 5000\n",
-                "path_to_images = '../data/small_images/'\n",
+                "path_to_images = \"../data/small_images/\"\n",
                 "\n",
-                "random_ids = np.random.choice(os.listdir(path_to_images), \n",
-                "                              n_images, \n",
-                "                              replace=False)\n",
+                "random_ids = np.random.choice(os.listdir(path_to_images), n_images, replace=False)\n",
                 "\n",
                 "random_ids = np.sort(random_ids)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -71,34 +70,41 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def display_palette(palette_colours, image_size=100, big=False):\n",
-                "    palette_size=len(palette_colours)\n",
-                "    \n",
+                "    palette_size = len(palette_colours)\n",
+                "\n",
                 "    scale = 1\n",
-                "    if big: scale = 5\n",
-                "    \n",
-                "    stretched_colours = [(lab2rgb(np.array(colour.tolist() * image_size * image_size * scale)\n",
-                "                                  .reshape(image_size * scale, image_size, 3)) * 255)\n",
-                "                         .astype(np.uint8) \n",
-                "                         for colour in palette_colours]\n",
-                "    \n",
-                "    palette_array = (np.hstack(stretched_colours)\n",
-                "                     .reshape((image_size * scale, \n",
-                "                               image_size * palette_size, \n",
-                "                               3)))\n",
+                "    if big:\n",
+                "        scale = 5\n",
+                "\n",
+                "    stretched_colours = [\n",
+                "        (\n",
+                "            lab2rgb(\n",
+                "                np.array(colour.tolist() * image_size * image_size * scale).reshape(\n",
+                "                    image_size * scale, image_size, 3\n",
+                "                )\n",
+                "            )\n",
+                "            * 255\n",
+                "        ).astype(np.uint8)\n",
+                "        for colour in palette_colours\n",
+                "    ]\n",
+                "\n",
+                "    palette_array = np.hstack(stretched_colours).reshape(\n",
+                "        (image_size * scale, image_size * palette_size, 3)\n",
+                "    )\n",
                 "\n",
                 "    return Image.fromarray(palette_array)\n",
                 "\n",
+                "\n",
                 "def get_palette(image, palette_size=5, image_size=75):\n",
-                "    image = image.resize((image_size, image_size),\n",
-                "                         resample=Image.BILINEAR)\n",
+                "    image = image.resize((image_size, image_size), resample=Image.BILINEAR)\n",
                 "    lab_image = rgb2lab(np.array(image)).reshape(-1, 3)\n",
                 "    clusters = KMeans(n_clusters=palette_size).fit(lab_image)\n",
                 "    return clusters.cluster_centers_"
             ]
         },
         {
             "cell_type": "markdown",
@@ -113,23 +119,23 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "image_dict = {}\n",
                 "palette_dict = {}\n",
                 "\n",
                 "for image_id in tqdm(random_ids):\n",
-                "    try: \n",
+                "    try:\n",
                 "        image = Image.open(path_to_images + image_id)\n",
-                "        \n",
+                "\n",
                 "        if len(np.array(image).shape) != 3:\n",
-                "            image = Image.fromarray(np.stack((image,)*3, -1))\n",
-                "        \n",
+                "            image = Image.fromarray(np.stack((image,) * 3, -1))\n",
+                "\n",
                 "        image_dict[image_id] = image\n",
                 "        palette_dict[image_id] = get_palette(image)\n",
-                "    except: \n",
+                "    except:\n",
                 "        pass\n",
                 "\n",
                 "image_ids = np.sort(list(image_dict.keys()))\n",
                 "len(image_ids)"
             ]
         },
         {
@@ -149,18 +155,18 @@
                 "def colour_distance(colour_1, colour_2):\n",
                 "    return sum([(a - b) ** 2 for a, b in zip(colour_1, colour_2)]) ** 0.5\n",
                 "\n",
                 "\n",
                 "def assignment_switch(query_palette, palette_dict):\n",
                 "    rearranged = []\n",
                 "    for other_palette in palette_dict.values():\n",
-                "        distances = [[colour_distance(c1, c2)\n",
-                "                      for c2 in other_palette]\n",
-                "                     for c1 in query_palette]\n",
-                "        \n",
+                "        distances = [\n",
+                "            [colour_distance(c1, c2) for c2 in other_palette] for c1 in query_palette\n",
+                "        ]\n",
+                "\n",
                 "        _, rearrangement = linear_sum_assignment(distances)\n",
                 "        rearranged.append([other_palette[i] for i in rearrangement])\n",
                 "\n",
                 "    return np.array(rearranged)"
             ]
         },
         {
@@ -222,17 +228,18 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "def vectorised_palette_distance(rearranged, query_palette):\n",
                 "    query = query_palette.reshape(-1, 1, 3)\n",
                 "    palettes = [p.squeeze() for p in np.split(rearranged, 5, axis=1)]\n",
                 "\n",
-                "    colour_distances = np.stack([cdist(q, p, metric='cosine') \n",
-                "                                 for q, p in zip(query, palettes)])\n",
-                "    \n",
+                "    colour_distances = np.stack(\n",
+                "        [cdist(q, p, metric=\"cosine\") for q, p in zip(query, palettes)]\n",
+                "    )\n",
+                "\n",
                 "    palette_distances = np.sum(colour_distances.squeeze(), axis=0)\n",
                 "    return palette_distances"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -297,23 +304,27 @@
                 "res = 300\n",
                 "n_similar = 49\n",
                 "size = int(n_similar ** 0.5)\n",
                 "\n",
                 "big_image = np.empty((int(res * size), int(res * size), 3)).astype(np.uint8)\n",
                 "grid = np.array(list(itertools.product(range(size), range(size))))\n",
                 "\n",
-                "most_similar_ids = palette_distances[query_id].sort_values().index.values[1 : n_similar+1]\n",
-                "similar_images = [image_dict[image_id].resize((res, res), resample=Image.BILINEAR) \n",
-                "                  for image_id in most_similar_ids]\n",
+                "most_similar_ids = (\n",
+                "    palette_distances[query_id].sort_values().index.values[1 : n_similar + 1]\n",
+                ")\n",
+                "similar_images = [\n",
+                "    image_dict[image_id].resize((res, res), resample=Image.BILINEAR)\n",
+                "    for image_id in most_similar_ids\n",
+                "]\n",
                 "\n",
                 "for pos, image in zip(grid, similar_images):\n",
                 "    block_t, block_l = pos * res\n",
                 "    block_b, block_r = (pos + 1) * res\n",
-                "    \n",
-                "    big_image[block_t : block_b, block_l : block_r] = np.array(image)\n",
+                "\n",
+                "    big_image[block_t:block_b, block_l:block_r] = np.array(image)\n",
                 "\n",
                 "Image.fromarray(big_image)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -326,36 +337,37 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "palette_distances.to_pickle('../src/api/palette_distances.pkl')"
+                "palette_distances.to_pickle(\"../src/api/palette_distances.pkl\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "import pickle\n",
-                "with open('../src/api/palettes.pkl', 'wb') as f:\n",
+                "\n",
+                "with open(\"../src/api/palettes.pkl\", \"wb\") as f:\n",
                 "    pickle.dump(palette_dict, f)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open('../src/api/palettes.pkl', 'rb') as f:\n",
-                "    print(np.array(list(pickle.load(f).values())).shape    )"
+                "with open(\"../src/api/palettes.pkl\", \"rb\") as f:\n",
+                "    print(np.array(list(pickle.load(f).values())).shape)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
```

### Comparing `weco-datascience-0.1.8/research_notebooks/palette/notebooks/06 - vectorised assignment switching.ipynb` & `weco-datascience-0.1.9/notebooks/palette/notebooks/06 - vectorised assignment switching.ipynb`

 * *Files 7% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9949491993513733%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("white")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\')], delete: [4, 3]}}, 1: {\'source\': '*

 * *            '{insert: [(1, \'path_to_images = "../data/small_images/"\\n\'), (3, \'random_ids = '*

 * *            "np.random.choice(os.listdir(path_to_images), n_images, replace=False)\\n')], delete: "*

 * *            "[5, 4, 3, 1]}}, 2: {'source': {insert: [(1, '    palette_size = "*

 * *            "len(palette_colours)\\n') […]*

```diff
@@ -5,16 +5,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('white')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"white\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import os\n",
                 "import itertools\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
                 "from PIL import Image\n",
                 "\n",
@@ -30,50 +31,55 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "n_images = 5000\n",
-                "path_to_images = '../data/small_images/'\n",
+                "path_to_images = \"../data/small_images/\"\n",
                 "\n",
-                "random_ids = np.random.choice(os.listdir(path_to_images), \n",
-                "                              n_images, \n",
-                "                              replace=False)\n",
+                "random_ids = np.random.choice(os.listdir(path_to_images), n_images, replace=False)\n",
                 "\n",
                 "random_ids = np.sort(random_ids)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def display_palette(palette_colours, image_size=100, big=False):\n",
-                "    palette_size=len(palette_colours)\n",
-                "    \n",
+                "    palette_size = len(palette_colours)\n",
+                "\n",
                 "    scale = 1\n",
-                "    if big: scale = 5\n",
-                "    \n",
-                "    stretched_colours = [(lab2rgb(np.array(colour.tolist() * image_size * image_size * scale)\n",
-                "                                  .reshape(image_size * scale, image_size, 3)) * 255)\n",
-                "                         .astype(np.uint8) \n",
-                "                         for colour in palette_colours]\n",
-                "    \n",
-                "    palette_array = (np.hstack(stretched_colours)\n",
-                "                     .reshape((image_size * scale, \n",
-                "                               image_size * palette_size, \n",
-                "                               3)))\n",
+                "    if big:\n",
+                "        scale = 5\n",
+                "\n",
+                "    stretched_colours = [\n",
+                "        (\n",
+                "            lab2rgb(\n",
+                "                np.array(colour.tolist() * image_size * image_size * scale).reshape(\n",
+                "                    image_size * scale, image_size, 3\n",
+                "                )\n",
+                "            )\n",
+                "            * 255\n",
+                "        ).astype(np.uint8)\n",
+                "        for colour in palette_colours\n",
+                "    ]\n",
+                "\n",
+                "    palette_array = np.hstack(stretched_colours).reshape(\n",
+                "        (image_size * scale, image_size * palette_size, 3)\n",
+                "    )\n",
                 "\n",
                 "    return Image.fromarray(palette_array)\n",
                 "\n",
+                "\n",
                 "def get_palette(image, palette_size=5, image_size=75):\n",
-                "    image = image.resize((image_size, image_size),\n",
-                "                         resample=Image.BILINEAR)\n",
+                "    image = image.resize((image_size, image_size), resample=Image.BILINEAR)\n",
                 "    lab_image = rgb2lab(np.array(image)).reshape(-1, 3)\n",
                 "    clusters = KMeans(n_clusters=palette_size).fit(lab_image)\n",
                 "    return clusters.cluster_centers_"
             ]
         },
         {
             "cell_type": "code",
@@ -81,23 +87,23 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "image_dict = {}\n",
                 "palette_dict = {}\n",
                 "\n",
                 "for image_id in tqdm(random_ids):\n",
-                "    try: \n",
+                "    try:\n",
                 "        image = Image.open(path_to_images + image_id)\n",
-                "        \n",
+                "\n",
                 "        if len(np.array(image).shape) != 3:\n",
-                "            image = Image.fromarray(np.stack((image,)*3, -1))\n",
-                "        \n",
+                "            image = Image.fromarray(np.stack((image,) * 3, -1))\n",
+                "\n",
                 "        image_dict[image_id] = image\n",
                 "        palette_dict[image_id] = get_palette(image)\n",
-                "    except: \n",
+                "    except:\n",
                 "        pass\n",
                 "\n",
                 "image_ids = np.sort(list(image_dict.keys()))\n",
                 "len(image_ids)"
             ]
         },
         {
@@ -119,15 +125,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "np.linalg.norm((query_palette.reshape(5, 3)-palettes[0]), axis=1)"
+                "np.linalg.norm((query_palette.reshape(5, 3) - palettes[0]), axis=1)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -150,34 +156,34 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%%time\n",
                 "big = np.stack([list(itertools.permutations(palette, 5)) for palette in palettes])\n",
-                "np.linalg.norm(big-query_palette, axis=3).sum(axis=2).min(axis=1)"
+                "np.linalg.norm(big - query_palette, axis=3).sum(axis=2).min(axis=1)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "np.linalg.norm(big-query_palette, axis=3).sum(axis=2).min(axis=1)"
+                "np.linalg.norm(big - query_palette, axis=3).sum(axis=2).min(axis=1)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%%timeit\n",
-                "np.argsort(np.linalg.norm(big-query_palette, axis=3).sum(axis=2).min(axis=1))"
+                "np.argsort(np.linalg.norm(big - query_palette, axis=3).sum(axis=2).min(axis=1))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
```

### Comparing `weco-datascience-0.1.8/research_notebooks/palette/notebooks/07 - data generation for palette explorer app.ipynb` & `weco-datascience-0.1.9/notebooks/palette/notebooks/07 - data generation for palette explorer app.ipynb`

 * *Files 8% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9954973947621006%*

 * *Differences: {"'cells'": "{2: {'source': {insert: [(2, 'path_to_images = "*

 * *            '"/Users/pimh/datasets/small_images/"\\n\'), (4, \'random_ids = '*

 * *            "np.random.choice(os.listdir(path_to_images), n_images, replace=False)\\n')], delete: "*

 * *            "[6, 5, 4, 2]}}, 4: {'source': {insert: [(1, '    image = image.resize((image_size, "*

 * *            "image_size), resample=Image.BILINEAR)\\n')], delete: [2, 1]}}, 5: {'source': {insert: "*

 * *            "[(3, '    try:\\n'), (4, '        image = Image.open(path_to […]*

```diff
@@ -28,19 +28,17 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "n_images = 10000\n",
                 "\n",
-                "path_to_images = '/Users/pimh/datasets/small_images/'\n",
+                "path_to_images = \"/Users/pimh/datasets/small_images/\"\n",
                 "\n",
-                "random_ids = np.random.choice(os.listdir(path_to_images), \n",
-                "                              n_images, \n",
-                "                              replace=False)\n",
+                "random_ids = np.random.choice(os.listdir(path_to_images), n_images, replace=False)\n",
                 "\n",
                 "random_ids = np.sort(random_ids)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -51,36 +49,35 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def get_palette(image, palette_size=5, image_size=75):\n",
-                "    image = image.resize((image_size, image_size),\n",
-                "                         resample=Image.BILINEAR)\n",
+                "    image = image.resize((image_size, image_size), resample=Image.BILINEAR)\n",
                 "    lab_image = rgb2lab(np.array(image)).reshape(-1, 3)\n",
                 "    clusters = KMeans(n_clusters=palette_size).fit(lab_image)\n",
                 "    return clusters.cluster_centers_"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "palette_dict = {}\n",
                 "\n",
                 "for image_id in tqdm(random_ids):\n",
-                "    try: \n",
-                "        image = Image.open(path_to_images + image_id)        \n",
+                "    try:\n",
+                "        image = Image.open(path_to_images + image_id)\n",
                 "        if len(np.array(image).shape) != 3:\n",
-                "            image = Image.fromarray(np.stack((image,)*3, -1))\n",
+                "            image = Image.fromarray(np.stack((image,) * 3, -1))\n",
                 "        palette_dict[image_id] = get_palette(image)\n",
-                "    except: \n",
+                "    except:\n",
                 "        pass\n",
                 "\n",
                 "image_ids = np.sort(list(palette_dict.keys()))\n",
                 "palettes = [palette_dict[image_id] for image_id in image_ids]"
             ]
         },
         {
@@ -92,31 +89,32 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "all_possible_palettes = np.stack([list(itertools.permutations(palette, 5)) \n",
-                "                                  for palette in palettes])"
+                "all_possible_palettes = np.stack(\n",
+                "    [list(itertools.permutations(palette, 5)) for palette in palettes]\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "palette_distances = {}\n",
                 "\n",
                 "for image_id, palette in tqdm(palette_dict.items()):\n",
-                "    distances = (np.linalg.norm(all_possible_palettes - palette, axis=3)\n",
-                "                 .sum(axis=2)\n",
-                "                 .min(axis=1))\n",
-                "    \n",
+                "    distances = (\n",
+                "        np.linalg.norm(all_possible_palettes - palette, axis=3).sum(axis=2).min(axis=1)\n",
+                "    )\n",
+                "\n",
                 "    palette_distances[image_id] = dict(zip(image_ids, distances))\n",
                 "\n",
                 "palette_distances = pd.DataFrame(palette_distances)"
             ]
         },
         {
             "cell_type": "markdown",
@@ -127,24 +125,24 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "palette_distances.to_pickle('../../../apis/palette_api/data/palette_distances.pkl')"
+                "palette_distances.to_pickle(\"../../../apis/palette_api/data/palette_distances.pkl\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open('../../../apis/palette_api/data/palettes.pkl', 'wb') as f:\n",
+                "with open(\"../../../apis/palette_api/data/palettes.pkl\", \"wb\") as f:\n",
                 "    pickle.dump(palette_dict, f)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
```

### Comparing `weco-datascience-0.1.8/research_notebooks/palette/notebooks/08 - get palettes.ipynb` & `weco-datascience-0.1.9/notebooks/palette/notebooks/08 - get palettes.ipynb`

 * *Files 6% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9974626386345136%*

 * *Differences: {"'cells'": "{0: {'source': {insert: [(0, 'import boto3\\n')], delete: [0]}}, 2: {'source': "*

 * *            '{insert: [(0, \'sts = boto3.client("sts")\\n\'), (2, \'    '*

 * *            'RoleArn="arn:aws:iam::760097843905:role/calm-assumable_read_role",\\n\'), (3, \'    '*

 * *            'RoleSessionName="AssumeRoleSession1",\\n\'), (5, \'credentials = '*

 * *            'assumed_role_object["Credentials"]\\n\'), (7, \'s3_platform = boto3.client(\\n\'), '*

 * *            '(8, \'    "s3",\\n\'), (9, \'    aws_access_key_id=cre […]*

```diff
@@ -2,15 +2,15 @@
     "cells": [
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "import boto3 \n",
+                "import boto3\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
                 "from skimage.color import rgb2lab, lab2rgb\n",
                 "import os\n",
                 "from io import BytesIO\n",
                 "from tqdm import tqdm\n",
                 "from PIL import Image\n",
@@ -28,64 +28,65 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "sts = boto3.client('sts')\n",
+                "sts = boto3.client(\"sts\")\n",
                 "assumed_role_object = sts.assume_role(\n",
-                "    RoleArn='arn:aws:iam::760097843905:role/calm-assumable_read_role',\n",
-                "    RoleSessionName='AssumeRoleSession1'\n",
+                "    RoleArn=\"arn:aws:iam::760097843905:role/calm-assumable_read_role\",\n",
+                "    RoleSessionName=\"AssumeRoleSession1\",\n",
                 ")\n",
-                "credentials = assumed_role_object['Credentials']\n",
+                "credentials = assumed_role_object[\"Credentials\"]\n",
                 "\n",
-                "s3_platform = boto3.client('s3',\n",
-                "    aws_access_key_id=credentials['AccessKeyId'],\n",
-                "    aws_secret_access_key=credentials['SecretAccessKey'],\n",
-                "    aws_session_token=credentials['SessionToken']\n",
+                "s3_platform = boto3.client(\n",
+                "    \"s3\",\n",
+                "    aws_access_key_id=credentials[\"AccessKeyId\"],\n",
+                "    aws_secret_access_key=credentials[\"SecretAccessKey\"],\n",
+                "    aws_session_token=credentials[\"SessionToken\"],\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "s3_data_science = boto3.client('s3')"
+                "s3_data_science = boto3.client(\"s3\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def get_s3_keys_as_generator(bucket):\n",
                 "    \"\"\"Generate all the keys in an S3 bucket.\"\"\"\n",
-                "    kwargs = {'Bucket': bucket}\n",
+                "    kwargs = {\"Bucket\": bucket}\n",
                 "    while True:\n",
                 "        resp = s3_platform.list_objects_v2(**kwargs)\n",
-                "        for obj in resp['Contents']:\n",
-                "            yield obj['Key']\n",
+                "        for obj in resp[\"Contents\"]:\n",
+                "            yield obj[\"Key\"]\n",
                 "\n",
                 "        try:\n",
-                "            kwargs['ContinuationToken'] = resp['NextContinuationToken']\n",
+                "            kwargs[\"ContinuationToken\"] = resp[\"NextContinuationToken\"]\n",
                 "        except KeyError:\n",
                 "            break"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "bucket_name = 'wellcomecollection-miro-images-public'\n",
+                "bucket_name = \"wellcomecollection-miro-images-public\"\n",
                 "all_keys = list(get_s3_keys_as_generator(bucket_name))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -113,20 +114,20 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "palette_dicts = []\n",
-                "for i in tqdm(range(n_items_in_bucket+1)):\n",
+                "for i in tqdm(range(n_items_in_bucket + 1)):\n",
                 "    try:\n",
                 "        binary_data = s3_data_science.get_object(\n",
-                "            Bucket='model-core-data',\n",
-                "            Key='palette_similarity/palette_dict_{}.pkl'.format(i),\n",
-                "        )['Body'].read()\n",
+                "            Bucket=\"model-core-data\",\n",
+                "            Key=\"palette_similarity/palette_dict_{}.pkl\".format(i),\n",
+                "        )[\"Body\"].read()\n",
                 "        palette_dict = pickle.load(BytesIO(binary_data))\n",
                 "        palette_dicts.append(palette_dict)\n",
                 "    except:\n",
                 "        pass"
             ]
         },
         {
@@ -165,16 +166,16 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "not_yet_processed_keys = [\n",
-                "    object_key \n",
-                "    for object_key in all_keys \n",
+                "    object_key\n",
+                "    for object_key in all_keys\n",
                 "    if id_from_object_key(object_key) not in already_processed_ids\n",
                 "]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
@@ -195,20 +196,21 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def get_image(object_key):\n",
                 "    image_object = s3_platform.get_object(Bucket=bucket_name, Key=object_key)\n",
-                "    image = Image.open(BytesIO(image_object['Body'].read()))\n",
-                "    if image.mode != 'RGB':\n",
-                "        image = image.convert('RGB')\n",
+                "    image = Image.open(BytesIO(image_object[\"Body\"].read()))\n",
+                "    if image.mode != \"RGB\":\n",
+                "        image = image.convert(\"RGB\")\n",
                 "    image = image.resize((75, 75), resample=Image.BILINEAR)\n",
                 "    return image\n",
                 "\n",
+                "\n",
                 "def get_palette(image, palette_size=5):\n",
                 "    lab_image = rgb2lab(np.array(image)).reshape(-1, 3)\n",
                 "    clusters = KMeans(n_clusters=palette_size).fit(lab_image)\n",
                 "    return clusters.cluster_centers_"
             ]
         },
         {
@@ -216,29 +218,31 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "chunk_size, palette_dict = 1000, {}\n",
                 "\n",
                 "for i, object_key in enumerate(tqdm(not_yet_processed_keys)):\n",
-                "    try: \n",
+                "    try:\n",
                 "        image = get_image(object_key)\n",
                 "        image_id = id_from_object_key(object_key)\n",
                 "        palette_dict[image_id] = get_palette(image)\n",
                 "    except:\n",
                 "        pass\n",
                 "\n",
-                "    if ((i % chunk_size == 0) and (i != 0)):\n",
-                "        s3_data_science = boto3.client('s3')\n",
+                "    if (i % chunk_size == 0) and (i != 0):\n",
+                "        s3_data_science = boto3.client(\"s3\")\n",
                 "        s3_data_science.put_object(\n",
-                "            Bucket='model-core-data',\n",
-                "            Key='palette_similarity/palette_dict_{}.pkl'.format((i // chunk_size) + n_items_in_bucket),\n",
-                "            Body=pickle.dumps(palette_dict)\n",
+                "            Bucket=\"model-core-data\",\n",
+                "            Key=\"palette_similarity/palette_dict_{}.pkl\".format(\n",
+                "                (i // chunk_size) + n_items_in_bucket\n",
+                "            ),\n",
+                "            Body=pickle.dumps(palette_dict),\n",
                 "        )\n",
-                "        palette_dict = {}\n"
+                "        palette_dict = {}"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# save the data"
```

### Comparing `weco-datascience-0.1.8/research_notebooks/palette/notebooks/09 - merge palettes.ipynb` & `weco-datascience-0.1.9/notebooks/palette/notebooks/09 - merge palettes.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9974358974358974%*

 * *Differences: {"'cells'": "{0: {'source': {insert: [(0, 'import boto3\\n')], delete: [0]}}, 1: {'source': ['s3 = "*

 * *            'boto3.client("s3")\']}, 2: {\'source\': {insert: [(2, \'    kwargs = {"Bucket": '*

 * *            'bucket}\\n\'), (5, \'        for obj in resp["Contents"]:\\n\'), (6, \'            '*

 * *            'yield obj["Key"]\\n\'), (9, \'            kwargs["ContinuationToken"] = '*

 * *            'resp["NextContinuationToken"]\\n\')], delete: [9, 6, 5, 2]}}, 3: {\'source\': '*

 * *            '{insert: [(0, \'bucket_na […]*

```diff
@@ -2,15 +2,15 @@
     "cells": [
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "import boto3 \n",
+                "import boto3\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
                 "from skimage.color import rgb2lab, lab2rgb\n",
                 "import os\n",
                 "from io import BytesIO\n",
                 "from tqdm import tqdm\n",
                 "from PIL import Image\n",
@@ -21,55 +21,55 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "s3 = boto3.client('s3')"
+                "s3 = boto3.client(\"s3\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def get_s3_keys_as_generator(bucket):\n",
                 "    \"\"\"Generate all the keys in an S3 bucket.\"\"\"\n",
-                "    kwargs = {'Bucket': bucket}\n",
+                "    kwargs = {\"Bucket\": bucket}\n",
                 "    while True:\n",
                 "        resp = s3.list_objects_v2(**kwargs)\n",
-                "        for obj in resp['Contents']:\n",
-                "            yield obj['Key']\n",
+                "        for obj in resp[\"Contents\"]:\n",
+                "            yield obj[\"Key\"]\n",
                 "\n",
                 "        try:\n",
-                "            kwargs['ContinuationToken'] = resp['NextContinuationToken']\n",
+                "            kwargs[\"ContinuationToken\"] = resp[\"NextContinuationToken\"]\n",
                 "        except KeyError:\n",
                 "            break"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "bucket_name = 'model-core-data'\n",
+                "bucket_name = \"model-core-data\"\n",
                 "all_keys = list(get_s3_keys_as_generator(bucket_name))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "palette_dict_keys = [\n",
-                "    key for key in all_keys if key.startswith('palette_similarity/palette_dict_')\n",
+                "    key for key in all_keys if key.startswith(\"palette_similarity/palette_dict_\")\n",
                 "]"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -78,33 +78,27 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "a = s3.get_object(\n",
-                "        Bucket='model-core-data', \n",
-                "        Key=palette_dict_keys[3]\n",
-                "    )['Body'].read()\n"
+                "a = s3.get_object(Bucket=\"model-core-data\", Key=palette_dict_keys[3])[\"Body\"].read()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "palette_dict = {}\n",
                 "\n",
                 "for key in tqdm(palette_dict_keys):\n",
-                "    chunk_bytes = s3.get_object(\n",
-                "        Bucket='model-core-data', \n",
-                "        Key=key\n",
-                "    )['Body'].read()\n",
+                "    chunk_bytes = s3.get_object(Bucket=\"model-core-data\", Key=key)[\"Body\"].read()\n",
                 "    chunk = pickle.loads(chunk_bytes)\n",
                 "    palette_dict.update(chunk)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
@@ -150,16 +144,16 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "np.save('/storage/data/palette/all_palettes', palettes)\n",
-                "np.save('/storage/data/palette/all_image_ids', np.array(image_ids))"
+                "np.save(\"/storage/data/palette/all_palettes\", palettes)\n",
+                "np.save(\"/storage/data/palette/all_image_ids\", np.array(image_ids))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
```

### Comparing `weco-datascience-0.1.8/research_notebooks/palette/notebooks/10 - dataset for palette embedding network.ipynb` & `weco-datascience-0.1.9/notebooks/palette/notebooks/10 - dataset for palette embedding network.ipynb`

 * *Files 7% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9969230769230769%*

 * *Differences: {"'cells'": "{3: {'source': {insert: [(2, '\\n')]}}, 4: {'source': ['lab_palettes = "*

 * *            "np.stack([rgb_to_lab(palette) for palette in tqdm(rgb_palettes)])']}, 9: {'source': "*

 * *            "['distance_matrix = np.stack(\\n', '    [\\n', '        "*

 * *            "(np.linalg.norm(palette_permutations - palette, axis=3).sum(axis=2).min(axis=1))\\n', "*

 * *            "'        for palette in tqdm(lab_palettes)\\n', '    ]\\n', ')']}, 12: {'source': "*

 * *            '[\'np.save("/storage/data/palette/lab_palette […]*

```diff
@@ -33,26 +33,25 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def rgb_to_lab(rgb_palette):\n",
                 "    return rgb2lab(rgb_palette.reshape(-1, 1, 3) / 255).squeeze()\n",
                 "\n",
+                "\n",
                 "lab_palette = rgb_to_lab(rgb_palettes[1])"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "lab_palettes = np.stack([\n",
-                "    rgb_to_lab(palette) for palette in tqdm(rgb_palettes)\n",
-                "])"
+                "lab_palettes = np.stack([rgb_to_lab(palette) for palette in tqdm(rgb_palettes)])"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# create permutations"
@@ -87,20 +86,20 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "distance_matrix = np.stack([(\n",
-                "        np.linalg.norm(palette_permutations - palette, axis=3)\n",
-                "        .sum(axis=2)\n",
-                "        .min(axis=1))\n",
-                "    for palette in tqdm(lab_palettes)\n",
-                "])"
+                "distance_matrix = np.stack(\n",
+                "    [\n",
+                "        (np.linalg.norm(palette_permutations - palette, axis=3).sum(axis=2).min(axis=1))\n",
+                "        for palette in tqdm(lab_palettes)\n",
+                "    ]\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -117,17 +116,17 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "np.save('/storage/data/palette/lab_palettes', lab_palettes)\n",
-                "np.save('/storage/data/palette/palette_permutations', palette_permutations)\n",
-                "np.save('/storage/data/palette/distance_matrix', distance_matrix)"
+                "np.save(\"/storage/data/palette/lab_palettes\", lab_palettes)\n",
+                "np.save(\"/storage/data/palette/palette_permutations\", palette_permutations)\n",
+                "np.save(\"/storage/data/palette/distance_matrix\", distance_matrix)"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
             "display_name": "Environment (conda_pytorch_p36)",
             "language": "python",
```

### Comparing `weco-datascience-0.1.8/research_notebooks/palette/notebooks/11 - siamese network.ipynb` & `weco-datascience-0.1.9/notebooks/palette/notebooks/11 - siamese network.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9967902446716499%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(7, \'\\n\'), (8, \'device = torch.device("cuda" if '*

 * *            'torch.cuda.is_available() else "cpu")\')], delete: [7]}}, 2: {\'source\': [\'palettes '*

 * *            '= torch.Tensor(np.load("/storage/data/palette/lab_palettes.npy"))\\n\', '*

 * *            "'palette_permutations = torch.Tensor(\\n', '    "*

 * *            'np.load("/storage/data/palette/palette_permutations.npy")\\n\', \')\\n\', \'distances '*

 * *            '= torch.Tensor(np.load("/storage/data/palette/distanc […]*

```diff
@@ -9,15 +9,16 @@
                 "import numpy as np\n",
                 "\n",
                 "from tqdm import tqdm\n",
                 "\n",
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# load data"
@@ -25,17 +26,19 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "palettes = torch.Tensor(np.load('/storage/data/palette/lab_palettes.npy'))\n",
-                "palette_permutations = torch.Tensor(np.load('/storage/data/palette/palette_permutations.npy'))\n",
-                "distances = torch.Tensor(np.load('/storage/data/palette/distance_matrix.npy'))"
+                "palettes = torch.Tensor(np.load(\"/storage/data/palette/lab_palettes.npy\"))\n",
+                "palette_permutations = torch.Tensor(\n",
+                "    np.load(\"/storage/data/palette/palette_permutations.npy\")\n",
+                ")\n",
+                "distances = torch.Tensor(np.load(\"/storage/data/palette/distance_matrix.npy\"))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -83,35 +86,37 @@
                 "class PaletteDistanceDataset(Dataset):\n",
                 "    def __init__(self, palette_permutations, distances, length):\n",
                 "        self.palette_permutations = palette_permutations\n",
                 "        self.dim_1 = palette_permutations.shape[0]\n",
                 "        self.dim_2 = palette_permutations.shape[1]\n",
                 "        self.distances = distances\n",
                 "        self.length = length\n",
-                "        \n",
+                "\n",
                 "    def __getitem__(self, ix):\n",
                 "        ix_1, ix_2 = np.random.randint(self.dim_1, size=2)\n",
                 "        sub_ix_1, sub_ix_2 = np.random.randint(self.dim_2, size=2)\n",
-                "        \n",
+                "\n",
                 "        palette_1 = self.palette_permutations[ix_1, sub_ix_1]\n",
                 "        palette_2 = self.palette_permutations[ix_2, sub_ix_2]\n",
                 "        target_distance = self.distances[ix_1, ix_2]\n",
                 "        return palette_1, palette_2, target_distance\n",
-                "    \n",
+                "\n",
                 "    def __len__(self):\n",
                 "        return self.length"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train_dataset = PaletteDistanceDataset(train_palettes, train_distances, length=100_000_000)\n",
+                "train_dataset = PaletteDistanceDataset(\n",
+                "    train_palettes, train_distances, length=100_000_000\n",
+                ")\n",
                 "test_dataset = PaletteDistanceDataset(test_palettes, test_distances, length=1_000_000)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -124,25 +129,18 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "train_loader = DataLoader(\n",
-                "    dataset=train_dataset,  \n",
-                "    batch_size=4096,\n",
-                "    num_workers=5,\n",
-                "    shuffle=True\n",
+                "    dataset=train_dataset, batch_size=4096, num_workers=5, shuffle=True\n",
                 ")\n",
                 "\n",
-                "test_loader = DataLoader(\n",
-                "    dataset=test_dataset,  \n",
-                "    batch_size=4096,\n",
-                "    shuffle=True\n",
-                ")"
+                "test_loader = DataLoader(dataset=test_dataset, batch_size=4096, shuffle=True)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# write network"
@@ -154,35 +152,39 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "class PaletteEmbedder(nn.Module):\n",
                 "    def __init__(self):\n",
                 "        super().__init__()\n",
                 "        self.first_transform = nn.Sequential(\n",
-                "            nn.Linear(3, 6), nn.ReLU(),\n",
-                "            nn.Linear(6, 12)\n",
+                "            nn.Linear(3, 6), nn.ReLU(), nn.Linear(6, 12)\n",
                 "        )\n",
                 "        self.second_transform = nn.Sequential(\n",
-                "            nn.Linear(60, 128), nn.ReLU(),\n",
-                "            nn.Linear(128, 128), nn.ReLU(),\n",
-                "            nn.Linear(128, 256), nn.ReLU(),\n",
-                "            nn.Linear(256, 256), nn.ReLU(),\n",
-                "            nn.Linear(256, 128), nn.ReLU(),\n",
-                "            nn.Linear(128, 128), nn.ReLU(),\n",
-                "            nn.Linear(128, 30)\n",
+                "            nn.Linear(60, 128),\n",
+                "            nn.ReLU(),\n",
+                "            nn.Linear(128, 128),\n",
+                "            nn.ReLU(),\n",
+                "            nn.Linear(128, 256),\n",
+                "            nn.ReLU(),\n",
+                "            nn.Linear(256, 256),\n",
+                "            nn.ReLU(),\n",
+                "            nn.Linear(256, 128),\n",
+                "            nn.ReLU(),\n",
+                "            nn.Linear(128, 128),\n",
+                "            nn.ReLU(),\n",
+                "            nn.Linear(128, 30),\n",
                 "        )\n",
                 "\n",
                 "    def forward(self, input_palettes):\n",
                 "        batch_size = input_palettes.shape[0]\n",
                 "        intermediate = self.first_transform(input_palettes)\n",
                 "        flattened = intermediate.reshape(batch_size, -1)\n",
                 "        embedded = self.second_transform(flattened)\n",
                 "        return embedded\n",
                 "\n",
-                "    \n",
                 "\n",
                 "class SiameseNetwork(nn.Module):\n",
                 "    def __init__(self):\n",
                 "        super().__init__()\n",
                 "        self.palette_embedder = PaletteEmbedder()\n",
                 "\n",
                 "    def forward(self, palettes_1, palettes_2):\n",
@@ -224,62 +226,62 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "if device.type == 'cuda':\n",
+                "if device.type == \"cuda\":\n",
                 "    model.cuda()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def train(model, train_loader, distance_metric, loss_function, optimiser, n_epochs):\n",
                 "    for epoch in range(n_epochs):\n",
                 "        model.train()\n",
                 "        train_loop = tqdm(train_loader)\n",
                 "        for palettes_1, palettes_2, target_distances in train_loop:\n",
-                "            if device.type == 'cuda':\n",
+                "            if device.type == \"cuda\":\n",
                 "                palettes_1 = palettes_1.cuda(non_blocking=True)\n",
                 "                palettes_2 = palettes_2.cuda(non_blocking=True)\n",
                 "                target_distances = target_distances.cuda(non_blocking=True)\n",
-                "            \n",
+                "\n",
                 "            optimiser.zero_grad()\n",
                 "            embeddings_1, embeddings_2 = model(palettes_1, palettes_2)\n",
                 "\n",
                 "            pred_distances = distance_metric(embeddings_1, embeddings_2)\n",
                 "            loss = loss_function(target_distances, pred_distances)\n",
                 "            loss.backward()\n",
                 "            optimiser.step()\n",
                 "\n",
                 "            train_losses.append(np.sqrt(loss.cpu().item()))\n",
-                "            train_loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
-                "            train_loop.set_postfix({'loss': np.mean(train_losses[-100:])})\n",
-                "        \n",
+                "            train_loop.set_description(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
+                "            train_loop.set_postfix({\"loss\": np.mean(train_losses[-100:])})\n",
+                "\n",
                 "        model.eval()\n",
                 "        test_loop = tqdm(test_loader)\n",
                 "        for palettes_1, palettes_2, target_distances in test_loop:\n",
-                "            if device.type == 'cuda':\n",
+                "            if device.type == \"cuda\":\n",
                 "                palettes_1 = palettes_1.cuda(non_blocking=True)\n",
                 "                palettes_2 = palettes_2.cuda(non_blocking=True)\n",
                 "                target_distances = target_distances.cuda(non_blocking=True)\n",
                 "\n",
                 "            embeddings_1, embeddings_2 = model(palettes_1, palettes_2)\n",
                 "\n",
                 "            pred_distances = distance_metric(embeddings_1, embeddings_2)\n",
                 "            loss = loss_function(target_distances, pred_distances)\n",
                 "\n",
                 "            test_losses.append(np.sqrt(loss.cpu().item()))\n",
-                "            test_loop.set_description('Test')\n",
-                "            test_loop.set_postfix({'loss': np.mean(test_losses[-100:])})"
+                "            test_loop.set_description(\"Test\")\n",
+                "            test_loop.set_postfix({\"loss\": np.mean(test_losses[-100:])})"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -289,15 +291,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "torch.save(model.state_dict(), '/storage/code/palette/model_state_dict.pt')"
+                "torch.save(model.state_dict(), \"/storage/code/palette/model_state_dict.pt\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# plot losses"
@@ -308,16 +310,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import pandas as pd"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
```

### Comparing `weco-datascience-0.1.8/research_notebooks/palette/notebooks/12 - embed all palettes.ipynb` & `weco-datascience-0.1.9/notebooks/palette/notebooks/12 - embed all palettes.ipynb`

 * *Files 4% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9949860423967567%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\'), (17, \'\\n\'), (18, \'device = '*

 * *            'torch.device("cuda" if torch.cuda.is_available() else "cpu")\')], delete: [16, 4, '*

 * *            "3]}}, 2: {'source': ['palettes = "*

 * *            'torch.Tensor(np.load("/storage/data/palette/all_palettes.npy"))\\n\', \'image_ids = '*

 * *            'np.load("/storage/data/palette/all_image_ids.npy")\']}, 4: […]*

```diff
@@ -5,28 +5,30 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import pandas as pd\n",
                 "from IPython.core.display import display, HTML\n",
                 "\n",
                 "import numpy as np\n",
                 "\n",
                 "from tqdm import tqdm\n",
                 "\n",
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# load data"
@@ -34,16 +36,16 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "palettes = torch.Tensor(np.load('/storage/data/palette/all_palettes.npy'))\n",
-                "image_ids = np.load('/storage/data/palette/all_image_ids.npy')"
+                "palettes = torch.Tensor(np.load(\"/storage/data/palette/all_palettes.npy\"))\n",
+                "image_ids = np.load(\"/storage/data/palette/all_image_ids.npy\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# load model"
@@ -55,35 +57,39 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "class PaletteEmbedder(nn.Module):\n",
                 "    def __init__(self):\n",
                 "        super().__init__()\n",
                 "        self.initial_transform = nn.Sequential(\n",
-                "            nn.Linear(3, 6), nn.ReLU(),\n",
-                "            nn.Linear(6, 12)\n",
+                "            nn.Linear(3, 6), nn.ReLU(), nn.Linear(6, 12)\n",
                 "        )\n",
                 "        self.embedder = nn.Sequential(\n",
-                "            nn.Linear(60, 128), nn.ReLU(),\n",
-                "            nn.Linear(128, 128), nn.ReLU(),\n",
-                "            nn.Linear(128, 256), nn.ReLU(),\n",
-                "            nn.Linear(256, 256), nn.ReLU(),\n",
-                "            nn.Linear(256, 128), nn.ReLU(),\n",
-                "            nn.Linear(128, 128), nn.ReLU(),\n",
-                "            nn.Linear(128, 30)\n",
+                "            nn.Linear(60, 128),\n",
+                "            nn.ReLU(),\n",
+                "            nn.Linear(128, 128),\n",
+                "            nn.ReLU(),\n",
+                "            nn.Linear(128, 256),\n",
+                "            nn.ReLU(),\n",
+                "            nn.Linear(256, 256),\n",
+                "            nn.ReLU(),\n",
+                "            nn.Linear(256, 128),\n",
+                "            nn.ReLU(),\n",
+                "            nn.Linear(128, 128),\n",
+                "            nn.ReLU(),\n",
+                "            nn.Linear(128, 30),\n",
                 "        )\n",
                 "\n",
                 "    def forward(self, input_palettes):\n",
                 "        batch_size = input_palettes.shape[0]\n",
                 "        intermediate = self.initial_transform(input_palettes)\n",
                 "        flattened = intermediate.reshape(batch_size, -1)\n",
                 "        embedded = self.embedder(flattened)\n",
                 "        return embedded\n",
                 "\n",
-                "    \n",
                 "\n",
                 "class SiameseNetwork(nn.Module):\n",
                 "    def __init__(self):\n",
                 "        super().__init__()\n",
                 "        self.palette_embedder = PaletteEmbedder()\n",
                 "\n",
                 "    def forward(self, palettes_1, palettes_2):\n",
@@ -95,18 +101,17 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "model = SiameseNetwork()\n",
-                "model.load_state_dict(torch.load(\n",
-                "    '/storage/code/palette/model_state_dict.pt',\n",
-                "    map_location='cpu'\n",
-                "))\n",
+                "model.load_state_dict(\n",
+                "    torch.load(\"/storage/code/palette/model_state_dict.pt\", map_location=\"cpu\")\n",
+                ")\n",
                 "model.eval()\n",
                 "embedder = model.palette_embedder"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -151,52 +156,56 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "distances = cdist(sample, sample, metric='euclidean')"
+                "distances = cdist(sample, sample, metric=\"euclidean\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_url = 'https://iiif.wellcomecollection.org/image/{}.jpg/full/960,/0/default.jpg'"
+                "image_url = \"https://iiif.wellcomecollection.org/image/{}.jpg/full/960,/0/default.jpg\""
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "test_ix = np.random.randint(10000)\n",
-                "display(HTML(\n",
-                "    \"<a href='{}' target='_blank'>query image</a>\".format(\n",
-                "        image_url.format(image_ids[test_ix])\n",
+                "display(\n",
+                "    HTML(\n",
+                "        \"<a href='{}' target='_blank'>query image</a>\".format(\n",
+                "            image_url.format(image_ids[test_ix])\n",
+                "        )\n",
                 "    )\n",
-                "))"
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "for ix in distances[test_ix].argsort()[1:6]:\n",
-                "    display(HTML(\n",
-                "        \"<a href='{}' target='_blank'>image</a>\".format(\n",
-                "            image_url.format(image_ids[ix])\n",
+                "    display(\n",
+                "        HTML(\n",
+                "            \"<a href='{}' target='_blank'>image</a>\".format(\n",
+                "                image_url.format(image_ids[ix])\n",
+                "            )\n",
                 "        )\n",
-                "    ))"
+                "    )"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# save the embeddings"
@@ -204,15 +213,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "np.save('/storage/data/palette/embedded_palettes', embedded_palettes.detach().numpy())"
+                "np.save(\"/storage/data/palette/embedded_palettes\", embedded_palettes.detach().numpy())"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
```

### Comparing `weco-datascience-0.1.8/research_notebooks/palette/notebooks/13 - create an nmslib index.ipynb` & `weco-datascience-0.1.9/notebooks/palette/notebooks/13 - create an nmslib index.ipynb`

 * *Files 3% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.994512987012987%*

 * *Differences: {"'cells'": "{1: {'source': ['image_ids = "*

 * *            'np.load("/storage/data/palette/all_image_ids.npy")\\n\', \'embedded_palettes = '*

 * *            'np.load("/storage/data/palette/embedded_palettes.npy")\']}, 2: {\'source\': {insert: '*

 * *            '[(0, \'index = nmslib.init(method="hnsw", space="l2")\\n\'), (2, '*

 * *            '\'index.createIndex({"post": 2}, print_progress=True)\')], delete: [2, 0]}}, 4: '*

 * *            "{'source': {insert: [(4, 'image_url = "*

 * *            '"https://iiif.wellcomecollection. […]*

```diff
@@ -13,27 +13,27 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "image_ids = np.load('/storage/data/palette/all_image_ids.npy')\n",
-                "embedded_palettes = np.load('/storage/data/palette/embedded_palettes.npy')"
+                "image_ids = np.load(\"/storage/data/palette/all_image_ids.npy\")\n",
+                "embedded_palettes = np.load(\"/storage/data/palette/embedded_palettes.npy\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "index = nmslib.init(method='hnsw', space='l2')\n",
+                "index = nmslib.init(method=\"hnsw\", space=\"l2\")\n",
                 "index.addDataPointBatch(embedded_palettes)\n",
-                "index.createIndex({'post': 2}, print_progress=True)"
+                "index.createIndex({\"post\": 2}, print_progress=True)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# test"
@@ -45,32 +45,36 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "query_ix = np.random.choice(len(image_ids))\n",
                 "query_id = image_ids[query_ix]\n",
                 "query_embedding = embedded_palettes[query_ix]\n",
                 "\n",
-                "image_url = 'https://iiif.wellcomecollection.org/image/{}.jpg/full/960,/0/default.jpg'\n",
+                "image_url = \"https://iiif.wellcomecollection.org/image/{}.jpg/full/960,/0/default.jpg\"\n",
                 "\n",
-                "display(HTML(\n",
-                "    \"<a href='{}' target='_blank'>query image</a>\".format(\n",
-                "        image_url.format(query_id)\n",
+                "display(\n",
+                "    HTML(\n",
+                "        \"<a href='{}' target='_blank'>query image</a>\".format(\n",
+                "            image_url.format(query_id)\n",
+                "        )\n",
                 "    )\n",
-                "))\n",
+                ")\n",
                 "\n",
                 "print()\n",
                 "\n",
                 "neighbour_indexes, neighbour_distances = index.knnQuery(query_embedding, k=6)\n",
                 "\n",
                 "for ix in neighbour_indexes[1:]:\n",
-                "    display(HTML(\n",
-                "        \"<a href='{}' target='_blank'>image</a>\".format(\n",
-                "            image_url.format(image_ids[ix])\n",
+                "    display(\n",
+                "        HTML(\n",
+                "            \"<a href='{}' target='_blank'>image</a>\".format(\n",
+                "                image_url.format(image_ids[ix])\n",
+                "            )\n",
                 "        )\n",
-                "    ))"
+                "    )"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# save"
@@ -78,15 +82,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "index.saveIndex('/storage/data/palette/palette_embeddings.hnsw', save_data=True)"
+                "index.saveIndex(\"/storage/data/palette/palette_embeddings.hnsw\", save_data=True)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# stick it in s3 as well"
@@ -94,30 +98,28 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "(    \"<a href='{}' target='_blank'>query image</a>\".format(\n",
-                "        image_url.format(query_id)\n",
-                "))"
+                "(\"<a href='{}' target='_blank'>query image</a>\".format(image_url.format(query_id)))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "for i, ix in enumerate(neighbour_indexes[1:]):\n",
                 "    print(\n",
-                "    (\n",
+                "        (\n",
                 "            \"<a href='{0}' target='_blank'>result {1}</a>\".format(\n",
-                "                image_url.format(image_ids[ix]), i+1\n",
+                "                image_url.format(image_ids[ix]), i + 1\n",
                 "            )\n",
                 "        )\n",
                 "    )"
             ]
         },
         {
             "cell_type": "code",
```

### Comparing `weco-datascience-0.1.8/research_notebooks/palette/src/utils.py` & `weco-datascience-0.1.9/notebooks/palette/src/utils.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/research_notebooks/search_intentions/Progression_of_search.ipynb` & `weco-datascience-0.1.9/notebooks/search_intentions/notebooks/Progression_of_search.ipynb`

 * *Files 16% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9897572345214889%*

 * *Differences: {"'cells'": "{0: {'source': {insert: [(3, 'from elasticsearch import helpers\\n'), (4, 'from "*

 * *            "collections.abc import MutableMapping')], delete: [4, 3]}}, 1: {'source': {insert: "*

 * *            '[(0, \'def flatten(nested_dict, parent_key=""):\\n\'), (8, \'\\n\'), (9, \'\\n\'), '*

 * *            '(11, \'    "sort": [{"timestamp": "desc"}],\\n\'), (12, \'    "query": '*

 * *            '{"match_phrase": {"event": "Search"}},\\n\'), (13, \'    "size": 100000,\\n\'), (16, '*

 * *            '\'response = es.search […]*

```diff
@@ -5,150 +5,130 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "import pandas as pd\n",
                 "import numpy as np\n",
                 "\n",
-                "from elasticsearch import helpers \n",
-                "from collections.abc import MutableMapping\n"
+                "from elasticsearch import helpers\n",
+                "from collections.abc import MutableMapping"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "def flatten(nested_dict, parent_key=''):\n",
+                "def flatten(nested_dict, parent_key=\"\"):\n",
                 "    items = []\n",
                 "    for k, v in nested_dict.items():\n",
                 "        if isinstance(v, MutableMapping):\n",
                 "            items.extend(flatten(v, k).items())\n",
                 "        else:\n",
                 "            items.append((k, v))\n",
                 "    return dict(items)\n",
-                "    \n",
-                "    \n",
+                "\n",
+                "\n",
                 "query = {\n",
-                "  \"sort\": [\n",
-                "    {\n",
-                "      \"timestamp\": \"desc\"\n",
-                "    }\n",
-                "  ],\n",
-                "  \"query\": {\n",
-                "    \"match_phrase\": {\n",
-                "      \"event\": \"Search\"\n",
-                "    }\n",
-                "  },\n",
-                "  \"size\": 100000\n",
+                "    \"sort\": [{\"timestamp\": \"desc\"}],\n",
+                "    \"query\": {\"match_phrase\": {\"event\": \"Search\"}},\n",
+                "    \"size\": 100000,\n",
                 "}\n",
                 "\n",
-                "response = es.search(\n",
-                "    body=query,\n",
-                "    index=\"search_relevance_implicit\"\n",
-                ")\n",
+                "response = es.search(body=query, index=\"search_relevance_implicit\")\n",
                 "\n",
-                "df = pd.DataFrame([\n",
-                "    flatten(event['_source']) for event in response['hits']['hits']\n",
-                "])"
+                "df = pd.DataFrame([flatten(event[\"_source\"]) for event in response[\"hits\"][\"hits\"]])"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "for col in df.columns: \n",
-                "    print(col) \n"
+                "for col in df.columns:\n",
+                "    print(col)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# grab only the columns needed\n",
-                "df2=df[['query', 'timestamp', 'anonymousId','network','event']]\n",
+                "df2 = df[[\"query\", \"timestamp\", \"anonymousId\", \"network\", \"event\"]]\n",
                 "\n",
-                "#note: Python client automatically indexes from latest to earliest.  To check start date:\n",
+                "# note: Python client automatically indexes from latest to earliest.  To check start date:\n",
                 "\n",
-                "sorted=df2.sort_values(by=['timestamp'], ascending=True) \n",
+                "sorted = df2.sort_values(by=[\"timestamp\"], ascending=True)\n",
                 "sorted.head(2)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "#data cleansing\n",
-                "df3=df2.loc[(df2['network'] != 'StaffCorporateDevices') & (df2['event'] == 'Search')]\n",
-                "            \n",
-                "#note: Python client automatically indexes from latest to earliest.  To check start date:  'Search result selected']\n",
+                "# data cleansing\n",
+                "df3 = df2.loc[(df2[\"network\"] != \"StaffCorporateDevices\") & (df2[\"event\"] == \"Search\")]\n",
+                "\n",
+                "# note: Python client automatically indexes from latest to earliest.  To check start date:  'Search result selected']\n",
                 "df3.head()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "#count searches and only keep data for users who search 3 times or more\n",
-                "counts=df3.groupby('anonymousId').count()[['query']]\n",
-                "counts2=counts.loc[(counts['query'] >3)]\n",
-                "#counts2.head()"
+                "# count searches and only keep data for users who search 3 times or more\n",
+                "counts = df3.groupby(\"anonymousId\").count()[[\"query\"]]\n",
+                "counts2 = counts.loc[(counts[\"query\"] > 3)]\n",
+                "# counts2.head()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "counts3=counts2.drop(columns=['query'])\n",
-                "#counts3.head()\n"
+                "counts3 = counts2.drop(columns=[\"query\"])\n",
+                "# counts3.head()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "#merge \n",
-                "searches_3plus=counts3.merge(df3, how='left', on='anonymousId')\n",
-                "\n",
-                "\n",
-                "\n",
-                "\n",
-                "\n",
-                "\n",
+                "# merge\n",
+                "searches_3plus = counts3.merge(df3, how=\"left\", on=\"anonymousId\")\n",
                 "\n",
                 "\n",
                 "pd.to_datetime(stamps, format=\"%Y%m%d:%H:%M:%S.%f\").sort_values()\n",
                 "\n",
-                "searches_3plus.sort_values(by='timestamp', ascending=False)\n",
-                "searches_3plus.head()\n",
-                "\n"
+                "searches_3plus.sort_values(by=\"timestamp\", ascending=False)\n",
+                "searches_3plus.head()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "#output data to csv to check\n",
+                "# output data to csv to check\n",
                 "\n",
-                "searches_3plus.to_csv('searches_3plus.csv') "
+                "searches_3plus.to_csv(\"searches_3plus.csv\")"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
             "display_name": "Python 3",
             "language": "python",
```

### Comparing `weco-datascience-0.1.8/research_notebooks/search_intentions/Top_workIds_2days.ipynb` & `weco-datascience-0.1.9/notebooks/analysis/notebooks/breadth_metric.ipynb`

 * *Files 20% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9805483449477352%*

 * *Differences: {"'cells'": "{0: {'source': ['with open (“./passwords.json”, “r”)\\n', 'as f:\\n', 'sensitive_data "*

 * *            "= json.load(f)']}, 1: {'source': {insert: [(0, 'from collections.abc import "*

 * *            "MutableMapping\\n'), (2, 'import numpy as np\\n'), (3, 'import pandas as pd\\n'), (4, "*

 * *            "'from elasticsearch import helpers\\n'), (5, 'from elasticsearch.helpers import "*

 * *            "scan\\n'), (6, '\\n'), (7, '\\n'), (8, 'def flatten(nested_dict, "*

 * *            'parent_key=""):\\n\'), (9, \'   […]*

```diff
@@ -2,179 +2,180 @@
     "cells": [
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "from elasticsearch import helpers \n",
+                "with open (\u201c./passwords.json\u201d, \u201cr\u201d)\n",
+                "as f:\n",
+                "sensitive_data = json.load(f)"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
                 "from collections.abc import MutableMapping\n",
-                "import pandas as pd\n",
+                "\n",
                 "import numpy as np\n",
+                "import pandas as pd\n",
+                "from elasticsearch import helpers\n",
+                "from elasticsearch.helpers import scan\n",
                 "\n",
-                "def flatten(nested_dict, parent_key=''):\n",
+                "\n",
+                "def flatten(nested_dict, parent_key=\"\"):\n",
                 "    items = []\n",
                 "    for k, v in nested_dict.items():\n",
                 "        if isinstance(v, MutableMapping):\n",
                 "            items.extend(flatten(v, k).items())\n",
                 "        else:\n",
                 "            items.append((k, v))\n",
                 "    return dict(items)\n",
-                "    \n",
-                "    \n",
+                "\n",
+                "\n",
                 "query = {\n",
-                "  \"sort\": [\n",
-                "    {\n",
-                "      \"timestamp\": \"desc\"\n",
-                "    }\n",
-                "  ],\n",
-                "  \"query\": {\n",
-                "    \"match_phrase\": {\n",
-                "      \"event\": \"Search result selected\"\n",
-                "    }\n",
-                "  },\n",
-                "  \"size\": 10000\n",
+                "    \"sort\": [{\"timestamp\": \"desc\"}],\n",
+                "    \"query\": {\"match_phrase\": {\"event\": \"Search result selected\"}},\n",
                 "}\n",
                 "\n",
-                "response = es.search(\n",
-                "    body=query,\n",
-                "    index=\"search_relevance_implicit\"\n",
+                "\n",
+                "# note: scan works fast because it grabs data unsorted.\n",
+                "# grabs 100,000 without scan. seems to have trouble past 500,000\n",
+                "response = helpers.scan(\n",
+                "    es,\n",
+                "    query=query,\n",
+                "    preserve_order=True,\n",
+                "    index=\"search_relevance_implicit\",\n",
                 ")\n",
+                "n_events_to_fetch = 250000\n",
                 "\n",
-                "df = pd.DataFrame([\n",
-                "    flatten(event['_source']) for event in response['hits']['hits']\n",
-                "])"
+                "\n",
+                "df = pd.DataFrame(\n",
+                "    [flatten(next(response)[\"_source\"]) for _ in range(n_events_to_fetch)]\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "#note: Python client automatically indexes from latest to earliest.  To check start date:\n",
+                "# note: Python client automatically indexes from latest to earliest.\n",
+                "\n",
+                "# \"timestamp\": datetime(2010, 10, 10, 10, 10, 10)\n",
                 "\n",
-                "sorted=df.sort_values(by=['timestamp'], ascending=True) \n",
+                "# To check start date:\n",
+                "\n",
+                "sorted = df.sort_values(by=[\"timestamp\"], ascending=True)\n",
                 "sorted.head()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "#data cleansing\n",
-                "\n",
-                "df=df.loc[df['network'] != 'StaffCorporateDevices'] #remove staff usage\n",
-                "#df=df.loc[df['timestamp'] > '2019-11-07 00:00:00']  #only use searches after AND implemented\n",
-                "#df=df.loc[df['event'] != 'Search landing']\n",
-                "\n",
-                "df2=df.loc[df['timestamp'] >= '2020-03-03 00:00:00']  #grabs 2 days' of data, ie from 1/3/20\n",
+                "# remove staff usage, limit time frame to 1/7/20 - 30/9/20\n",
+                "df2 = df.loc[\n",
+                "    (df[\"network\"] != \"StaffCorporateDevices\")\n",
+                "    & (df[\"timestamp\"] >= \"2020-07-01\")\n",
+                "    & (df[\"timestamp\"] < \"2020-10-01\")\n",
+                "]\n",
                 "\n",
+                "# grab only the columns needed\n",
+                "df2 = df2[[\"id\", \"resultWorkType\", \"anonymousId\", \"timestamp\"]]\n",
                 "\n",
+                "# sort the dataframe\n",
+                "df2.sort_values(by=[\"anonymousId\", \"id\"], inplace=True)\n",
                 "df2.head(5)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df.sort_values(by=['anonymousId','timestamp'])\n",
-                "unique_selects=df.drop_duplicates(subset='anonymousId', keep='first')\n",
-                "unique_selects.head(5)"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": null,
-            "metadata": {},
-            "outputs": [],
-            "source": [
-                "#How many workIds?\n",
-                "summary=unique_selects.groupby('id').count()[['anonymousId']]\n",
-                "summary\n"
+                "# dedupe\n",
+                "\n",
+                "df2.sort_values(by=[\"id\", \"anonymousId\"])\n",
+                "df3 = df2.drop_duplicates(subset=[\"anonymousId\", \"id\"], keep=\"first\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "#How many workIds viewed once?\n",
-                "viewed_once=summary.loc[summary['anonymousId']<=1]\n",
-                "count=viewed_once['anonymousId'].count()\n",
-                "print(count)"
+                "# How many workIds?\n",
+                "summary = df3.groupby(\"id\").count()[[\"anonymousId\"]]\n",
+                "print(summary)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "#How many workIds viewed twice?\n",
-                "viewed_twice=summary.loc[summary['anonymousId']==2]\n",
-                "count=viewed_twice['anonymousId'].count()\n",
-                "print(count)"
+                "# How many workIds?\n",
+                "summary[\"anonymousId\"].count()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "#Create bar chart for workIds viewed more than twice.\n",
-                "\n",
-                "summary.sort_values(by='anonymousId', ascending=False)[:19].plot.bar(legend=False)"
+                "# create index for dataframe\n",
+                "sorted = summary.sort_values(by=[\"anonymousId\"], ascending=False)\n",
+                "sorted.head()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "#calculate skewness\n",
+                "# output data to csv to check\n",
                 "\n",
-                "skew=summary.skew()\n",
-                "print(skew)"
+                "summary.to_csv(\"selects_by_workId.csv\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
-            "source": [
-                "#output data to csv to check\n",
-                "\n",
-                "summary.to_csv('skew_for_2_days.csv') "
-            ]
+            "source": []
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.4"
+            "version": "3.9.5"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 4
 }
```

### Comparing `weco-datascience-0.1.8/research_notebooks/search_intentions/Top_workIds_3days.ipynb` & `weco-datascience-0.1.9/notebooks/search_intentions/notebooks/Top_workIds_2days.ipynb`

 * *Files 21% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9908988970588235%*

 * *Differences: {"'cells'": "{0: {'source': {insert: [(0, 'from elasticsearch import helpers\\n'), (1, 'from "*

 * *            "collections.abc import MutableMapping\\n'), (5, '\\n'), (6, 'def flatten(nested_dict, "*

 * *            'parent_key=""):\\n\'), (14, \'\\n\'), (15, \'\\n\'), (17, \'    "sort": '*

 * *            '[{"timestamp": "desc"}],\\n\'), (18, \'    "query": {"match_phrase": {"event": '*

 * *            '"Search result selected"}},\\n\'), (19, \'    "size": 10000,\\n\'), (22, \'response = '*

 * *            'es.search(body=query, […]*

```diff
@@ -2,161 +2,151 @@
     "cells": [
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "from elasticsearch import helpers \n",
-                "from collections import MutableMapping\n",
+                "from elasticsearch import helpers\n",
+                "from collections.abc import MutableMapping\n",
                 "import pandas as pd\n",
                 "import numpy as np\n",
                 "\n",
-                "def flatten(nested_dict, parent_key=''):\n",
+                "\n",
+                "def flatten(nested_dict, parent_key=\"\"):\n",
                 "    items = []\n",
                 "    for k, v in nested_dict.items():\n",
                 "        if isinstance(v, MutableMapping):\n",
                 "            items.extend(flatten(v, k).items())\n",
                 "        else:\n",
                 "            items.append((k, v))\n",
                 "    return dict(items)\n",
-                "    \n",
-                "    \n",
+                "\n",
+                "\n",
                 "query = {\n",
-                "  \"sort\": [\n",
-                "    {\n",
-                "      \"timestamp\": \"desc\"\n",
-                "    }\n",
-                "  ],\n",
-                "  \"query\": {\n",
-                "    \"match_phrase\": {\n",
-                "      \"event\": \"Search result selected\"\n",
-                "    }\n",
-                "  },\n",
-                "  \"size\": 10000\n",
+                "    \"sort\": [{\"timestamp\": \"desc\"}],\n",
+                "    \"query\": {\"match_phrase\": {\"event\": \"Search result selected\"}},\n",
+                "    \"size\": 10000,\n",
                 "}\n",
                 "\n",
-                "response = es.search(\n",
-                "    body=query,\n",
-                "    index=\"search_relevance_implicit\"\n",
-                ")\n",
+                "response = es.search(body=query, index=\"search_relevance_implicit\")\n",
                 "\n",
-                "df = pd.DataFrame([\n",
-                "    flatten(event['_source']) for event in response['hits']['hits']\n",
-                "])"
+                "df = pd.DataFrame([flatten(event[\"_source\"]) for event in response[\"hits\"][\"hits\"]])"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "#note: Python client automatically indexes from latest to earliest.  To check start date:\n",
+                "# note: Python client automatically indexes from latest to earliest.  To check start date:\n",
                 "\n",
-                "sorted=df.sort_values(by=['timestamp'], ascending=True) \n",
+                "sorted = df.sort_values(by=[\"timestamp\"], ascending=True)\n",
                 "sorted.head()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "#data cleansing\n",
-                "\n",
-                "df=df.loc[df['network'] != 'StaffCorporateDevices'] #remove staff usage\n",
-                "#df=df.loc[df['timestamp'] > '2019-11-07 00:00:00']  #only use searches after AND implemented\n",
-                "#df=df.loc[df['event'] != 'Search landing']\n",
+                "# data cleansing\n",
                 "\n",
-                "df2=df.loc[df['timestamp'] >= '2020-03-01 00:00:00']  #grabs 2 days' of data, ie from 1/3/20\n",
+                "df = df.loc[df[\"network\"] != \"StaffCorporateDevices\"]  # remove staff usage\n",
+                "# df=df.loc[df['timestamp'] > '2019-11-07 00:00:00']  #only use searches after AND implemented\n",
+                "# df=df.loc[df['event'] != 'Search landing']\n",
+                "\n",
+                "df2 = df.loc[\n",
+                "    df[\"timestamp\"] >= \"2020-03-03 00:00:00\"\n",
+                "]  # grabs 2 days' of data, ie from 1/3/20\n",
                 "\n",
                 "\n",
                 "df2.head(5)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df.sort_values(by=['anonymousId','timestamp'])\n",
-                "unique_selects=df.drop_duplicates(subset='anonymousId', keep='first')\n",
+                "df.sort_values(by=[\"anonymousId\", \"timestamp\"])\n",
+                "unique_selects = df.drop_duplicates(subset=\"anonymousId\", keep=\"first\")\n",
                 "unique_selects.head(5)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "#How many workIds?\n",
-                "summary=unique_selects.groupby('id').count()[['anonymousId']]\n",
-                "summary\n"
+                "summary = unique_selects.groupby(\"id\").count()[[\"anonymousId\"]]\n",
+                "summary"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "#How many workIds viewed once?\n",
-                "viewed_once=summary.loc[summary['anonymousId']<=1]\n",
-                "count=viewed_once['anonymousId'].count()\n",
+                "viewed_once = summary.loc[summary[\"anonymousId\"] <= 1]\n",
+                "count = viewed_once[\"anonymousId\"].count()\n",
                 "print(count)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "#How many workIds viewed twice?\n",
-                "viewed_twice=summary.loc[summary['anonymousId']==2]\n",
-                "count=viewed_twice['anonymousId'].count()\n",
+                "viewed_twice = summary.loc[summary[\"anonymousId\"] == 2]\n",
+                "count = viewed_twice[\"anonymousId\"].count()\n",
                 "print(count)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "#Create bar chart for workIds viewed more than twice.\n",
+                "# Create bar chart for workIds viewed more than twice.\n",
                 "\n",
-                "summary.sort_values(by='anonymousId', ascending=False)[:20].plot.bar(legend=False)"
+                "summary.sort_values(by=\"anonymousId\", ascending=False)[:19].plot.bar(legend=False)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "#calculate skewness\n",
+                "# calculate skewness\n",
                 "\n",
-                "skew=summary.skew()\n",
+                "skew = summary.skew()\n",
                 "print(skew)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "#output data to csv to check\n",
+                "# output data to csv to check\n",
                 "\n",
-                "summary.to_csv('skew_for_2_days.csv') "
+                "summary.to_csv(\"skew_for_2_days.csv\")"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
             "display_name": "Python 3",
             "language": "python",
```

### Comparing `weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/01 - knowledge graph embeddings.ipynb` & `weco-datascience-0.1.9/notebooks/wikipedia/notebooks/01 - knowledge graph embeddings.ipynb`

 * *Files 6% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9961336232167285%*

 * *Differences: {"'cells'": '{1: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("white")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\'), (13, \'from sklearn.cluster import '*

 * *            'AgglomerativeClustering\')], delete: [12, 4, 3]}}, 3: {\'source\': {insert: [(0, "# '*

 * *            'titles = set(wikipedia.page(\'Wikipedia:Featured_articles\').links)\\n"), (1, '*

 * *            '\'titles = set(wikipedia.page("WP:GA/ALL").links)\\n\')], delete: [1, 0]}}, 4: '*

 * *            "{'source': {inse […]*

```diff
@@ -12,24 +12,25 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('white')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"white\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import pickle\n",
                 "import wikipedia\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
                 "from umap import UMAP\n",
                 "from tqdm import tqdm_notebook as tqdm\n",
-                "from sklearn.cluster import AgglomerativeClustering "
+                "from sklearn.cluster import AgglomerativeClustering"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# just using the featured articles\n",
@@ -38,45 +39,44 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "#titles = set(wikipedia.page('Wikipedia:Featured_articles').links)\n",
-                "titles = set(wikipedia.page('WP:GA/ALL').links)\n",
+                "# titles = set(wikipedia.page('Wikipedia:Featured_articles').links)\n",
+                "titles = set(wikipedia.page(\"WP:GA/ALL\").links)\n",
                 "len(titles)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "graph_dict = {}\n",
                 "\n",
                 "for title in tqdm(titles):\n",
                 "    try:\n",
-                "        possible_links = {link for link in wikipedia.page(title).links\n",
-                "                          if link != title}\n",
-                "        \n",
+                "        possible_links = {link for link in wikipedia.page(title).links if link != title}\n",
+                "\n",
                 "        links_to_keep = list(titles.intersection(possible_links))\n",
                 "        graph_dict[title] = links_to_keep\n",
                 "    except (wikipedia.DisambiguationError, wikipedia.PageError):\n",
-                "        print(f'couldn\\'t resolve page: {title}\\n')"
+                "        print(f\"couldn't resolve page: {title}\\n\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open('featured_article_links.pkl', 'rb') as fp:\n",
+                "with open(\"featured_article_links.pkl\", \"rb\") as fp:\n",
                 "    graph_dict = pickle.load(fp)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -129,66 +129,68 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "embedding = UMAP(n_components=2, metric='cosine').fit_transform(adjacency_matrix)\n",
-                "large_embedding = UMAP(n_components=300, metric='cosine').fit_transform(adjacency_matrix)"
+                "embedding = UMAP(n_components=2, metric=\"cosine\").fit_transform(adjacency_matrix)\n",
+                "large_embedding = UMAP(n_components=300, metric=\"cosine\").fit_transform(\n",
+                "    adjacency_matrix\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "df = pd.DataFrame(embedding)\n",
                 "cluster = AgglomerativeClustering(n_clusters=40)\n",
-                "df['cluster'] = cluster.fit_predict(large_embedding)"
+                "df[\"cluster\"] = cluster.fit_predict(large_embedding)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df.plot.scatter(x=0, y=1, s=2, c=df['cluster'], cmap='Paired');"
+                "df.plot.scatter(x=0, y=1, s=2, c=df[\"cluster\"], cmap=\"Paired\");"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "chosen_cluster = 26\n",
-                "df['selected'] = df['cluster'] == chosen_cluster\n",
-                "df.plot.scatter(x=0, y=1, s=2, c=df['selected'], cmap='Paired');"
+                "df[\"selected\"] = df[\"cluster\"] == chosen_cluster\n",
+                "df.plot.scatter(x=0, y=1, s=2, c=df[\"selected\"], cmap=\"Paired\");"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "for i in df[df['cluster'] == chosen_cluster].index.values:\n",
+                "for i in df[df[\"cluster\"] == chosen_cluster].index.values:\n",
                 "    print(index_to_title[i])"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open('featured_article_links.pkl', 'wb') as fp:\n",
+                "with open(\"featured_article_links.pkl\", \"wb\") as fp:\n",
                 "    pickle.dump(graph_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -249,31 +251,32 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('white')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"white\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import pickle\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
                 "import networkx as nx\n",
                 "from umap import UMAP\n",
                 "from itertools import combinations\n",
                 "from tqdm import tqdm_notebook as tqdm\n",
-                "from sklearn.cluster import AgglomerativeClustering \n",
+                "from sklearn.cluster import AgglomerativeClustering\n",
                 "\n",
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader\n",
                 "\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# assemble the data"
@@ -281,29 +284,26 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open('/mnt/efs/wikipedia/good_article_links.pkl', 'rb') as fp:\n",
+                "with open(\"/mnt/efs/wikipedia/good_article_links.pkl\", \"rb\") as fp:\n",
                 "    graph_dict = pickle.load(fp)\n",
                 "    G = nx.from_dict_of_lists(graph_dict)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "adjacency_matrix = torch.Tensor(\n",
-                "    nx.adjacency_matrix(G)\n",
-                "    .todense()\n",
-                ")"
+                "adjacency_matrix = torch.Tensor(nx.adjacency_matrix(G).todense())"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -313,29 +313,25 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "embedding = (\n",
-                "    UMAP(n_components=2, metric='cosine')\n",
-                "    .fit_transform(adjacency_matrix)\n",
-                ")"
+                "embedding = UMAP(n_components=2, metric=\"cosine\").fit_transform(adjacency_matrix)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "large_embedding = (\n",
-                "    UMAP(n_components=300, metric='cosine')\n",
-                "    .fit_transform(adjacency_matrix)\n",
+                "large_embedding = UMAP(n_components=300, metric=\"cosine\").fit_transform(\n",
+                "    adjacency_matrix\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -356,39 +352,40 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "cluster = AgglomerativeClustering(n_clusters)\n",
-                "df['cluster'] = cluster.fit_predict(embedding)"
+                "df[\"cluster\"] = cluster.fit_predict(embedding)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df.plot.scatter(x=0, y=1, c=df['cluster'], cmap='Paired');"
+                "df.plot.scatter(x=0, y=1, c=df[\"cluster\"], cmap=\"Paired\");"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "for selected_cluster in range(n_clusters):\n",
-                "    df['selected_cluster'] = df['cluster'] == selected_cluster\n",
-                "    print(np.random.choice(\n",
-                "        df.index.values[df['cluster'] == selected_cluster], \n",
-                "        size=10, \n",
-                "        replace=False\n",
-                "    ), '\\n\\n')"
+                "    df[\"selected_cluster\"] = df[\"cluster\"] == selected_cluster\n",
+                "    print(\n",
+                "        np.random.choice(\n",
+                "            df.index.values[df[\"cluster\"] == selected_cluster], size=10, replace=False\n",
+                "        ),\n",
+                "        \"\\n\\n\",\n",
+                "    )"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
```

### Comparing `weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/02 - stealing links from wikipedia.ipynb` & `weco-datascience-0.1.9/notebooks/wikipedia/notebooks/02 - stealing links from wikipedia.ipynb`

 * *Files 3% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9973324542582991%*

 * *Differences: {"'cells'": '{3: {\'source\': [\'title = "Francis Crick (1916-2004): archives"\']}, 4: '*

 * *            '{\'source\': {insert: [(0, \'archive_material = """\\n\'), (27, \'"""\')], delete: '*

 * *            '[27, 0]}}, 7: {\'source\': {insert: [(1, \'    re.sub(r"\\\\ \\\\(.*\\\\)", "", link) '*

 * *            "for link in wikipedia.page(title).links\\n')], delete: [2, 1]}}, 10: {'source': "*

 * *            "{insert: [(8, '            archive_material = archive_material.replace(\\n'), (9, "*

 * *            "'                li […]*

```diff
@@ -32,24 +32,24 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "title = 'Francis Crick (1916-2004): archives'"
+                "title = \"Francis Crick (1916-2004): archives\""
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "archive_material = '''\n",
+                "archive_material = \"\"\"\n",
                 "<p><b>Career</b>\n",
                 "<p>Francis Harry Compton <span class=\"HIT\">Crick</span> was born 8 June, 1916, in Northampton, England, the elder child of Harry <span class=\"HIT\">Crick</span> and Annie Elizabeth Wilkins.  He was educated at Northampton Grammar School and Mill Hill School, London.  Subsequently, <span class=\"HIT\">Crick</span> studied physics at University College London (UCL), obtaining his BSc in 1937.  He remained at UCL, and commenced doctoral research under Professor E N da C Andrade, investigating the viscosity of water at temperatures above 100\u00baC, but his study was interrupted by the outbreak of war in 1939.</p>\n",
                 "\n",
                 "<p>During World War II, <span class=\"HIT\">Crick</span> was a scientist at the British Admiralty Research Laboratory, working on non-contact magnetic and acoustic mines.  He continued to work at the Admiralty immediately after the war.  In 1947, he obtained a Medical Council Research Studentship and re-commenced graduate study, this time at Strangeways Laboratory in Cambridge.  There, he worked under Arthur Hughes, studying the physical properties of cytoplasm in cultured fibroblast cells, but did not submit a dissertation.  During this period, <span class=\"HIT\">Crick</span> began to read widely and purposefully in biology and chemistry, developing a particular interest in the nature of genetic material and in protein structure.  In June, 1949, <span class=\"HIT\">Crick</span> joined the staff of the Medical Research Council Unit at the Cavendish Laboratory, Cambridge.  With the encouragement of Sir Edward Mellenby, Secretary to the MRC, he also re-registered his research degree at Gonville and Caius College, Cambridge.</p>\n",
                 "\n",
                 "<p>At the Cavendish, headed by Sir Lawrence Bragg, <span class=\"HIT\">Crick</span> joined a small team that included Max Perutz and John Kendrew, investigating the structure of proteins through X-ray crystallography, an investigative technique which was then entirely new to <span class=\"HIT\">Crick</span>.  He proved a rapid learner.  Together with W Cochran and V Vand, <span class=\"HIT\">Crick</span> determined the general theory of X-ray diffraction patterns produced by continuous and discontinuous helices.  The theory of helices formed a major component of his PhD thesis, by now entirely concerned with X-ray crystallography.  Drafted during 1952-53, <i>X-ray diffraction: polypeptides and proteins</i> was submitted to Gonville and Caius College, Cambridge, in July, 1953, shortly after the publication of his first papers with James D Watson on the structure of DNA.</p>\n",
                 "\n",
@@ -68,15 +68,15 @@
                 "<p><span class=\"HIT\">Crick</span> died on 28 July 2004 at Thornton Hospital, San Diego.</p>\n",
                 "\n",
                 "<p><b>Publications</b><br>\n",
                 "In addition to his many scientific papers, <span class=\"HIT\">Crick</span> published:  <i>Of Molecules and Men</i> (1966), <i>Life Itself: Its Origin and Nature</i> (1981), <i>What Mad Pursuit: A Personal View of Scientific Discovery</i> (1988), and <i>The Astonishing Hypothesis: The Scientific Search for the Soul</i> (1994).</p>\n",
                 "\n",
                 "<p><b>Awards and Honours</b><br>\n",
                 "In 1962, <span class=\"HIT\">Crick</span> shared the Nobel Prize in Physiology or Medicine with James Watson and Maurice Wilkins, in recognition of their respective contributions to the discovery of the structure of DNA.  Although <span class=\"HIT\">Crick</span> consistently declined honorary degrees, he was the recipient of a number of awards and honours.  They include:  Fellow of the Royal Society (1959), Warren Triennial Prize (1959), Albert Lasker Award (1960), Le Prix Charles-L\u00e9opold Mayer (1961), Royal Society Royal Medal (1972), Royal Society Copley Medal (1975), Order of Merit (1991), and University of California (San Diego Division of Biological Sciences) inaugural Life Sciences Achievement Award (2003). Francis <span class=\"HIT\">Crick</span> was a Fellow of University College, London, Honorary Fellow of Churchill College Cambridge, and Honorary Fellow of Caius College, Cambridge.</p>\n",
-                "'''"
+                "\"\"\""
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# get the links from crick's wikipedia page"
@@ -94,16 +94,15 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "wikipedia_page_links = [\n",
-                "    re.sub(r'\\ \\(.*\\)', '', link)\n",
-                "    for link in wikipedia.page(title).links\n",
+                "    re.sub(r\"\\ \\(.*\\)\", \"\", link) for link in wikipedia.page(title).links\n",
                 "]\n",
                 "\n",
                 "wikipedia_page_links.sort(key=lambda x: len(x.split()), reverse=True)"
             ]
         },
         {
             "cell_type": "code",
@@ -131,19 +130,21 @@
                 "found_link_titles = []\n",
                 "\n",
                 "for link_title in tqdm(wikipedia_page_links):\n",
                 "    if link_title in archive_material:\n",
                 "        try:\n",
                 "            url = wikipedia.page(link_title).url\n",
                 "            link_html = f'<a href=\"{url}\">{link_title}</a>'\n",
-                "            archive_material = archive_material.replace(link_title, str(hash(link_html)))\n",
+                "            archive_material = archive_material.replace(\n",
+                "                link_title, str(hash(link_html))\n",
+                "            )\n",
                 "            html_dict[str(hash(link_html))] = link_html\n",
                 "            found_link_titles.append(link_title)\n",
                 "        except wikipedia.DisambiguationError:\n",
-                "            print('couldn\\'t disambiguate', link_title)\n",
+                "            print(\"couldn't disambiguate\", link_title)\n",
                 "\n",
                 "for key, value in html_dict.items():\n",
                 "    archive_material = archive_material.replace(key, value)"
             ]
         },
         {
             "cell_type": "code",
@@ -167,53 +168,47 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "def find_authorities(name):\n",
                 "    url = wikipedia.page(name).url\n",
                 "    html = requests.get(url).content\n",
                 "    soup = BeautifulSoup(html)\n",
-                "    \n",
-                "    wikidata_id = [link.get('href').split('/')[-1]\n",
-                "                   for link in soup.find_all('a')\n",
-                "                   if link.text == 'Wikidata item'][0]\n",
                 "\n",
-                "    wikidata_url = f'https://www.wikidata.org/wiki/Special:EntityData/{wikidata_id}.json'\n",
+                "    wikidata_id = [\n",
+                "        link.get(\"href\").split(\"/\")[-1]\n",
+                "        for link in soup.find_all(\"a\")\n",
+                "        if link.text == \"Wikidata item\"\n",
+                "    ][0]\n",
+                "\n",
+                "    wikidata_url = (\n",
+                "        f\"https://www.wikidata.org/wiki/Special:EntityData/{wikidata_id}.json\"\n",
+                "    )\n",
                 "    wikidata_json = requests.get(wikidata_url).json()\n",
                 "\n",
                 "    try:\n",
-                "        LCSH_id = (wikidata_json\n",
-                "                   ['entities']\n",
-                "                   [wikidata_id]\n",
-                "                   ['claims']\n",
-                "                   ['P244']\n",
-                "                   [0]\n",
-                "                   ['mainsnak']\n",
-                "                   ['datavalue']\n",
-                "                   ['value'])\n",
-                "    except: \n",
+                "        LCSH_id = wikidata_json[\"entities\"][wikidata_id][\"claims\"][\"P244\"][0][\n",
+                "            \"mainsnak\"\n",
+                "        ][\"datavalue\"][\"value\"]\n",
+                "    except:\n",
                 "        LCSH_id = None\n",
                 "\n",
-                "    authorities = {\n",
-                "        'name': name,\n",
-                "        'wikidata_id': wikidata_id,\n",
-                "        'LCSH_id': LCSH_id\n",
-                "    }\n",
-                "    \n",
+                "    authorities = {\"name\": name, \"wikidata_id\": wikidata_id, \"LCSH_id\": LCSH_id}\n",
+                "\n",
                 "    return authorities"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "authorities = (pd.DataFrame([find_authorities(title) \n",
-                "                             for title in tqdm(found_link_titles)])\n",
-                "               .drop_duplicates('wikidata_id'))\n"
+                "authorities = pd.DataFrame(\n",
+                "    [find_authorities(title) for title in tqdm(found_link_titles)]\n",
+                ").drop_duplicates(\"wikidata_id\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
```

### Comparing `weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/03 - graph stuff.ipynb` & `weco-datascience-0.1.9/notebooks/wikipedia/notebooks/03 - graph stuff.ipynb`

 * *Files 8% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9990613553113553%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("white")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\')], delete: [4, 3]}}, 1: {\'source\': '*

 * *            '{insert: [(0, \'with open("/mnt/efs/wikipedia/good_article_links.pkl", "rb") as '*

 * *            "fp:\\n')], delete: [0]}}}"}*

```diff
@@ -5,30 +5,31 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('white')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"white\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import pickle\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
                 "import networkx as nx"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open('/mnt/efs/wikipedia/good_article_links.pkl', 'rb') as fp:\n",
+                "with open(\"/mnt/efs/wikipedia/good_article_links.pkl\", \"rb\") as fp:\n",
                 "    graph_dict = pickle.load(fp)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
```

### Comparing `weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/04 - collaborative filtering.ipynb` & `weco-datascience-0.1.9/notebooks/wikipedia/notebooks/04 - collaborative filtering.ipynb`

 * *Files 15% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9940959595959595%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("white")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\'), (17, \'from fastai.collab import '*

 * *            "*\\n')], delete: [16, 4, 3]}}, 1: {'source': {insert: [(0, 'with "*

 * *            'open("/mnt/efs/wikipedia/good_article_links.pkl", "rb") as fp:\\n\')], delete: [0]}}, '*

 * *            "2: {'source': {insert: [(3, 'no_edges = random.sample(set(combinations(list(G.nodes), "*

 * *            "r=2)), len(edges))')], […]*

```diff
@@ -5,93 +5,92 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('white')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"white\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import torch\n",
                 "import random\n",
                 "import pickle\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
                 "from tqdm import tqdm\n",
                 "import networkx as nx\n",
                 "from itertools import combinations\n",
                 "\n",
                 "from fastai import *\n",
-                "from fastai.collab import * \n",
+                "from fastai.collab import *\n",
                 "from fastai.train import *"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open('/mnt/efs/wikipedia/good_article_links.pkl', 'rb') as fp:\n",
+                "with open(\"/mnt/efs/wikipedia/good_article_links.pkl\", \"rb\") as fp:\n",
                 "    graph_dict = pickle.load(fp)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "G = nx.from_dict_of_lists(graph_dict)\n",
                 "\n",
                 "edges = set(G.edges)\n",
-                "no_edges = random.sample(set(combinations(list(G.nodes), r=2)),\n",
-                "                         len(edges))"
+                "no_edges = random.sample(set(combinations(list(G.nodes), r=2)), len(edges))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "df_0s = pd.DataFrame(no_edges)\n",
-                "df_0s.columns = ['source', 'target']\n",
-                "df_0s['value'] = 0"
+                "df_0s.columns = [\"source\", \"target\"]\n",
+                "df_0s[\"value\"] = 0"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "df_1s = pd.DataFrame(list(edges))\n",
-                "df_1s.columns = ['source', 'target']\n",
-                "df_1s['value'] = 1"
+                "df_1s.columns = [\"source\", \"target\"]\n",
+                "df_1s[\"value\"] = 1"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df = pd.concat([df_0s.sample(len(df_1s)), \n",
-                "                df_1s]).sample(frac=1)"
+                "df = pd.concat([df_0s.sample(len(df_1s)), df_1s]).sample(frac=1)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df.to_pickle('/mnt/efs/wikipedia/sample_good_article_links.pkl')"
+                "df.to_pickle(\"/mnt/efs/wikipedia/sample_good_article_links.pkl\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -101,19 +100,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "learn = collab_learner(\n",
-                "    databunch, \n",
-                "    n_factors=50, \n",
-                "    y_range=(0, 1)\n",
-                ")"
+                "learn = collab_learner(databunch, n_factors=50, y_range=(0, 1))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -132,15 +127,19 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "learn.fit_one_cycle(10, 5e-3, wd=0.1, )"
+                "learn.fit_one_cycle(\n",
+                "    10,\n",
+                "    5e-3,\n",
+                "    wd=0.1,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
```

### Comparing `weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/05 - autoencoders.ipynb` & `weco-datascience-0.1.9/notebooks/wikipedia/notebooks/05 - autoencoders.ipynb`

 * *Files 3% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9959698718789627%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("white")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\'), (14, \'from sklearn.cluster import '*

 * *            'AgglomerativeClustering\\n\'), (20, \'device = torch.device("cuda" if '*

 * *            'torch.cuda.is_available() else "cpu")\')], delete: [19, 13, 4, 3]}}, 2: {\'source\': '*

 * *            '{insert: [(0, \'with open("/mnt/efs/wikipedia/good_article_links.pkl", "rb") as '*

 * *            "fp:\\n')], delete: [0] […]*

```diff
@@ -5,31 +5,32 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('white')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"white\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import pickle\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
                 "import networkx as nx\n",
                 "from umap import UMAP\n",
                 "from itertools import combinations\n",
                 "from tqdm import tqdm_notebook as tqdm\n",
-                "from sklearn.cluster import AgglomerativeClustering \n",
+                "from sklearn.cluster import AgglomerativeClustering\n",
                 "\n",
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader\n",
                 "\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# assemble the data"
@@ -37,29 +38,26 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open('/mnt/efs/wikipedia/good_article_links.pkl', 'rb') as fp:\n",
+                "with open(\"/mnt/efs/wikipedia/good_article_links.pkl\", \"rb\") as fp:\n",
                 "    graph_dict = pickle.load(fp)\n",
                 "    G = nx.from_dict_of_lists(graph_dict)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "adjacency_matrix = torch.Tensor(\n",
-                "    nx.adjacency_matrix(G)\n",
-                "    .todense()\n",
-                ")"
+                "adjacency_matrix = torch.Tensor(nx.adjacency_matrix(G).todense())"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### dataset"
@@ -103,18 +101,15 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "batch_size = 64\n",
                 "\n",
                 "dataloader = DataLoader(\n",
-                "    dataset=dataset, \n",
-                "    batch_size=batch_size,\n",
-                "    shuffle=True,\n",
-                "    num_workers=5\n",
+                "    dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=5\n",
                 ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -130,33 +125,31 @@
                 "class Encoder(nn.Module):\n",
                 "    def __init__(self, input_size, embedding_size=50):\n",
                 "        super().__init__()\n",
                 "        self.input_size = input_size\n",
                 "        self.embedding_size = embedding_size\n",
                 "        # use the multiplicative midpoint between the two sizes\n",
                 "        self.mid_size = int(\n",
-                "            self.input_size // \n",
-                "            np.sqrt(self.input_size / self.embedding_size)\n",
+                "            self.input_size // np.sqrt(self.input_size / self.embedding_size)\n",
                 "        )\n",
                 "        print()\n",
                 "        self.encode = nn.Sequential(\n",
-                "            nn.Linear(in_features=self.input_size,\n",
-                "                      out_features=self.mid_size),\n",
+                "            nn.Linear(in_features=self.input_size, out_features=self.mid_size),\n",
                 "            nn.ReLU(inplace=True),\n",
                 "            nn.Dropout(),\n",
-                "            nn.Linear(in_features=self.mid_size,\n",
-                "                      out_features=self.embedding_size),\n",
+                "            nn.Linear(in_features=self.mid_size, out_features=self.embedding_size),\n",
                 "            nn.ReLU(inplace=True),\n",
                 "            nn.Dropout(),\n",
-                "            nn.Linear(in_features=self.embedding_size,\n",
-                "                      out_features=self.embedding_size),\n",
+                "            nn.Linear(\n",
+                "                in_features=self.embedding_size, out_features=self.embedding_size\n",
+                "            ),\n",
                 "        )\n",
                 "\n",
                 "    def forward(self, x):\n",
-                "         return self.encode(x)"
+                "        return self.encode(x)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -164,33 +157,31 @@
                 "class Decoder(nn.Module):\n",
                 "    def __init__(self, output_size, embedding_size=50):\n",
                 "        super().__init__()\n",
                 "        self.output_size = output_size\n",
                 "        self.embedding_size = embedding_size\n",
                 "        # use the multiplicative midpoint between the two sizes\n",
                 "        self.mid_size = int(\n",
-                "            self.output_size // \n",
-                "            np.sqrt(self.output_size / self.embedding_size)\n",
+                "            self.output_size // np.sqrt(self.output_size / self.embedding_size)\n",
                 "        )\n",
                 "\n",
                 "        self.decode = nn.Sequential(\n",
-                "            nn.Linear(in_features=self.embedding_size,\n",
-                "                      out_features=self.embedding_size),\n",
+                "            nn.Linear(\n",
+                "                in_features=self.embedding_size, out_features=self.embedding_size\n",
+                "            ),\n",
                 "            nn.ReLU(inplace=True),\n",
                 "            nn.Dropout(),\n",
-                "            nn.Linear(in_features=self.embedding_size,\n",
-                "                      out_features=self.mid_size),\n",
+                "            nn.Linear(in_features=self.embedding_size, out_features=self.mid_size),\n",
                 "            nn.ReLU(inplace=True),\n",
                 "            nn.Dropout(),\n",
-                "            nn.Linear(in_features=self.mid_size,\n",
-                "                      out_features=self.output_size),\n",
+                "            nn.Linear(in_features=self.mid_size, out_features=self.output_size),\n",
                 "        )\n",
                 "\n",
                 "    def forward(self, x):\n",
-                "         return self.decode(x)"
+                "        return self.decode(x)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -200,15 +191,15 @@
                 "        super().__init__()\n",
                 "        self.embedding_size = embedding_size\n",
                 "        self.input_size = input_size\n",
                 "        self.output_size = input_size\n",
                 "\n",
                 "        self.encoder = Encoder(self.input_size, self.embedding_size)\n",
                 "        self.decoder = Decoder(self.output_size, self.embedding_size)\n",
-                "        \n",
+                "\n",
                 "    def forward(self, x):\n",
                 "        embedding = self.encoder(x)\n",
                 "        decoded = self.decoder(embedding)\n",
                 "        return nn.Sigmoid()(decoded)"
             ]
         },
         {
@@ -229,18 +220,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "autoencoder = Autoencoder(\n",
-                "    input_size=len(G.nodes),\n",
-                "    embedding_size=20\n",
-                ").to(device)"
+                "autoencoder = Autoencoder(input_size=len(G.nodes), embedding_size=20).to(device)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -259,16 +247,16 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "losses = []\n",
                 "\n",
-                "def train(model, train_loader, n_epochs, loss_function, \n",
-                "          optimiser, device=device):\n",
+                "\n",
+                "def train(model, train_loader, n_epochs, loss_function, optimiser, device=device):\n",
                 "    model.train()\n",
                 "    for epoch in range(n_epochs):\n",
                 "        loop = tqdm(train_loader)\n",
                 "        for batch in loop:\n",
                 "            data = batch.cuda(non_blocking=True)\n",
                 "            target = batch.cuda(non_blocking=True)\n",
                 "\n",
@@ -277,15 +265,15 @@
                 "\n",
                 "            loss = loss_function(prediction, target)\n",
                 "            losses.append(loss.item())\n",
                 "\n",
                 "            loss.backward()\n",
                 "            optimiser.step()\n",
                 "\n",
-                "            loop.set_description(f'Epoch {epoch + 1}/{n_epochs}')\n",
+                "            loop.set_description(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
                 "            loop.set_postfix(loss=loss.item())"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -301,19 +289,21 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train(model=autoencoder,\n",
-                "      train_loader=dataloader,\n",
-                "      loss_function=loss_function,\n",
-                "      optimiser=optimiser,\n",
-                "      n_epochs=10)"
+                "train(\n",
+                "    model=autoencoder,\n",
+                "    train_loader=dataloader,\n",
+                "    loss_function=loss_function,\n",
+                "    optimiser=optimiser,\n",
+                "    n_epochs=10,\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -326,61 +316,54 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "with torch.no_grad():\n",
                 "    embeddings_50d = (\n",
-                "        autoencoder.encoder(\n",
-                "            adjacency_matrix\n",
-                "            .to(device)\n",
-                "        )\n",
-                "        .detach().cpu().numpy()\n",
+                "        autoencoder.encoder(adjacency_matrix.to(device)).detach().cpu().numpy()\n",
                 "    )"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "embeddings_2d = (\n",
-                "    UMAP(n_components=2, metric='cosine')\n",
-                "    .fit_transform(embeddings_50d)\n",
-                ")"
+                "embeddings_2d = UMAP(n_components=2, metric=\"cosine\").fit_transform(embeddings_50d)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "df = pd.DataFrame(embeddings_2d)\n",
                 "cluster = AgglomerativeClustering()\n",
-                "df['cluster'] = cluster.fit_predict(embeddings_50d)"
+                "df[\"cluster\"] = cluster.fit_predict(embeddings_50d)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df.plot.scatter(x=0, y=1, c=df['cluster'], cmap='Paired');"
+                "df.plot.scatter(x=0, y=1, c=df[\"cluster\"], cmap=\"Paired\");"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "node_names[df[df['cluster'] == 1].index.values]"
+                "node_names[df[df[\"cluster\"] == 1].index.values]"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# query with nmslib"
@@ -389,17 +372,18 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "import nmslib\n",
-                "index = nmslib.init(method='hnsw')\n",
+                "\n",
+                "index = nmslib.init(method=\"hnsw\")\n",
                 "index.addDataPointBatch(embeddings_50d)\n",
-                "index.createIndex({'post': 2}, print_progress=True)"
+                "index.createIndex({\"post\": 2}, print_progress=True)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
```

### Comparing `weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/06 - Singular Value Decomposition.ipynb` & `weco-datascience-0.1.9/notebooks/wikipedia/notebooks/06 - Singular Value Decomposition.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9962745859213251%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("white")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\'), (15, \'from sklearn.cluster import '*

 * *            "AgglomerativeClustering\\n')], delete: [14, 4, 3]}}, 1: {'source': {insert: [(0, "*

 * *            '\'with open("/mnt/efs/wikipedia/good_article_links.pkl", "rb") as fp:\\n\'), (2, '*

 * *            "'    G = nx.from_dict_of_lists(graph_dict)  # .to_undirected()\\n')], delete: [2, "*

 * *            "0]}}, 3: {'so […]*

```diff
@@ -5,38 +5,39 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('white')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"white\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import torch\n",
                 "import pickle\n",
                 "import wikipedia\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
                 "import networkx as nx\n",
                 "from umap import UMAP\n",
                 "from tqdm import tqdm_notebook as tqdm\n",
-                "from sklearn.cluster import AgglomerativeClustering \n",
+                "from sklearn.cluster import AgglomerativeClustering\n",
                 "from scipy.sparse.linalg import svds"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open('/mnt/efs/wikipedia/good_article_links.pkl', 'rb') as fp:\n",
+                "with open(\"/mnt/efs/wikipedia/good_article_links.pkl\", \"rb\") as fp:\n",
                 "    graph_dict = pickle.load(fp)\n",
-                "    G = nx.from_dict_of_lists(graph_dict)#.to_undirected()\n",
+                "    G = nx.from_dict_of_lists(graph_dict)  # .to_undirected()\n",
                 "    node_names = list(G.nodes)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -47,23 +48,21 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "adjacency_matrix = (\n",
-                "    nx.adjacency_matrix(G)\n",
-                "    .todense()\n",
-                ")\n",
+                "adjacency_matrix = nx.adjacency_matrix(G).todense()\n",
                 "\n",
-                "df = pd.DataFrame(adjacency_matrix,\n",
-                "                  index=node_names,\n",
-                "                  columns=node_names,\n",
-                "                  ).fillna(0)"
+                "df = pd.DataFrame(\n",
+                "    adjacency_matrix,\n",
+                "    index=node_names,\n",
+                "    columns=node_names,\n",
+                ").fillna(0)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -95,17 +94,15 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "US = np.dot(U, np.diag(S))\n",
                 "predictions = np.dot(US, Vt) + np.array(user_means).reshape(-1, 1)\n",
                 "\n",
-                "predictions = pd.DataFrame(np.round(predictions), \n",
-                "                           index=node_names,\n",
-                "                           columns=node_names)"
+                "predictions = pd.DataFrame(np.round(predictions), index=node_names, columns=node_names)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -132,17 +129,15 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "distance_matrix = pd.DataFrame(\n",
-                "    cdist(Vt.T, Vt.T, metric='cosine'),\n",
-                "    index=node_names,\n",
-                "    columns=node_names\n",
+                "    cdist(Vt.T, Vt.T, metric=\"cosine\"), index=node_names, columns=node_names\n",
                 ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -167,18 +162,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "embeddings_2d = (\n",
-                "    UMAP(n_components=2)\n",
-                "    .fit_transform(U)\n",
-                ")"
+                "embeddings_2d = UMAP(n_components=2).fit_transform(U)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -190,39 +182,40 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "cluster = AgglomerativeClustering(300)\n",
-                "df['cluster'] = cluster.fit_predict(embeddings_2d)"
+                "df[\"cluster\"] = cluster.fit_predict(embeddings_2d)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "df.plot.scatter(x=0, y=1, c=df['cluster'], cmap='Paired');"
+                "df.plot.scatter(x=0, y=1, c=df[\"cluster\"], cmap=\"Paired\");"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "for selected_cluster in range(df['cluster'].max() + 1):\n",
-                "    df['selected_cluster'] = df['cluster'] == selected_cluster\n",
-                "    print(np.random.choice(\n",
-                "        df.index.values[df['cluster'] == selected_cluster], \n",
-                "        size=10, \n",
-                "        replace=False\n",
-                "    ), '\\n\\n')"
+                "for selected_cluster in range(df[\"cluster\"].max() + 1):\n",
+                "    df[\"selected_cluster\"] = df[\"cluster\"] == selected_cluster\n",
+                "    print(\n",
+                "        np.random.choice(\n",
+                "            df.index.values[df[\"cluster\"] == selected_cluster], size=10, replace=False\n",
+                "        ),\n",
+                "        \"\\n\\n\",\n",
+                "    )"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
```

### Comparing `weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/07 - labelling existing links in wikipedia text.ipynb` & `weco-datascience-0.1.9/notebooks/wikipedia/notebooks/07 - labelling existing links in wikipedia text.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9973489858906526%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("white")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\')], delete: [4, 3]}}, 2: {\'source\': '*

 * *            '{insert: [(0, \'file_path = "/mnt/efs/wikipedia/dumps/text/AA/wiki_00"\\n\'), (2, '*

 * *            '\'with open(file_path, "rb") as f:\\n\'), (3, \'    file = '*

 * *            'f.read().decode("latin1")\')], delete: [3, 2, 0]}}, 4: {\'source\': {insert: [(0, '*

 * *            '\'pattern = r"(?:<doc.+>)((.|\\\\s […]*

```diff
@@ -5,16 +5,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('white')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"white\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import re\n",
                 "import numpy as np\n",
                 "from bs4 import BeautifulSoup\n",
                 "from tqdm import tqdm_notebook as tqdm\n",
                 "\n",
                 "from fastai.text import Tokenizer"
@@ -55,18 +56,18 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "file_path = '/mnt/efs/wikipedia/dumps/text/AA/wiki_00'\n",
+                "file_path = \"/mnt/efs/wikipedia/dumps/text/AA/wiki_00\"\n",
                 "\n",
-                "with open(file_path, 'rb') as f:\n",
-                "    file = f.read().decode('latin1')"
+                "with open(file_path, \"rb\") as f:\n",
+                "    file = f.read().decode(\"latin1\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "Each file is made up of multiple articles, so we'll split them by `<doc>` tokens"
@@ -74,15 +75,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "pattern = r'(?:<doc.+>)((.|\\s|\\S)*?)(?:<\\/doc>)'\n",
+                "pattern = r\"(?:<doc.+>)((.|\\s|\\S)*?)(?:<\\/doc>)\"\n",
                 "docs = [doc[0] for doc in re.findall(pattern, file)]"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -92,45 +93,45 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def label_linkable_tokens(article_html, tokenizer=Tokenizer(), label_all=True):\n",
-                "    parsed_html = BeautifulSoup(article_html, 'html.parser')\n",
+                "    parsed_html = BeautifulSoup(article_html, \"html.parser\")\n",
                 "\n",
-                "    link_text = [link.text for link in parsed_html.find_all('a')]\n",
+                "    link_text = [link.text for link in parsed_html.find_all(\"a\")]\n",
                 "    tokenised_links = tokenizer.process_all(link_text)\n",
                 "\n",
                 "    tokenised_text = tokenizer.process_all([parsed_html.text])[0]\n",
-                "    \n",
+                "\n",
                 "    target = np.zeros(len(tokenised_text))\n",
-                "    \n",
+                "\n",
                 "    for link in tokenised_links:\n",
                 "        start_positions = kmp(tokenised_text, link)\n",
-                "        if label_all:            \n",
+                "        if label_all:\n",
                 "            for pos in start_positions:\n",
                 "                target[pos : pos + len(link)] = 1\n",
                 "        elif label_all == False and len(start_positions) > 0:\n",
                 "            pos = start_positions[0]\n",
                 "            target[pos : pos + len(link)] = 1\n",
-                "        else: \n",
+                "        else:\n",
                 "            pass\n",
                 "\n",
                 "    return tokenised_text, target"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def kmp(sequence, sub):\n",
-                "    \"\"\"         \n",
+                "    \"\"\"\n",
                 "    Knuth\u2013Morris\u2013Pratt algorithm, returning the starting position\n",
                 "    of a specified sub within another, larger sequence.\n",
                 "    Often used for string matching.\n",
                 "    \"\"\"\n",
                 "    partial = [0]\n",
                 "    for i in range(1, len(sub)):\n",
                 "        j = partial[i - 1]\n",
@@ -138,16 +139,17 @@
                 "            j = partial[j - 1]\n",
                 "        partial.append(j + 1 if sub[j] == sub[i] else j)\n",
                 "\n",
                 "    positions, j = [], 0\n",
                 "    for i in range(len(sequence)):\n",
                 "        while j > 0 and sequence[i] != sub[j]:\n",
                 "            j = partial[j - 1]\n",
-                "        if sequence[i] == sub[j]: j += 1\n",
-                "        if j == len(sub): \n",
+                "        if sequence[i] == sub[j]:\n",
+                "            j += 1\n",
+                "        if j == len(sub):\n",
                 "            positions.append(i - (j - 1))\n",
                 "            j = 0\n",
                 "\n",
                 "    return positions"
             ]
         },
         {
```

### Comparing `weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/08 - word-level BiLSTM link labeller.ipynb` & `weco-datascience-0.1.9/notebooks/wikipedia/notebooks/08 - word-level BiLSTM link labeller.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.99561568125304%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\'), (23, '*

 * *            '\'nltk.download("punkt")\\n\'), (24, \'device = torch.device("cuda" if '*

 * *            'torch.cuda.is_available() else "cpu")\')], delete: [23, 22, 4, 3]}}, 2: {\'source\': '*

 * *            '{insert: [(0, \'base_path = "/mnt/efs/wikipedia/dumps/text/"\\n\'), (3, \'all_text = '*

 * *            '""\\n\'), (6, \'        with open(base_pat […]*

```diff
@@ -5,16 +5,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import re\n",
                 "import os\n",
                 "import io\n",
                 "import nltk\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
@@ -24,16 +25,16 @@
                 "from sklearn.model_selection import train_test_split\n",
                 "\n",
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader\n",
                 "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
                 "\n",
-                "nltk.download('punkt')\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "nltk.download(\"punkt\")\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# data"
@@ -41,24 +42,24 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "base_path = '/mnt/efs/wikipedia/dumps/text/'\n",
+                "base_path = \"/mnt/efs/wikipedia/dumps/text/\"\n",
                 "paths = np.random.choice(os.listdir(base_path), size=1)\n",
                 "\n",
-                "all_text = ''\n",
+                "all_text = \"\"\n",
                 "for path in paths:\n",
                 "    for filename in tqdm(os.listdir(base_path + path)):\n",
-                "        with open(base_path + path + '/' + filename, 'rb') as f:\n",
-                "            all_text += f.read().decode('latin1')\n",
+                "        with open(base_path + path + \"/\" + filename, \"rb\") as f:\n",
+                "            all_text += f.read().decode(\"latin1\")\n",
                 "\n",
-                "pattern = r'(?:<doc.+>)((.|\\s|\\S)*?)(?:<\\/doc>)'\n",
+                "pattern = r\"(?:<doc.+>)((.|\\s|\\S)*?)(?:<\\/doc>)\"\n",
                 "articles = [article[0] for article in re.findall(pattern, all_text)]"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -68,44 +69,44 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def tokenize(sentence):\n",
-                "    '''moses tokeniser'''\n",
-                "    seq = ' '.join(word_tokenize(sentence))\n",
+                "    \"\"\"moses tokeniser\"\"\"\n",
+                "    seq = \" \".join(word_tokenize(sentence))\n",
                 "    seq = seq.replace(\" n't \", \"n 't \")\n",
                 "    return seq.split()\n",
                 "\n",
                 "\n",
                 "def label_linkable_tokens(sentence, label_all=True):\n",
-                "    parsed_html = BeautifulSoup(sentence, 'html.parser')\n",
+                "    parsed_html = BeautifulSoup(sentence, \"html.parser\")\n",
                 "\n",
-                "    link_text = [link.text for link in parsed_html.find_all('a')]\n",
+                "    link_text = [link.text for link in parsed_html.find_all(\"a\")]\n",
                 "    tokenised_links = [tokenize(link) for link in link_text]\n",
                 "    tokenised_text = tokenize(parsed_html.text)\n",
                 "    target_sequence = np.zeros(len(tokenised_text))\n",
                 "\n",
                 "    for link in tokenised_links:\n",
                 "        start_positions = kmp(tokenised_text, link)\n",
-                "        if label_all:            \n",
+                "        if label_all:\n",
                 "            for pos in start_positions:\n",
                 "                target_sequence[pos : pos + len(link)] = 1\n",
                 "        elif label_all == False and len(start_positions) > 0:\n",
                 "            pos = start_positions[0]\n",
                 "            target_sequence[pos : pos + len(link)] = 1\n",
-                "        else: \n",
+                "        else:\n",
                 "            pass\n",
                 "\n",
-                "    return tokenised_text, target_sequence.reshape(-1,1)\n",
+                "    return tokenised_text, target_sequence.reshape(-1, 1)\n",
                 "\n",
                 "\n",
                 "def kmp(sequence, sub):\n",
-                "    \"\"\"         \n",
+                "    \"\"\"\n",
                 "    Knuth\u2013Morris\u2013Pratt algorithm, returning the starting position\n",
                 "    of a specified subsequence within another, larger sequence.\n",
                 "    Usually used for string matching.\n",
                 "    \"\"\"\n",
                 "    partial = [0]\n",
                 "    for i in range(1, len(sub)):\n",
                 "        j = partial[i - 1]\n",
@@ -113,16 +114,17 @@
                 "            j = partial[j - 1]\n",
                 "        partial.append(j + 1 if sub[j] == sub[i] else j)\n",
                 "\n",
                 "    positions, j = [], 0\n",
                 "    for i in range(len(sequence)):\n",
                 "        while j > 0 and sequence[i] != sub[j]:\n",
                 "            j = partial[j - 1]\n",
-                "        if sequence[i] == sub[j]: j += 1\n",
-                "        if j == len(sub): \n",
+                "        if sequence[i] == sub[j]:\n",
+                "            j += 1\n",
+                "        if j == len(sub):\n",
                 "            positions.append(i - (j - 1))\n",
                 "            j = 0\n",
                 "\n",
                 "    return positions"
             ]
         },
         {
@@ -132,15 +134,15 @@
             "outputs": [],
             "source": [
                 "token_sequences, target_sequences = [], []\n",
                 "\n",
                 "for i, article in enumerate(tqdm(articles)):\n",
                 "    for j, sentence in enumerate(sent_tokenize(article)):\n",
                 "        try:\n",
-                "            tokenized_sentence, target_sequence = label_linkable_tokens(sentence)        \n",
+                "            tokenized_sentence, target_sequence = label_linkable_tokens(sentence)\n",
                 "            token_sequences.append(tokenized_sentence)\n",
                 "            target_sequences.append(target_sequence)\n",
                 "        except:\n",
                 "            pass"
             ]
         },
         {
@@ -152,42 +154,40 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "wv_path = '/mnt/efs/text/word_vectors/wiki-news-300d-1M.vec'\n",
-                "wv_file = io.open(wv_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
+                "wv_path = \"/mnt/efs/text/word_vectors/wiki-news-300d-1M.vec\"\n",
+                "wv_file = io.open(wv_path, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\")\n",
                 "lines_to_parse = list(wv_file)[1:]\n",
                 "\n",
-                "fasttext = {line.split()[0]: np.array(line.split()[1:]).astype(np.float32)\n",
-                "            for line in tqdm(lines_to_parse)}"
+                "fasttext = {\n",
+                "    line.split()[0]: np.array(line.split()[1:]).astype(np.float32)\n",
+                "    for line in tqdm(lines_to_parse)\n",
+                "}"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "article_vocabulary = list(set([\n",
-                "    tok \n",
-                "    for seq in token_sequences \n",
-                "    for tok in seq\n",
-                "]))"
+                "article_vocabulary = list(set([tok for seq in token_sequences for tok in seq]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "special_cases = ['xxunk', 'xxpad']\n",
+                "special_cases = [\"xxunk\", \"xxpad\"]\n",
                 "\n",
                 "for case in special_cases:\n",
                 "    fasttext[case] = np.random.random(300)\n",
                 "\n",
                 "article_vocabulary = np.append(article_vocabulary, special_cases)"
             ]
         },
@@ -203,20 +203,20 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "word_vector_embedding_matrix = torch.FloatTensor([\n",
-                "    fasttext[token]\n",
-                "    if token in fasttext\n",
-                "    else fasttext['xxunk']\n",
-                "    for token in article_vocabulary\n",
-                "])"
+                "word_vector_embedding_matrix = torch.FloatTensor(\n",
+                "    [\n",
+                "        fasttext[token] if token in fasttext else fasttext[\"xxunk\"]\n",
+                "        for token in article_vocabulary\n",
+                "    ]\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# dataset and dataloader"
@@ -226,123 +226,111 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class SentenceDataset(Dataset):\n",
                 "    def __init__(self, token_sequences, target_sequences):\n",
-                "        self.token_index_sequences = np.array([\n",
-                "            self.indexify(seq) for seq in token_sequences\n",
-                "        ])\n",
+                "        self.token_index_sequences = np.array(\n",
+                "            [self.indexify(seq) for seq in token_sequences]\n",
+                "        )\n",
                 "        self.target_sequences = np.array(target_sequences)\n",
                 "\n",
-                "        #impose length constraint\n",
+                "        # impose length constraint\n",
                 "        where_big_enough = np.where([len(seq) > 3 for seq in target_sequences])\n",
                 "        self.token_index_sequences = self.token_index_sequences[where_big_enough]\n",
                 "        self.target_sequences = self.target_sequences[where_big_enough]\n",
                 "\n",
                 "    def __getitem__(self, index):\n",
                 "        token_index_sequence = self.token_index_sequences[index]\n",
                 "        target_sequence = self.target_sequences[index]\n",
                 "        length = len(token_index_sequence)\n",
                 "        return token_index_sequence, target_sequence, length\n",
                 "\n",
                 "    def __len__(self):\n",
                 "        return len(self.token_index_sequences)\n",
-                "    \n",
+                "\n",
                 "    def indexify(self, token_sequence):\n",
-                "        index_sequence = np.array([\n",
-                "            token_to_index[token] for token in token_sequence\n",
-                "        ])\n",
+                "        index_sequence = np.array([token_to_index[token] for token in token_sequence])\n",
                 "        return index_sequence"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def pad(sequences, pad_value):\n",
                 "    pad_length = max([len(seq) for seq in sequences])\n",
                 "    padded = np.full((len(sequences), pad_length, 1), pad_value)\n",
                 "    for i, seq in enumerate(sequences):\n",
-                "        padded[i][pad_length - len(seq):] = seq.reshape(-1,1)\n",
+                "        padded[i][pad_length - len(seq) :] = seq.reshape(-1, 1)\n",
                 "    return padded.squeeze()\n",
                 "\n",
                 "\n",
                 "def collate_fn(batch):\n",
                 "    indexes, targets, lengths = zip(*batch)\n",
-                "    \n",
-                "    sorted_lengths, sort_indicies = (\n",
-                "        torch.Tensor(lengths)\n",
-                "        .sort(dim=0, descending=True)\n",
-                "    )\n",
-                "    \n",
+                "\n",
+                "    sorted_lengths, sort_indicies = torch.Tensor(lengths).sort(dim=0, descending=True)\n",
+                "\n",
                 "    sorted_indexes = np.array(indexes)[sort_indicies]\n",
                 "    sorted_targets = np.array(targets)[sort_indicies]\n",
-                "    \n",
-                "    padded_indexes = pad(sorted_indexes, token_to_index['xxpad'])\n",
+                "\n",
+                "    padded_indexes = pad(sorted_indexes, token_to_index[\"xxpad\"])\n",
                 "    padded_targets = pad(sorted_targets, 0)\n",
-                "        \n",
+                "\n",
                 "    return padded_indexes, padded_targets, sorted_lengths"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train_token_sequences, test_token_sequences, train_target_sequences, test_target_sequences = (\n",
-                "    train_test_split(\n",
-                "        token_sequences, \n",
-                "        target_sequences, \n",
-                "        test_size=0.20, \n",
-                "        random_state=42)\n",
-                ")"
+                "(\n",
+                "    train_token_sequences,\n",
+                "    test_token_sequences,\n",
+                "    train_target_sequences,\n",
+                "    test_target_sequences,\n",
+                ") = train_test_split(token_sequences, target_sequences, test_size=0.20, random_state=42)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train_dataset = SentenceDataset(\n",
-                "    train_token_sequences, \n",
-                "    train_target_sequences\n",
-                ")\n",
+                "train_dataset = SentenceDataset(train_token_sequences, train_target_sequences)\n",
                 "\n",
                 "train_loader = DataLoader(\n",
                 "    dataset=train_dataset,\n",
                 "    batch_size=64,\n",
                 "    num_workers=5,\n",
                 "    shuffle=True,\n",
-                "    collate_fn=collate_fn\n",
+                "    collate_fn=collate_fn,\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "test_dataset = SentenceDataset(\n",
-                "    test_token_sequences, \n",
-                "    test_target_sequences\n",
-                ")\n",
+                "test_dataset = SentenceDataset(test_token_sequences, test_target_sequences)\n",
                 "\n",
                 "test_loader = DataLoader(\n",
                 "    dataset=test_dataset,\n",
                 "    batch_size=64,\n",
                 "    num_workers=5,\n",
                 "    shuffle=True,\n",
-                "    collate_fn=collate_fn\n",
+                "    collate_fn=collate_fn,\n",
                 ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -353,58 +341,52 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class LinkLabeller(nn.Module):\n",
                 "    def __init__(self, word_vectors, hidden_dim=1024):\n",
-                "        super(LinkLabeller, self).__init__()        \n",
+                "        super(LinkLabeller, self).__init__()\n",
                 "        self.hidden_dim = hidden_dim\n",
                 "        self.embedding = nn.Embedding.from_pretrained(word_vectors)\n",
                 "        self.enc_lstm = nn.LSTM(\n",
-                "            input_size=300, \n",
-                "            hidden_size=self.hidden_dim, \n",
+                "            input_size=300,\n",
+                "            hidden_size=self.hidden_dim,\n",
                 "            num_layers=2,\n",
-                "            bidirectional=True, \n",
-                "            dropout=0.2\n",
+                "            bidirectional=True,\n",
+                "            dropout=0.2,\n",
                 "        )\n",
-                "                \n",
+                "\n",
                 "        self.head = nn.Sequential(\n",
                 "            nn.Linear(self.hidden_dim * 2, self.hidden_dim // 32),\n",
                 "            nn.ReLU(),\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(self.hidden_dim // 32, 2),\n",
                 "        )\n",
-                "    \n",
+                "\n",
                 "    def forward(self, index_sequence, sequence_lengths):\n",
                 "        word_vectors = self.embedding(index_sequence)\n",
-                "        \n",
-                "        packed = pack_padded_sequence(\n",
-                "            word_vectors,\n",
-                "            sequence_lengths,\n",
-                "            batch_first=True)\n",
-                "        \n",
+                "\n",
+                "        packed = pack_padded_sequence(word_vectors, sequence_lengths, batch_first=True)\n",
+                "\n",
                 "        embedded_packed, _ = self.enc_lstm(packed)\n",
-                "        \n",
-                "        embedded, batch_lengths = pad_packed_sequence(\n",
-                "            embedded_packed, \n",
-                "            batch_first=True\n",
-                "        )\n",
-                "        \n",
+                "\n",
+                "        embedded, batch_lengths = pad_packed_sequence(embedded_packed, batch_first=True)\n",
+                "\n",
                 "        categorised = self.head(embedded)\n",
                 "        return categorised"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "#model_path = '/mnt/efs/models/20180114_link_labeller.pt'\n",
+                "# model_path = '/mnt/efs/models/20180114_link_labeller.pt'\n",
                 "model = LinkLabeller(word_vector_embedding_matrix).to(device)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -458,42 +440,42 @@
                 "            preds = model(indexes, sequence_lengths).permute(0, 2, 1)\n",
                 "\n",
                 "            loss = loss_function(preds, targets)\n",
                 "            loss.backward()\n",
                 "            optimiser.step()\n",
                 "\n",
                 "            losses.append(loss.item())\n",
-                "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
+                "            loop.set_description(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
                 "            loop.set_postfix(loss=np.mean(losses[-100:]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "train(\n",
                 "    model=model,\n",
                 "    train_loader=train_loader,\n",
                 "    loss_function=loss_function,\n",
                 "    optimiser=optimiser,\n",
-                "    n_epochs=3\n",
+                "    n_epochs=3,\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "loss_data = pd.Series(losses[20:]).rolling(window=300).mean()\n",
-                "ax = loss_data.plot();\n",
-                "#ax.set_ylim(0.4, 0.5);"
+                "ax = loss_data.plot()\n",
+                "# ax.set_ylim(0.4, 0.5);"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# test the model on unseen data"
@@ -505,15 +487,15 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "random_ixs = np.random.randint(len(test_token_sequences), size=30)\n",
                 "\n",
                 "for random_ix in random_ixs:\n",
                 "    i, t, l = test_dataset.__getitem__(random_ix)\n",
-                "    \n",
+                "\n",
                 "    p = model(torch.LongTensor([i]).cuda(), torch.Tensor([l]).cuda())[0]\n",
                 "    p = nn.LogSoftmax(dim=1)(p).argmax(dim=1)\n",
                 "    for bksgh in range(len(i)):\n",
                 "        print(p[bksgh].item(), int(t[bksgh][0]), index_to_token[i[bksgh]])\n",
                 "    print()"
             ]
         },
@@ -526,28 +508,26 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "torch.save(model.state_dict(), \n",
-                "           '/mnt/efs/models/20180117_link_labeller.pt')\n",
+                "torch.save(model.state_dict(), \"/mnt/efs/models/20180117_link_labeller.pt\")\n",
                 "\n",
-                "torch.save(word_vector_embedding_matrix, \n",
-                "           '/mnt/efs/models/20180117_embeddings.pt')"
+                "torch.save(word_vector_embedding_matrix, \"/mnt/efs/models/20180117_embeddings.pt\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "model_path = '/mnt/efs/models/20180114_link_labeller.pt'\n",
+                "model_path = \"/mnt/efs/models/20180114_link_labeller.pt\"\n",
                 "model = LinkLabeller(word_vector_embedding_matrix)\n",
                 "model.load_state_dict(torch.load(model_path))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
```

### Comparing `weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/09 - character based language model, targeting tokens.ipynb` & `weco-datascience-0.1.9/notebooks/wikipedia/notebooks/09 - character based language model, targeting tokens.ipynb`

 * *Files 14% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9969866284739963%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\'), (23, '*

 * *            '\'nltk.download("punkt")\\n\'), (24, \'device = torch.device("cuda" if '*

 * *            'torch.cuda.is_available() else "cpu")\')], delete: [23, 22, 4, 3]}}, 2: {\'source\': '*

 * *            '{insert: [(0, \'base_path = "/mnt/efs/wikipedia/dumps/text/"\\n\'), (3, \'all_text = '*

 * *            '""\\n\'), (6, \'        with open(base_pat […]*

```diff
@@ -5,16 +5,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import re\n",
                 "import os\n",
                 "import io\n",
                 "import nltk\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
@@ -24,16 +25,16 @@
                 "from sklearn.preprocessing import OneHotEncoder\n",
                 "\n",
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader\n",
                 "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
                 "\n",
-                "nltk.download('punkt')\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "nltk.download(\"punkt\")\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# data"
@@ -41,24 +42,24 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "base_path = '/mnt/efs/wikipedia/dumps/text/'\n",
+                "base_path = \"/mnt/efs/wikipedia/dumps/text/\"\n",
                 "paths = np.random.choice(os.listdir(base_path), size=1)\n",
                 "\n",
-                "all_text = ''\n",
+                "all_text = \"\"\n",
                 "for path in paths:\n",
                 "    for filename in tqdm(os.listdir(base_path + path)):\n",
-                "        with open(base_path + path + '/' + filename, 'rb') as f:\n",
-                "            all_text += f.read().decode('latin1')\n",
+                "        with open(base_path + path + \"/\" + filename, \"rb\") as f:\n",
+                "            all_text += f.read().decode(\"latin1\")\n",
                 "\n",
-                "pattern = r'(?:<doc.+>)((.|\\s|\\S)*?)(?:<\\/doc>)'\n",
+                "pattern = r\"(?:<doc.+>)((.|\\s|\\S)*?)(?:<\\/doc>)\"\n",
                 "articles = [article[0] for article in re.findall(pattern, all_text)]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -77,44 +78,44 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def tokenize(sentence):\n",
-                "    '''moses tokeniser'''\n",
-                "    seq = ' '.join(word_tokenize(sentence))\n",
+                "    \"\"\"moses tokeniser\"\"\"\n",
+                "    seq = \" \".join(word_tokenize(sentence))\n",
                 "    seq = seq.replace(\" n't \", \"n 't \")\n",
                 "    return seq.split()\n",
                 "\n",
                 "\n",
                 "def label_linkable_tokens(sentence, label_all=True):\n",
-                "    parsed_html = BeautifulSoup(sentence, 'html.parser')\n",
+                "    parsed_html = BeautifulSoup(sentence, \"html.parser\")\n",
                 "\n",
-                "    link_text = [link.text for link in parsed_html.find_all('a')]\n",
+                "    link_text = [link.text for link in parsed_html.find_all(\"a\")]\n",
                 "    tokenised_links = [tokenize(link) for link in link_text]\n",
                 "    tokenised_text = tokenize(parsed_html.text)\n",
                 "    target_sequence = np.zeros(len(tokenised_text))\n",
                 "\n",
                 "    for link in tokenised_links:\n",
                 "        start_positions = kmp(tokenised_text, link)\n",
-                "        if label_all:            \n",
+                "        if label_all:\n",
                 "            for pos in start_positions:\n",
                 "                target_sequence[pos : pos + len(link)] = 1\n",
                 "        elif label_all == False and len(start_positions) > 0:\n",
                 "            pos = start_positions[0]\n",
                 "            target_sequence[pos : pos + len(link)] = 1\n",
-                "        else: \n",
+                "        else:\n",
                 "            pass\n",
                 "\n",
-                "    return tokenised_text, target_sequence.reshape(-1,1)\n",
+                "    return tokenised_text, target_sequence.reshape(-1, 1)\n",
                 "\n",
                 "\n",
                 "def kmp(sequence, sub):\n",
-                "    \"\"\"         \n",
+                "    \"\"\"\n",
                 "    Knuth\u2013Morris\u2013Pratt algorithm, returning the starting position\n",
                 "    of a specified subsequence within another, larger sequence.\n",
                 "    Usually used for string matching.\n",
                 "    \"\"\"\n",
                 "    partial = [0]\n",
                 "    for i in range(1, len(sub)):\n",
                 "        j = partial[i - 1]\n",
@@ -122,16 +123,17 @@
                 "            j = partial[j - 1]\n",
                 "        partial.append(j + 1 if sub[j] == sub[i] else j)\n",
                 "\n",
                 "    positions, j = [], 0\n",
                 "    for i in range(len(sequence)):\n",
                 "        while j > 0 and sequence[i] != sub[j]:\n",
                 "            j = partial[j - 1]\n",
-                "        if sequence[i] == sub[j]: j += 1\n",
-                "        if j == len(sub): \n",
+                "        if sequence[i] == sub[j]:\n",
+                "            j += 1\n",
+                "        if j == len(sub):\n",
                 "            positions.append(i - (j - 1))\n",
                 "            j = 0\n",
                 "\n",
                 "    return positions"
             ]
         },
         {
@@ -141,15 +143,15 @@
             "outputs": [],
             "source": [
                 "token_sequences, target_sequences = [], []\n",
                 "\n",
                 "for i, article in enumerate(tqdm(articles)):\n",
                 "    for j, sentence in enumerate(sent_tokenize(article)):\n",
                 "        try:\n",
-                "            tokenized_sentence, target_sequence = label_linkable_tokens(sentence)        \n",
+                "            tokenized_sentence, target_sequence = label_linkable_tokens(sentence)\n",
                 "            token_sequences.append(tokenized_sentence)\n",
                 "            target_sequences.append(target_sequence)\n",
                 "        except:\n",
                 "            pass"
             ]
         },
         {
@@ -170,28 +172,24 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "unique_characters = set(' '.join([\n",
-                "    token \n",
-                "    for seq in token_sequences\n",
-                "    for token in seq\n",
-                "]))"
+                "unique_characters = set(\" \".join([token for seq in token_sequences for token in seq]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "special_cases = ['xxunk', 'xxpad', 'xxbos', 'xxeos']\n",
+                "special_cases = [\"xxunk\", \"xxpad\", \"xxbos\", \"xxeos\"]\n",
                 "\n",
                 "for case in special_cases:\n",
                 "    unique_characters.add(case)"
             ]
         },
         {
             "cell_type": "code",
@@ -212,19 +210,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "article_vocabulary = set([\n",
-                "    tok \n",
-                "    for seq in token_sequences \n",
-                "    for tok in seq\n",
-                "])"
+                "article_vocabulary = set([tok for seq in token_sequences for tok in seq])"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -255,119 +249,103 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class SentenceDataset(Dataset):\n",
                 "    def __init__(self, token_seqs):\n",
                 "        self.token_seqs = np.array(token_seqs)\n",
-                "        \n",
-                "        #impose length constraint\n",
+                "\n",
+                "        # impose length constraint\n",
                 "        where_big_enough = np.where([len(seq) > 3 for seq in token_seqs])\n",
                 "        self.token_seqs = self.token_seqs[where_big_enough]\n",
                 "\n",
-                "        #find prediction points for language model\n",
-                "        self.exit_ix_seqs = [\n",
-                "            self.find_exit_points(seq) for seq in self.token_seqs\n",
-                "        ]\n",
-                "\n",
-                "        #indexify\n",
-                "        self.char_ix_seqs = [\n",
-                "            self.indexify_chars(seq) for seq in self.token_seqs\n",
-                "        ]\n",
-                "\n",
-                "        self.token_wv_seqs = [\n",
-                "            self.indexify_tokens(seq) for seq in self.token_seqs\n",
-                "        ]\n",
-                "        \n",
+                "        # find prediction points for language model\n",
+                "        self.exit_ix_seqs = [self.find_exit_points(seq) for seq in self.token_seqs]\n",
+                "\n",
+                "        # indexify\n",
+                "        self.char_ix_seqs = [self.indexify_chars(seq) for seq in self.token_seqs]\n",
+                "\n",
+                "        self.token_wv_seqs = [self.indexify_tokens(seq) for seq in self.token_seqs]\n",
+                "\n",
                 "    def __getitem__(self, ix):\n",
                 "        char_ix_seq = self.char_ix_seqs[ix]\n",
                 "        token_ix_seq = self.token_ix_seqs[ix]\n",
                 "        exit_ix_seq = self.exit_ix_seqs[ix]\n",
                 "        return char_ix_seq, token_ix_seq, exit_ix_seq\n",
                 "\n",
                 "    def __len__(self):\n",
                 "        return len(self.token_seqs)\n",
-                "    \n",
+                "\n",
                 "    def indexify_tokens(self, token_seq):\n",
                 "        ix_seq = np.array(\n",
-                "            [token_to_ix[token] for token in token_seq] + \n",
-                "            [token_to_ix['xxeos']]\n",
+                "            [token_to_ix[token] for token in token_seq] + [token_to_ix[\"xxeos\"]]\n",
                 "        )\n",
                 "        return torch.LongTensor(ix_seq)\n",
-                "    \n",
+                "\n",
                 "    def indexify_chars(self, token_seq):\n",
                 "        ix_seq = np.array(\n",
-                "            [char_to_ix[char] for char in ' '.join(token_seq)] + \n",
-                "            [char_to_ix[' '], char_to_ix['xxeos']]\n",
+                "            [char_to_ix[char] for char in \" \".join(token_seq)]\n",
+                "            + [char_to_ix[\" \"], char_to_ix[\"xxeos\"]]\n",
                 "        )\n",
                 "        return torch.LongTensor(ix_seq)\n",
-                "    \n",
+                "\n",
                 "    def find_exit_points(self, token_seq):\n",
-                "        exit_positions = np.cumsum([\n",
-                "            len(token) + 1 for token in token_seq\n",
-                "        ])\n",
-                "        return torch.LongTensor(exit_positions) -1"
+                "        exit_positions = np.cumsum([len(token) + 1 for token in token_seq])\n",
+                "        return torch.LongTensor(exit_positions) - 1"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def collate_fn(batch):\n",
                 "    char_ix_seqs, token_ix_seqs, exit_ix_seqs = zip(*batch)\n",
                 "\n",
-                "    char_seq_lens = torch.LongTensor([\n",
-                "        len(char_seq) for char_seq in char_ix_seqs\n",
-                "    ])\n",
-                "    \n",
-                "    sorted_lengths, sort_indicies = char_seq_lens.sort(\n",
-                "        dim=0, \n",
-                "        descending=True\n",
-                "    )\n",
-                "    \n",
+                "    char_seq_lens = torch.LongTensor([len(char_seq) for char_seq in char_ix_seqs])\n",
+                "\n",
+                "    sorted_lengths, sort_indicies = char_seq_lens.sort(dim=0, descending=True)\n",
+                "\n",
                 "    sorted_char_seqs = [char_ix_seqs[i] for i in sort_indicies]\n",
                 "    sorted_token_seqs = [token_ix_seqs[i] for i in sort_indicies]\n",
                 "    sorted_exit_seqs = [exit_ix_seqs[i] for i in sort_indicies]\n",
-                "    \n",
+                "\n",
                 "    padded_char_seqs = pad_sequence(\n",
-                "        sequences=sorted_char_seqs, \n",
-                "        padding_value=char_to_ix['xxpad'],\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_char_seqs, padding_value=char_to_ix[\"xxpad\"], batch_first=True\n",
                 "    )\n",
-                "    \n",
+                "\n",
                 "    padded_token_seqs = pad_sequence(\n",
-                "        sequences=sorted_token_seqs, \n",
-                "        padding_value=token_to_ix['xxpad'],\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_token_seqs,\n",
+                "        padding_value=token_to_ix[\"xxpad\"],\n",
+                "        batch_first=True,\n",
                 "    )\n",
-                "    \n",
+                "\n",
                 "    padded_exit_seqs = pad_sequence(\n",
-                "        sequences=sorted_exit_seqs, \n",
-                "        padding_value=0,\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_exit_seqs, padding_value=0, batch_first=True\n",
                 "    )\n",
-                "    \n",
+                "\n",
                 "    return padded_char_seqs, padded_token_seqs, padded_exit_seqs, sorted_lengths"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "train_dataset = SentenceDataset(token_sequences)\n",
                 "\n",
-                "train_loader = DataLoader(dataset=train_dataset,\n",
-                "                          batch_size=32,\n",
-                "                          num_workers=5,\n",
-                "                          shuffle=True,\n",
-                "                          collate_fn=collate_fn)"
+                "train_loader = DataLoader(\n",
+                "    dataset=train_dataset,\n",
+                "    batch_size=32,\n",
+                "    num_workers=5,\n",
+                "    shuffle=True,\n",
+                "    collate_fn=collate_fn,\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# testing the thing"
@@ -376,39 +354,41 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class LanguageModel(nn.Module):\n",
-                "    def __init__(self,\n",
-                "                 input_dim=len(unique_characters),\n",
-                "                 embedding_dim=200, \n",
-                "                 hidden_dim=512,\n",
-                "                 output_dim=len(article_vocabulary)):\n",
+                "    def __init__(\n",
+                "        self,\n",
+                "        input_dim=len(unique_characters),\n",
+                "        embedding_dim=200,\n",
+                "        hidden_dim=512,\n",
+                "        output_dim=len(article_vocabulary),\n",
+                "    ):\n",
                 "\n",
                 "        super(LanguageModel, self).__init__()\n",
                 "\n",
                 "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
                 "\n",
                 "        self.enc_lstm = nn.LSTM(\n",
                 "            input_size=embedding_dim,\n",
                 "            hidden_size=hidden_dim,\n",
                 "            num_layers=1,\n",
                 "            bidirectional=False,\n",
-                "            dropout=0.2\n",
+                "            dropout=0.2,\n",
                 "        )\n",
                 "\n",
-                "#        self.head = nn.Sequential(\n",
-                "#            nn.Linear(hidden_dim, hidden_dim),\n",
-                "#            nn.ReLU(),\n",
-                "#            nn.Dropout(0.3),\n",
-                "#            nn.Linear(hidden_dim, output_dim),\n",
-                "#        )\n",
-                "        \n",
+                "        #        self.head = nn.Sequential(\n",
+                "        #            nn.Linear(hidden_dim, hidden_dim),\n",
+                "        #            nn.ReLU(),\n",
+                "        #            nn.Dropout(0.3),\n",
+                "        #            nn.Linear(hidden_dim, output_dim),\n",
+                "        #        )\n",
+                "\n",
                 "        self.head = nn.Linear(hidden_dim, output_dim)\n",
                 "\n",
                 "    def forward(self, padded_char_seqs, exit_ix_seqs, sorted_lengths):\n",
                 "        x = self.embedding(padded_char_seqs)\n",
                 "\n",
                 "        x = pack_padded_sequence(x, lengths=sorted_lengths, batch_first=True)\n",
                 "\n",
@@ -475,36 +455,36 @@
                 "            token_seqs = torch.LongTensor(token_seqs).cuda(non_blocking=True)[:, 1:]\n",
                 "            exit_ix_seqs = torch.LongTensor(exit_ix_seqs).cuda(non_blocking=True)\n",
                 "            lengths = torch.LongTensor(lengths).cuda(non_blocking=True)\n",
                 "\n",
                 "            optimiser.zero_grad()\n",
                 "            preds = model(char_seqs, exit_ix_seqs, lengths)\n",
                 "            preds = preds.permute(0, 2, 1)\n",
-                "            \n",
+                "\n",
                 "            loss = loss_function(preds, token_seqs)\n",
                 "            loss.backward()\n",
                 "            optimiser.step()\n",
                 "\n",
                 "            losses.append(loss.item())\n",
-                "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
+                "            loop.set_description(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
                 "            loop.set_postfix(loss=np.mean(losses[-100:]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "train(\n",
                 "    model=model,\n",
                 "    train_loader=train_loader,\n",
                 "    loss_function=loss_function,\n",
                 "    optimiser=optimiser,\n",
-                "    n_epochs=1\n",
+                "    n_epochs=1,\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
```

### Comparing `weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/10 - character level language model, targeting wvs.ipynb` & `weco-datascience-0.1.9/notebooks/wikipedia/notebooks/10 - character level language model, targeting wvs.ipynb`

 * *Files 3% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9972027047314652%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\'), (23, '*

 * *            '\'nltk.download("punkt")\\n\'), (24, \'device = torch.device("cuda" if '*

 * *            'torch.cuda.is_available() else "cpu")\')], delete: [23, 22, 4, 3]}}, 2: {\'source\': '*

 * *            '{insert: [(0, \'base_path = "/mnt/efs/wikipedia/dumps/text/"\\n\'), (3, \'all_text = '*

 * *            '""\\n\'), (6, \'        with open(base_pat […]*

```diff
@@ -5,16 +5,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import re\n",
                 "import os\n",
                 "import io\n",
                 "import nltk\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
@@ -24,16 +25,16 @@
                 "from sklearn.preprocessing import OneHotEncoder\n",
                 "\n",
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader\n",
                 "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
                 "\n",
-                "nltk.download('punkt')\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "nltk.download(\"punkt\")\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# data"
@@ -41,24 +42,24 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "base_path = '/mnt/efs/wikipedia/dumps/text/'\n",
+                "base_path = \"/mnt/efs/wikipedia/dumps/text/\"\n",
                 "paths = np.random.choice(os.listdir(base_path), size=1)\n",
                 "\n",
-                "all_text = ''\n",
+                "all_text = \"\"\n",
                 "for path in paths:\n",
                 "    for filename in tqdm(os.listdir(base_path + path)):\n",
-                "        with open(base_path + path + '/' + filename, 'rb') as f:\n",
-                "            all_text += f.read().decode('latin1')\n",
+                "        with open(base_path + path + \"/\" + filename, \"rb\") as f:\n",
+                "            all_text += f.read().decode(\"latin1\")\n",
                 "\n",
-                "pattern = r'(?:<doc.+>)((.|\\s|\\S)*?)(?:<\\/doc>)'\n",
+                "pattern = r\"(?:<doc.+>)((.|\\s|\\S)*?)(?:<\\/doc>)\"\n",
                 "articles = [article[0] for article in re.findall(pattern, all_text)]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -77,44 +78,44 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def tokenize(sentence):\n",
-                "    '''moses tokeniser'''\n",
-                "    seq = ' '.join(word_tokenize(sentence))\n",
+                "    \"\"\"moses tokeniser\"\"\"\n",
+                "    seq = \" \".join(word_tokenize(sentence))\n",
                 "    seq = seq.replace(\" n't \", \"n 't \")\n",
                 "    return seq.split()\n",
                 "\n",
                 "\n",
                 "def label_linkable_tokens(sentence, label_all=True):\n",
-                "    parsed_html = BeautifulSoup(sentence, 'html.parser')\n",
+                "    parsed_html = BeautifulSoup(sentence, \"html.parser\")\n",
                 "\n",
-                "    link_text = [link.text for link in parsed_html.find_all('a')]\n",
+                "    link_text = [link.text for link in parsed_html.find_all(\"a\")]\n",
                 "    tokenised_links = [tokenize(link) for link in link_text]\n",
                 "    tokenised_text = tokenize(parsed_html.text)\n",
                 "    target_sequence = np.zeros(len(tokenised_text))\n",
                 "\n",
                 "    for link in tokenised_links:\n",
                 "        start_positions = kmp(tokenised_text, link)\n",
-                "        if label_all:            \n",
+                "        if label_all:\n",
                 "            for pos in start_positions:\n",
                 "                target_sequence[pos : pos + len(link)] = 1\n",
                 "        elif label_all == False and len(start_positions) > 0:\n",
                 "            pos = start_positions[0]\n",
                 "            target_sequence[pos : pos + len(link)] = 1\n",
-                "        else: \n",
+                "        else:\n",
                 "            pass\n",
                 "\n",
-                "    return tokenised_text, target_sequence.reshape(-1,1)\n",
+                "    return tokenised_text, target_sequence.reshape(-1, 1)\n",
                 "\n",
                 "\n",
                 "def kmp(sequence, sub):\n",
-                "    \"\"\"         \n",
+                "    \"\"\"\n",
                 "    Knuth\u2013Morris\u2013Pratt algorithm, returning the starting position\n",
                 "    of a specified subsequence within another, larger sequence.\n",
                 "    Usually used for string matching.\n",
                 "    \"\"\"\n",
                 "    partial = [0]\n",
                 "    for i in range(1, len(sub)):\n",
                 "        j = partial[i - 1]\n",
@@ -122,16 +123,17 @@
                 "            j = partial[j - 1]\n",
                 "        partial.append(j + 1 if sub[j] == sub[i] else j)\n",
                 "\n",
                 "    positions, j = [], 0\n",
                 "    for i in range(len(sequence)):\n",
                 "        while j > 0 and sequence[i] != sub[j]:\n",
                 "            j = partial[j - 1]\n",
-                "        if sequence[i] == sub[j]: j += 1\n",
-                "        if j == len(sub): \n",
+                "        if sequence[i] == sub[j]:\n",
+                "            j += 1\n",
+                "        if j == len(sub):\n",
                 "            positions.append(i - (j - 1))\n",
                 "            j = 0\n",
                 "\n",
                 "    return positions"
             ]
         },
         {
@@ -141,15 +143,15 @@
             "outputs": [],
             "source": [
                 "token_sequences, target_sequences = [], []\n",
                 "\n",
                 "for i, article in enumerate(tqdm(articles)):\n",
                 "    for j, sentence in enumerate(sent_tokenize(article)):\n",
                 "        try:\n",
-                "            tokenized_sentence, target_sequence = label_linkable_tokens(sentence)        \n",
+                "            tokenized_sentence, target_sequence = label_linkable_tokens(sentence)\n",
                 "            token_sequences.append(tokenized_sentence)\n",
                 "            target_sequences.append(target_sequence)\n",
                 "        except:\n",
                 "            pass"
             ]
         },
         {
@@ -170,28 +172,24 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "unique_characters = set(' '.join([\n",
-                "    token \n",
-                "    for seq in token_sequences\n",
-                "    for token in seq\n",
-                "]))"
+                "unique_characters = set(\" \".join([token for seq in token_sequences for token in seq]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "special_cases = ['xxunk', 'xxpad', 'xxbos', 'xxeos']\n",
+                "special_cases = [\"xxunk\", \"xxpad\", \"xxbos\", \"xxeos\"]\n",
                 "\n",
                 "for case in special_cases:\n",
                 "    unique_characters.add(case)"
             ]
         },
         {
             "cell_type": "code",
@@ -213,34 +211,33 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "n_wvs = 100000\n",
-                "wv_path = '/mnt/efs/text/word_vectors/wiki-news-300d-1M.vec'\n",
-                "wv_file = io.open(wv_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
+                "wv_path = \"/mnt/efs/text/word_vectors/wiki-news-300d-1M.vec\"\n",
+                "wv_file = io.open(wv_path, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\")\n",
                 "lines_to_parse = list(wv_file)[1:n_wvs]\n",
                 "\n",
-                "fasttext = {line.split()[0]: np.array(line.split()[1:]).astype(np.float32)\n",
-                "            for line in tqdm(lines_to_parse)}"
+                "fasttext = {\n",
+                "    line.split()[0]: np.array(line.split()[1:]).astype(np.float32)\n",
+                "    for line in tqdm(lines_to_parse)\n",
+                "}"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "from collections import Counter\n",
-                "all_tokens = [\n",
-                "    tok \n",
-                "    for seq in token_sequences \n",
-                "    for tok in seq\n",
-                "]\n",
+                "\n",
+                "all_tokens = [tok for seq in token_sequences for tok in seq]\n",
                 "\n",
                 "article_vocabulary, _ = zip(*Counter(all_tokens).most_common(n_wvs))\n",
                 "article_vocabulary = set(article_vocabulary)"
             ]
         },
         {
             "cell_type": "code",
@@ -275,20 +272,20 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "word_vector_embedding_matrix = torch.FloatTensor([\n",
-                "    fasttext[token]\n",
-                "    if token in fasttext\n",
-                "    else fasttext['xxunk']\n",
-                "    for token in article_vocabulary\n",
-                "])"
+                "word_vector_embedding_matrix = torch.FloatTensor(\n",
+                "    [\n",
+                "        fasttext[token] if token in fasttext else fasttext[\"xxunk\"]\n",
+                "        for token in article_vocabulary\n",
+                "    ]\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# dataset and dataloader"
@@ -300,160 +297,136 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "class SentenceDataset(Dataset):\n",
                 "    def __init__(self, token_seqs, word_vector_embedding_matrix):\n",
                 "        self.wv_embedding = nn.Embedding.from_pretrained(word_vector_embedding_matrix)\n",
                 "\n",
-                "        #impose length constraint\n",
+                "        # impose length constraint\n",
                 "        where_big_enough = np.where([len(seq) > 3 for seq in token_seqs])\n",
                 "        self.token_seqs = np.array(token_seqs)[where_big_enough]\n",
                 "\n",
-                "        #find prediction points for language model\n",
-                "        self.exit_ix_seqs = [\n",
-                "            self.find_exit_points(seq) for seq in self.token_seqs\n",
-                "        ]\n",
-                "\n",
-                "        #indexify\n",
-                "        self.char_ix_seqs = [\n",
-                "            self.indexify_chars(seq) for seq in self.token_seqs\n",
-                "        ]\n",
-                "\n",
-                "        self.token_wv_seqs = [\n",
-                "            self.vectorise_tokens(seq) for seq in self.token_seqs\n",
-                "        ]\n",
+                "        # find prediction points for language model\n",
+                "        self.exit_ix_seqs = [self.find_exit_points(seq) for seq in self.token_seqs]\n",
+                "\n",
+                "        # indexify\n",
+                "        self.char_ix_seqs = [self.indexify_chars(seq) for seq in self.token_seqs]\n",
+                "\n",
+                "        self.token_wv_seqs = [self.vectorise_tokens(seq) for seq in self.token_seqs]\n",
                 "\n",
                 "    def __getitem__(self, ix):\n",
                 "        char_ix_seq = self.char_ix_seqs[ix]\n",
                 "        token_wv_seq = self.token_wv_seqs[ix]\n",
                 "        exit_ix_seq = self.exit_ix_seqs[ix]\n",
                 "        return char_ix_seq, token_wv_seq, exit_ix_seq\n",
                 "\n",
                 "    def __len__(self):\n",
                 "        return len(self.token_seqs)\n",
-                "    \n",
+                "\n",
                 "    def vectorise_tokens(self, token_seq):\n",
-                "        ix_seq = torch.LongTensor(np.array(\n",
-                "            [token_to_ix[token]\n",
-                "             if token in article_vocabulary\n",
-                "             else token_to_ix['xxunk']\n",
-                "             for token in token_seq] + \n",
-                "            [token_to_ix['xxeos']]\n",
-                "        ))\n",
+                "        ix_seq = torch.LongTensor(\n",
+                "            np.array(\n",
+                "                [\n",
+                "                    token_to_ix[token]\n",
+                "                    if token in article_vocabulary\n",
+                "                    else token_to_ix[\"xxunk\"]\n",
+                "                    for token in token_seq\n",
+                "                ]\n",
+                "                + [token_to_ix[\"xxeos\"]]\n",
+                "            )\n",
+                "        )\n",
                 "        wv_seq = self.wv_embedding(ix_seq)\n",
                 "        return wv_seq\n",
-                "    \n",
+                "\n",
                 "    def indexify_chars(self, token_seq):\n",
                 "        ix_seq = np.array(\n",
-                "            [char_to_ix[char] for char in ' '.join(token_seq)] + \n",
-                "            [char_to_ix[' '], char_to_ix['xxeos']]\n",
+                "            [char_to_ix[char] for char in \" \".join(token_seq)]\n",
+                "            + [char_to_ix[\" \"], char_to_ix[\"xxeos\"]]\n",
                 "        )\n",
                 "        return torch.LongTensor(ix_seq)\n",
-                "    \n",
+                "\n",
                 "    def find_exit_points(self, token_seq):\n",
-                "        exit_positions = np.cumsum([\n",
-                "            len(token) + 1 for token in token_seq\n",
-                "        ])\n",
-                "        return torch.LongTensor(exit_positions) -1"
+                "        exit_positions = np.cumsum([len(token) + 1 for token in token_seq])\n",
+                "        return torch.LongTensor(exit_positions) - 1"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def collate_fn(batch):\n",
                 "    char_ix_seqs, token_wv_seqs, exit_ix_seqs = zip(*batch)\n",
                 "\n",
-                "    char_seq_lens = torch.LongTensor([\n",
-                "        len(char_seq) for char_seq in char_ix_seqs\n",
-                "    ])\n",
-                "    \n",
-                "    sorted_lengths, sort_indicies = char_seq_lens.sort(\n",
-                "        dim=0, \n",
-                "        descending=True\n",
-                "    )\n",
-                "    \n",
+                "    char_seq_lens = torch.LongTensor([len(char_seq) for char_seq in char_ix_seqs])\n",
+                "\n",
+                "    sorted_lengths, sort_indicies = char_seq_lens.sort(dim=0, descending=True)\n",
+                "\n",
                 "    sorted_char_seqs = [char_ix_seqs[i] for i in sort_indicies]\n",
                 "    sorted_wv_seqs = [token_wv_seqs[i] for i in sort_indicies]\n",
                 "    sorted_exit_seqs = [exit_ix_seqs[i] for i in sort_indicies]\n",
-                "    \n",
+                "\n",
                 "    padded_char_seqs = pad_sequence(\n",
-                "        sequences=sorted_char_seqs, \n",
-                "        padding_value=char_to_ix['xxpad'],\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_char_seqs, padding_value=char_to_ix[\"xxpad\"], batch_first=True\n",
                 "    )\n",
-                "    \n",
+                "\n",
                 "    padded_wv_seqs = pad_sequence(\n",
-                "        sequences=sorted_wv_seqs, \n",
-                "        padding_value=token_to_ix['xxpad'],\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_wv_seqs, padding_value=token_to_ix[\"xxpad\"], batch_first=True\n",
                 "    )\n",
-                "    \n",
+                "\n",
                 "    padded_exit_seqs = pad_sequence(\n",
-                "        sequences=sorted_exit_seqs, \n",
-                "        padding_value=0,\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_exit_seqs, padding_value=0, batch_first=True\n",
                 "    )\n",
-                "    \n",
+                "\n",
                 "    return padded_char_seqs, padded_wv_seqs, padded_exit_seqs, sorted_lengths"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "train_token_sequences, test_token_sequences = train_test_split(\n",
-                "    token_sequences, \n",
-                "    target_sequences, \n",
-                "    test_size=0.20, \n",
-                "    random_state=42\n",
+                "    token_sequences, target_sequences, test_size=0.20, random_state=42\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train_dataset = SentenceDataset(\n",
-                "    train_token_sequences, \n",
-                "    word_vector_embedding_matrix\n",
-                ")\n",
+                "train_dataset = SentenceDataset(train_token_sequences, word_vector_embedding_matrix)\n",
                 "\n",
                 "train_loader = DataLoader(\n",
-                "    dataset=train_dataset,  \n",
+                "    dataset=train_dataset,\n",
                 "    batch_size=32,\n",
                 "    num_workers=5,\n",
                 "    shuffle=True,\n",
-                "    collate_fn=collate_fn\n",
+                "    collate_fn=collate_fn,\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "test_dataset = SentenceDataset(\n",
-                "    test_token_sequences, \n",
-                "    word_vector_embedding_matrix\n",
-                ")\n",
+                "test_dataset = SentenceDataset(test_token_sequences, word_vector_embedding_matrix)\n",
                 "\n",
                 "test_loader = DataLoader(\n",
-                "    dataset=test_dataset,  \n",
+                "    dataset=test_dataset,\n",
                 "    batch_size=32,\n",
                 "    num_workers=5,\n",
                 "    shuffle=True,\n",
-                "    collate_fn=collate_fn\n",
+                "    collate_fn=collate_fn,\n",
                 ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -463,29 +436,28 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class LanguageModel(nn.Module):\n",
-                "    def __init__(self,\n",
-                "                 input_dim=len(unique_characters),\n",
-                "                 embedding_dim=50, \n",
-                "                 hidden_dim=512):\n",
+                "    def __init__(\n",
+                "        self, input_dim=len(unique_characters), embedding_dim=50, hidden_dim=512\n",
+                "    ):\n",
                 "\n",
                 "        super(LanguageModel, self).__init__()\n",
                 "\n",
                 "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
                 "\n",
                 "        self.enc_lstm = nn.LSTM(\n",
                 "            input_size=embedding_dim,\n",
                 "            hidden_size=hidden_dim,\n",
                 "            num_layers=1,\n",
                 "            bidirectional=False,\n",
-                "            #dropout=0.2\n",
+                "            # dropout=0.2\n",
                 "        )\n",
                 "\n",
                 "        self.head = nn.Sequential(\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim, hidden_dim),\n",
                 "            nn.ReLU(),\n",
                 "            nn.Dropout(0.3),\n",
@@ -554,47 +526,47 @@
                 "            char_seqs = torch.LongTensor(char_seqs).cuda(non_blocking=True)\n",
                 "            target_wvs = torch.FloatTensor(target_wvs).cuda(non_blocking=True)[:, 1:]\n",
                 "            exit_ix_seqs = torch.LongTensor(exit_ix_seqs).cuda(non_blocking=True)\n",
                 "            lengths = torch.LongTensor(lengths).cuda(non_blocking=True)\n",
                 "\n",
                 "            optimiser.zero_grad()\n",
                 "            preds = model(char_seqs, exit_ix_seqs, lengths)\n",
-                "            \n",
+                "\n",
                 "            loss = loss_function(preds, target_wvs, flags)\n",
                 "            loss.backward()\n",
                 "            optimiser.step()\n",
                 "\n",
                 "            losses.append(loss.item())\n",
-                "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
+                "            loop.set_description(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
                 "            loop.set_postfix(loss=np.mean(losses[-100:]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "train(\n",
                 "    model=model,\n",
                 "    train_loader=train_loader,\n",
                 "    loss_function=loss_function,\n",
                 "    optimiser=optimiser,\n",
-                "    n_epochs=1\n",
+                "    n_epochs=1,\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "loss_data = pd.Series(losses[20:]).rolling(window=50).mean()\n",
-                "ax = loss_data.plot();\n",
+                "ax = loss_data.plot()\n",
                 "ax.set_ylim(0, 0.06);"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -609,15 +581,15 @@
             "source": [
                 "sentence = 'Traditionally, the term \"philosophy\" referred to any body of knowledge.'\n",
                 "token_seq = tokenize(sentence)\n",
                 "\n",
                 "exit_ix_seq = np.cumsum([len(token) + 1 for token in token_seq]) - 1\n",
                 "exit_ix_seq = torch.LongTensor(exit_ix_seq).cuda()\n",
                 "\n",
-                "char_seq = ' '.join(token_seq)\n",
+                "char_seq = \" \".join(token_seq)\n",
                 "char_ix_seq = torch.LongTensor([[char_to_ix[c] for c in char_seq]]).cuda()\n",
                 "\n",
                 "x = model.embedding(char_ix_seq)\n",
                 "x, _ = model.enc_lstm(x)\n",
                 "x = x[0, exit_ix_seq]"
             ]
         },
```

### Comparing `weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/11- making the character-level network bidirectional.ipynb` & `weco-datascience-0.1.9/notebooks/wikipedia/notebooks/11- making the character-level network bidirectional.ipynb`

 * *Files 8% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9969016585804436%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 20)\\n\'), (23, '*

 * *            '\'nltk.download("punkt")\\n\'), (24, \'device = torch.device("cuda" if '*

 * *            'torch.cuda.is_available() else "cpu")\')], delete: [23, 22, 4, 3]}}, 2: {\'source\': '*

 * *            '{insert: [(0, \'base_path = "/mnt/efs/wikipedia/dumps/text/"\\n\'), (3, \'all_text = '*

 * *            '""\\n\'), (6, \'        with open(base_pat […]*

```diff
@@ -5,16 +5,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (20, 20)\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
                 "\n",
                 "import re\n",
                 "import os\n",
                 "import io\n",
                 "import nltk\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
@@ -24,16 +25,16 @@
                 "from sklearn.model_selection import train_test_split\n",
                 "\n",
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader\n",
                 "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
                 "\n",
-                "nltk.download('punkt')\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "nltk.download(\"punkt\")\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# data"
@@ -41,24 +42,24 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "base_path = '/mnt/efs/wikipedia/dumps/text/'\n",
+                "base_path = \"/mnt/efs/wikipedia/dumps/text/\"\n",
                 "paths = np.random.choice(os.listdir(base_path), size=1)\n",
                 "\n",
-                "all_text = ''\n",
+                "all_text = \"\"\n",
                 "for path in paths:\n",
                 "    for filename in tqdm(os.listdir(base_path + path)):\n",
-                "        with open(base_path + path + '/' + filename, 'rb') as f:\n",
-                "            all_text += f.read().decode('latin1')\n",
+                "        with open(base_path + path + \"/\" + filename, \"rb\") as f:\n",
+                "            all_text += f.read().decode(\"latin1\")\n",
                 "\n",
-                "pattern = r'(?:<doc.+>)((.|\\s|\\S)*?)(?:<\\/doc>)'\n",
+                "pattern = r\"(?:<doc.+>)((.|\\s|\\S)*?)(?:<\\/doc>)\"\n",
                 "articles = [article[0] for article in re.findall(pattern, all_text)]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -77,44 +78,44 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def tokenize(sentence):\n",
-                "    '''moses tokeniser'''\n",
-                "    seq = ' '.join(word_tokenize(sentence))\n",
+                "    \"\"\"moses tokeniser\"\"\"\n",
+                "    seq = \" \".join(word_tokenize(sentence))\n",
                 "    seq = seq.replace(\" n't \", \"n 't \")\n",
                 "    return seq.split()\n",
                 "\n",
                 "\n",
                 "def label_linkable_tokens(sentence, label_all=True):\n",
-                "    parsed_html = BeautifulSoup(sentence, 'html.parser')\n",
+                "    parsed_html = BeautifulSoup(sentence, \"html.parser\")\n",
                 "\n",
-                "    link_text = [link.text for link in parsed_html.find_all('a')]\n",
+                "    link_text = [link.text for link in parsed_html.find_all(\"a\")]\n",
                 "    tokenised_links = [tokenize(link) for link in link_text]\n",
                 "    tokenised_text = tokenize(parsed_html.text)\n",
                 "    target_sequence = np.zeros(len(tokenised_text))\n",
                 "\n",
                 "    for link in tokenised_links:\n",
                 "        start_positions = kmp(tokenised_text, link)\n",
-                "        if label_all:            \n",
+                "        if label_all:\n",
                 "            for pos in start_positions:\n",
                 "                target_sequence[pos : pos + len(link)] = 1\n",
                 "        elif label_all == False and len(start_positions) > 0:\n",
                 "            pos = start_positions[0]\n",
                 "            target_sequence[pos : pos + len(link)] = 1\n",
-                "        else: \n",
+                "        else:\n",
                 "            pass\n",
                 "\n",
-                "    return tokenised_text, target_sequence.reshape(-1,1)\n",
+                "    return tokenised_text, target_sequence.reshape(-1, 1)\n",
                 "\n",
                 "\n",
                 "def kmp(sequence, sub):\n",
-                "    \"\"\"         \n",
+                "    \"\"\"\n",
                 "    Knuth\u2013Morris\u2013Pratt algorithm, returning the starting position\n",
                 "    of a specified subsequence within another, larger sequence.\n",
                 "    Usually used for string matching.\n",
                 "    \"\"\"\n",
                 "    partial = [0]\n",
                 "    for i in range(1, len(sub)):\n",
                 "        j = partial[i - 1]\n",
@@ -122,16 +123,17 @@
                 "            j = partial[j - 1]\n",
                 "        partial.append(j + 1 if sub[j] == sub[i] else j)\n",
                 "\n",
                 "    positions, j = [], 0\n",
                 "    for i in range(len(sequence)):\n",
                 "        while j > 0 and sequence[i] != sub[j]:\n",
                 "            j = partial[j - 1]\n",
-                "        if sequence[i] == sub[j]: j += 1\n",
-                "        if j == len(sub): \n",
+                "        if sequence[i] == sub[j]:\n",
+                "            j += 1\n",
+                "        if j == len(sub):\n",
                 "            positions.append(i - (j - 1))\n",
                 "            j = 0\n",
                 "\n",
                 "    return positions"
             ]
         },
         {
@@ -141,15 +143,15 @@
             "outputs": [],
             "source": [
                 "token_sequences, target_sequences = [], []\n",
                 "\n",
                 "for i, article in enumerate(tqdm(articles)):\n",
                 "    for j, sentence in enumerate(sent_tokenize(article)):\n",
                 "        try:\n",
-                "            tokenized_sentence, target_sequence = label_linkable_tokens(sentence)        \n",
+                "            tokenized_sentence, target_sequence = label_linkable_tokens(sentence)\n",
                 "            token_sequences.append(tokenized_sentence)\n",
                 "            target_sequences.append(target_sequence)\n",
                 "        except:\n",
                 "            pass"
             ]
         },
         {
@@ -170,28 +172,24 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "unique_characters = set(' '.join([\n",
-                "    token \n",
-                "    for seq in token_sequences\n",
-                "    for token in seq\n",
-                "]))"
+                "unique_characters = set(\" \".join([token for seq in token_sequences for token in seq]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "special_cases = ['xxunk', 'xxpad', 'xxbos', 'xxeos']\n",
+                "special_cases = [\"xxunk\", \"xxpad\", \"xxbos\", \"xxeos\"]\n",
                 "\n",
                 "for case in special_cases:\n",
                 "    unique_characters.add(case)"
             ]
         },
         {
             "cell_type": "code",
@@ -213,34 +211,33 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "n_wvs = 1000000000\n",
-                "wv_path = '/mnt/efs/text/word_vectors/wiki-news-300d-1M.vec'\n",
-                "wv_file = io.open(wv_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
+                "wv_path = \"/mnt/efs/text/word_vectors/wiki-news-300d-1M.vec\"\n",
+                "wv_file = io.open(wv_path, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\")\n",
                 "lines_to_parse = list(wv_file)[1:n_wvs]\n",
                 "\n",
-                "fasttext = {line.split()[0]: np.array(line.split()[1:]).astype(np.float32)\n",
-                "            for line in tqdm(lines_to_parse)}"
+                "fasttext = {\n",
+                "    line.split()[0]: np.array(line.split()[1:]).astype(np.float32)\n",
+                "    for line in tqdm(lines_to_parse)\n",
+                "}"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "from collections import Counter\n",
-                "all_tokens = [\n",
-                "    tok \n",
-                "    for seq in token_sequences \n",
-                "    for tok in seq\n",
-                "]\n",
+                "\n",
+                "all_tokens = [tok for seq in token_sequences for tok in seq]\n",
                 "\n",
                 "article_vocabulary, _ = zip(*Counter(all_tokens).most_common(n_wvs))\n",
                 "article_vocabulary = set(article_vocabulary)"
             ]
         },
         {
             "cell_type": "code",
@@ -275,20 +272,20 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "word_vector_embedding_matrix = torch.FloatTensor([\n",
-                "    fasttext[token]\n",
-                "    if token in fasttext\n",
-                "    else fasttext['xxunk']\n",
-                "    for token in article_vocabulary\n",
-                "])"
+                "word_vector_embedding_matrix = torch.FloatTensor(\n",
+                "    [\n",
+                "        fasttext[token] if token in fasttext else fasttext[\"xxunk\"]\n",
+                "        for token in article_vocabulary\n",
+                "    ]\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# dataset and dataloader"
@@ -300,159 +297,138 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "class SentenceDataset(Dataset):\n",
                 "    def __init__(self, token_seqs, word_vector_embedding_matrix):\n",
                 "        self.wv_embedding = nn.Embedding.from_pretrained(word_vector_embedding_matrix)\n",
                 "\n",
-                "        #impose length constraint\n",
+                "        # impose length constraint\n",
                 "        where_big_enough = np.where([len(seq) > 3 for seq in token_seqs])\n",
                 "        self.token_seqs = np.array(token_seqs)[where_big_enough]\n",
                 "\n",
-                "        #indexify\n",
-                "        self.char_ix_seqs = [\n",
-                "            self.indexify_chars(seq) for seq in self.token_seqs\n",
-                "        ]\n",
-                "\n",
-                "        self.token_wv_seqs = [\n",
-                "            self.vectorise_tokens(seq) for seq in self.token_seqs\n",
-                "        ]\n",
-                "        \n",
-                "        #find prediction points for language model\n",
-                "        self.exit_ix_seqs = [\n",
-                "            self.find_exit_points(seq) for seq in self.char_ix_seqs\n",
-                "        ]\n",
+                "        # indexify\n",
+                "        self.char_ix_seqs = [self.indexify_chars(seq) for seq in self.token_seqs]\n",
+                "\n",
+                "        self.token_wv_seqs = [self.vectorise_tokens(seq) for seq in self.token_seqs]\n",
+                "\n",
+                "        # find prediction points for language model\n",
+                "        self.exit_ix_seqs = [self.find_exit_points(seq) for seq in self.char_ix_seqs]\n",
                 "\n",
                 "    def __getitem__(self, ix):\n",
                 "        char_ix_seq = self.char_ix_seqs[ix]\n",
                 "        token_wv_seq = self.token_wv_seqs[ix]\n",
                 "        exit_ix_seq = self.exit_ix_seqs[ix]\n",
                 "        return char_ix_seq, token_wv_seq, exit_ix_seq\n",
                 "\n",
                 "    def __len__(self):\n",
                 "        return len(self.token_seqs)\n",
-                "    \n",
+                "\n",
                 "    def vectorise_tokens(self, token_seq):\n",
-                "        ix_seq = torch.LongTensor(np.array(\n",
-                "            [token_to_ix['xxbos']] +\n",
-                "            [token_to_ix[token]\n",
-                "             if token in article_vocabulary\n",
-                "             else token_to_ix['xxunk']\n",
-                "             for token in token_seq] + \n",
-                "            [token_to_ix['xxeos']]\n",
-                "        ))\n",
+                "        ix_seq = torch.LongTensor(\n",
+                "            np.array(\n",
+                "                [token_to_ix[\"xxbos\"]]\n",
+                "                + [\n",
+                "                    token_to_ix[token]\n",
+                "                    if token in article_vocabulary\n",
+                "                    else token_to_ix[\"xxunk\"]\n",
+                "                    for token in token_seq\n",
+                "                ]\n",
+                "                + [token_to_ix[\"xxeos\"]]\n",
+                "            )\n",
+                "        )\n",
                 "        wv_seq = self.wv_embedding(ix_seq)\n",
                 "        return wv_seq\n",
-                "    \n",
+                "\n",
                 "    def indexify_chars(self, token_seq):\n",
                 "        ix_seq = np.array(\n",
-                "            [char_to_ix['xxbos'], char_to_ix[' ']] +\n",
-                "            [char_to_ix[char] for char in ' '.join(token_seq)] + \n",
-                "            [char_to_ix[' '], char_to_ix['xxeos']]\n",
+                "            [char_to_ix[\"xxbos\"], char_to_ix[\" \"]]\n",
+                "            + [char_to_ix[char] for char in \" \".join(token_seq)]\n",
+                "            + [char_to_ix[\" \"], char_to_ix[\"xxeos\"]]\n",
                 "        )\n",
                 "        return torch.LongTensor(ix_seq)\n",
-                "    \n",
+                "\n",
                 "    def find_exit_points(self, char_ix_seq):\n",
-                "        binary = (char_ix_seq == char_to_ix[' '])\n",
+                "        binary = char_ix_seq == char_to_ix[\" \"]\n",
                 "        return binary.nonzero().squeeze()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def collate_fn(batch):\n",
                 "    char_ix_seqs, token_wv_seqs, exit_ix_seqs = zip(*batch)\n",
                 "\n",
-                "    char_seq_lens = torch.LongTensor([\n",
-                "        len(char_seq) for char_seq in char_ix_seqs\n",
-                "    ])\n",
-                "    \n",
-                "    sorted_lengths, sort_indicies = char_seq_lens.sort(\n",
-                "        dim=0, \n",
-                "        descending=True\n",
-                "    )\n",
-                "    \n",
+                "    char_seq_lens = torch.LongTensor([len(char_seq) for char_seq in char_ix_seqs])\n",
+                "\n",
+                "    sorted_lengths, sort_indicies = char_seq_lens.sort(dim=0, descending=True)\n",
+                "\n",
                 "    sorted_char_seqs = [char_ix_seqs[i] for i in sort_indicies]\n",
                 "    sorted_wv_seqs = [token_wv_seqs[i] for i in sort_indicies]\n",
                 "    sorted_exit_seqs = [exit_ix_seqs[i] for i in sort_indicies]\n",
-                "    \n",
+                "\n",
                 "    padded_char_seqs = pad_sequence(\n",
-                "        sequences=sorted_char_seqs, \n",
-                "        padding_value=char_to_ix['xxpad'],\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_char_seqs, padding_value=char_to_ix[\"xxpad\"], batch_first=True\n",
                 "    )\n",
-                "    \n",
+                "\n",
                 "    padded_wv_seqs = pad_sequence(\n",
-                "        sequences=sorted_wv_seqs, \n",
-                "        padding_value=token_to_ix['xxpad'],\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_wv_seqs, padding_value=token_to_ix[\"xxpad\"], batch_first=True\n",
                 "    )\n",
-                "    \n",
+                "\n",
                 "    padded_exit_seqs = pad_sequence(\n",
-                "        sequences=sorted_exit_seqs, \n",
-                "        padding_value=0,\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_exit_seqs, padding_value=0, batch_first=True\n",
                 "    )\n",
-                "    \n",
+                "\n",
                 "    return padded_char_seqs, padded_wv_seqs, padded_exit_seqs, sorted_lengths"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "train_token_sequences, test_token_sequences = train_test_split(\n",
-                "    token_sequences, \n",
-                "    test_size=0.20, \n",
-                "    random_state=42\n",
+                "    token_sequences, test_size=0.20, random_state=42\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train_dataset = SentenceDataset(\n",
-                "    train_token_sequences, \n",
-                "    word_vector_embedding_matrix\n",
-                ")\n",
+                "train_dataset = SentenceDataset(train_token_sequences, word_vector_embedding_matrix)\n",
                 "\n",
                 "train_loader = DataLoader(\n",
-                "    dataset=train_dataset,  \n",
+                "    dataset=train_dataset,\n",
                 "    batch_size=32,\n",
                 "    num_workers=5,\n",
                 "    shuffle=True,\n",
-                "    collate_fn=collate_fn\n",
+                "    collate_fn=collate_fn,\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "test_dataset = SentenceDataset(\n",
-                "    test_token_sequences, \n",
-                "    word_vector_embedding_matrix\n",
-                ")\n",
+                "test_dataset = SentenceDataset(test_token_sequences, word_vector_embedding_matrix)\n",
                 "\n",
                 "test_loader = DataLoader(\n",
-                "    dataset=test_dataset,  \n",
+                "    dataset=test_dataset,\n",
                 "    batch_size=32,\n",
                 "    num_workers=5,\n",
                 "    shuffle=True,\n",
-                "    collate_fn=collate_fn\n",
+                "    collate_fn=collate_fn,\n",
                 ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -462,39 +438,38 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class LanguageModel(nn.Module):\n",
-                "    def __init__(self,\n",
-                "                 input_dim=len(unique_characters),\n",
-                "                 embedding_dim=50, \n",
-                "                 hidden_dim=512):\n",
+                "    def __init__(\n",
+                "        self, input_dim=len(unique_characters), embedding_dim=50, hidden_dim=512\n",
+                "    ):\n",
                 "\n",
                 "        super(LanguageModel, self).__init__()\n",
                 "\n",
                 "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
                 "\n",
                 "        self.enc_lstm = nn.LSTM(\n",
                 "            input_size=embedding_dim,\n",
                 "            hidden_size=hidden_dim,\n",
                 "            num_layers=1,\n",
-                "            #dropout=0.2,\n",
+                "            # dropout=0.2,\n",
                 "            bidirectional=True,\n",
                 "        )\n",
                 "\n",
                 "        self.head_fwd = nn.Sequential(\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim, hidden_dim),\n",
                 "            nn.ReLU(),\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim, 300),\n",
                 "        )\n",
-                "        \n",
+                "\n",
                 "        self.head_bwd = nn.Sequential(\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim, hidden_dim),\n",
                 "            nn.ReLU(),\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim, 300),\n",
                 "        )\n",
@@ -502,23 +477,23 @@
                 "    def forward(self, padded_char_seqs, exit_ix_seqs, sorted_lengths):\n",
                 "        x = self.embedding(padded_char_seqs)\n",
                 "\n",
                 "        x = pack_padded_sequence(x, lengths=sorted_lengths, batch_first=True)\n",
                 "\n",
                 "        x, _ = self.enc_lstm(x)\n",
                 "        out, _ = pad_packed_sequence(x, batch_first=True)\n",
-                "        \n",
+                "\n",
                 "        # pop out the character embeddings at position of the end of each token\n",
                 "        out = torch.stack([out[i, exit_ix_seqs[i]] for i in range(len(out))])\n",
-                "        \n",
+                "\n",
                 "        out_fwd, out_bwd = torch.chunk(out, 2, 2)\n",
                 "\n",
                 "        pred_fwd = self.head_fwd(out_fwd)\n",
                 "        pred_bwd = self.head_bwd(out_bwd)\n",
-                "        \n",
+                "\n",
                 "        return torch.cat([pred_fwd, pred_bwd], dim=1)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -567,50 +542,50 @@
                 "            exit_ix_seqs = torch.LongTensor(exit_ix_seqs).cuda(non_blocking=True)\n",
                 "            lengths = torch.LongTensor(lengths).cuda(non_blocking=True)\n",
                 "\n",
                 "            target_wvs = torch.FloatTensor(target_wvs).cuda(non_blocking=True)\n",
                 "            target_fwd = target_wvs[:, 1:]\n",
                 "            target_bwd = target_wvs[:, :-1]\n",
                 "            target = torch.cat([target_fwd, target_bwd], dim=1)\n",
-                "            \n",
+                "\n",
                 "            optimiser.zero_grad()\n",
                 "            preds = model(char_seqs, exit_ix_seqs, lengths)\n",
-                "            \n",
+                "\n",
                 "            loss = loss_function(preds, target, flags)\n",
                 "            loss.backward()\n",
                 "            optimiser.step()\n",
                 "\n",
                 "            losses.append(loss.item())\n",
-                "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
+                "            loop.set_description(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
                 "            loop.set_postfix(loss=np.mean(losses[-100:]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "train(\n",
                 "    model=model,\n",
                 "    train_loader=train_loader,\n",
                 "    loss_function=loss_function,\n",
                 "    optimiser=optimiser,\n",
-                "    n_epochs=1\n",
+                "    n_epochs=1,\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "loss_data = pd.Series(losses[20:]).rolling(window=50).mean()\n",
-                "ax = loss_data.plot();\n",
+                "ax = loss_data.plot()\n",
                 "ax.set_ylim(0.02, 0.05);"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -646,15 +621,15 @@
                 "    exit_ix_seqs = torch.LongTensor(exit_ix_seqs).cuda(non_blocking=True)\n",
                 "    lengths = torch.LongTensor(lengths).cuda(non_blocking=True)\n",
                 "\n",
                 "    target_wvs = torch.FloatTensor(target_wvs).cuda(non_blocking=True)\n",
                 "    target_fwd = target_wvs[:, 1:]\n",
                 "    target_bwd = target_wvs[:, :-1]\n",
                 "    target = torch.cat([target_fwd, target_bwd], dim=1)\n",
-                "    \n",
+                "\n",
                 "    optimiser.zero_grad()\n",
                 "    preds = model(char_seqs, exit_ix_seqs, lengths)\n",
                 "    preds.shape"
             ]
         },
         {
             "cell_type": "code",
@@ -688,18 +663,18 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "j = 25\n",
                 "\n",
                 "for i in range(preds.shape[1]):\n",
-                "    #query_wv = target[j][i].cpu().numpy().reshape(1,-1)\n",
-                "    query_wv = preds[j][i].cpu().numpy().reshape(1,-1)\n",
-                "    \n",
-                "    distances = cdist(query_wv, ft, metric='cosine')[0]\n",
+                "    # query_wv = target[j][i].cpu().numpy().reshape(1,-1)\n",
+                "    query_wv = preds[j][i].cpu().numpy().reshape(1, -1)\n",
+                "\n",
+                "    distances = cdist(query_wv, ft, metric=\"cosine\")[0]\n",
                 "    closest = np.argsort(distances)\n",
                 "\n",
                 "    words = np.array(list(fasttext.keys()))\n",
                 "    word = words[closest][3]\n",
                 "    print(word)"
             ]
         },
```

### Comparing `weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/12 - concatenating word- and character-level embeddings.ipynb` & `weco-datascience-0.1.9/notebooks/wikipedia/notebooks/final network.ipynb`

 * *Files 4% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9842829194456237%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 15)\\n\'), (11, \'import pickle\\n\'), (25, '*

 * *            '\'nltk.download("punkt")\\n\'), (26, \'device = torch.device("cuda" if '*

 * *            'torch.cuda.is_available() else "cpu")\')], delete: [24, 23, 4, 3]}}, 2: {\'source\': '*

 * *            "['def remove_title(article):\\n', '    return "*

 * *            '"\\\\n\\\\n".join(article.split("\\\\n\\\\n")[1:])\'] […]*

```diff
@@ -5,36 +5,38 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (20, 15)\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 15)\n",
                 "\n",
                 "import re\n",
                 "import os\n",
                 "import io\n",
                 "import nltk\n",
+                "import pickle\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
                 "from bs4 import BeautifulSoup\n",
                 "from tqdm import tqdm_notebook as tqdm\n",
                 "from nltk import word_tokenize, sent_tokenize\n",
                 "from sklearn.model_selection import train_test_split\n",
                 "from IPython.core.display import display, HTML\n",
                 "\n",
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader\n",
                 "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
                 "\n",
-                "nltk.download('punkt')\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "nltk.download(\"punkt\")\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# data"
@@ -42,25 +44,35 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "base_path = '/mnt/efs/wikipedia/dumps/text/'\n",
+                "def remove_title(article):\n",
+                "    return \"\\n\\n\".join(article.split(\"\\n\\n\")[1:])"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "base_path = \"/mnt/efs/wikipedia/dumps/text/\"\n",
                 "paths = np.random.choice(os.listdir(base_path), size=2)\n",
                 "\n",
-                "all_text = ''\n",
+                "all_text = \"\"\n",
                 "for path in paths:\n",
                 "    for filename in tqdm(os.listdir(base_path + path)):\n",
-                "        with open(base_path + path + '/' + filename, 'rb') as f:\n",
-                "            all_text += f.read().decode('latin1')\n",
+                "        with open(base_path + path + \"/\" + filename, \"rb\") as f:\n",
+                "            all_text += f.read().decode(\"latin1\")\n",
                 "\n",
-                "pattern = r'(?:<doc.+>)((.|\\s|\\S)*?)(?:<\\/doc>)'\n",
-                "articles = [article[0] for article in re.findall(pattern, all_text)]"
+                "pattern = r\"(?:<doc.+>)((.|\\s|\\S)*?)(?:<\\/doc>)\"\n",
+                "articles = [remove_title(article[0]) for article in re.findall(pattern, all_text)]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -76,53 +88,24 @@
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
-            "source": []
-        },
-        {
-            "cell_type": "code",
-            "execution_count": null,
-            "metadata": {},
-            "outputs": [],
             "source": [
                 "def tokenize(sentence):\n",
-                "    '''moses tokeniser'''\n",
-                "    seq = ' '.join(word_tokenize(sentence))\n",
+                "    \"\"\"moses tokeniser\"\"\"\n",
+                "    seq = \" \".join(word_tokenize(sentence))\n",
                 "    seq = seq.replace(\" n't \", \"n 't \")\n",
                 "    return seq.split()\n",
                 "\n",
                 "\n",
-                "def label_linkable_tokens(sentence, label_all=True):\n",
-                "    parsed_html = BeautifulSoup(sentence, 'html.parser')\n",
-                "\n",
-                "    link_text = [link.text for link in parsed_html.find_all('a')]\n",
-                "    tokenised_links = [tokenize(link) for link in link_text]\n",
-                "    tokenised_text = tokenize(parsed_html.text)\n",
-                "    target_sequence = np.zeros(len(tokenised_text))\n",
-                "\n",
-                "    for link in tokenised_links:\n",
-                "        start_positions = kmp(tokenised_text, link)\n",
-                "        if label_all:            \n",
-                "            for pos in start_positions:\n",
-                "                target_sequence[pos : pos + len(link)] = 1\n",
-                "        elif label_all == False and len(start_positions) > 0:\n",
-                "            pos = start_positions[0]\n",
-                "            target_sequence[pos : pos + len(link)] = 1\n",
-                "        else: \n",
-                "            pass\n",
-                "\n",
-                "    return tokenised_text, target_sequence\n",
-                "\n",
-                "\n",
                 "def kmp(sequence, sub):\n",
-                "    \"\"\"         \n",
+                "    \"\"\"\n",
                 "    Knuth\u2013Morris\u2013Pratt algorithm, returning the starting position\n",
                 "    of a specified subsequence within another, larger sequence.\n",
                 "    Usually used for string matching.\n",
                 "    \"\"\"\n",
                 "    partial = [0]\n",
                 "    for i in range(1, len(sub)):\n",
                 "        j = partial[i - 1]\n",
@@ -130,47 +113,68 @@
                 "            j = partial[j - 1]\n",
                 "        partial.append(j + 1 if sub[j] == sub[i] else j)\n",
                 "\n",
                 "    positions, j = [], 0\n",
                 "    for i in range(len(sequence)):\n",
                 "        while j > 0 and sequence[i] != sub[j]:\n",
                 "            j = partial[j - 1]\n",
-                "        if sequence[i] == sub[j]: j += 1\n",
-                "        if j == len(sub): \n",
+                "        if sequence[i] == sub[j]:\n",
+                "            j += 1\n",
+                "        if j == len(sub):\n",
                 "            positions.append(i - (j - 1))\n",
                 "            j = 0\n",
                 "\n",
-                "    return positions"
+                "    return positions\n",
+                "\n",
+                "\n",
+                "def label(tokenised_sequences, link_tokens):\n",
+                "    target_sequences = []\n",
+                "\n",
+                "    for i, sequence in enumerate(tokenised_sequences):\n",
+                "        target_sequence = np.zeros(len(sequence))\n",
+                "\n",
+                "        for link in link_tokens:\n",
+                "            start_positions = kmp(sequence, link)\n",
+                "            for pos in start_positions:\n",
+                "                target_sequence[pos : pos + len(link)] = 1\n",
+                "\n",
+                "        target_sequences.append(target_sequence)\n",
+                "\n",
+                "    return target_sequences\n",
+                "\n",
+                "\n",
+                "def label_linkable_tokens(text, label_all=True):\n",
+                "    parsed_html = BeautifulSoup(text, \"html.parser\")\n",
+                "\n",
+                "    link_tokens = [tokenize(link.text) for link in parsed_html.find_all(\"a\")]\n",
+                "\n",
+                "    tokenised_sequences = [\n",
+                "        tokenize(sentence) for sentence in sent_tokenize(parsed_html.text)\n",
+                "    ]\n",
+                "\n",
+                "    target_sequences = label(tokenised_sequences, link_tokens)\n",
+                "\n",
+                "    return tokenised_sequences, target_sequences"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "token_sequences, target_sequences = [], []\n",
                 "\n",
-                "for i, article in enumerate(tqdm(articles)):\n",
-                "    for j, sentence in enumerate(sent_tokenize(article)):\n",
-                "        try:\n",
-                "            tokenized_sentence, target_sequence = label_linkable_tokens(sentence)        \n",
-                "            token_sequences.append(tokenized_sentence)\n",
-                "            target_sequences.append(target_sequence)\n",
-                "        except:\n",
-                "            pass"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": null,
-            "metadata": {},
-            "outputs": [],
-            "source": [
-                "len(token_sequences)"
+                "for article in tqdm(articles):\n",
+                "    try:\n",
+                "        tokenised_seqs, target_seqs = label_linkable_tokens(article)\n",
+                "        token_sequences.extend(tokenised_seqs)\n",
+                "        target_sequences.extend(target_seqs)\n",
+                "    except:\n",
+                "        pass"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# character level inputs"
@@ -178,28 +182,24 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "unique_characters = set(' '.join([\n",
-                "    token \n",
-                "    for seq in token_sequences\n",
-                "    for token in seq\n",
-                "]))"
+                "unique_characters = set(\" \".join([token for seq in token_sequences for token in seq]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "special_cases = ['xxunk', 'xxpad', 'xxbos', 'xxeos']\n",
+                "special_cases = [\"xxunk\", \"xxpad\", \"xxbos\", \"xxeos\"]\n",
                 "\n",
                 "for case in special_cases:\n",
                 "    unique_characters.add(case)"
             ]
         },
         {
             "cell_type": "code",
@@ -220,36 +220,35 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "wv_path = '/mnt/efs/text/word_vectors/wiki-news-300d-1M.vec'\n",
-                "wv_file = io.open(wv_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
+                "wv_path = \"/mnt/efs/text/word_vectors/wiki-news-300d-1M.vec\"\n",
+                "wv_file = io.open(wv_path, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\")\n",
                 "lines_to_parse = list(wv_file)[1:]\n",
                 "\n",
-                "fasttext = {line.split()[0]: np.array(line.split()[1:]).astype(np.float32)\n",
-                "            for line in tqdm(lines_to_parse)}"
+                "fasttext = {\n",
+                "    line.split()[0]: np.array(line.split()[1:]).astype(np.float32)\n",
+                "    for line in tqdm(lines_to_parse)\n",
+                "}"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "from collections import Counter\n",
-                "all_tokens = [\n",
-                "    tok \n",
-                "    for seq in token_sequences \n",
-                "    for tok in seq\n",
-                "]\n",
                 "\n",
-                "article_vocabulary, _ = zip(*Counter(all_tokens).most_common(1000000))\n",
+                "all_tokens = [tok for seq in token_sequences for tok in seq]\n",
+                "\n",
+                "article_vocabulary, _ = zip(*Counter(all_tokens).most_common(10000000000))\n",
                 "article_vocabulary = set(article_vocabulary)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -282,20 +281,20 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "word_vector_embedding_matrix = torch.FloatTensor([\n",
-                "    fasttext[token]\n",
-                "    if token in fasttext\n",
-                "    else fasttext['xxunk']\n",
-                "    for token in article_vocabulary\n",
-                "])"
+                "word_vector_embedding_matrix = torch.FloatTensor(\n",
+                "    [\n",
+                "        fasttext[token] if token in fasttext else fasttext[\"xxunk\"]\n",
+                "        for token in article_vocabulary\n",
+                "    ]\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# dataset and dataloader"
@@ -305,111 +304,96 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class SentenceDataset(Dataset):\n",
                 "    def __init__(self, token_seqs, target_seqs):\n",
-                "        #impose length constraint\n",
+                "        # impose length constraint\n",
                 "        where_big_enough = np.where([len(seq) > 3 for seq in token_seqs])\n",
                 "        self.token_seqs = np.array(token_seqs)[where_big_enough]\n",
                 "        self.target_seqs = np.array(target_seqs)[where_big_enough]\n",
                 "\n",
-                "        #indexify\n",
-                "        self.char_ix_seqs = [\n",
-                "            self.indexify_chars(seq) for seq in self.token_seqs\n",
-                "        ]\n",
-                "\n",
-                "        self.token_seqs = [\n",
-                "            self.indexify_tokens(seq) for seq in self.token_seqs\n",
-                "        ]\n",
-                "        \n",
-                "        #find prediction points for language model\n",
-                "        self.exit_ix_seqs = [\n",
-                "            self.find_exit_points(seq) for seq in self.char_ix_seqs\n",
-                "        ]\n",
+                "        # indexify\n",
+                "        self.char_ix_seqs = [self.indexify_chars(seq) for seq in self.token_seqs]\n",
+                "\n",
+                "        self.token_seqs = [self.indexify_tokens(seq) for seq in self.token_seqs]\n",
+                "\n",
+                "        # find prediction points for language model\n",
+                "        self.exit_ix_seqs = [self.find_exit_points(seq) for seq in self.char_ix_seqs]\n",
                 "\n",
                 "    def __getitem__(self, ix):\n",
                 "        char_ix_seq = self.char_ix_seqs[ix]\n",
                 "        token_seq = self.token_seqs[ix]\n",
                 "        exit_ix_seq = self.exit_ix_seqs[ix]\n",
                 "        target_seq = self.target_seqs[ix]\n",
                 "        return char_ix_seq, token_seq, exit_ix_seq, target_seq\n",
                 "\n",
                 "    def __len__(self):\n",
                 "        return len(self.token_seqs)\n",
-                "    \n",
+                "\n",
                 "    def indexify_tokens(self, token_seq):\n",
                 "        ix_seq = np.array(\n",
-                "            [token_to_ix[token]\n",
-                "             if token in article_vocabulary\n",
-                "             else token_to_ix['xxunk']\n",
-                "             for token in token_seq]\n",
+                "            [\n",
+                "                token_to_ix[token]\n",
+                "                if token in article_vocabulary\n",
+                "                else token_to_ix[\"xxunk\"]\n",
+                "                for token in token_seq\n",
+                "            ]\n",
                 "        )\n",
                 "        return torch.LongTensor(ix_seq)\n",
-                "    \n",
+                "\n",
                 "    def indexify_chars(self, token_seq):\n",
                 "        ix_seq = np.array(\n",
-                "            [char_to_ix['xxbos'], char_to_ix[' ']] +\n",
-                "            [char_to_ix[char] for char in ' '.join(token_seq)] + \n",
-                "            [char_to_ix[' '], char_to_ix['xxeos']]\n",
+                "            [char_to_ix[\"xxbos\"], char_to_ix[\" \"]]\n",
+                "            + [char_to_ix[char] for char in \" \".join(token_seq)]\n",
+                "            + [char_to_ix[\" \"], char_to_ix[\"xxeos\"]]\n",
                 "        )\n",
                 "        return torch.LongTensor(ix_seq)\n",
-                "    \n",
+                "\n",
                 "    def find_exit_points(self, char_ix_seq):\n",
-                "        binary = (char_ix_seq == char_to_ix[' '])\n",
+                "        binary = char_ix_seq == char_to_ix[\" \"]\n",
                 "        return binary.nonzero().squeeze()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def collate_fn(batch):\n",
                 "    char_ix_seqs, token_seqs, exit_ix_seqs, target_seqs = zip(*batch)\n",
                 "\n",
-                "    char_seq_lens = torch.LongTensor([\n",
-                "        len(char_seq) for char_seq in char_ix_seqs\n",
-                "    ])\n",
-                "    \n",
-                "    sorted_char_lengths, sort_indicies = char_seq_lens.sort(\n",
-                "        dim=0, \n",
-                "        descending=True\n",
-                "    )\n",
-                "    \n",
+                "    char_seq_lens = torch.LongTensor([len(char_seq) for char_seq in char_ix_seqs])\n",
+                "\n",
+                "    sorted_char_lengths, sort_indicies = char_seq_lens.sort(dim=0, descending=True)\n",
+                "\n",
                 "    sorted_char_seqs = [char_ix_seqs[i] for i in sort_indicies]\n",
                 "    sorted_token_seqs = [token_seqs[i] for i in sort_indicies]\n",
                 "    sorted_exit_seqs = [exit_ix_seqs[i] for i in sort_indicies]\n",
                 "    sorted_target_seqs = [torch.LongTensor(target_seqs[i]) for i in sort_indicies]\n",
                 "    sorted_token_lengths = torch.LongTensor([len(seq) for seq in sorted_token_seqs])\n",
-                "    \n",
+                "\n",
                 "    padded_char_seqs = pad_sequence(\n",
-                "        sequences=sorted_char_seqs, \n",
-                "        padding_value=char_to_ix['xxpad'],\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_char_seqs, padding_value=char_to_ix[\"xxpad\"], batch_first=True\n",
                 "    )\n",
-                "    \n",
+                "\n",
                 "    padded_token_seqs = pad_sequence(\n",
-                "        sequences=sorted_token_seqs, \n",
-                "        padding_value=token_to_ix['xxpad'],\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_token_seqs,\n",
+                "        padding_value=token_to_ix[\"xxpad\"],\n",
+                "        batch_first=True,\n",
                 "    )\n",
-                "    \n",
+                "\n",
                 "    padded_exit_seqs = pad_sequence(\n",
-                "        sequences=sorted_exit_seqs, \n",
-                "        padding_value=0,\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_exit_seqs, padding_value=0, batch_first=True\n",
                 "    )\n",
-                "    \n",
+                "\n",
                 "    padded_target_seqs = pad_sequence(\n",
-                "        sequences=sorted_target_seqs, \n",
-                "        padding_value=0,\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_target_seqs, padding_value=0, batch_first=True\n",
                 "    )\n",
                 "\n",
                 "    return (\n",
                 "        padded_char_seqs,\n",
                 "        padded_token_seqs,\n",
                 "        padded_exit_seqs,\n",
                 "        sorted_char_lengths,\n",
@@ -421,58 +405,49 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "train_tokens, test_tokens, train_targets, test_targets = train_test_split(\n",
-                "    token_sequences, \n",
-                "    target_sequences,\n",
-                "    test_size=0.20, \n",
-                "    random_state=42\n",
+                "    token_sequences, target_sequences, test_size=0.05, random_state=42\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train_dataset = SentenceDataset(\n",
-                "    train_tokens, \n",
-                "    train_targets\n",
-                ")\n",
+                "train_dataset = SentenceDataset(train_tokens, train_targets)\n",
                 "\n",
                 "train_loader = DataLoader(\n",
-                "    dataset=train_dataset,  \n",
-                "    batch_size=32,\n",
+                "    dataset=train_dataset,\n",
+                "    batch_size=64,\n",
                 "    num_workers=5,\n",
                 "    shuffle=True,\n",
-                "    collate_fn=collate_fn\n",
+                "    collate_fn=collate_fn,\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "test_dataset = SentenceDataset(\n",
-                "    test_tokens, \n",
-                "    test_targets\n",
-                ")\n",
+                "test_dataset = SentenceDataset(test_tokens, test_targets)\n",
                 "\n",
                 "test_loader = DataLoader(\n",
-                "    dataset=test_dataset,  \n",
+                "    dataset=test_dataset,\n",
                 "    batch_size=1,\n",
                 "    num_workers=5,\n",
                 "    shuffle=True,\n",
-                "    collate_fn=collate_fn\n",
+                "    collate_fn=collate_fn,\n",
                 ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -501,15 +476,15 @@
                 "        self.head_fwd = nn.Sequential(\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
                 "            nn.ReLU(),\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim // 2, output_dim),\n",
                 "        )\n",
-                "        \n",
+                "\n",
                 "        self.head_bwd = nn.Sequential(\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
                 "            nn.ReLU(),\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim // 2, output_dim),\n",
                 "        )\n",
@@ -517,89 +492,80 @@
                 "    def forward(self, char_seqs, exit_seqs, lengths):\n",
                 "        x = self.embedding(char_seqs)\n",
                 "\n",
                 "        x = pack_padded_sequence(x, lengths=lengths, batch_first=True)\n",
                 "\n",
                 "        x, _ = self.char_level_lstm(x)\n",
                 "        out, _ = pad_packed_sequence(x, batch_first=True)\n",
-                "        \n",
+                "\n",
                 "        # pop out the character embeddings at position of the end of each token\n",
                 "        out = torch.stack([out[i, exit_seqs[i]] for i in range(len(out))])\n",
-                "        \n",
+                "\n",
                 "        out_fwd, out_bwd = torch.chunk(out, 2, 2)\n",
                 "\n",
                 "        pred_fwd = self.head_fwd(out_fwd[:, 1:])\n",
                 "        pred_bwd = self.head_bwd(out_bwd[:, :-1])\n",
-                "        \n",
+                "\n",
                 "        return pred_fwd, pred_bwd"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class LinkLabeller(nn.Module):\n",
-                "    def __init__(self, word_vector_embedding_matrix, hidden_dim=512):\n",
+                "    def __init__(self, word_vector_embedding_matrix, hidden_dim=1024):\n",
                 "        super(LinkLabeller, self).__init__()\n",
                 "        self.wv_embedding = nn.Embedding.from_pretrained(word_vector_embedding_matrix)\n",
                 "\n",
                 "        self.cln = CharacterLevelNetwork(\n",
                 "            input_dim=len(unique_characters),\n",
                 "            embedding_dim=50,\n",
-                "            hidden_dim=256,\n",
-                "            output_dim=50\n",
+                "            hidden_dim=128,\n",
+                "            output_dim=50,\n",
                 "        )\n",
-                "        \n",
-                "        self.lstm_input_size = (\n",
-                "            word_vector_embedding_matrix.shape[1] + \n",
-                "            (self.cln.output_dim * 2)\n",
+                "\n",
+                "        self.lstm_input_size = word_vector_embedding_matrix.shape[1] + (\n",
+                "            self.cln.output_dim * 2\n",
                 "        )\n",
-                "        \n",
+                "\n",
                 "        self.word_level_lstm = nn.LSTM(\n",
                 "            input_size=self.lstm_input_size,\n",
                 "            hidden_size=hidden_dim,\n",
                 "            num_layers=2,\n",
                 "            bidirectional=True,\n",
-                "            dropout=0.2\n",
+                "            dropout=0.2,\n",
                 "        )\n",
-                "                \n",
+                "\n",
                 "        self.head = nn.Sequential(\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim * 2, hidden_dim // 32),\n",
                 "            nn.ReLU(),\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim // 32, 2),\n",
                 "        )\n",
-                "    \n",
-                "    def forward(self, char_seqs, token_seqs, exit_seqs, c_lens, t_lens):        \n",
+                "\n",
+                "    def forward(self, char_seqs, token_seqs, exit_seqs, c_lens, t_lens):\n",
                 "        wv_seqs = self.wv_embedding(token_seqs)\n",
                 "        char_fwd, char_bwd = self.cln(char_seqs, exit_seqs, c_lens)\n",
-                "        \n",
-                "        concats = torch.cat(\n",
-                "            [char_fwd, char_bwd, wv_seqs], \n",
-                "            dim=2\n",
-                "        )\n",
-                "        \n",
-                "        sorted_lengths, sort_indicies = t_lens.sort(\n",
-                "            dim=0,\n",
-                "            descending=True\n",
-                "        )\n",
-                "        \n",
+                "\n",
+                "        concats = torch.cat([char_fwd, char_bwd, wv_seqs], dim=2)\n",
+                "\n",
+                "        sorted_lengths, sort_indicies = t_lens.sort(dim=0, descending=True)\n",
+                "\n",
                 "        concats = torch.stack([concats[i] for i in sort_indicies])\n",
-                "        \n",
-                "        packed = pack_padded_sequence(\n",
-                "            concats,\n",
-                "            lengths=sorted_lengths,\n",
-                "            batch_first=True\n",
-                "        )\n",
                 "\n",
-                "        embedded, _ = self.word_level_lstm(concats)\n",
-                "        output = self.head(embedded).permute(0,2,1)\n",
+                "        packed = pack_padded_sequence(concats, lengths=sorted_lengths, batch_first=True)\n",
+                "\n",
+                "        packed_embedded, _ = self.word_level_lstm(packed)\n",
+                "        embedded, _ = pad_packed_sequence(packed_embedded)\n",
+                "\n",
+                "        output = self.head(embedded).permute(1, 2, 0)\n",
                 "        return output, sort_indicies"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -635,15 +601,15 @@
             "source": [
                 "losses = []\n",
                 "\n",
                 "torch.backends.cudnn.benchmark = True\n",
                 "\n",
                 "trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
                 "\n",
-                "optimiser = optim.Adam(trainable_parameters, lr=0.0001)\n",
+                "optimiser = optim.Adam(trainable_parameters, lr=0.001)\n",
                 "\n",
                 "loss_function = nn.CrossEntropyLoss(weight=class_weights.cuda())"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
@@ -660,50 +626,52 @@
                 "            exit_seqs = torch.LongTensor(exit_seqs).cuda(non_blocking=True)\n",
                 "            c_lens = torch.LongTensor(c_lens).cuda(non_blocking=True)\n",
                 "            t_lens = torch.LongTensor(t_lens).cuda(non_blocking=True)\n",
                 "            targets = torch.LongTensor(targets).cuda(non_blocking=True)\n",
                 "\n",
                 "            optimiser.zero_grad()\n",
                 "            preds, sort_indicies = model(c_seqs, t_seqs, exit_seqs, c_lens, t_lens)\n",
-                "            \n",
+                "\n",
                 "            targets = torch.stack([targets[i] for i in sort_indicies])\n",
-                "            \n",
+                "\n",
                 "            loss = loss_function(preds, targets)\n",
                 "            loss.backward()\n",
                 "            optimiser.step()\n",
                 "\n",
                 "            losses.append(loss.item())\n",
-                "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
-                "            loop.set_postfix(loss=np.mean(losses[-100:]))"
+                "            loop.set_description(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
+                "            loop.set_postfix(loss=np.mean(losses[-100:]))\n",
+                "\n",
+                "        torch.save(model.state_dict(), \"/mnt/efs/models/model_state_dict.pt\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "train(\n",
                 "    model=model,\n",
                 "    train_loader=train_loader,\n",
                 "    loss_function=loss_function,\n",
                 "    optimiser=optimiser,\n",
-                "    n_epochs=3\n",
+                "    n_epochs=3,\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "loss_data = pd.Series(losses[20:]).rolling(window=100).mean()\n",
-                "ax = loss_data.plot();\n",
-                "ax.set_ylim(0.12, 0.28);"
+                "ax = loss_data.plot()\n",
+                "ax.set_ylim(0.1, 0.3);"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# test the model on unseen data"
@@ -711,62 +679,112 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "output = ''\n",
+                "iterable_loader = iter(test_loader)"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "def format_output(tokens, targets, preds):\n",
+                "    target_string, pred_string = \"\", \"\"\n",
+                "\n",
+                "    for token_id, target, pred in zip(tokens, targets, preds):\n",
+                "        token = ix_to_token[token_id.item()]\n",
                 "\n",
-                "for i, (c_seqs, t_seqs, exit_seqs, c_lens, t_lens, targets) in enumerate(test_loader):\n",
-                "    while i < 10:\n",
-                "        c_seqs = torch.LongTensor(c_seqs).cuda()\n",
-                "        t_seqs = torch.LongTensor(t_seqs).cuda()\n",
-                "        exit_seqs = torch.LongTensor(exit_seqs).cuda()\n",
-                "        c_lens = torch.LongTensor(c_lens).cuda()\n",
-                "        t_lens = torch.LongTensor(t_lens).cuda()\n",
-                "        targets = torch.LongTensor(targets).cuda()\n",
-                "\n",
-                "        optimiser.zero_grad()\n",
-                "        preds, sort_indicies = model(c_seqs, t_seqs, exit_seqs, c_lens, t_lens)\n",
-                "        preds = nn.LogSoftmax(dim=1)(preds).argmax(dim=1)\n",
-                "        \n",
-                "        targets = torch.stack([targets[i] for i in sort_indicies])\n",
-                "        \n",
-                "        target_string = []\n",
-                "        pred_string = []\n",
-                "\n",
-                "        for i in range(len(t_seqs[0])):\n",
-                "            if targets[0][i].item() == 1:\n",
-                "                target_string.append('<b>' + ix_to_token[t_seqs[0][i].item()] + '</b>')\n",
-                "            else:\n",
-                "                target_string.append(ix_to_token[t_seqs[0][i].item()])\n",
-                "            if preds[0][i].item() == 1:\n",
-                "                pred_string.append('<b>' + ix_to_token[t_seqs[0][i].item()] + '</b>')\n",
-                "            else:\n",
-                "                pred_string.append(ix_to_token[t_seqs[0][i].item()])\n",
+                "        if target.item() == 1:\n",
+                "            target_string += \"<b>\" + token + \"</b> \"\n",
+                "        else:\n",
+                "            target_string += token + \" \"\n",
+                "\n",
+                "        if pred.item() == 1:\n",
+                "            pred_string += \"<b>\" + token + \"</b> \"\n",
+                "        else:\n",
+                "            pred_string += token + \" \"\n",
+                "\n",
+                "    output_string = (\n",
+                "        \"PRED:<br>\"\n",
+                "        + pred_string\n",
+                "        + \"<br><br>TARG:<br>\"\n",
+                "        + target_string\n",
+                "        + \"<br><br>------------------------<br><br>\"\n",
+                "    )\n",
                 "\n",
-                "        output += 'PRED:    ' + ' '.join(pred_string) + '<br>'\n",
-                "        output += 'TARG:    ' + ' '.join(target_string) + '</p>'"
+                "    return output_string"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
+                "output = \"\"\n",
+                "samples = [next(iterable_loader) for i in range(10)]\n",
+                "\n",
+                "for (c_seqs, t_seqs, exit_seqs, c_lens, t_lens, targets) in samples:\n",
+                "    c_seqs = torch.LongTensor(c_seqs).cuda(non_blocking=True)\n",
+                "    t_seqs = torch.LongTensor(t_seqs).cuda(non_blocking=True)\n",
+                "    exit_seqs = torch.LongTensor(exit_seqs).cuda(non_blocking=True)\n",
+                "    c_lens = torch.LongTensor(c_lens).cuda(non_blocking=True)\n",
+                "    t_lens = torch.LongTensor(t_lens).cuda(non_blocking=True)\n",
+                "    targets = torch.LongTensor(targets).cuda(non_blocking=True)\n",
+                "\n",
+                "    optimiser.zero_grad()\n",
+                "    preds, sort_indicies = model(c_seqs, t_seqs, exit_seqs, c_lens, t_lens)\n",
+                "    preds = nn.LogSoftmax(dim=1)(preds).argmax(dim=1)\n",
+                "\n",
+                "    targets = torch.stack([targets[i] for i in sort_indicies])\n",
+                "\n",
+                "    target_string = []\n",
+                "    pred_string = []\n",
+                "\n",
+                "    output += format_output(t_seqs[0], targets[0], preds[0])\n",
+                "\n",
                 "display(HTML(output))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
+            "source": [
+                "torch.save(model.state_dict(), \"/mnt/efs/models/nerd/model_state_dict.pt\")\n",
+                "\n",
+                "with open(\"/mnt/efs/models/nerd/token_to_ix.pkl\", \"wb\") as f:\n",
+                "    pickle.dump(token_to_ix, f)\n",
+                "\n",
+                "with open(\"/mnt/efs/models/nerd/ix_to_token.pkl\", \"wb\") as f:\n",
+                "    pickle.dump(ix_to_token, f)\n",
+                "\n",
+                "with open(\"/mnt/efs/models/nerd/char_to_ix.pkl\", \"wb\") as f:\n",
+                "    pickle.dump(char_to_ix, f)\n",
+                "\n",
+                "with open(\"/mnt/efs/models/nerd/unique_characters.pkl\", \"wb\") as f:\n",
+                "    pickle.dump(unique_characters, f)\n",
+                "\n",
+                "with open(\"/mnt/efs/models/nerd/article_vocabulary.pkl\", \"wb\") as f:\n",
+                "    pickle.dump(article_vocabulary, f)\n",
+                "\n",
+                "torch.save(word_vector_embedding_matrix, \"/mnt/efs/models/nerd/embedding_matrix.pt\")"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
             "source": []
         }
     ],
     "metadata": {
         "kernelspec": {
             "display_name": "Python [conda env:pytorch_p36]",
             "language": "python",
```

### Comparing `weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/13 - adding context vectors.ipynb` & `weco-datascience-0.1.9/notebooks/wikipedia/notebooks/13 - adding context vectors.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9962649713202294%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (30, 30)\\n\'), (27, '*

 * *            '\'nltk.download("punkt")\\n\'), (28, \'device = torch.device("cuda" if '*

 * *            'torch.cuda.is_available() else "cpu")\')], delete: [27, 26, 4, 3]}}, 2: {\'source\': '*

 * *            '{insert: [(0, \'base_path = "/mnt/efs/wikipedia/dumps/text/"\\n\'), (3, \'all_text = '*

 * *            '""\\n\'), (6, \'        with open(base_pat […]*

```diff
@@ -5,16 +5,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (30, 30)\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (30, 30)\n",
                 "\n",
                 "import re\n",
                 "import os\n",
                 "import io\n",
                 "import nltk\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
@@ -28,16 +29,16 @@
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader\n",
                 "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
                 "\n",
                 "from infersent import InferSent\n",
                 "\n",
-                "nltk.download('punkt')\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "nltk.download(\"punkt\")\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# data"
@@ -45,36 +46,34 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "base_path = '/mnt/efs/wikipedia/dumps/text/'\n",
+                "base_path = \"/mnt/efs/wikipedia/dumps/text/\"\n",
                 "paths = np.random.choice(os.listdir(base_path), size=1)\n",
                 "\n",
-                "all_text = ''\n",
+                "all_text = \"\"\n",
                 "for path in paths:\n",
                 "    for filename in tqdm(os.listdir(base_path + path)):\n",
-                "        with open(base_path + path + '/' + filename, 'rb') as f:\n",
-                "            all_text += f.read().decode('latin1')\n",
+                "        with open(base_path + path + \"/\" + filename, \"rb\") as f:\n",
+                "            all_text += f.read().decode(\"latin1\")\n",
                 "\n",
-                "pattern = r'(?:<doc.+>)((.|\\s|\\S)*?)(?:<\\/doc>)'\n",
+                "pattern = r\"(?:<doc.+>)((.|\\s|\\S)*?)(?:<\\/doc>)\"\n",
                 "articles = [article[0] for article in re.findall(pattern, all_text)]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "articles = [\n",
-                "    articles[ix] for ix in np.random.choice(range(len(articles)), 10000)\n",
-                "]"
+                "articles = [articles[ix] for ix in np.random.choice(range(len(articles)), 10000)]"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### cleaning pipeline"
@@ -83,45 +82,45 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def tokenize(sentence):\n",
-                "    '''moses tokeniser, matching infersent'''\n",
-                "    seq = ' '.join(word_tokenize(sentence))\n",
+                "    \"\"\"moses tokeniser, matching infersent\"\"\"\n",
+                "    seq = \" \".join(word_tokenize(sentence))\n",
                 "    seq = seq.replace(\" n't \", \"n 't \")\n",
                 "    return seq.split()\n",
                 "\n",
                 "\n",
                 "def label_linkable_tokens(sentence, label_all=True):\n",
-                "    parsed_html = BeautifulSoup(sentence, 'html.parser')\n",
-                "    \n",
-                "    link_text = [link.text for link in parsed_html.find_all('a')]\n",
+                "    parsed_html = BeautifulSoup(sentence, \"html.parser\")\n",
+                "\n",
+                "    link_text = [link.text for link in parsed_html.find_all(\"a\")]\n",
                 "    tokenised_links = [tokenize(link) for link in link_text]\n",
                 "    clean_sentence = parsed_html.text\n",
                 "    tokenised_text = tokenize(clean_sentence)\n",
                 "    target_sequence = np.zeros(len(tokenised_text))\n",
                 "\n",
                 "    for link in tokenised_links:\n",
                 "        start_positions = kmp(tokenised_text, link)\n",
-                "        if label_all:            \n",
+                "        if label_all:\n",
                 "            for pos in start_positions:\n",
                 "                target_sequence[pos : pos + len(link)] = 1\n",
                 "        elif label_all == False and len(start_positions) > 0:\n",
                 "            pos = start_positions[0]\n",
                 "            target_sequence[pos : pos + len(link)] = 1\n",
-                "        else: \n",
+                "        else:\n",
                 "            pass\n",
                 "\n",
                 "    return clean_sentence, tokenised_text, target_sequence\n",
                 "\n",
                 "\n",
                 "def kmp(sequence, sub):\n",
-                "    \"\"\"         \n",
+                "    \"\"\"\n",
                 "    Knuth\u2013Morris\u2013Pratt algorithm, returning the starting position\n",
                 "    of a specified subsequence within another, larger sequence.\n",
                 "    Usually used for string matching.\n",
                 "    \"\"\"\n",
                 "    partial = [0]\n",
                 "    for i in range(1, len(sub)):\n",
                 "        j = partial[i - 1]\n",
@@ -129,16 +128,17 @@
                 "            j = partial[j - 1]\n",
                 "        partial.append(j + 1 if sub[j] == sub[i] else j)\n",
                 "\n",
                 "    positions, j = [], 0\n",
                 "    for i in range(len(sequence)):\n",
                 "        while j > 0 and sequence[i] != sub[j]:\n",
                 "            j = partial[j - 1]\n",
-                "        if sequence[i] == sub[j]: j += 1\n",
-                "        if j == len(sub): \n",
+                "        if sequence[i] == sub[j]:\n",
+                "            j += 1\n",
+                "        if j == len(sub):\n",
                 "            positions.append(i - (j - 1))\n",
                 "            j = 0\n",
                 "\n",
                 "    return positions"
             ]
         },
         {
@@ -148,24 +148,24 @@
             "outputs": [],
             "source": [
                 "sequence_data, article_lengths = [], []\n",
                 "\n",
                 "for i, article in enumerate(tqdm(articles)):\n",
                 "    new_sentences = sent_tokenize(article)\n",
                 "    good_sentence_count = 0\n",
-                "    \n",
+                "\n",
                 "    for sentence in new_sentences:\n",
                 "        try:\n",
-                "            clean, tokenised, targets = label_linkable_tokens(sentence)        \n",
+                "            clean, tokenised, targets = label_linkable_tokens(sentence)\n",
                 "            sequence_data.append([clean, tokenised, targets, i])\n",
                 "            good_sentence_count += 1\n",
                 "        except:\n",
                 "            pass\n",
                 "    article_lengths.append(good_sentence_count)\n",
-                "    \n",
+                "\n",
                 "sentences, token_sequences, target_sequences, article_markers = zip(*sequence_data)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -199,28 +199,24 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "unique_characters = set(' '.join([\n",
-                "    token \n",
-                "    for seq in token_sequences\n",
-                "    for token in seq\n",
-                "]))"
+                "unique_characters = set(\" \".join([token for seq in token_sequences for token in seq]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "special_cases = ['xxunk', 'xxpad', 'xxbos', 'xxeos']\n",
+                "special_cases = [\"xxunk\", \"xxpad\", \"xxbos\", \"xxeos\"]\n",
                 "\n",
                 "for case in special_cases:\n",
                 "    unique_characters.add(case)"
             ]
         },
         {
             "cell_type": "code",
@@ -242,33 +238,31 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "n_wvs = 1000000\n",
-                "wv_path = '/mnt/efs/text/word_vectors/wiki-news-300d-1M.vec'\n",
-                "wv_file = io.open(wv_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
+                "wv_path = \"/mnt/efs/text/word_vectors/wiki-news-300d-1M.vec\"\n",
+                "wv_file = io.open(wv_path, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\")\n",
                 "lines_to_parse = list(wv_file)[1:n_wvs]\n",
                 "\n",
-                "fasttext = {line.split()[0]: np.array(line.split()[1:]).astype(np.float32)\n",
-                "            for line in tqdm(lines_to_parse)}"
+                "fasttext = {\n",
+                "    line.split()[0]: np.array(line.split()[1:]).astype(np.float32)\n",
+                "    for line in tqdm(lines_to_parse)\n",
+                "}"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "all_tokens = [\n",
-                "    tok \n",
-                "    for seq in token_sequences \n",
-                "    for tok in seq\n",
-                "]\n",
+                "all_tokens = [tok for seq in token_sequences for tok in seq]\n",
                 "\n",
                 "article_vocabulary, _ = zip(*Counter(all_tokens).most_common(n_wvs))\n",
                 "article_vocabulary = set(article_vocabulary)"
             ]
         },
         {
             "cell_type": "code",
@@ -303,20 +297,20 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "word_vector_embedding_matrix = torch.FloatTensor([\n",
-                "    fasttext[token]\n",
-                "    if token in fasttext\n",
-                "    else fasttext['xxunk']\n",
-                "    for token in article_vocabulary\n",
-                "])"
+                "word_vector_embedding_matrix = torch.FloatTensor(\n",
+                "    [\n",
+                "        fasttext[token] if token in fasttext else fasttext[\"xxunk\"]\n",
+                "        for token in article_vocabulary\n",
+                "    ]\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# infersent embeddings"
@@ -324,29 +318,35 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "model_path = '/mnt/efs/text/infersent/infersent2.pkl'\n",
-                "model_params = {'bsize': 32, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
-                "                'pool_type': 'max', 'dpout_model': 0.0, 'version': 2}\n",
+                "model_path = \"/mnt/efs/text/infersent/infersent2.pkl\"\n",
+                "model_params = {\n",
+                "    \"bsize\": 32,\n",
+                "    \"word_emb_dim\": 300,\n",
+                "    \"enc_lstm_dim\": 2048,\n",
+                "    \"pool_type\": \"max\",\n",
+                "    \"dpout_model\": 0.0,\n",
+                "    \"version\": 2,\n",
+                "}\n",
                 "\n",
                 "infersent = InferSent(model_params).to(device)\n",
                 "infersent.load_state_dict(torch.load(model_path))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "word_vector_path = '/mnt/efs/text/word_vectors/crawl-300d-2M.vec'\n",
+                "word_vector_path = \"/mnt/efs/text/word_vectors/crawl-300d-2M.vec\"\n",
                 "infersent.set_w2v_path(word_vector_path)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -357,17 +357,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "sentence_vectors = torch.Tensor(\n",
-                "    infersent.encode(sentences, tokenize=True)\n",
-                ")"
+                "sentence_vectors = torch.Tensor(infersent.encode(sentences, tokenize=True))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -401,87 +399,89 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class SentenceDataset(Dataset):\n",
-                "    def __init__(self, token_seqs, target_seqs, article_markers,\n",
-                "                 sent_vectors=sentence_vectors, article_vectors=article_vectors):\n",
+                "    def __init__(\n",
+                "        self,\n",
+                "        token_seqs,\n",
+                "        target_seqs,\n",
+                "        article_markers,\n",
+                "        sent_vectors=sentence_vectors,\n",
+                "        article_vectors=article_vectors,\n",
+                "    ):\n",
                 "        self.sent_vectors = sent_vectors\n",
                 "        self.article_vectors = article_vectors\n",
-                "        \n",
-                "        #impose length constraint\n",
+                "\n",
+                "        # impose length constraint\n",
                 "        where_big_enough = np.where([len(seq) > 3 for seq in token_seqs])\n",
                 "        self.token_seqs = np.array(token_seqs)[where_big_enough]\n",
                 "        self.target_seqs = np.array(target_seqs)[where_big_enough]\n",
                 "        self.article_markers = np.array(article_markers)[where_big_enough]\n",
-                "        \n",
-                "        #indexify\n",
-                "        self.char_ix_seqs = [\n",
-                "            self.indexify_chars(seq) for seq in self.token_seqs\n",
-                "        ]\n",
-                "\n",
-                "        self.token_seqs = [\n",
-                "            self.indexify_tokens(seq) for seq in self.token_seqs\n",
-                "        ]\n",
-                "        \n",
-                "        #find exit points for character level sequences\n",
-                "        self.exit_ix_seqs = [\n",
-                "            self.find_exit_points(seq) for seq in self.char_ix_seqs\n",
-                "        ]\n",
-                "        \n",
+                "\n",
+                "        # indexify\n",
+                "        self.char_ix_seqs = [self.indexify_chars(seq) for seq in self.token_seqs]\n",
+                "\n",
+                "        self.token_seqs = [self.indexify_tokens(seq) for seq in self.token_seqs]\n",
+                "\n",
+                "        # find exit points for character level sequences\n",
+                "        self.exit_ix_seqs = [self.find_exit_points(seq) for seq in self.char_ix_seqs]\n",
+                "\n",
                 "        self.max_ix = len(sent_vectors) - 1\n",
-                "        \n",
+                "\n",
                 "    def __getitem__(self, ix):\n",
                 "        char_ix_seq = self.char_ix_seqs[ix]\n",
                 "        token_seq = self.token_seqs[ix]\n",
                 "        exit_ix_seq = self.exit_ix_seqs[ix]\n",
                 "        target_seq = self.target_seqs[ix]\n",
                 "        context_vector = self.get_context_vector(ix)\n",
                 "        return char_ix_seq, token_seq, exit_ix_seq, target_seq, context_vector\n",
                 "\n",
                 "    def __len__(self):\n",
                 "        return len(self.token_seqs)\n",
-                "    \n",
+                "\n",
                 "    def indexify_tokens(self, token_seq):\n",
                 "        ix_seq = np.array(\n",
-                "            [token_to_ix[token]\n",
-                "             if token in article_vocabulary\n",
-                "             else token_to_ix['xxunk']\n",
-                "             for token in token_seq]\n",
+                "            [\n",
+                "                token_to_ix[token]\n",
+                "                if token in article_vocabulary\n",
+                "                else token_to_ix[\"xxunk\"]\n",
+                "                for token in token_seq\n",
+                "            ]\n",
                 "        )\n",
                 "        return torch.LongTensor(ix_seq)\n",
-                "    \n",
+                "\n",
                 "    def indexify_chars(self, token_seq):\n",
                 "        ix_seq = np.array(\n",
-                "            [char_to_ix['xxbos'], char_to_ix[' ']] +\n",
-                "            [char_to_ix[char] for char in ' '.join(token_seq)] + \n",
-                "            [char_to_ix[' '], char_to_ix['xxeos']]\n",
+                "            [char_to_ix[\"xxbos\"], char_to_ix[\" \"]]\n",
+                "            + [char_to_ix[char] for char in \" \".join(token_seq)]\n",
+                "            + [char_to_ix[\" \"], char_to_ix[\"xxeos\"]]\n",
                 "        )\n",
                 "        return torch.LongTensor(ix_seq)\n",
-                "    \n",
+                "\n",
                 "    def find_exit_points(self, char_ix_seq):\n",
-                "        binary = (char_ix_seq == char_to_ix[' '])\n",
+                "        binary = char_ix_seq == char_to_ix[\" \"]\n",
                 "        return binary.nonzero().squeeze()\n",
-                "    \n",
+                "\n",
                 "    def get_context_vector(self, ix):\n",
                 "        if ix in [0, 1, 2]:\n",
                 "            previous_1 = torch.rand(4096)\n",
                 "            previous_2 = torch.rand(4096)\n",
                 "            following = self.sent_vectors[ix + 1]\n",
                 "        elif ix == self.max_ix:\n",
                 "            previous_1 = self.sent_vectors[ix - 1]\n",
                 "            previous_2 = self.sent_vectors[ix - 2]\n",
                 "            following = torch.rand(14096)\n",
                 "        else:\n",
                 "            previous_1 = self.sent_vectors[ix - 1]\n",
                 "            previous_2 = self.sent_vectors[ix - 2]\n",
                 "            following = self.sent_vectors[ix + 1]\n",
-                "            \n",
+                "\n",
                 "        article_vector = self.article_vectors[self.article_markers[ix]]\n",
                 "        previous = torch.max(torch.stack([previous_1, previous_2]), 0)[0]\n",
                 "        context_vector = torch.cat([article_vector, previous, following])\n",
                 "        return context_vector"
             ]
         },
         {
@@ -489,52 +489,41 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def collate_fn(batch):\n",
                 "    char_ix_seqs, token_seqs, exit_ix_seqs, target_seqs, context_vectors = zip(*batch)\n",
                 "\n",
-                "    char_seq_lens = torch.LongTensor([\n",
-                "        len(char_seq) for char_seq in char_ix_seqs\n",
-                "    ])\n",
-                "    \n",
-                "    sorted_char_lengths, sort_indicies = char_seq_lens.sort(\n",
-                "        dim=0,\n",
-                "        descending=True\n",
-                "    )\n",
+                "    char_seq_lens = torch.LongTensor([len(char_seq) for char_seq in char_ix_seqs])\n",
+                "\n",
+                "    sorted_char_lengths, sort_indicies = char_seq_lens.sort(dim=0, descending=True)\n",
                 "\n",
                 "    sorted_char_seqs = [char_ix_seqs[i] for i in sort_indicies]\n",
                 "    sorted_token_seqs = [token_seqs[i] for i in sort_indicies]\n",
                 "    sorted_exit_seqs = [exit_ix_seqs[i] for i in sort_indicies]\n",
                 "    sorted_context_vectors = torch.stack([context_vectors[i] for i in sort_indicies])\n",
                 "    sorted_target_seqs = [torch.LongTensor(target_seqs[i]) for i in sort_indicies]\n",
                 "    sorted_token_lengths = torch.LongTensor([len(seq) for seq in sorted_token_seqs])\n",
-                "    \n",
+                "\n",
                 "    padded_char_seqs = pad_sequence(\n",
-                "        sequences=sorted_char_seqs,\n",
-                "        padding_value=char_to_ix['xxpad'],\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_char_seqs, padding_value=char_to_ix[\"xxpad\"], batch_first=True\n",
                 "    )\n",
-                "    \n",
+                "\n",
                 "    padded_token_seqs = pad_sequence(\n",
                 "        sequences=sorted_token_seqs,\n",
-                "        padding_value=token_to_ix['xxpad'],\n",
-                "        batch_first=True\n",
+                "        padding_value=token_to_ix[\"xxpad\"],\n",
+                "        batch_first=True,\n",
                 "    )\n",
-                "    \n",
+                "\n",
                 "    padded_exit_seqs = pad_sequence(\n",
-                "        sequences=sorted_exit_seqs,\n",
-                "        padding_value=0,\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_exit_seqs, padding_value=0, batch_first=True\n",
                 "    )\n",
-                "    \n",
+                "\n",
                 "    padded_target_seqs = pad_sequence(\n",
-                "        sequences=sorted_target_seqs,\n",
-                "        padding_value=0,\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_target_seqs, padding_value=0, batch_first=True\n",
                 "    )\n",
                 "\n",
                 "    return (\n",
                 "        padded_char_seqs,\n",
                 "        padded_token_seqs,\n",
                 "        padded_exit_seqs,\n",
                 "        sorted_context_vectors,\n",
@@ -546,64 +535,57 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "(train_tokens, test_tokens, \n",
-                " train_targets, test_targets,\n",
-                " train_markers, test_markers) = train_test_split(\n",
-                "    token_sequences, \n",
-                "    target_sequences,\n",
-                "    article_markers,\n",
-                "    test_size=0.20, \n",
-                "    random_state=42\n",
+                "(\n",
+                "    train_tokens,\n",
+                "    test_tokens,\n",
+                "    train_targets,\n",
+                "    test_targets,\n",
+                "    train_markers,\n",
+                "    test_markers,\n",
+                ") = train_test_split(\n",
+                "    token_sequences, target_sequences, article_markers, test_size=0.20, random_state=42\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train_dataset = SentenceDataset(\n",
-                "    train_tokens,\n",
-                "    train_targets,\n",
-                "    train_markers\n",
-                ")\n",
+                "train_dataset = SentenceDataset(train_tokens, train_targets, train_markers)\n",
                 "\n",
                 "train_loader = DataLoader(\n",
-                "    dataset=train_dataset,  \n",
+                "    dataset=train_dataset,\n",
                 "    batch_size=64,\n",
                 "    num_workers=5,\n",
                 "    shuffle=True,\n",
-                "    collate_fn=collate_fn\n",
+                "    collate_fn=collate_fn,\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "test_dataset = SentenceDataset(\n",
-                "    test_tokens,\n",
-                "    test_targets,\n",
-                "    test_markers\n",
-                ")\n",
+                "test_dataset = SentenceDataset(test_tokens, test_targets, test_markers)\n",
                 "\n",
                 "test_loader = DataLoader(\n",
-                "    dataset=test_dataset,  \n",
+                "    dataset=test_dataset,\n",
                 "    batch_size=1,\n",
                 "    num_workers=5,\n",
                 "    shuffle=True,\n",
-                "    collate_fn=collate_fn\n",
+                "    collate_fn=collate_fn,\n",
                 ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -633,15 +615,15 @@
                 "        self.head_fwd = nn.Sequential(\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
                 "            nn.ReLU(),\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim // 2, output_dim),\n",
                 "        )\n",
-                "        \n",
+                "\n",
                 "        self.head_bwd = nn.Sequential(\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
                 "            nn.ReLU(),\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim // 2, output_dim),\n",
                 "        )\n",
@@ -649,23 +631,23 @@
                 "    def forward(self, char_seqs, exit_seqs, lengths):\n",
                 "        x = self.embedding(char_seqs)\n",
                 "\n",
                 "        x = pack_padded_sequence(x, lengths=lengths, batch_first=True)\n",
                 "\n",
                 "        x, _ = self.char_level_lstm(x)\n",
                 "        out, _ = pad_packed_sequence(x, batch_first=True)\n",
-                "        \n",
+                "\n",
                 "        # pop out the character embeddings at position of the end of each token\n",
                 "        out = torch.stack([out[i, exit_seqs[i]] for i in range(len(out))])\n",
-                "        \n",
+                "\n",
                 "        out_fwd, out_bwd = torch.chunk(out, 2, 2)\n",
                 "\n",
                 "        pred_fwd = self.head_fwd(out_fwd[:, 1:])\n",
                 "        pred_bwd = self.head_bwd(out_bwd[:, :-1])\n",
-                "        \n",
+                "\n",
                 "        return pred_fwd, pred_bwd"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -677,80 +659,65 @@
                 "        self.wv_embedding = nn.Embedding.from_pretrained(word_vector_embedding_matrix)\n",
                 "        self.distil_dim = 512\n",
                 "\n",
                 "        self.cln = CharacterLevelNetwork(\n",
                 "            input_dim=len(unique_characters),\n",
                 "            embedding_dim=30,\n",
                 "            hidden_dim=256,\n",
-                "            output_dim=30\n",
+                "            output_dim=30,\n",
                 "        )\n",
-                "        \n",
-                "        self.lstm_input_size = (\n",
-                "            word_vector_embedding_matrix.shape[1] + \n",
-                "            (self.cln.output_dim * 2)\n",
+                "\n",
+                "        self.lstm_input_size = word_vector_embedding_matrix.shape[1] + (\n",
+                "            self.cln.output_dim * 2\n",
                 "        )\n",
-                "        \n",
+                "\n",
                 "        self.word_level_lstm = nn.LSTM(\n",
                 "            input_size=self.lstm_input_size,\n",
                 "            hidden_size=hidden_dim,\n",
                 "            num_layers=1,\n",
                 "            bidirectional=True,\n",
-                "            #dropout=0.2\n",
+                "            # dropout=0.2\n",
                 "        )\n",
-                "        \n",
+                "\n",
                 "        self.distill = nn.Sequential(\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(4096 * 3, 2048),\n",
                 "            nn.ReLU(),\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(2048, self.distil_dim),\n",
                 "        )\n",
-                "        \n",
+                "\n",
                 "        self.head = nn.Sequential(\n",
                 "            nn.Dropout(0.3),\n",
-                "            nn.Linear(\n",
-                "                hidden_dim * 2 + self.distil_dim, \n",
-                "                hidden_dim // 16\n",
-                "            ),\n",
+                "            nn.Linear(hidden_dim * 2 + self.distil_dim, hidden_dim // 16),\n",
                 "            nn.ReLU(),\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim // 16, 2),\n",
                 "        )\n",
-                "    \n",
-                "    def forward(self, char_seqs, token_seqs, exit_seqs, context, c_lens, t_lens):        \n",
+                "\n",
+                "    def forward(self, char_seqs, token_seqs, exit_seqs, context, c_lens, t_lens):\n",
                 "        wv_seqs = self.wv_embedding(token_seqs)\n",
                 "        char_fwd, char_bwd = self.cln(char_seqs, exit_seqs, c_lens)\n",
-                "        \n",
-                "        concats = torch.cat(\n",
-                "            [char_fwd, char_bwd, wv_seqs], \n",
-                "            dim=2\n",
-                "        )\n",
-                "        \n",
-                "        sorted_lengths, sort_indicies = t_lens.sort(\n",
-                "            dim=0,\n",
-                "            descending=True\n",
-                "        )\n",
-                "        \n",
+                "\n",
+                "        concats = torch.cat([char_fwd, char_bwd, wv_seqs], dim=2)\n",
+                "\n",
+                "        sorted_lengths, sort_indicies = t_lens.sort(dim=0, descending=True)\n",
+                "\n",
                 "        concats = torch.stack([concats[i] for i in sort_indicies])\n",
-                "        \n",
-                "        packed = pack_padded_sequence(\n",
-                "            concats,\n",
-                "            lengths=sorted_lengths,\n",
-                "            batch_first=True\n",
-                "        )\n",
-                "        \n",
+                "\n",
+                "        packed = pack_padded_sequence(concats, lengths=sorted_lengths, batch_first=True)\n",
+                "\n",
                 "        embedded, _ = self.word_level_lstm(concats)\n",
-                "        \n",
+                "\n",
                 "        distilled_context = torch.cat(\n",
-                "            [self.distill(context).unsqueeze(1)] * embedded.shape[1], \n",
-                "            dim=1\n",
+                "            [self.distill(context).unsqueeze(1)] * embedded.shape[1], dim=1\n",
                 "        )\n",
-                "        \n",
+                "\n",
                 "        with_context = torch.cat([distilled_context, embedded], dim=2)\n",
-                "        output = self.head(with_context).permute(0,2,1)\n",
+                "        output = self.head(with_context).permute(0, 2, 1)\n",
                 "        return output, sort_indicies"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -818,50 +785,52 @@
                 "            exit_seqs = torch.LongTensor(exit_seqs).cuda(non_blocking=True)\n",
                 "            context = torch.Tensor(context).cuda(non_blocking=True)\n",
                 "            c_lens = torch.LongTensor(c_lens).cuda(non_blocking=True)\n",
                 "            t_lens = torch.LongTensor(t_lens).cuda(non_blocking=True)\n",
                 "            targets = torch.LongTensor(targets).cuda(non_blocking=True)\n",
                 "\n",
                 "            optimiser.zero_grad()\n",
-                "            preds, sort_indicies = model(c_seqs, t_seqs, exit_seqs, context, c_lens, t_lens)\n",
-                "            \n",
+                "            preds, sort_indicies = model(\n",
+                "                c_seqs, t_seqs, exit_seqs, context, c_lens, t_lens\n",
+                "            )\n",
+                "\n",
                 "            targets = torch.stack([targets[i] for i in sort_indicies])\n",
-                "            \n",
+                "\n",
                 "            loss = loss_function(preds, targets)\n",
                 "            loss.backward()\n",
                 "            optimiser.step()\n",
                 "\n",
                 "            losses.append(loss.item())\n",
-                "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
+                "            loop.set_description(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
                 "            loop.set_postfix(loss=np.mean(losses[-100:]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "train(\n",
                 "    model=model,\n",
                 "    train_loader=train_loader,\n",
                 "    loss_function=loss_function,\n",
                 "    optimiser=optimiser,\n",
-                "    n_epochs=8\n",
+                "    n_epochs=8,\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "loss_data = pd.Series(losses[20:]).rolling(window=500).mean()\n",
-                "ax = loss_data.plot();\n",
+                "ax = loss_data.plot()\n",
                 "ax.set_ylim(0.12, 0.26);"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -871,69 +840,77 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def format_output(tokens, targets, preds):\n",
-                "    target_string, pred_string = '', ''\n",
+                "    target_string, pred_string = \"\", \"\"\n",
                 "\n",
                 "    for token_id, target, pred in zip(tokens, targets, preds):\n",
                 "        token = tokenizer.convert_ids_to_tokens([token_id.item()])[0]\n",
                 "\n",
-                "        if target == 1: \n",
-                "            target_string += '<b>' + token + '</b> '\n",
-                "        else: \n",
-                "            target_string += token + ' '\n",
+                "        if target == 1:\n",
+                "            target_string += \"<b>\" + token + \"</b> \"\n",
+                "        else:\n",
+                "            target_string += token + \" \"\n",
                 "\n",
-                "        if pred == 1: pred_string += '<b>' + token + '</b> '\n",
-                "        else: pred_string += token + ' '\n",
+                "        if pred == 1:\n",
+                "            pred_string += \"<b>\" + token + \"</b> \"\n",
+                "        else:\n",
+                "            pred_string += token + \" \"\n",
                 "\n",
                 "    output_string = (\n",
-                "        'PRED:<br>' + \n",
-                "        clean(pred_string) +\n",
-                "        '<br><br>TARG:<br>' +\n",
-                "        clean(target_string) +\n",
-                "        '<br><br>--------<br><br>'\n",
+                "        \"PRED:<br>\"\n",
+                "        + clean(pred_string)\n",
+                "        + \"<br><br>TARG:<br>\"\n",
+                "        + clean(target_string)\n",
+                "        + \"<br><br>--------<br><br>\"\n",
                 "    )\n",
-                "    \n",
+                "\n",
                 "    return output_string"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "output = ''\n",
+                "output = \"\"\n",
                 "\n",
-                "for i, (c_seqs, t_seqs, exit_seqs, context, c_lens, t_lens, targets) in enumerate(test_loader):\n",
+                "for i, (c_seqs, t_seqs, exit_seqs, context, c_lens, t_lens, targets) in enumerate(\n",
+                "    test_loader\n",
+                "):\n",
                 "    if i < 10:\n",
                 "        try:\n",
                 "            c_seqs = torch.LongTensor(c_seqs).cuda()\n",
                 "            t_seqs = torch.LongTensor(t_seqs).cuda()\n",
                 "            exit_seqs = torch.LongTensor(exit_seqs).cuda()\n",
                 "            context = torch.Tensor(context).cuda()\n",
                 "            c_lens = torch.LongTensor(c_lens).cuda()\n",
                 "            t_lens = torch.LongTensor(t_lens).cuda()\n",
                 "            targets = torch.LongTensor(targets).cuda()\n",
                 "\n",
                 "            optimiser.zero_grad()\n",
-                "            preds, sort_indicies = model(c_seqs, t_seqs, exit_seqs, context, c_lens, t_lens)\n",
+                "            preds, sort_indicies = model(\n",
+                "                c_seqs, t_seqs, exit_seqs, context, c_lens, t_lens\n",
+                "            )\n",
                 "            preds = nn.LogSoftmax(dim=1)(preds).argmax(dim=1)\n",
                 "\n",
                 "            targets = torch.stack([targets[i] for i in sort_indicies])\n",
                 "\n",
                 "            target_string = []\n",
                 "            pred_string = []\n",
                 "\n",
                 "            output += format_output(tokens[0], targets[0], preds)\n",
-                "        except: pass\n",
-                "    else: break\n",
+                "        except:\n",
+                "            pass\n",
+                "    else:\n",
+                "        break\n",
                 "\n",
                 "display(HTML(output))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -943,26 +920,22 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "test_dataset = SentenceDataset(\n",
-                "    test_tokens,\n",
-                "    test_targets,\n",
-                "    test_markers\n",
-                ")\n",
+                "test_dataset = SentenceDataset(test_tokens, test_targets, test_markers)\n",
                 "\n",
                 "test_loader = DataLoader(\n",
-                "    dataset=test_dataset,  \n",
+                "    dataset=test_dataset,\n",
                 "    batch_size=1,\n",
                 "    num_workers=5,\n",
                 "    shuffle=True,\n",
-                "    collate_fn=collate_fn\n",
+                "    collate_fn=collate_fn,\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -993,15 +966,15 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "cpu_preds = preds.squeeze().detach().cpu().numpy()[1]\n",
-                "pd.Series(cpu_preds).plot.bar(color='#606060');"
+                "pd.Series(cpu_preds).plot.bar(color=\"#606060\");"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -1022,27 +995,31 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def to_hex_colour(val):\n",
-                "    return '#%02x%02x%02x' % (val, 0, 0)"
+                "    return \"#%02x%02x%02x\" % (val, 0, 0)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "coloured_text = ' '.join(['<b>'] + [\n",
-                "    f'<font color=\"{to_hex_colour(val)}\">{word}</font>' \n",
-                "    for word, val in list(zip(words, vals))\n",
-                "] + ['<b>'])"
+                "coloured_text = \" \".join(\n",
+                "    [\"<b>\"]\n",
+                "    + [\n",
+                "        f'<font color=\"{to_hex_colour(val)}\">{word}</font>'\n",
+                "        for word, val in list(zip(words, vals))\n",
+                "    ]\n",
+                "    + [\"<b>\"]\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -1071,26 +1048,22 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "test_dataset = SentenceDataset(\n",
-                "    test_tokens,\n",
-                "    test_targets,\n",
-                "    test_markers\n",
-                ")\n",
+                "test_dataset = SentenceDataset(test_tokens, test_targets, test_markers)\n",
                 "\n",
                 "test_loader = DataLoader(\n",
-                "    dataset=test_dataset,  \n",
+                "    dataset=test_dataset,\n",
                 "    batch_size=128,\n",
                 "    num_workers=5,\n",
                 "    shuffle=True,\n",
-                "    collate_fn=collate_fn\n",
+                "    collate_fn=collate_fn,\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -1122,18 +1095,18 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "import pickle\n",
                 "\n",
                 "for i in tqdm(range(len(predictions))):\n",
-                "    torch.save(predictions[i], f'/mnt/efs/disambiguation_data/predictions/{i}.pt')\n",
-                "    torch.save(tokens[i], f'/mnt/efs/disambiguation_data/tokens/{i}.pt')\n",
-                "    \n",
-                "with open('/mnt/efs/disambiguation_data/ix_to_token.pkl', 'wb') as f:\n",
+                "    torch.save(predictions[i], f\"/mnt/efs/disambiguation_data/predictions/{i}.pt\")\n",
+                "    torch.save(tokens[i], f\"/mnt/efs/disambiguation_data/tokens/{i}.pt\")\n",
+                "\n",
+                "with open(\"/mnt/efs/disambiguation_data/ix_to_token.pkl\", \"wb\") as f:\n",
                 "    pickle.dump(ix_to_token, f)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
```

### Comparing `weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/14 - disambiguating.ipynb` & `weco-datascience-0.1.9/notebooks/wikipedia/notebooks/14 - disambiguating.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9969606717687075%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (30, 30)\\n\')], delete: [4, 3]}}, 2: {\'source\': '*

 * *            '{insert: [(0, \'with open("/mnt/efs/disambiguation_data/ix_to_token.pkl", "rb") as '*

 * *            "f:\\n'), (2, '\\n')], delete: [2, 0]}}, 3: {'source': {insert: [(1, 'base_path = "*

 * *            '"/mnt/efs/disambiguation_data/{}/{}.pt"\\n\'), (3, \'tokens = '*

 * *            'torch.load(base_path.format […]*

```diff
@@ -5,16 +5,17 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (30, 30)\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (30, 30)\n",
                 "\n",
                 "import wikipedia\n",
                 "import torch\n",
                 "from torch import nn\n",
                 "import pickle\n",
                 "import pandas as pd\n",
                 "import numpy as np\n",
@@ -35,31 +36,33 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "with open('/mnt/efs/disambiguation_data/ix_to_token.pkl', 'rb') as f:\n",
+                "with open(\"/mnt/efs/disambiguation_data/ix_to_token.pkl\", \"rb\") as f:\n",
                 "    ix_to_token = pickle.load(f)\n",
-                "    \n",
+                "\n",
                 "token_to_ix = {token: ix for ix, token in ix_to_token.items()}"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "ix = np.random.choice(100)\n",
-                "base_path = '/mnt/efs/disambiguation_data/{}/{}.pt'\n",
+                "base_path = \"/mnt/efs/disambiguation_data/{}/{}.pt\"\n",
                 "\n",
-                "tokens = torch.load(base_path.format('tokens', ix), map_location='cpu').numpy()\n",
-                "predictions = torch.load(base_path.format('predictions', ix), map_location='cpu').numpy()"
+                "tokens = torch.load(base_path.format(\"tokens\", ix), map_location=\"cpu\").numpy()\n",
+                "predictions = torch.load(\n",
+                "    base_path.format(\"predictions\", ix), map_location=\"cpu\"\n",
+                ").numpy()"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# do science"
@@ -77,16 +80,16 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "token_seq = [ix_to_token[ix] for ix in tokens[index] if ix != token_to_ix['xxpad']]\n",
-                "' '.join(token_seq)"
+                "token_seq = [ix_to_token[ix] for ix in tokens[index] if ix != token_to_ix[\"xxpad\"]]\n",
+                "\" \".join(token_seq)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -96,15 +99,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "#pd.Series(val_seq).plot.bar();"
+                "# pd.Series(val_seq).plot.bar();"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -129,79 +132,77 @@
             "outputs": [],
             "source": [
                 "standard_threshold = 0\n",
                 "lower_threshold = -0.5\n",
                 "\n",
                 "for i in range(len(val_seq)):\n",
                 "    p, v, n = prev_seq[i], val_seq[i], next_seq[i]\n",
-                "    to_include = (\n",
-                "        (v > standard_threshold) or \n",
-                "        ((v > lower_threshold) and \n",
-                "         ((p > standard_threshold) or\n",
-                "          (n > standard_threshold)))\n",
+                "    to_include = (v > standard_threshold) or (\n",
+                "        (v > lower_threshold) and ((p > standard_threshold) or (n > standard_threshold))\n",
                 "    )\n",
-                "    #print('\\t'.join([str(round(p, 2)), \n",
-                "    #                 str(round(v, 2)), \n",
-                "    #                 str(round(n, 2)), \n",
+                "    # print('\\t'.join([str(round(p, 2)),\n",
+                "    #                 str(round(v, 2)),\n",
+                "    #                 str(round(n, 2)),\n",
                 "    #                 str(to_include)]))\n",
-                "    \n",
-                "    indexes_to_include.append(to_include)\n",
-                "    "
+                "\n",
+                "    indexes_to_include.append(to_include)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "groups = [list(group) for group in mit.consecutive_groups(np.where(indexes_to_include)[0])]\n",
+                "groups = [\n",
+                "    list(group) for group in mit.consecutive_groups(np.where(indexes_to_include)[0])\n",
+                "]\n",
                 "bool_groups = [list(group) for _, group in groupby(indexes_to_include)]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "links = []\n",
                 "\n",
                 "for group in groups:\n",
                 "    try:\n",
-                "        text = ' '.join([token_seq[g] for g in group])\n",
+                "        text = \" \".join([token_seq[g] for g in group])\n",
                 "        url = wikipedia.page(text).url\n",
                 "        links.append(f'<a href=\"{url}\">{text}</a>')\n",
                 "\n",
                 "    except:\n",
-                "        links.append(' '.join([token_seq[g] for g in group]))"
+                "        links.append(\" \".join([token_seq[g] for g in group]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "new_token_seq = deepcopy(token_seq)\n",
                 "\n",
                 "for i, group in enumerate(groups):\n",
                 "    new_token_seq[group[0]] = links[i]\n",
                 "    if len(group) > 1:\n",
                 "        for g in group[1:]:\n",
-                "            new_token_seq[g] = ''"
+                "            new_token_seq[g] = \"\""
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "display(HTML(' '.join(new_token_seq)))"
+                "display(HTML(\" \".join(new_token_seq)))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
```

### Comparing `weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/final network.ipynb` & `weco-datascience-0.1.9/notebooks/wikipedia/notebooks/12 - concatenating word- and character-level embeddings.ipynb`

 * *Files 13% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9842888248968757%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'\\n\'), (4, \'sns.set_style("whitegrid")\\n\'), (5, '*

 * *            '\'plt.rcParams["figure.figsize"] = (20, 15)\\n\'), (24, '*

 * *            '\'nltk.download("punkt")\\n\'), (25, \'device = torch.device("cuda" if '*

 * *            'torch.cuda.is_available() else "cpu")\')], delete: [25, 24, 10, 4, 3]}}, 2: '*

 * *            '{\'source\': [\'base_path = "/mnt/efs/wikipedia/dumps/text/"\\n\', \'paths = '*

 * *            'np.random.choice(os.listdir(base_path), size=2)\\n\', \'\\n […]*

```diff
@@ -5,37 +5,37 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "%matplotlib inline\n",
                 "import matplotlib.pyplot as plt\n",
                 "import seaborn as sns\n",
-                "sns.set_style('whitegrid')\n",
-                "plt.rcParams['figure.figsize'] = (20, 15)\n",
+                "\n",
+                "sns.set_style(\"whitegrid\")\n",
+                "plt.rcParams[\"figure.figsize\"] = (20, 15)\n",
                 "\n",
                 "import re\n",
                 "import os\n",
                 "import io\n",
                 "import nltk\n",
-                "import pickle\n",
                 "import numpy as np\n",
                 "import pandas as pd\n",
                 "from bs4 import BeautifulSoup\n",
                 "from tqdm import tqdm_notebook as tqdm\n",
                 "from nltk import word_tokenize, sent_tokenize\n",
                 "from sklearn.model_selection import train_test_split\n",
                 "from IPython.core.display import display, HTML\n",
                 "\n",
                 "import torch\n",
                 "from torch import nn, optim\n",
                 "from torch.utils.data import Dataset, DataLoader\n",
                 "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
                 "\n",
-                "nltk.download('punkt')\n",
-                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
+                "nltk.download(\"punkt\")\n",
+                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# data"
@@ -43,38 +43,25 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "def remove_title(article):\n",
-                "    return '\\n\\n'.join(article.split('\\n\\n')[1:])"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": null,
-            "metadata": {},
-            "outputs": [],
-            "source": [
-                "base_path = '/mnt/efs/wikipedia/dumps/text/'\n",
+                "base_path = \"/mnt/efs/wikipedia/dumps/text/\"\n",
                 "paths = np.random.choice(os.listdir(base_path), size=2)\n",
                 "\n",
-                "all_text = ''\n",
+                "all_text = \"\"\n",
                 "for path in paths:\n",
                 "    for filename in tqdm(os.listdir(base_path + path)):\n",
-                "        with open(base_path + path + '/' + filename, 'rb') as f:\n",
-                "            all_text += f.read().decode('latin1')\n",
+                "        with open(base_path + path + \"/\" + filename, \"rb\") as f:\n",
+                "            all_text += f.read().decode(\"latin1\")\n",
                 "\n",
-                "pattern = r'(?:<doc.+>)((.|\\s|\\S)*?)(?:<\\/doc>)'\n",
-                "articles = [\n",
-                "    remove_title(article[0])\n",
-                "    for article in re.findall(pattern, all_text)\n",
-                "]"
+                "pattern = r\"(?:<doc.+>)((.|\\s|\\S)*?)(?:<\\/doc>)\"\n",
+                "articles = [article[0] for article in re.findall(pattern, all_text)]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -90,24 +77,53 @@
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
+            "source": []
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
             "source": [
                 "def tokenize(sentence):\n",
-                "    '''moses tokeniser'''\n",
-                "    seq = ' '.join(word_tokenize(sentence))\n",
+                "    \"\"\"moses tokeniser\"\"\"\n",
+                "    seq = \" \".join(word_tokenize(sentence))\n",
                 "    seq = seq.replace(\" n't \", \"n 't \")\n",
                 "    return seq.split()\n",
                 "\n",
                 "\n",
+                "def label_linkable_tokens(sentence, label_all=True):\n",
+                "    parsed_html = BeautifulSoup(sentence, \"html.parser\")\n",
+                "\n",
+                "    link_text = [link.text for link in parsed_html.find_all(\"a\")]\n",
+                "    tokenised_links = [tokenize(link) for link in link_text]\n",
+                "    tokenised_text = tokenize(parsed_html.text)\n",
+                "    target_sequence = np.zeros(len(tokenised_text))\n",
+                "\n",
+                "    for link in tokenised_links:\n",
+                "        start_positions = kmp(tokenised_text, link)\n",
+                "        if label_all:\n",
+                "            for pos in start_positions:\n",
+                "                target_sequence[pos : pos + len(link)] = 1\n",
+                "        elif label_all == False and len(start_positions) > 0:\n",
+                "            pos = start_positions[0]\n",
+                "            target_sequence[pos : pos + len(link)] = 1\n",
+                "        else:\n",
+                "            pass\n",
+                "\n",
+                "    return tokenised_text, target_sequence\n",
+                "\n",
+                "\n",
                 "def kmp(sequence, sub):\n",
-                "    \"\"\"         \n",
+                "    \"\"\"\n",
                 "    Knuth\u2013Morris\u2013Pratt algorithm, returning the starting position\n",
                 "    of a specified subsequence within another, larger sequence.\n",
                 "    Usually used for string matching.\n",
                 "    \"\"\"\n",
                 "    partial = [0]\n",
                 "    for i in range(1, len(sub)):\n",
                 "        j = partial[i - 1]\n",
@@ -115,71 +131,48 @@
                 "            j = partial[j - 1]\n",
                 "        partial.append(j + 1 if sub[j] == sub[i] else j)\n",
                 "\n",
                 "    positions, j = [], 0\n",
                 "    for i in range(len(sequence)):\n",
                 "        while j > 0 and sequence[i] != sub[j]:\n",
                 "            j = partial[j - 1]\n",
-                "        if sequence[i] == sub[j]: j += 1\n",
-                "        if j == len(sub): \n",
+                "        if sequence[i] == sub[j]:\n",
+                "            j += 1\n",
+                "        if j == len(sub):\n",
                 "            positions.append(i - (j - 1))\n",
                 "            j = 0\n",
                 "\n",
-                "    return positions\n",
-                "\n",
-                "\n",
-                "def label(tokenised_sequences, link_tokens):\n",
-                "    target_sequences = []\n",
-                "    \n",
-                "    for i, sequence in enumerate(tokenised_sequences):\n",
-                "        target_sequence = np.zeros(len(sequence))\n",
-                "        \n",
-                "        for link in link_tokens:\n",
-                "            start_positions = kmp(sequence, link)\n",
-                "            for pos in start_positions:\n",
-                "                target_sequence [pos : pos + len(link)] = 1\n",
-                "\n",
-                "        target_sequences.append(target_sequence)\n",
-                "\n",
-                "    return target_sequences\n",
-                "\n",
-                "\n",
-                "def label_linkable_tokens(text, label_all=True):\n",
-                "    parsed_html = BeautifulSoup(text, 'html.parser')\n",
-                "\n",
-                "    link_tokens = [\n",
-                "        tokenize(link.text)\n",
-                "        for link in parsed_html.find_all('a')\n",
-                "    ]\n",
-                "    \n",
-                "    tokenised_sequences = [\n",
-                "        tokenize(sentence) \n",
-                "        for sentence in sent_tokenize(parsed_html.text)\n",
-                "    ]\n",
-                "    \n",
-                "    target_sequences = label(tokenised_sequences, link_tokens)\n",
-                "    \n",
-                "    return tokenised_sequences, target_sequences"
+                "    return positions"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "token_sequences, target_sequences = [], []\n",
                 "\n",
-                "for article in tqdm(articles):\n",
-                "    try:\n",
-                "        tokenised_seqs, target_seqs = label_linkable_tokens(article)        \n",
-                "        token_sequences.extend(tokenised_seqs)\n",
-                "        target_sequences.extend(target_seqs)\n",
-                "    except:\n",
-                "        pass"
+                "for i, article in enumerate(tqdm(articles)):\n",
+                "    for j, sentence in enumerate(sent_tokenize(article)):\n",
+                "        try:\n",
+                "            tokenized_sentence, target_sequence = label_linkable_tokens(sentence)\n",
+                "            token_sequences.append(tokenized_sentence)\n",
+                "            target_sequences.append(target_sequence)\n",
+                "        except:\n",
+                "            pass"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "len(token_sequences)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# character level inputs"
@@ -187,28 +180,24 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "unique_characters = set(' '.join([\n",
-                "    token \n",
-                "    for seq in token_sequences\n",
-                "    for token in seq\n",
-                "]))"
+                "unique_characters = set(\" \".join([token for seq in token_sequences for token in seq]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "special_cases = ['xxunk', 'xxpad', 'xxbos', 'xxeos']\n",
+                "special_cases = [\"xxunk\", \"xxpad\", \"xxbos\", \"xxeos\"]\n",
                 "\n",
                 "for case in special_cases:\n",
                 "    unique_characters.add(case)"
             ]
         },
         {
             "cell_type": "code",
@@ -229,36 +218,35 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "wv_path = '/mnt/efs/text/word_vectors/wiki-news-300d-1M.vec'\n",
-                "wv_file = io.open(wv_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
+                "wv_path = \"/mnt/efs/text/word_vectors/wiki-news-300d-1M.vec\"\n",
+                "wv_file = io.open(wv_path, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\")\n",
                 "lines_to_parse = list(wv_file)[1:]\n",
                 "\n",
-                "fasttext = {line.split()[0]: np.array(line.split()[1:]).astype(np.float32)\n",
-                "            for line in tqdm(lines_to_parse)}"
+                "fasttext = {\n",
+                "    line.split()[0]: np.array(line.split()[1:]).astype(np.float32)\n",
+                "    for line in tqdm(lines_to_parse)\n",
+                "}"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "from collections import Counter\n",
-                "all_tokens = [\n",
-                "    tok \n",
-                "    for seq in token_sequences \n",
-                "    for tok in seq\n",
-                "]\n",
                 "\n",
-                "article_vocabulary, _ = zip(*Counter(all_tokens).most_common(10000000000))\n",
+                "all_tokens = [tok for seq in token_sequences for tok in seq]\n",
+                "\n",
+                "article_vocabulary, _ = zip(*Counter(all_tokens).most_common(1000000))\n",
                 "article_vocabulary = set(article_vocabulary)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -291,20 +279,20 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "word_vector_embedding_matrix = torch.FloatTensor([\n",
-                "    fasttext[token]\n",
-                "    if token in fasttext\n",
-                "    else fasttext['xxunk']\n",
-                "    for token in article_vocabulary\n",
-                "])"
+                "word_vector_embedding_matrix = torch.FloatTensor(\n",
+                "    [\n",
+                "        fasttext[token] if token in fasttext else fasttext[\"xxunk\"]\n",
+                "        for token in article_vocabulary\n",
+                "    ]\n",
+                ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# dataset and dataloader"
@@ -314,111 +302,96 @@
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class SentenceDataset(Dataset):\n",
                 "    def __init__(self, token_seqs, target_seqs):\n",
-                "        #impose length constraint\n",
+                "        # impose length constraint\n",
                 "        where_big_enough = np.where([len(seq) > 3 for seq in token_seqs])\n",
                 "        self.token_seqs = np.array(token_seqs)[where_big_enough]\n",
                 "        self.target_seqs = np.array(target_seqs)[where_big_enough]\n",
                 "\n",
-                "        #indexify\n",
-                "        self.char_ix_seqs = [\n",
-                "            self.indexify_chars(seq) for seq in self.token_seqs\n",
-                "        ]\n",
-                "\n",
-                "        self.token_seqs = [\n",
-                "            self.indexify_tokens(seq) for seq in self.token_seqs\n",
-                "        ]\n",
-                "        \n",
-                "        #find prediction points for language model\n",
-                "        self.exit_ix_seqs = [\n",
-                "            self.find_exit_points(seq) for seq in self.char_ix_seqs\n",
-                "        ]\n",
+                "        # indexify\n",
+                "        self.char_ix_seqs = [self.indexify_chars(seq) for seq in self.token_seqs]\n",
+                "\n",
+                "        self.token_seqs = [self.indexify_tokens(seq) for seq in self.token_seqs]\n",
+                "\n",
+                "        # find prediction points for language model\n",
+                "        self.exit_ix_seqs = [self.find_exit_points(seq) for seq in self.char_ix_seqs]\n",
                 "\n",
                 "    def __getitem__(self, ix):\n",
                 "        char_ix_seq = self.char_ix_seqs[ix]\n",
                 "        token_seq = self.token_seqs[ix]\n",
                 "        exit_ix_seq = self.exit_ix_seqs[ix]\n",
                 "        target_seq = self.target_seqs[ix]\n",
                 "        return char_ix_seq, token_seq, exit_ix_seq, target_seq\n",
                 "\n",
                 "    def __len__(self):\n",
                 "        return len(self.token_seqs)\n",
-                "    \n",
+                "\n",
                 "    def indexify_tokens(self, token_seq):\n",
                 "        ix_seq = np.array(\n",
-                "            [token_to_ix[token]\n",
-                "             if token in article_vocabulary\n",
-                "             else token_to_ix['xxunk']\n",
-                "             for token in token_seq]\n",
+                "            [\n",
+                "                token_to_ix[token]\n",
+                "                if token in article_vocabulary\n",
+                "                else token_to_ix[\"xxunk\"]\n",
+                "                for token in token_seq\n",
+                "            ]\n",
                 "        )\n",
                 "        return torch.LongTensor(ix_seq)\n",
-                "    \n",
+                "\n",
                 "    def indexify_chars(self, token_seq):\n",
                 "        ix_seq = np.array(\n",
-                "            [char_to_ix['xxbos'], char_to_ix[' ']] +\n",
-                "            [char_to_ix[char] for char in ' '.join(token_seq)] + \n",
-                "            [char_to_ix[' '], char_to_ix['xxeos']]\n",
+                "            [char_to_ix[\"xxbos\"], char_to_ix[\" \"]]\n",
+                "            + [char_to_ix[char] for char in \" \".join(token_seq)]\n",
+                "            + [char_to_ix[\" \"], char_to_ix[\"xxeos\"]]\n",
                 "        )\n",
                 "        return torch.LongTensor(ix_seq)\n",
-                "    \n",
+                "\n",
                 "    def find_exit_points(self, char_ix_seq):\n",
-                "        binary = (char_ix_seq == char_to_ix[' '])\n",
+                "        binary = char_ix_seq == char_to_ix[\" \"]\n",
                 "        return binary.nonzero().squeeze()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def collate_fn(batch):\n",
                 "    char_ix_seqs, token_seqs, exit_ix_seqs, target_seqs = zip(*batch)\n",
                 "\n",
-                "    char_seq_lens = torch.LongTensor([\n",
-                "        len(char_seq) for char_seq in char_ix_seqs\n",
-                "    ])\n",
-                "    \n",
-                "    sorted_char_lengths, sort_indicies = char_seq_lens.sort(\n",
-                "        dim=0, \n",
-                "        descending=True\n",
-                "    )\n",
-                "    \n",
+                "    char_seq_lens = torch.LongTensor([len(char_seq) for char_seq in char_ix_seqs])\n",
+                "\n",
+                "    sorted_char_lengths, sort_indicies = char_seq_lens.sort(dim=0, descending=True)\n",
+                "\n",
                 "    sorted_char_seqs = [char_ix_seqs[i] for i in sort_indicies]\n",
                 "    sorted_token_seqs = [token_seqs[i] for i in sort_indicies]\n",
                 "    sorted_exit_seqs = [exit_ix_seqs[i] for i in sort_indicies]\n",
                 "    sorted_target_seqs = [torch.LongTensor(target_seqs[i]) for i in sort_indicies]\n",
                 "    sorted_token_lengths = torch.LongTensor([len(seq) for seq in sorted_token_seqs])\n",
-                "    \n",
+                "\n",
                 "    padded_char_seqs = pad_sequence(\n",
-                "        sequences=sorted_char_seqs, \n",
-                "        padding_value=char_to_ix['xxpad'],\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_char_seqs, padding_value=char_to_ix[\"xxpad\"], batch_first=True\n",
                 "    )\n",
-                "    \n",
+                "\n",
                 "    padded_token_seqs = pad_sequence(\n",
-                "        sequences=sorted_token_seqs, \n",
-                "        padding_value=token_to_ix['xxpad'],\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_token_seqs,\n",
+                "        padding_value=token_to_ix[\"xxpad\"],\n",
+                "        batch_first=True,\n",
                 "    )\n",
-                "    \n",
+                "\n",
                 "    padded_exit_seqs = pad_sequence(\n",
-                "        sequences=sorted_exit_seqs, \n",
-                "        padding_value=0,\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_exit_seqs, padding_value=0, batch_first=True\n",
                 "    )\n",
-                "    \n",
+                "\n",
                 "    padded_target_seqs = pad_sequence(\n",
-                "        sequences=sorted_target_seqs, \n",
-                "        padding_value=0,\n",
-                "        batch_first=True\n",
+                "        sequences=sorted_target_seqs, padding_value=0, batch_first=True\n",
                 "    )\n",
                 "\n",
                 "    return (\n",
                 "        padded_char_seqs,\n",
                 "        padded_token_seqs,\n",
                 "        padded_exit_seqs,\n",
                 "        sorted_char_lengths,\n",
@@ -430,58 +403,49 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "train_tokens, test_tokens, train_targets, test_targets = train_test_split(\n",
-                "    token_sequences, \n",
-                "    target_sequences,\n",
-                "    test_size=0.05, \n",
-                "    random_state=42\n",
+                "    token_sequences, target_sequences, test_size=0.20, random_state=42\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "train_dataset = SentenceDataset(\n",
-                "    train_tokens, \n",
-                "    train_targets\n",
-                ")\n",
+                "train_dataset = SentenceDataset(train_tokens, train_targets)\n",
                 "\n",
                 "train_loader = DataLoader(\n",
-                "    dataset=train_dataset,  \n",
-                "    batch_size=64,\n",
+                "    dataset=train_dataset,\n",
+                "    batch_size=32,\n",
                 "    num_workers=5,\n",
                 "    shuffle=True,\n",
-                "    collate_fn=collate_fn\n",
+                "    collate_fn=collate_fn,\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "test_dataset = SentenceDataset(\n",
-                "    test_tokens, \n",
-                "    test_targets\n",
-                ")\n",
+                "test_dataset = SentenceDataset(test_tokens, test_targets)\n",
                 "\n",
                 "test_loader = DataLoader(\n",
-                "    dataset=test_dataset,  \n",
+                "    dataset=test_dataset,\n",
                 "    batch_size=1,\n",
                 "    num_workers=5,\n",
                 "    shuffle=True,\n",
-                "    collate_fn=collate_fn\n",
+                "    collate_fn=collate_fn,\n",
                 ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -510,15 +474,15 @@
                 "        self.head_fwd = nn.Sequential(\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
                 "            nn.ReLU(),\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim // 2, output_dim),\n",
                 "        )\n",
-                "        \n",
+                "\n",
                 "        self.head_bwd = nn.Sequential(\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
                 "            nn.ReLU(),\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim // 2, output_dim),\n",
                 "        )\n",
@@ -526,91 +490,78 @@
                 "    def forward(self, char_seqs, exit_seqs, lengths):\n",
                 "        x = self.embedding(char_seqs)\n",
                 "\n",
                 "        x = pack_padded_sequence(x, lengths=lengths, batch_first=True)\n",
                 "\n",
                 "        x, _ = self.char_level_lstm(x)\n",
                 "        out, _ = pad_packed_sequence(x, batch_first=True)\n",
-                "        \n",
+                "\n",
                 "        # pop out the character embeddings at position of the end of each token\n",
                 "        out = torch.stack([out[i, exit_seqs[i]] for i in range(len(out))])\n",
-                "        \n",
+                "\n",
                 "        out_fwd, out_bwd = torch.chunk(out, 2, 2)\n",
                 "\n",
                 "        pred_fwd = self.head_fwd(out_fwd[:, 1:])\n",
                 "        pred_bwd = self.head_bwd(out_bwd[:, :-1])\n",
-                "        \n",
+                "\n",
                 "        return pred_fwd, pred_bwd"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class LinkLabeller(nn.Module):\n",
-                "    def __init__(self, word_vector_embedding_matrix, hidden_dim=1024):\n",
+                "    def __init__(self, word_vector_embedding_matrix, hidden_dim=512):\n",
                 "        super(LinkLabeller, self).__init__()\n",
                 "        self.wv_embedding = nn.Embedding.from_pretrained(word_vector_embedding_matrix)\n",
                 "\n",
                 "        self.cln = CharacterLevelNetwork(\n",
                 "            input_dim=len(unique_characters),\n",
                 "            embedding_dim=50,\n",
-                "            hidden_dim=128,\n",
-                "            output_dim=50\n",
+                "            hidden_dim=256,\n",
+                "            output_dim=50,\n",
                 "        )\n",
-                "        \n",
-                "        self.lstm_input_size = (\n",
-                "            word_vector_embedding_matrix.shape[1] + \n",
-                "            (self.cln.output_dim * 2)\n",
+                "\n",
+                "        self.lstm_input_size = word_vector_embedding_matrix.shape[1] + (\n",
+                "            self.cln.output_dim * 2\n",
                 "        )\n",
-                "        \n",
+                "\n",
                 "        self.word_level_lstm = nn.LSTM(\n",
                 "            input_size=self.lstm_input_size,\n",
                 "            hidden_size=hidden_dim,\n",
                 "            num_layers=2,\n",
                 "            bidirectional=True,\n",
-                "            dropout=0.2\n",
+                "            dropout=0.2,\n",
                 "        )\n",
-                "                \n",
+                "\n",
                 "        self.head = nn.Sequential(\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim * 2, hidden_dim // 32),\n",
                 "            nn.ReLU(),\n",
                 "            nn.Dropout(0.3),\n",
                 "            nn.Linear(hidden_dim // 32, 2),\n",
                 "        )\n",
-                "    \n",
-                "    def forward(self, char_seqs, token_seqs, exit_seqs, c_lens, t_lens):        \n",
+                "\n",
+                "    def forward(self, char_seqs, token_seqs, exit_seqs, c_lens, t_lens):\n",
                 "        wv_seqs = self.wv_embedding(token_seqs)\n",
                 "        char_fwd, char_bwd = self.cln(char_seqs, exit_seqs, c_lens)\n",
-                "        \n",
-                "        concats = torch.cat(\n",
-                "            [char_fwd, char_bwd, wv_seqs], \n",
-                "            dim=2\n",
-                "        )\n",
-                "        \n",
-                "        sorted_lengths, sort_indicies = t_lens.sort(\n",
-                "            dim=0,\n",
-                "            descending=True\n",
-                "        )\n",
-                "        \n",
+                "\n",
+                "        concats = torch.cat([char_fwd, char_bwd, wv_seqs], dim=2)\n",
+                "\n",
+                "        sorted_lengths, sort_indicies = t_lens.sort(dim=0, descending=True)\n",
+                "\n",
                 "        concats = torch.stack([concats[i] for i in sort_indicies])\n",
-                "        \n",
-                "        packed = pack_padded_sequence(\n",
-                "            concats,\n",
-                "            lengths=sorted_lengths,\n",
-                "            batch_first=True\n",
-                "        )\n",
                 "\n",
-                "        packed_embedded, _ = self.word_level_lstm(packed)\n",
-                "        embedded, _ = pad_packed_sequence(packed_embedded)\n",
+                "        packed = pack_padded_sequence(concats, lengths=sorted_lengths, batch_first=True)\n",
                 "\n",
-                "        output = self.head(embedded).permute(1,2,0)\n",
+                "        embedded, _ = self.word_level_lstm(concats)\n",
+                "        output = self.head(embedded).permute(0, 2, 1)\n",
                 "        return output, sort_indicies"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
@@ -646,15 +597,15 @@
             "source": [
                 "losses = []\n",
                 "\n",
                 "torch.backends.cudnn.benchmark = True\n",
                 "\n",
                 "trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
                 "\n",
-                "optimiser = optim.Adam(trainable_parameters, lr=0.001)\n",
+                "optimiser = optim.Adam(trainable_parameters, lr=0.0001)\n",
                 "\n",
                 "loss_function = nn.CrossEntropyLoss(weight=class_weights.cuda())"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
@@ -671,52 +622,50 @@
                 "            exit_seqs = torch.LongTensor(exit_seqs).cuda(non_blocking=True)\n",
                 "            c_lens = torch.LongTensor(c_lens).cuda(non_blocking=True)\n",
                 "            t_lens = torch.LongTensor(t_lens).cuda(non_blocking=True)\n",
                 "            targets = torch.LongTensor(targets).cuda(non_blocking=True)\n",
                 "\n",
                 "            optimiser.zero_grad()\n",
                 "            preds, sort_indicies = model(c_seqs, t_seqs, exit_seqs, c_lens, t_lens)\n",
-                "            \n",
+                "\n",
                 "            targets = torch.stack([targets[i] for i in sort_indicies])\n",
-                "            \n",
+                "\n",
                 "            loss = loss_function(preds, targets)\n",
                 "            loss.backward()\n",
                 "            optimiser.step()\n",
                 "\n",
                 "            losses.append(loss.item())\n",
-                "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
-                "            loop.set_postfix(loss=np.mean(losses[-100:]))\n",
-                "        \n",
-                "        torch.save(model.state_dict(), '/mnt/efs/models/model_state_dict.pt')"
+                "            loop.set_description(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
+                "            loop.set_postfix(loss=np.mean(losses[-100:]))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "train(\n",
                 "    model=model,\n",
                 "    train_loader=train_loader,\n",
                 "    loss_function=loss_function,\n",
                 "    optimiser=optimiser,\n",
-                "    n_epochs=3\n",
+                "    n_epochs=3,\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "loss_data = pd.Series(losses[20:]).rolling(window=100).mean()\n",
-                "ax = loss_data.plot();\n",
-                "ax.set_ylim(0.1, 0.3);"
+                "ax = loss_data.plot()\n",
+                "ax.set_ylim(0.12, 0.28);"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# test the model on unseen data"
@@ -724,110 +673,62 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "iterable_loader = iter(test_loader)"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": null,
-            "metadata": {},
-            "outputs": [],
-            "source": [
-                "def format_output(tokens, targets, preds):\n",
-                "    target_string, pred_string = '', ''\n",
-                "\n",
-                "    for token_id, target, pred in zip(tokens, targets, preds):\n",
-                "        token = ix_to_token[token_id.item()]\n",
+                "output = \"\"\n",
                 "\n",
-                "        if target.item() == 1: \n",
-                "            target_string += '<b>' + token + '</b> '\n",
-                "        else: \n",
-                "            target_string += token + ' '\n",
+                "for i, (c_seqs, t_seqs, exit_seqs, c_lens, t_lens, targets) in enumerate(test_loader):\n",
+                "    while i < 10:\n",
+                "        c_seqs = torch.LongTensor(c_seqs).cuda()\n",
+                "        t_seqs = torch.LongTensor(t_seqs).cuda()\n",
+                "        exit_seqs = torch.LongTensor(exit_seqs).cuda()\n",
+                "        c_lens = torch.LongTensor(c_lens).cuda()\n",
+                "        t_lens = torch.LongTensor(t_lens).cuda()\n",
+                "        targets = torch.LongTensor(targets).cuda()\n",
+                "\n",
+                "        optimiser.zero_grad()\n",
+                "        preds, sort_indicies = model(c_seqs, t_seqs, exit_seqs, c_lens, t_lens)\n",
+                "        preds = nn.LogSoftmax(dim=1)(preds).argmax(dim=1)\n",
+                "\n",
+                "        targets = torch.stack([targets[i] for i in sort_indicies])\n",
+                "\n",
+                "        target_string = []\n",
+                "        pred_string = []\n",
+                "\n",
+                "        for i in range(len(t_seqs[0])):\n",
+                "            if targets[0][i].item() == 1:\n",
+                "                target_string.append(\"<b>\" + ix_to_token[t_seqs[0][i].item()] + \"</b>\")\n",
+                "            else:\n",
+                "                target_string.append(ix_to_token[t_seqs[0][i].item()])\n",
+                "            if preds[0][i].item() == 1:\n",
+                "                pred_string.append(\"<b>\" + ix_to_token[t_seqs[0][i].item()] + \"</b>\")\n",
+                "            else:\n",
+                "                pred_string.append(ix_to_token[t_seqs[0][i].item()])\n",
                 "\n",
-                "        if pred.item() == 1: pred_string += '<b>' + token + '</b> '\n",
-                "        else: pred_string += token + ' '\n",
-                "\n",
-                "    output_string = (\n",
-                "        'PRED:<br>' + \n",
-                "        pred_string +\n",
-                "        '<br><br>TARG:<br>' +\n",
-                "        target_string +\n",
-                "        '<br><br>------------------------<br><br>'\n",
-                "    )\n",
-                "    \n",
-                "    return output_string"
+                "        output += \"PRED:    \" + \" \".join(pred_string) + \"<br>\"\n",
+                "        output += \"TARG:    \" + \" \".join(target_string) + \"</p>\""
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "output = ''\n",
-                "samples = [next(iterable_loader) for i in range(10)]\n",
-                "\n",
-                "for (c_seqs, t_seqs, exit_seqs, c_lens, t_lens, targets) in samples:\n",
-                "    c_seqs = torch.LongTensor(c_seqs).cuda(non_blocking=True)\n",
-                "    t_seqs = torch.LongTensor(t_seqs).cuda(non_blocking=True)\n",
-                "    exit_seqs = torch.LongTensor(exit_seqs).cuda(non_blocking=True)\n",
-                "    c_lens = torch.LongTensor(c_lens).cuda(non_blocking=True)\n",
-                "    t_lens = torch.LongTensor(t_lens).cuda(non_blocking=True)\n",
-                "    targets = torch.LongTensor(targets).cuda(non_blocking=True)\n",
-                "\n",
-                "    optimiser.zero_grad()\n",
-                "    preds, sort_indicies = model(c_seqs, t_seqs, exit_seqs, c_lens, t_lens)\n",
-                "    preds = nn.LogSoftmax(dim=1)(preds).argmax(dim=1)\n",
-                "\n",
-                "    targets = torch.stack([targets[i] for i in sort_indicies])\n",
-                "\n",
-                "    target_string = []\n",
-                "    pred_string = []\n",
-                "\n",
-                "    output += format_output(t_seqs[0], targets[0], preds[0])\n",
-                "\n",
                 "display(HTML(output))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
-            "source": [
-                "torch.save(model.state_dict(), '/mnt/efs/models/nerd/model_state_dict.pt')\n",
-                "\n",
-                "with open('/mnt/efs/models/nerd/token_to_ix.pkl', 'wb') as f:\n",
-                "    pickle.dump(token_to_ix, f)\n",
-                "\n",
-                "with open('/mnt/efs/models/nerd/ix_to_token.pkl', 'wb') as f:\n",
-                "    pickle.dump(ix_to_token, f)\n",
-                "    \n",
-                "with open('/mnt/efs/models/nerd/char_to_ix.pkl', 'wb') as f:\n",
-                "    pickle.dump(char_to_ix, f)\n",
-                "\n",
-                "with open('/mnt/efs/models/nerd/unique_characters.pkl', 'wb') as f:\n",
-                "    pickle.dump(unique_characters, f)\n",
-                "\n",
-                "with open('/mnt/efs/models/nerd/article_vocabulary.pkl', 'wb') as f:\n",
-                "    pickle.dump(article_vocabulary, f)\n",
-                "\n",
-                "torch.save(word_vector_embedding_matrix, '/mnt/efs/models/nerd/embedding_matrix.pt')"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": null,
-            "metadata": {},
-            "outputs": [],
             "source": []
         }
     ],
     "metadata": {
         "kernelspec": {
             "display_name": "Python [conda env:pytorch_p36]",
             "language": "python",
```

### Comparing `weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/reading from VHS.ipynb` & `weco-datascience-0.1.9/notebooks/wikipedia/notebooks/reading from VHS.ipynb`

 * *Files 16% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9945512820512821%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(3, \'dynamodb = boto3.resource("dynamodb", '*

 * *            '"eu-west-1")\\n\'), (10, \'table = dynamodb.Table("vhs-calm")\')], delete: [10, 3]}}, '*

 * *            '1: {\'source\': [\'table.get_item(Key={"id": '*

 * *            '"0063a877-ea25-4a1b-aebf-6e79f4e9f1c8"})\']}}'}*

```diff
@@ -5,33 +5,31 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "import boto3\n",
                 "\n",
                 "# Get the service resource.\n",
-                "dynamodb = boto3.resource('dynamodb', 'eu-west-1')\n",
+                "dynamodb = boto3.resource(\"dynamodb\", \"eu-west-1\")\n",
                 "\n",
                 "# Instantiate a table resource object without actually\n",
                 "# creating a DynamoDB table. Note that the attributes of this table\n",
                 "# are lazy-loaded: a request is not made nor are the attribute\n",
                 "# values populated until the attributes\n",
                 "# on the table resource are accessed or its load() method is called.\n",
-                "table = dynamodb.Table('vhs-calm')\n"
+                "table = dynamodb.Table(\"vhs-calm\")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "table.get_item(\n",
-                "    Key={'id': '0063a877-ea25-4a1b-aebf-6e79f4e9f1c8'}\n",
-                ")"
+                "table.get_item(Key={\"id\": \"0063a877-ea25-4a1b-aebf-6e79f4e9f1c8\"})"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
```

### Comparing `weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/scratch.ipynb` & `weco-datascience-0.1.9/notebooks/wikipedia/notebooks/scratch.ipynb`

 * *Files 12% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9955810069036689%*

 * *Differences: {"'cells'": "{1: {'source': {insert: [(7, '                    name, number = "*

 * *            'line.rstrip("\\\\n").split("\\\\t")\\n\')], delete: [7]}}, 2: {\'source\': [\'path = '*

 * *            '"/mnt/efs/wikipedia/deeptype/wikidata/"\\n\', \'num_names_to_load = 43710495\\n\', '*

 * *            '\'prefix = "enwiki"\']}, 3: {\'source\': {insert: [(1, \'    join(path, '*

 * *            '"wikidata_wikititle2wikidata.tsv"), num_names_to_load, prefix=prefix\\n\')], delete: '*

 * *            "[3, 2, 1]}}, 6: {'source': {insert […]*

```diff
@@ -18,15 +18,15 @@
                 "def load_names(path, num, prefix):\n",
                 "    names = {}\n",
                 "    errors = 0  # debug\n",
                 "    if num > 0:\n",
                 "        with open(path, \"rt\", encoding=\"UTF-8\") as fin:\n",
                 "            for line in fin:\n",
                 "                try:\n",
-                "                    name, number = line.rstrip('\\n').split('\\t')\n",
+                "                    name, number = line.rstrip(\"\\n\").split(\"\\t\")\n",
                 "                except ValueError:\n",
                 "                    errors += 1\n",
                 "                number = int(number)\n",
                 "                if number >= num:\n",
                 "                    break\n",
                 "                else:\n",
                 "                    if name.startswith(prefix):\n",
@@ -37,29 +37,27 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "path='/mnt/efs/wikipedia/deeptype/wikidata/'\n",
-                "num_names_to_load=43710495\n",
-                "prefix=\"enwiki\""
+                "path = \"/mnt/efs/wikipedia/deeptype/wikidata/\"\n",
+                "num_names_to_load = 43710495\n",
+                "prefix = \"enwiki\""
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "known_names = load_names(\n",
-                "    join(path, \"wikidata_wikititle2wikidata.tsv\"),\n",
-                "    num_names_to_load,\n",
-                "    prefix=prefix\n",
+                "    join(path, \"wikidata_wikititle2wikidata.tsv\"), num_names_to_load, prefix=prefix\n",
                 ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -94,35 +92,37 @@
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def load_wikidata_ids(path, verbose=True):\n",
-                "    wikidata_ids_inverted_path = join(path, 'wikidata_ids_inverted.marisa')\n",
+                "    wikidata_ids_inverted_path = join(path, \"wikidata_ids_inverted.marisa\")\n",
                 "    with open(join(path, \"wikidata_ids.txt\"), \"rt\") as fin:\n",
                 "        ids = fin.read().splitlines()\n",
                 "    if exists(wikidata_ids_inverted_path):\n",
-                "        print('exists')\n",
+                "        print(\"exists\")\n",
                 "        if verbose:\n",
                 "            print(\"loading wikidata id -> index\")\n",
-                "        name2index = MarisaAsDict(marisa_trie.RecordTrie('i').load(wikidata_ids_inverted_path))\n",
+                "        name2index = MarisaAsDict(\n",
+                "            marisa_trie.RecordTrie(\"i\").load(wikidata_ids_inverted_path)\n",
+                "        )\n",
                 "        if verbose:\n",
                 "            print(\"done\")\n",
                 "    else:\n",
                 "        if verbose:\n",
                 "            print(\"building trie\")\n",
                 "\n",
                 "        name2index = MarisaAsDict(\n",
-                "            marisa_trie.RecordTrie('i', [(name, (k,)) for k, name in enumerate(ids)])\n",
+                "            marisa_trie.RecordTrie(\"i\", [(name, (k,)) for k, name in enumerate(ids)])\n",
                 "        )\n",
                 "        name2index.marisa.save(wikidata_ids_inverted_path)\n",
                 "        if verbose:\n",
                 "            print(\"done\")\n",
-                "    return (ids, name2index)\n"
+                "    return (ids, name2index)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
@@ -141,26 +141,24 @@
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "article2id = marisa_trie.RecordTrie('i').load(\n",
-                "    join(path, \"wikititle2wikidata.marisa\")\n",
-                ")"
+                "article2id = marisa_trie.RecordTrie(\"i\").load(join(path, \"wikititle2wikidata.marisa\"))"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "article2id['europe'][0][0]"
+                "article2id[\"europe\"][0][0]"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# fuzz"
@@ -172,35 +170,35 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "import numpy as np\n",
                 "from nltk import word_tokenize\n",
                 "from more_itertools import consecutive_groups\n",
                 "\n",
+                "\n",
                 "def tokenize(sentence):\n",
-                "    '''moses tokeniser'''\n",
-                "    seq = ' '.join(word_tokenize(sentence))\n",
+                "    \"\"\"moses tokeniser\"\"\"\n",
+                "    seq = \" \".join(word_tokenize(sentence))\n",
                 "    seq = seq.replace(\" n't \", \"n 't \")\n",
                 "    return seq.split()\n",
                 "\n",
-                "s = 'a bunch of text with a name like Francis Crick in it, and then Francis Crick in it again later'\n",
+                "\n",
+                "s = \"a bunch of text with a name like Francis Crick in it, and then Francis Crick in it again later\"\n",
                 "tokens = tokenize(s)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "outputs = np.array([\n",
-                "    1 if token == 'Francis' or token == 'Crick' \n",
-                "    else 0 \n",
-                "    for token in tokens\n",
-                "])"
+                "outputs = np.array(\n",
+                "    [1 if token == \"Francis\" or token == \"Crick\" else 0 for token in tokens]\n",
+                ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
```

### Comparing `weco-datascience-0.1.8/research_notebooks/wikipedia/notebooks/test bpe.ipynb` & `weco-datascience-0.1.9/notebooks/wikipedia/notebooks/test bpe.ipynb`

 * *Files 7% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.994954954954955%*

 * *Differences: {"'cells'": "{0: {'source': {insert: [(5, '\\n'), (18, '    bs = (\\n'), (19, '        "*

 * *            'list(range(ord("!"), ord("~") + 1))\\n\'), (20, \'        + list(range(ord("¡"), '*

 * *            'ord("¬") + 1))\\n\'), (21, \'        + list(range(ord("®"), ord("ÿ") + 1))\\n\'), '*

 * *            "(22, '    )\\n'), (25, '    for b in range(2 ** 8):\\n'), (28, '            "*

 * *            "cs.append(2 ** 8 + n)\\n'), (32, '\\n'), (44, '\\n'), (47, '    def __init__(self, "*

 * *            'encoder, bpe_merges, errors= […]*

```diff
@@ -8,143 +8,157 @@
             "source": [
                 "import os\n",
                 "import json\n",
                 "import requests\n",
                 "import regex as re\n",
                 "from functools import lru_cache\n",
                 "\n",
+                "\n",
                 "@lru_cache()\n",
                 "def bytes_to_unicode():\n",
                 "    \"\"\"\n",
                 "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
                 "    The reversible bpe codes work on unicode strings.\n",
                 "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
                 "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
                 "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
                 "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
                 "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
                 "    \"\"\"\n",
-                "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"\u00a1\"), ord(\"\u00ac\")+1))+list(range(ord(\"\u00ae\"), ord(\"\u00ff\")+1))\n",
+                "    bs = (\n",
+                "        list(range(ord(\"!\"), ord(\"~\") + 1))\n",
+                "        + list(range(ord(\"\u00a1\"), ord(\"\u00ac\") + 1))\n",
+                "        + list(range(ord(\"\u00ae\"), ord(\"\u00ff\") + 1))\n",
+                "    )\n",
                 "    cs = bs[:]\n",
                 "    n = 0\n",
-                "    for b in range(2**8):\n",
+                "    for b in range(2 ** 8):\n",
                 "        if b not in bs:\n",
                 "            bs.append(b)\n",
-                "            cs.append(2**8+n)\n",
+                "            cs.append(2 ** 8 + n)\n",
                 "            n += 1\n",
                 "    cs = [chr(n) for n in cs]\n",
                 "    return dict(zip(bs, cs))\n",
                 "\n",
+                "\n",
                 "def get_pairs(word):\n",
                 "    \"\"\"Return set of symbol pairs in a word.\n",
                 "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
                 "    \"\"\"\n",
                 "    pairs = set()\n",
                 "    prev_char = word[0]\n",
                 "    for char in word[1:]:\n",
                 "        pairs.add((prev_char, char))\n",
                 "        prev_char = char\n",
                 "    return pairs\n",
                 "\n",
+                "\n",
                 "class Encoder:\n",
-                "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
+                "    def __init__(self, encoder, bpe_merges, errors=\"replace\"):\n",
                 "        self.encoder = encoder\n",
-                "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
-                "        self.errors = errors # how to handle errors in decoding\n",
+                "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
+                "        self.errors = errors  # how to handle errors in decoding\n",
                 "        self.byte_encoder = bytes_to_unicode()\n",
-                "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
+                "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
                 "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
                 "        self.cache = {}\n",
                 "\n",
                 "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
-                "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
+                "        self.pat = re.compile(\n",
+                "            r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
+                "        )\n",
                 "\n",
                 "    def bpe(self, token):\n",
                 "        if token in self.cache:\n",
                 "            return self.cache[token]\n",
                 "        word = tuple(token)\n",
                 "        pairs = get_pairs(word)\n",
                 "\n",
                 "        if not pairs:\n",
                 "            return token\n",
                 "\n",
                 "        while True:\n",
-                "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
+                "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n",
                 "            if bigram not in self.bpe_ranks:\n",
                 "                break\n",
                 "            first, second = bigram\n",
                 "            new_word = []\n",
                 "            i = 0\n",
                 "            while i < len(word):\n",
                 "                try:\n",
                 "                    j = word.index(first, i)\n",
                 "                    new_word.extend(word[i:j])\n",
                 "                    i = j\n",
                 "                except:\n",
                 "                    new_word.extend(word[i:])\n",
                 "                    break\n",
                 "\n",
-                "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
-                "                    new_word.append(first+second)\n",
+                "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
+                "                    new_word.append(first + second)\n",
                 "                    i += 2\n",
                 "                else:\n",
                 "                    new_word.append(word[i])\n",
                 "                    i += 1\n",
                 "            new_word = tuple(new_word)\n",
                 "            word = new_word\n",
                 "            if len(word) == 1:\n",
                 "                break\n",
                 "            else:\n",
                 "                pairs = get_pairs(word)\n",
-                "        word = ' '.join(word)\n",
+                "        word = \" \".join(word)\n",
                 "        self.cache[token] = word\n",
                 "        return word\n",
                 "\n",
                 "    def encode(self, text):\n",
                 "        bpe_tokens = []\n",
                 "        for token in re.findall(self.pat, text):\n",
-                "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
-                "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
+                "            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
+                "            bpe_tokens.extend(\n",
+                "                self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \")\n",
+                "            )\n",
                 "        return bpe_tokens\n",
                 "\n",
                 "    def decode(self, tokens):\n",
-                "        text = ''.join([self.decoder[token] for token in tokens])\n",
-                "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
+                "        text = \"\".join([self.decoder[token] for token in tokens])\n",
+                "        text = bytearray([self.byte_decoder[c] for c in text]).decode(\n",
+                "            \"utf-8\", errors=self.errors\n",
+                "        )\n",
                 "        return text\n",
                 "\n",
+                "\n",
                 "def get_encoder(bpe_path, encoder_path):\n",
-                "    with open(encoder_path, encoding='utf-8') as f:\n",
+                "    with open(encoder_path, encoding=\"utf-8\") as f:\n",
                 "        encoder = json.load(f)\n",
-                "    with open(bpe_path, encoding='utf-8') as f:\n",
+                "    with open(bpe_path, encoding=\"utf-8\") as f:\n",
                 "        bpe_data = f.read()\n",
-                "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
+                "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]]\n",
                 "    return Encoder(\n",
                 "        encoder=encoder,\n",
                 "        bpe_merges=bpe_merges,\n",
                 "    )"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
-                "text = requests.get('https://www.gutenberg.org/files/58884/58884-0.txt').text"
+                "text = requests.get(\"https://www.gutenberg.org/files/58884/58884-0.txt\").text"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "enc = get_encoder(\n",
-                "    bpe_path='/mnt/efs/wikipedia/gpt-2/vocab.bpe', \n",
-                "    encoder_path='/mnt/efs/wikipedia/gpt-2/encoder.json'\n",
+                "    bpe_path=\"/mnt/efs/wikipedia/gpt-2/vocab.bpe\",\n",
+                "    encoder_path=\"/mnt/efs/wikipedia/gpt-2/encoder.json\",\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
```

### Comparing `weco-datascience-0.1.8/research_notebooks/wikipedia/src/utils.py` & `weco-datascience-0.1.9/notebooks/wikipedia/src/utils.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/scripts/download_all_images.py` & `weco-datascience-0.1.9/scripts/download_all_images.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/scripts/download_oai_harvest.py` & `weco-datascience-0.1.9/scripts/download_oai_harvest.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/scripts/generate_api_identifiers.py` & `weco-datascience-0.1.9/scripts/generate_api_identifiers.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/scripts/populate_dummy_calm_vhs.py` & `weco-datascience-0.1.9/scripts/populate_dummy_calm_vhs.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/scripts/put_wikidata_embeddings_in_dynamodb.py` & `weco-datascience-0.1.9/scripts/put_wikidata_embeddings_in_dynamodb.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/terraform/apis/locals.tf` & `weco-datascience-0.1.9/terraform/apis/locals.tf`

 * *Files 23% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 locals {
   nginx_container_image          = "wellcome/nginx_api-gw:77d1ba9b060a184097a26bc685735be343b1a754"
   nginx_listener_port            = "9000"
-  public_subnets                 = "${data.terraform_remote_state.accounts_data.datascience_vpc_public_subnets}"
-  private_subnets                = "${data.terraform_remote_state.accounts_data.datascience_vpc_private_subnets}"
-  vpc_id                         = "${data.terraform_remote_state.accounts_data.datascience_vpc_id}"
+  public_subnets                 = data.terraform_remote_state.accounts_data.outputs.datascience_vpc_public_subnets
+  private_subnets                = data.terraform_remote_state.accounts_data.outputs.datascience_vpc_private_subnets
+  vpc_id                         = data.terraform_remote_state.accounts_data.outputs.datascience_vpc_id
   namespace_id                = "datascience"
   cluster_id                  = "apis"
   miro_read_role              = "arn:aws:iam::760097843905:role/sourcedata-miro-assumable_read_role"
 }
 
 data "aws_vpc" "vpc" {
-  id = "${local.vpc_id}"
+  id = local.vpc_id
 }
```

### Comparing `weco-datascience-0.1.8/terraform/apis/modules/service/api/api_gateway.tf` & `weco-datascience-0.1.9/terraform/apis/modules/service/api/api_gateway.tf`

 * *Files 14% similar despite different names*

```diff
@@ -1,59 +1,59 @@
 resource "aws_api_gateway_resource" "api" {
-  rest_api_id = "${var.api_gateway_rest_api_id}"
-  parent_id   = "${var.api_gateway_root_resource_id}"
-  path_part   = "${var.namespace}"
+  rest_api_id = var.api_gateway_rest_api_id
+  parent_id   = var.api_gateway_root_resource_id
+  path_part   = var.namespace
 }
 
 resource "aws_api_gateway_resource" "api_proxy" {
-  rest_api_id = "${var.api_gateway_rest_api_id}"
-  parent_id   = "${aws_api_gateway_resource.api.id}"
+  rest_api_id = var.api_gateway_rest_api_id
+  parent_id   = aws_api_gateway_resource.api.id
   path_part   = "{proxy+}"
 }
 
 resource "aws_api_gateway_method" "api" {
-  rest_api_id   = "${var.api_gateway_rest_api_id}"
-  resource_id   = "${aws_api_gateway_resource.api.id}"
+  rest_api_id   = var.api_gateway_rest_api_id
+  resource_id   = aws_api_gateway_resource.api.id
   http_method   = "GET"
   authorization = "NONE"
 }
 
 resource "aws_api_gateway_method" "api_proxy" {
-  rest_api_id   = "${var.api_gateway_rest_api_id}"
-  resource_id   = "${aws_api_gateway_resource.api_proxy.id}"
+  rest_api_id   = var.api_gateway_rest_api_id
+  resource_id   = aws_api_gateway_resource.api_proxy.id
   http_method   = "GET"
   authorization = "NONE"
 
   request_parameters = {
     "method.request.path.proxy" = true
   }
 }
 
 resource "aws_api_gateway_integration" "api" {
-  rest_api_id = "${var.api_gateway_rest_api_id}"
-  resource_id = "${aws_api_gateway_method.api.resource_id}"
-  http_method = "${aws_api_gateway_method.api.http_method}"
+  rest_api_id = var.api_gateway_rest_api_id
+  resource_id = aws_api_gateway_method.api.resource_id
+  http_method = aws_api_gateway_method.api.http_method
 
   integration_http_method = "GET"
   type                    = "HTTP_PROXY"
   uri                     = "http://${var.lb_dns_name}:${var.listener_port}"
 
   connection_type = "VPC_LINK"
-  connection_id   = "${var.api_gateway_vpc_link_id}"
+  connection_id   = var.api_gateway_vpc_link_id
 }
 
 resource "aws_api_gateway_integration" "api_proxy" {
-  rest_api_id = "${var.api_gateway_rest_api_id}"
-  resource_id = "${aws_api_gateway_method.api_proxy.resource_id}"
-  http_method = "${aws_api_gateway_method.api_proxy.http_method}"
+  rest_api_id = var.api_gateway_rest_api_id
+  resource_id = aws_api_gateway_method.api_proxy.resource_id
+  http_method = aws_api_gateway_method.api_proxy.http_method
 
   integration_http_method = "GET"
   type                    = "HTTP_PROXY"
   uri                     = "http://${var.lb_dns_name}:${var.listener_port}/{proxy}"
 
   connection_type = "VPC_LINK"
-  connection_id   = "${var.api_gateway_vpc_link_id}"
+  connection_id   = var.api_gateway_vpc_link_id
 
   request_parameters = {
-    integration.request.path.proxy = "method.request.path.proxy"
+    "integration.request.path.proxy" = "method.request.path.proxy"
   }
 }
```

### Comparing `weco-datascience-0.1.8/terraform/apis/modules/service/api/iam.tf` & `weco-datascience-0.1.9/terraform/apis/modules/service/api/iam.tf`

 * *Files 17% similar despite different names*

```diff
@@ -1,56 +1,56 @@
 locals {
-  assume_policy_count = "${length(var.assumable_roles) > 0 ? 1 : 0}"
+  assume_policy_count = length(var.assumable_roles) > 0 ? 1 : 0
 }
 
 data "aws_iam_policy_document" "allow_assume_roles" {
   statement {
     actions = [
       "sts:AssumeRole",
     ]
 
-    resources = ["${var.assumable_roles}"]
+    resources = var.assumable_roles
   }
 }
 
 data "aws_iam_policy_document" "allow_read_core_data" {
   statement {
     actions = [
       "s3:ListBucket",
       "s3:GetBucketLocation",
     ]
 
-    resources = ["${local.core_data_bucket}"]
+    resources = [local.core_data_bucket]
   }
 
   statement {
     actions = [
       "s3:GetObject",
     ]
 
     resources = ["${local.core_data_bucket}/*"]
   }
 }
 
 resource "aws_iam_policy" "allow_assume_roles" {
-  count       = "${local.assume_policy_count}"
+  count       = local.assume_policy_count
   name        = "${var.namespace}-allow-assume-roles"
   description = "Allows the specified roles to be assumed"
-  policy      = "${data.aws_iam_policy_document.allow_assume_roles.json}"
+  policy      = data.aws_iam_policy_document.allow_assume_roles.json
 }
 
 resource "aws_iam_policy" "allow_read_core_data" {
   name        = "${var.namespace}-allow-read-core-data"
   description = "Allows reading the core data bucket"
-  policy      = "${data.aws_iam_policy_document.allow_read_core_data.json}"
+  policy      = data.aws_iam_policy_document.allow_read_core_data.json
 }
 
 resource "aws_iam_role_policy_attachment" "attach_allow_assume_roles" {
-  count      = "${local.assume_policy_count}"
-  role       = "${module.task.task_role_name}"
-  policy_arn = "${aws_iam_policy.allow_assume_roles.arn}"
+  count      = local.assume_policy_count
+  role       = module.task.task_role_name
+  policy_arn = aws_iam_policy.allow_assume_roles[0].arn
 }
 
 resource "aws_iam_role_policy_attachment" "attach_allow_read_core_data" {
-  role       = "${module.task.task_role_name}"
-  policy_arn = "${aws_iam_policy.allow_read_core_data.arn}"
+  role       = module.task.task_role_name
+  policy_arn = aws_iam_policy.allow_read_core_data.arn
 }
```

### Comparing `weco-datascience-0.1.8/terraform/apis/modules/service/api/variables.tf` & `weco-datascience-0.1.9/terraform/apis/modules/service/api/variables.tf`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 variable "subnets" {
-  type = "list"
+  type = list(string)
 }
 
 variable "cluster_id" {}
 
 variable "namespace" {}
 variable "namespace_id" {}
 
@@ -15,21 +15,21 @@
   default = "80"
 }
 
 variable "nginx_container_image" {}
 variable "nginx_container_port" {}
 
 variable "security_group_ids" {
-  type = "list"
+  type = list(string)
 }
 
 variable "env_vars_length" {}
 
 variable "env_vars" {
-  type = "map"
+  type = map(string)
 }
 
 variable "lb_arn" {}
 variable "lb_dns_name" {}
 variable "listener_port" {}
 
 variable "api_gateway_rest_api_id" {}
@@ -69,10 +69,10 @@
 }
 
 variable "task_desired_count" {
   default = "3"
 }
 
 variable "assumable_roles" {
-  type    = "list"
+  type    = list(string)
   default = []
 }
```

### Comparing `weco-datascience-0.1.8/terraform/apis/terraform.tf` & `weco-datascience-0.1.9/terraform/apis/terraform.tf`

 * *Files 19% similar despite different names*

```diff
@@ -10,28 +10,25 @@
     region         = "eu-west-1"
   }
 }
 
 data "terraform_remote_state" "accounts_data" {
   backend = "s3"
 
-  config {
-    role_arn = "arn:aws:iam::760097843905:role/platform-developer"
+  config = {
+    role_arn = "arn:aws:iam::760097843905:role/platform-read_only"
 
     bucket = "wellcomecollection-platform-infra"
-    key    = "terraform/accounts/data.tfstate"
+    key    = "terraform/platform-infrastructure/accounts/data.tfstate"
     region = "eu-west-1"
   }
 }
 
 provider "aws" {
   assume_role {
     role_arn = "arn:aws:iam::964279923020:role/data-admin"
   }
 
-  region  = "eu-west-1"
-  version = "1.59.0"
+  region = "eu-west-1"
 }
 
-provider "template" {
-  version = "~> 2.1"
-}
+provider "template" {}
```

### Comparing `weco-datascience-0.1.8/terraform/data_stores/s3.tf` & `weco-datascience-0.1.9/terraform/data_stores/s3.tf`

 * *Files 13% similar despite different names*

```diff
@@ -14,26 +14,26 @@
   lifecycle {
     prevent_destroy = true
   }
 }
 
 # policies
 resource "aws_s3_bucket_policy" "model_core_data_policy" {
-  bucket = "${aws_s3_bucket.model_core_data.bucket}"
-  policy = "${data.aws_iam_policy_document.s3_read.json}"
+  bucket = aws_s3_bucket.model_core_data.bucket
+  policy = data.aws_iam_policy_document.s3_read.json
 }
 
 data "aws_iam_policy_document" "s3_read" {
   statement {
     actions = ["s3:*"]
 
     resources = [
-      "${aws_s3_bucket.model_core_data.arn}",
+      aws_s3_bucket.model_core_data.arn,
       "${aws_s3_bucket.model_core_data.arn}/*",
     ]
 
-    principals = {
+    principals {
       type        = "AWS"
       identifiers = ["arn:aws:iam::964279923020:role/datascience_ec2"]
     }
   }
 }
```

### Comparing `weco-datascience-0.1.8/ui/README.md` & `weco-datascience-0.1.9/api_interfaces/palette/README.md`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/ui/components/Header.tsx` & `weco-datascience-0.1.9/api_interfaces/palette/components/Header.tsx`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/ui/package-lock.json` & `weco-datascience-0.1.9/api_interfaces/palette/package-lock.json`

 * *Files 20% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9564513968230064%*

 * *Differences: {"'dependencies'": "{'@ampproject/toolbox-core': {'version': '2.8.0', 'resolved': "*

 * *                   "'https://registry.npmjs.org/@ampproject/toolbox-core/-/toolbox-core-2.8.0.tgz', "*

 * *                   "'integrity': "*

 * *                   "'sha512-YrMRrE9zfAChPlFLT+B4yoGEH6CR/Yerjm6SCxuFSPARK/LaytUV+ZhZ03tlMv5wUHDH2Lq8e/lGymME0CXBhA==', "*

 * *                   "'requires': {replace: OrderedDict([('cross-fetch', '3.1.2'), ('lru-cache', "*

 * *                   "'6.0.0')])}, 'dependencies': OrderedDict([('lru-cache […]*

```diff
@@ -1,72 +1,219 @@
 {
     "dependencies": {
         "@ampproject/toolbox-core": {
-            "integrity": "sha512-8aONoeOAVujavLUezSCtpUjg9khkVndpArbn25cLab6/UG+ZgrFPvU3A7z1TjBvB31bte4pXxH6U004BC0VdfA==",
+            "dependencies": {
+                "lru-cache": {
+                    "integrity": "sha512-Jo6dJ04CmSjuznwJSS3pUeWmd/H0ffTlkXXgwZi+eq1UCmqQwCh+eLsYOYCwY991i2Fah4h1BEMCx4qThGbsiA==",
+                    "requires": {
+                        "yallist": "^4.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/lru-cache/-/lru-cache-6.0.0.tgz",
+                    "version": "6.0.0"
+                }
+            },
+            "integrity": "sha512-YrMRrE9zfAChPlFLT+B4yoGEH6CR/Yerjm6SCxuFSPARK/LaytUV+ZhZ03tlMv5wUHDH2Lq8e/lGymME0CXBhA==",
             "requires": {
-                "node-fetch": "2.6.0"
+                "cross-fetch": "3.1.2",
+                "lru-cache": "6.0.0"
             },
-            "resolved": "https://registry.npmjs.org/@ampproject/toolbox-core/-/toolbox-core-1.0.1.tgz",
-            "version": "1.0.1"
+            "resolved": "https://registry.npmjs.org/@ampproject/toolbox-core/-/toolbox-core-2.8.0.tgz",
+            "version": "2.8.0"
         },
         "@ampproject/toolbox-optimizer": {
-            "integrity": "sha512-zz1cJsQWBvfg2h1ce2/bbgNdSkTjIY7PaF7QhWMzYVcfvdxGSAykA+Ajt+F13H6adNAtIn09s96z/+6pn7XiXQ==",
+            "dependencies": {
+                "terser": {
+                    "integrity": "sha512-fmr7M1f7DBly5cX2+rFDvmGBAaaZyPrHYK4mMdHEDAdNTqXSZgSOfqsfGq2HqPGT/1V0foZZuCZFx8CHKgAk3g==",
+                    "requires": {
+                        "commander": "^2.20.0",
+                        "source-map": "~0.6.1",
+                        "source-map-support": "~0.5.12"
+                    },
+                    "resolved": "https://registry.npmjs.org/terser/-/terser-4.6.7.tgz",
+                    "version": "4.6.7"
+                }
+            },
+            "integrity": "sha512-zroXqrV7mY77+/6hV7kaaWxp4LA85V0B/2vg7WdF+FrwiO9Wior/lIW8UbpRek6INjw0VOp1ED73MmGJkwaDhA==",
             "requires": {
-                "@ampproject/toolbox-core": "^1.0.1",
-                "@ampproject/toolbox-runtime-version": "^1.0.1",
+                "@ampproject/toolbox-core": "^2.0.0",
+                "@ampproject/toolbox-runtime-version": "^2.0.0",
+                "@ampproject/toolbox-script-csp": "^2.0.0",
+                "@ampproject/toolbox-validator-rules": "^2.0.0",
                 "css": "2.2.4",
-                "parse5": "5.1.0",
-                "parse5-htmlparser2-tree-adapter": "5.1.0"
+                "domhandler": "3.0.0",
+                "domutils": "2.0.0",
+                "htmlparser2": "4.1.0",
+                "normalize-html-whitespace": "1.0.0",
+                "terser": "4.6.7"
             },
-            "resolved": "https://registry.npmjs.org/@ampproject/toolbox-optimizer/-/toolbox-optimizer-1.0.1.tgz",
-            "version": "1.0.1"
+            "resolved": "https://registry.npmjs.org/@ampproject/toolbox-optimizer/-/toolbox-optimizer-2.0.1.tgz",
+            "version": "2.0.1"
         },
         "@ampproject/toolbox-runtime-version": {
-            "integrity": "sha512-OFky5rUfP9Hw/NlvEH+/8LqeSZ5DiXY2/RUvWSnY0r0/Uk4ooPyRCWEcVgRF7Y+wY+K1oro5UBZfE9MRYz+hpA==",
+            "integrity": "sha512-vkotDc6S3Q3Xm6LIPzWo2T1+yxvj+bIDrD4SObk6J4SVqilIlPEunLayS602Su+ZXqNC82VjEeD1ARAtc613dQ==",
             "requires": {
-                "@ampproject/toolbox-core": "^1.0.1"
+                "@ampproject/toolbox-core": "^2.8.0"
             },
-            "resolved": "https://registry.npmjs.org/@ampproject/toolbox-runtime-version/-/toolbox-runtime-version-1.0.1.tgz",
-            "version": "1.0.1"
+            "resolved": "https://registry.npmjs.org/@ampproject/toolbox-runtime-version/-/toolbox-runtime-version-2.8.0.tgz",
+            "version": "2.8.0"
+        },
+        "@ampproject/toolbox-script-csp": {
+            "integrity": "sha512-5/ytdTzhmdIyOkcEBskh5ZlLJ8V4bbe+1pY9LZQ8DfWrSOVD1pJ+LtAO/7lmTM+HXxMAKPYDRpvsJc0vvbY0tw==",
+            "resolved": "https://registry.npmjs.org/@ampproject/toolbox-script-csp/-/toolbox-script-csp-2.8.0.tgz",
+            "version": "2.8.0"
+        },
+        "@ampproject/toolbox-validator-rules": {
+            "integrity": "sha512-kbInwnzpEPVZkKigpKFkF/DQ2LsuZ5b8vrEFHjJ4P+meKVQg2QF/UWAQpIMMdjGe1AQBT+DWm91n9UyjgqfnWQ==",
+            "requires": {
+                "cross-fetch": "3.1.2"
+            },
+            "resolved": "https://registry.npmjs.org/@ampproject/toolbox-validator-rules/-/toolbox-validator-rules-2.8.0.tgz",
+            "version": "2.8.0"
         },
         "@babel/code-frame": {
             "integrity": "sha512-27d4lZoomVyo51VegxI20xZPuSHusqbQag/ztrBC7wegWoQ1nLREPVSKSW8byhTlzTKyNE4ifaTA6lCp7JjpFw==",
             "requires": {
                 "@babel/highlight": "^7.0.0"
             },
             "resolved": "https://registry.npmjs.org/@babel/code-frame/-/code-frame-7.5.5.tgz",
             "version": "7.5.5"
         },
         "@babel/core": {
             "dependencies": {
+                "@babel/generator": {
+                    "integrity": "sha512-cYDUpvIzhBVnMzRoY1fkSEhK/HmwEVwlyULYgn/tMQYd6Obag3ylCjONle3gdErfXBW61SVTlR9QR7uWlgeIkg==",
+                    "requires": {
+                        "@babel/types": "^7.14.8",
+                        "jsesc": "^2.5.1",
+                        "source-map": "^0.5.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/generator/-/generator-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "@babel/helper-function-name": {
+                    "integrity": "sha512-Gjna0AsXWfFvrAuX+VKcN/aNNWonizBj39yGwUzVDVTlMYJMK2Wp6xdpy72mfArFq5uK+NOuexfzZlzI1z9+AQ==",
+                    "requires": {
+                        "@babel/helper-get-function-arity": "^7.14.5",
+                        "@babel/template": "^7.14.5",
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-function-name/-/helper-function-name-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-get-function-arity": {
+                    "integrity": "sha512-I1Db4Shst5lewOM4V+ZKJzQ0JGGaZ6VY1jYvMghRjqs6DWgxLCIyFt30GlnKkfUeFLpJt2vzbMVEXVSXlIFYUg==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-get-function-arity/-/helper-get-function-arity-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-split-export-declaration": {
+                    "integrity": "sha512-hprxVPu6e5Kdp2puZUmvOGjaLv9TCe58E/Fl6hRq4YiVQxIcNvuq6uTM2r1mT/oPskuS9CgR+I94sqAYv0NGKA==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-split-export-declaration/-/helper-split-export-declaration-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/highlight": {
+                    "integrity": "sha512-qf9u2WFWVV0MppaL877j2dBtQIDgmidgjGk5VIMw3OadXvYaXn66U1BFlH2t4+t3i+8PhedppRv+i40ABzd+gg==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.5",
+                        "chalk": "^2.0.0",
+                        "js-tokens": "^4.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/highlight/-/highlight-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/parser": {
+                    "integrity": "sha512-syoCQFOoo/fzkWDeM0dLEZi5xqurb5vuyzwIMNZRNun+N/9A4cUZeQaE7dTrB8jGaKuJRBtEOajtnmw0I5hvvA==",
+                    "resolved": "https://registry.npmjs.org/@babel/parser/-/parser-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "@babel/template": {
+                    "dependencies": {
+                        "@babel/code-frame": {
+                            "integrity": "sha512-9pzDqyc6OLDaqe+zbACgFkb6fKMNG6CObKpnYXChRsvYGyEdc7CA2BaqeOM+vOtCS5ndmJicPJhKAwYRI6UfFw==",
+                            "requires": {
+                                "@babel/highlight": "^7.14.5"
+                            },
+                            "resolved": "https://registry.npmjs.org/@babel/code-frame/-/code-frame-7.14.5.tgz",
+                            "version": "7.14.5"
+                        }
+                    },
+                    "integrity": "sha512-6Z3Po85sfxRGachLULUhOmvAaOo7xCvqGQtxINai2mEGPFm6pQ4z5QInFnUrRpfoSV60BnjyF5F3c+15fxFV1g==",
+                    "requires": {
+                        "@babel/code-frame": "^7.14.5",
+                        "@babel/parser": "^7.14.5",
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/template/-/template-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/traverse": {
+                    "dependencies": {
+                        "@babel/code-frame": {
+                            "integrity": "sha512-9pzDqyc6OLDaqe+zbACgFkb6fKMNG6CObKpnYXChRsvYGyEdc7CA2BaqeOM+vOtCS5ndmJicPJhKAwYRI6UfFw==",
+                            "requires": {
+                                "@babel/highlight": "^7.14.5"
+                            },
+                            "resolved": "https://registry.npmjs.org/@babel/code-frame/-/code-frame-7.14.5.tgz",
+                            "version": "7.14.5"
+                        }
+                    },
+                    "integrity": "sha512-kexHhzCljJcFNn1KYAQ6A5wxMRzq9ebYpEDV4+WdNyr3i7O44tanbDOR/xjiG2F3sllan+LgwK+7OMk0EmydHg==",
+                    "requires": {
+                        "@babel/code-frame": "^7.14.5",
+                        "@babel/generator": "^7.14.8",
+                        "@babel/helper-function-name": "^7.14.5",
+                        "@babel/helper-hoist-variables": "^7.14.5",
+                        "@babel/helper-split-export-declaration": "^7.14.5",
+                        "@babel/parser": "^7.14.8",
+                        "@babel/types": "^7.14.8",
+                        "debug": "^4.1.0",
+                        "globals": "^11.1.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/traverse/-/traverse-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
                 "source-map": {
                     "integrity": "sha1-igOdLRAh0i0eoUyA2OpGi6LvP8w=",
                     "resolved": "https://registry.npmjs.org/source-map/-/source-map-0.5.7.tgz",
                     "version": "0.5.7"
                 }
             },
-            "integrity": "sha512-OvjIh6aqXtlsA8ujtGKfC7LYWksYSX8yQcM8Ay3LuvVeQ63lcOKgoZWVqcpFwkd29aYU9rVx7jxhfhiEDV9MZA==",
+            "integrity": "sha512-eeD7VEZKfhK1KUXGiyPFettgF3m513f8FoBSWiQ1xTvl1RAopLs42Wp9+Ze911I6H0N9lNqJMDgoZT7gHsipeQ==",
             "requires": {
-                "@babel/code-frame": "^7.0.0",
-                "@babel/generator": "^7.4.4",
-                "@babel/helpers": "^7.4.4",
-                "@babel/parser": "^7.4.5",
-                "@babel/template": "^7.4.4",
-                "@babel/traverse": "^7.4.5",
-                "@babel/types": "^7.4.4",
-                "convert-source-map": "^1.1.0",
+                "@babel/code-frame": "^7.5.5",
+                "@babel/generator": "^7.7.2",
+                "@babel/helpers": "^7.7.0",
+                "@babel/parser": "^7.7.2",
+                "@babel/template": "^7.7.0",
+                "@babel/traverse": "^7.7.2",
+                "@babel/types": "^7.7.2",
+                "convert-source-map": "^1.7.0",
                 "debug": "^4.1.0",
                 "json5": "^2.1.0",
-                "lodash": "^4.17.11",
+                "lodash": "^4.17.13",
                 "resolve": "^1.3.2",
                 "semver": "^5.4.1",
                 "source-map": "^0.5.0"
             },
-            "resolved": "https://registry.npmjs.org/@babel/core/-/core-7.4.5.tgz",
-            "version": "7.4.5"
+            "resolved": "https://registry.npmjs.org/@babel/core/-/core-7.7.2.tgz",
+            "version": "7.7.2"
         },
         "@babel/generator": {
             "dependencies": {
                 "source-map": {
                     "integrity": "sha1-igOdLRAh0i0eoUyA2OpGi6LvP8w=",
                     "resolved": "https://registry.npmjs.org/source-map/-/source-map-0.5.7.tgz",
                     "version": "0.5.7"
@@ -88,72 +235,170 @@
             "requires": {
                 "@babel/types": "^7.0.0"
             },
             "resolved": "https://registry.npmjs.org/@babel/helper-annotate-as-pure/-/helper-annotate-as-pure-7.0.0.tgz",
             "version": "7.0.0"
         },
         "@babel/helper-builder-binary-assignment-operator-visitor": {
-            "integrity": "sha512-qNSR4jrmJ8M1VMM9tibvyRAHXQs2PmaksQF7c1CGJNipfe3D8p+wgNwgso/P2A2r2mdgBWAXljNWR0QRZAMW8w==",
-            "requires": {
-                "@babel/helper-explode-assignable-expression": "^7.1.0",
-                "@babel/types": "^7.0.0"
-            },
-            "resolved": "https://registry.npmjs.org/@babel/helper-builder-binary-assignment-operator-visitor/-/helper-builder-binary-assignment-operator-visitor-7.1.0.tgz",
-            "version": "7.1.0"
-        },
-        "@babel/helper-builder-react-jsx": {
-            "integrity": "sha512-MjA9KgwCuPEkQd9ncSXvSyJ5y+j2sICHyrI0M3L+6fnS4wMSNDc1ARXsbTfbb2cXHn17VisSnU/sHFTCxVxSMw==",
-            "requires": {
-                "@babel/types": "^7.3.0",
-                "esutils": "^2.0.0"
+            "dependencies": {
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                }
             },
-            "resolved": "https://registry.npmjs.org/@babel/helper-builder-react-jsx/-/helper-builder-react-jsx-7.3.0.tgz",
-            "version": "7.3.0"
-        },
-        "@babel/helper-call-delegate": {
-            "integrity": "sha512-l79boDFJ8S1c5hvQvG+rc+wHw6IuH7YldmRKsYtpbawsxURu/paVy57FZMomGK22/JckepaikOkY0MoAmdyOlQ==",
+            "integrity": "sha512-YTA/Twn0vBXDVGJuAX6PwW7x5zQei1luDDo2Pl6q1qZ7hVNl0RZrhHCQG/ArGpR29Vl7ETiB8eJyrvpuRp300w==",
             "requires": {
-                "@babel/helper-hoist-variables": "^7.4.4",
-                "@babel/traverse": "^7.4.4",
-                "@babel/types": "^7.4.4"
+                "@babel/helper-explode-assignable-expression": "^7.14.5",
+                "@babel/types": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/helper-call-delegate/-/helper-call-delegate-7.4.4.tgz",
-            "version": "7.4.4"
+            "resolved": "https://registry.npmjs.org/@babel/helper-builder-binary-assignment-operator-visitor/-/helper-builder-binary-assignment-operator-visitor-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/helper-create-class-features-plugin": {
-            "integrity": "sha512-ZsxkyYiRA7Bg+ZTRpPvB6AbOFKTFFK4LrvTet8lInm0V468MWCaSYJE+I7v2z2r8KNLtYiV+K5kTCnR7dvyZjg==",
+            "dependencies": {
+                "@babel/code-frame": {
+                    "integrity": "sha512-9pzDqyc6OLDaqe+zbACgFkb6fKMNG6CObKpnYXChRsvYGyEdc7CA2BaqeOM+vOtCS5ndmJicPJhKAwYRI6UfFw==",
+                    "requires": {
+                        "@babel/highlight": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/code-frame/-/code-frame-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-annotate-as-pure": {
+                    "integrity": "sha512-EivH9EgBIb+G8ij1B2jAwSH36WnGvkQSEC6CkX/6v6ZFlw5fVOHvsgGF4uiEHO2GzMvunZb6tDLQEQSdrdocrA==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-annotate-as-pure/-/helper-annotate-as-pure-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-function-name": {
+                    "integrity": "sha512-Gjna0AsXWfFvrAuX+VKcN/aNNWonizBj39yGwUzVDVTlMYJMK2Wp6xdpy72mfArFq5uK+NOuexfzZlzI1z9+AQ==",
+                    "requires": {
+                        "@babel/helper-get-function-arity": "^7.14.5",
+                        "@babel/template": "^7.14.5",
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-function-name/-/helper-function-name-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-get-function-arity": {
+                    "integrity": "sha512-I1Db4Shst5lewOM4V+ZKJzQ0JGGaZ6VY1jYvMghRjqs6DWgxLCIyFt30GlnKkfUeFLpJt2vzbMVEXVSXlIFYUg==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-get-function-arity/-/helper-get-function-arity-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-split-export-declaration": {
+                    "integrity": "sha512-hprxVPu6e5Kdp2puZUmvOGjaLv9TCe58E/Fl6hRq4YiVQxIcNvuq6uTM2r1mT/oPskuS9CgR+I94sqAYv0NGKA==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-split-export-declaration/-/helper-split-export-declaration-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/highlight": {
+                    "integrity": "sha512-qf9u2WFWVV0MppaL877j2dBtQIDgmidgjGk5VIMw3OadXvYaXn66U1BFlH2t4+t3i+8PhedppRv+i40ABzd+gg==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.5",
+                        "chalk": "^2.0.0",
+                        "js-tokens": "^4.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/highlight/-/highlight-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/parser": {
+                    "integrity": "sha512-syoCQFOoo/fzkWDeM0dLEZi5xqurb5vuyzwIMNZRNun+N/9A4cUZeQaE7dTrB8jGaKuJRBtEOajtnmw0I5hvvA==",
+                    "resolved": "https://registry.npmjs.org/@babel/parser/-/parser-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "@babel/template": {
+                    "integrity": "sha512-6Z3Po85sfxRGachLULUhOmvAaOo7xCvqGQtxINai2mEGPFm6pQ4z5QInFnUrRpfoSV60BnjyF5F3c+15fxFV1g==",
+                    "requires": {
+                        "@babel/code-frame": "^7.14.5",
+                        "@babel/parser": "^7.14.5",
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/template/-/template-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                }
+            },
+            "integrity": "sha512-bpYvH8zJBWzeqi1o+co8qOrw+EXzQ/0c74gVmY205AWXy9nifHrOg77y+1zwxX5lXE7Icq4sPlSQ4O2kWBrteQ==",
             "requires": {
-                "@babel/helper-function-name": "^7.1.0",
-                "@babel/helper-member-expression-to-functions": "^7.5.5",
-                "@babel/helper-optimise-call-expression": "^7.0.0",
-                "@babel/helper-plugin-utils": "^7.0.0",
-                "@babel/helper-replace-supers": "^7.5.5",
-                "@babel/helper-split-export-declaration": "^7.4.4"
+                "@babel/helper-annotate-as-pure": "^7.14.5",
+                "@babel/helper-function-name": "^7.14.5",
+                "@babel/helper-member-expression-to-functions": "^7.14.7",
+                "@babel/helper-optimise-call-expression": "^7.14.5",
+                "@babel/helper-replace-supers": "^7.14.5",
+                "@babel/helper-split-export-declaration": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/helper-create-class-features-plugin/-/helper-create-class-features-plugin-7.5.5.tgz",
-            "version": "7.5.5"
+            "resolved": "https://registry.npmjs.org/@babel/helper-create-class-features-plugin/-/helper-create-class-features-plugin-7.14.8.tgz",
+            "version": "7.14.8"
         },
-        "@babel/helper-define-map": {
-            "integrity": "sha512-fTfxx7i0B5NJqvUOBBGREnrqbTxRh7zinBANpZXAVDlsZxYdclDp467G1sQ8VZYMnAURY3RpBUAgOYT9GfzHBg==",
+        "@babel/helper-create-regexp-features-plugin": {
+            "dependencies": {
+                "@babel/helper-annotate-as-pure": {
+                    "integrity": "sha512-EivH9EgBIb+G8ij1B2jAwSH36WnGvkQSEC6CkX/6v6ZFlw5fVOHvsgGF4uiEHO2GzMvunZb6tDLQEQSdrdocrA==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-annotate-as-pure/-/helper-annotate-as-pure-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                }
+            },
+            "integrity": "sha512-TLawwqpOErY2HhWbGJ2nZT5wSkR192QpN+nBg1THfBfftrlvOh+WbhrxXCH4q4xJ9Gl16BGPR/48JA+Ryiho/A==",
             "requires": {
-                "@babel/helper-function-name": "^7.1.0",
-                "@babel/types": "^7.5.5",
-                "lodash": "^4.17.13"
+                "@babel/helper-annotate-as-pure": "^7.14.5",
+                "regexpu-core": "^4.7.1"
             },
-            "resolved": "https://registry.npmjs.org/@babel/helper-define-map/-/helper-define-map-7.5.5.tgz",
-            "version": "7.5.5"
+            "resolved": "https://registry.npmjs.org/@babel/helper-create-regexp-features-plugin/-/helper-create-regexp-features-plugin-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/helper-explode-assignable-expression": {
-            "integrity": "sha512-NRQpfHrJ1msCHtKjbzs9YcMmJZOg6mQMmGRB+hbamEdG5PNpaSm95275VD92DvJKuyl0s2sFiDmMZ+EnnvufqA==",
+            "dependencies": {
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                }
+            },
+            "integrity": "sha512-Htb24gnGJdIGT4vnRKMdoXiOIlqOLmdiUYpAQ0mYfgVT/GDm8GOYhgi4GL+hMKrkiPRohO4ts34ELFsGAPQLDQ==",
             "requires": {
-                "@babel/traverse": "^7.1.0",
-                "@babel/types": "^7.0.0"
+                "@babel/types": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/helper-explode-assignable-expression/-/helper-explode-assignable-expression-7.1.0.tgz",
-            "version": "7.1.0"
+            "resolved": "https://registry.npmjs.org/@babel/helper-explode-assignable-expression/-/helper-explode-assignable-expression-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/helper-function-name": {
             "integrity": "sha512-A95XEoCpb3TO+KZzJ4S/5uW5fNe26DjBGqf1o9ucyLyCmi1dXq/B3c8iaWTfBk3VvetUxl16e8tIrd5teOCfGw==",
             "requires": {
                 "@babel/helper-get-function-arity": "^7.0.0",
                 "@babel/template": "^7.1.0",
                 "@babel/types": "^7.0.0"
@@ -166,131 +411,621 @@
             "requires": {
                 "@babel/types": "^7.0.0"
             },
             "resolved": "https://registry.npmjs.org/@babel/helper-get-function-arity/-/helper-get-function-arity-7.0.0.tgz",
             "version": "7.0.0"
         },
         "@babel/helper-hoist-variables": {
-            "integrity": "sha512-VYk2/H/BnYbZDDg39hr3t2kKyifAm1W6zHRfhx8jGjIHpQEBv9dry7oQ2f3+J703TLu69nYdxsovl0XYfcnK4w==",
+            "dependencies": {
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                }
+            },
+            "integrity": "sha512-R1PXiz31Uc0Vxy4OEOm07x0oSjKAdPPCh3tPivn/Eo8cvz6gveAeuyUUPB21Hoiif0uoPQSSdhIPS3352nvdyQ==",
             "requires": {
-                "@babel/types": "^7.4.4"
+                "@babel/types": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/helper-hoist-variables/-/helper-hoist-variables-7.4.4.tgz",
-            "version": "7.4.4"
+            "resolved": "https://registry.npmjs.org/@babel/helper-hoist-variables/-/helper-hoist-variables-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/helper-member-expression-to-functions": {
-            "integrity": "sha512-5qZ3D1uMclSNqYcXqiHoA0meVdv+xUEex9em2fqMnrk/scphGlGgg66zjMrPJESPwrFJ6sbfFQYUSa0Mz7FabA==",
+            "dependencies": {
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                }
+            },
+            "integrity": "sha512-TMUt4xKxJn6ccjcOW7c4hlwyJArizskAhoSTOCkA0uZ+KghIaci0Qg9R043kUMWI9mtQfgny+NQ5QATnZ+paaA==",
             "requires": {
-                "@babel/types": "^7.5.5"
+                "@babel/types": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/helper-member-expression-to-functions/-/helper-member-expression-to-functions-7.5.5.tgz",
-            "version": "7.5.5"
+            "resolved": "https://registry.npmjs.org/@babel/helper-member-expression-to-functions/-/helper-member-expression-to-functions-7.14.7.tgz",
+            "version": "7.14.7"
         },
         "@babel/helper-module-imports": {
             "integrity": "sha512-aP/hlLq01DWNEiDg4Jn23i+CXxW/owM4WpDLFUbpjxe4NS3BhLVZQ5i7E0ZrxuQ/vwekIeciyamgB1UIYxxM6A==",
             "requires": {
                 "@babel/types": "^7.0.0"
             },
             "resolved": "https://registry.npmjs.org/@babel/helper-module-imports/-/helper-module-imports-7.0.0.tgz",
             "version": "7.0.0"
         },
         "@babel/helper-module-transforms": {
-            "integrity": "sha512-jBeCvETKuJqeiaCdyaheF40aXnnU1+wkSiUs/IQg3tB85up1LyL8x77ClY8qJpuRJUcXQo+ZtdNESmZl4j56Pw==",
+            "dependencies": {
+                "@babel/code-frame": {
+                    "integrity": "sha512-9pzDqyc6OLDaqe+zbACgFkb6fKMNG6CObKpnYXChRsvYGyEdc7CA2BaqeOM+vOtCS5ndmJicPJhKAwYRI6UfFw==",
+                    "requires": {
+                        "@babel/highlight": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/code-frame/-/code-frame-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/generator": {
+                    "integrity": "sha512-cYDUpvIzhBVnMzRoY1fkSEhK/HmwEVwlyULYgn/tMQYd6Obag3ylCjONle3gdErfXBW61SVTlR9QR7uWlgeIkg==",
+                    "requires": {
+                        "@babel/types": "^7.14.8",
+                        "jsesc": "^2.5.1",
+                        "source-map": "^0.5.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/generator/-/generator-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "@babel/helper-function-name": {
+                    "integrity": "sha512-Gjna0AsXWfFvrAuX+VKcN/aNNWonizBj39yGwUzVDVTlMYJMK2Wp6xdpy72mfArFq5uK+NOuexfzZlzI1z9+AQ==",
+                    "requires": {
+                        "@babel/helper-get-function-arity": "^7.14.5",
+                        "@babel/template": "^7.14.5",
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-function-name/-/helper-function-name-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-get-function-arity": {
+                    "integrity": "sha512-I1Db4Shst5lewOM4V+ZKJzQ0JGGaZ6VY1jYvMghRjqs6DWgxLCIyFt30GlnKkfUeFLpJt2vzbMVEXVSXlIFYUg==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-get-function-arity/-/helper-get-function-arity-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-module-imports": {
+                    "integrity": "sha512-SwrNHu5QWS84XlHwGYPDtCxcA0hrSlL2yhWYLgeOc0w7ccOl2qv4s/nARI0aYZW+bSwAL5CukeXA47B/1NKcnQ==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-module-imports/-/helper-module-imports-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-split-export-declaration": {
+                    "integrity": "sha512-hprxVPu6e5Kdp2puZUmvOGjaLv9TCe58E/Fl6hRq4YiVQxIcNvuq6uTM2r1mT/oPskuS9CgR+I94sqAYv0NGKA==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-split-export-declaration/-/helper-split-export-declaration-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/highlight": {
+                    "integrity": "sha512-qf9u2WFWVV0MppaL877j2dBtQIDgmidgjGk5VIMw3OadXvYaXn66U1BFlH2t4+t3i+8PhedppRv+i40ABzd+gg==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.5",
+                        "chalk": "^2.0.0",
+                        "js-tokens": "^4.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/highlight/-/highlight-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/parser": {
+                    "integrity": "sha512-syoCQFOoo/fzkWDeM0dLEZi5xqurb5vuyzwIMNZRNun+N/9A4cUZeQaE7dTrB8jGaKuJRBtEOajtnmw0I5hvvA==",
+                    "resolved": "https://registry.npmjs.org/@babel/parser/-/parser-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "@babel/template": {
+                    "integrity": "sha512-6Z3Po85sfxRGachLULUhOmvAaOo7xCvqGQtxINai2mEGPFm6pQ4z5QInFnUrRpfoSV60BnjyF5F3c+15fxFV1g==",
+                    "requires": {
+                        "@babel/code-frame": "^7.14.5",
+                        "@babel/parser": "^7.14.5",
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/template/-/template-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/traverse": {
+                    "integrity": "sha512-kexHhzCljJcFNn1KYAQ6A5wxMRzq9ebYpEDV4+WdNyr3i7O44tanbDOR/xjiG2F3sllan+LgwK+7OMk0EmydHg==",
+                    "requires": {
+                        "@babel/code-frame": "^7.14.5",
+                        "@babel/generator": "^7.14.8",
+                        "@babel/helper-function-name": "^7.14.5",
+                        "@babel/helper-hoist-variables": "^7.14.5",
+                        "@babel/helper-split-export-declaration": "^7.14.5",
+                        "@babel/parser": "^7.14.8",
+                        "@babel/types": "^7.14.8",
+                        "debug": "^4.1.0",
+                        "globals": "^11.1.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/traverse/-/traverse-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "source-map": {
+                    "integrity": "sha1-igOdLRAh0i0eoUyA2OpGi6LvP8w=",
+                    "resolved": "https://registry.npmjs.org/source-map/-/source-map-0.5.7.tgz",
+                    "version": "0.5.7"
+                }
+            },
+            "integrity": "sha512-RyE+NFOjXn5A9YU1dkpeBaduagTlZ0+fccnIcAGbv1KGUlReBj7utF7oEth8IdIBQPcux0DDgW5MFBH2xu9KcA==",
             "requires": {
-                "@babel/helper-module-imports": "^7.0.0",
-                "@babel/helper-simple-access": "^7.1.0",
-                "@babel/helper-split-export-declaration": "^7.4.4",
-                "@babel/template": "^7.4.4",
-                "@babel/types": "^7.5.5",
-                "lodash": "^4.17.13"
+                "@babel/helper-module-imports": "^7.14.5",
+                "@babel/helper-replace-supers": "^7.14.5",
+                "@babel/helper-simple-access": "^7.14.8",
+                "@babel/helper-split-export-declaration": "^7.14.5",
+                "@babel/helper-validator-identifier": "^7.14.8",
+                "@babel/template": "^7.14.5",
+                "@babel/traverse": "^7.14.8",
+                "@babel/types": "^7.14.8"
             },
-            "resolved": "https://registry.npmjs.org/@babel/helper-module-transforms/-/helper-module-transforms-7.5.5.tgz",
-            "version": "7.5.5"
+            "resolved": "https://registry.npmjs.org/@babel/helper-module-transforms/-/helper-module-transforms-7.14.8.tgz",
+            "version": "7.14.8"
         },
         "@babel/helper-optimise-call-expression": {
-            "integrity": "sha512-u8nd9NQePYNQV8iPWu/pLLYBqZBa4ZaY1YWRFMuxrid94wKI1QNt67NEZ7GAe5Kc/0LLScbim05xZFWkAdrj9g==",
+            "dependencies": {
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                }
+            },
+            "integrity": "sha512-IqiLIrODUOdnPU9/F8ib1Fx2ohlgDhxnIDU7OEVi+kAbEZcyiF7BLU8W6PfvPi9LzztjS7kcbzbmL7oG8kD6VA==",
             "requires": {
-                "@babel/types": "^7.0.0"
+                "@babel/types": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/helper-optimise-call-expression/-/helper-optimise-call-expression-7.0.0.tgz",
-            "version": "7.0.0"
+            "resolved": "https://registry.npmjs.org/@babel/helper-optimise-call-expression/-/helper-optimise-call-expression-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/helper-plugin-utils": {
-            "integrity": "sha512-CYAOUCARwExnEixLdB6sDm2dIJ/YgEAKDM1MOeMeZu9Ld/bDgVo8aiWrXwcY7OBh+1Ea2uUcVRcxKk0GJvW7QA==",
-            "resolved": "https://registry.npmjs.org/@babel/helper-plugin-utils/-/helper-plugin-utils-7.0.0.tgz",
-            "version": "7.0.0"
-        },
-        "@babel/helper-regex": {
-            "integrity": "sha512-CkCYQLkfkiugbRDO8eZn6lRuR8kzZoGXCg3149iTk5se7g6qykSpy3+hELSwquhu+TgHn8nkLiBwHvNX8Hofcw==",
-            "requires": {
-                "lodash": "^4.17.13"
-            },
-            "resolved": "https://registry.npmjs.org/@babel/helper-regex/-/helper-regex-7.5.5.tgz",
-            "version": "7.5.5"
+            "integrity": "sha512-/37qQCE3K0vvZKwoK4XU/irIJQdIfCJuhU5eKnNxpFDsOkgFaUAwbv+RYw6eYgsC0E4hS7r5KqGULUogqui0fQ==",
+            "resolved": "https://registry.npmjs.org/@babel/helper-plugin-utils/-/helper-plugin-utils-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/helper-remap-async-to-generator": {
-            "integrity": "sha512-3fOK0L+Fdlg8S5al8u/hWE6vhufGSn0bN09xm2LXMy//REAF8kDCrYoOBKYmA8m5Nom+sV9LyLCwrFynA8/slg==",
+            "dependencies": {
+                "@babel/helper-annotate-as-pure": {
+                    "integrity": "sha512-EivH9EgBIb+G8ij1B2jAwSH36WnGvkQSEC6CkX/6v6ZFlw5fVOHvsgGF4uiEHO2GzMvunZb6tDLQEQSdrdocrA==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-annotate-as-pure/-/helper-annotate-as-pure-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                }
+            },
+            "integrity": "sha512-rLQKdQU+HYlxBwQIj8dk4/0ENOUEhA/Z0l4hN8BexpvmSMN9oA9EagjnhnDpNsRdWCfjwa4mn/HyBXO9yhQP6A==",
             "requires": {
-                "@babel/helper-annotate-as-pure": "^7.0.0",
-                "@babel/helper-wrap-function": "^7.1.0",
-                "@babel/template": "^7.1.0",
-                "@babel/traverse": "^7.1.0",
-                "@babel/types": "^7.0.0"
+                "@babel/helper-annotate-as-pure": "^7.14.5",
+                "@babel/helper-wrap-function": "^7.14.5",
+                "@babel/types": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/helper-remap-async-to-generator/-/helper-remap-async-to-generator-7.1.0.tgz",
-            "version": "7.1.0"
+            "resolved": "https://registry.npmjs.org/@babel/helper-remap-async-to-generator/-/helper-remap-async-to-generator-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/helper-replace-supers": {
-            "integrity": "sha512-XvRFWrNnlsow2u7jXDuH4jDDctkxbS7gXssrP4q2nUD606ukXHRvydj346wmNg+zAgpFx4MWf4+usfC93bElJg==",
+            "dependencies": {
+                "@babel/code-frame": {
+                    "integrity": "sha512-9pzDqyc6OLDaqe+zbACgFkb6fKMNG6CObKpnYXChRsvYGyEdc7CA2BaqeOM+vOtCS5ndmJicPJhKAwYRI6UfFw==",
+                    "requires": {
+                        "@babel/highlight": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/code-frame/-/code-frame-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/generator": {
+                    "integrity": "sha512-cYDUpvIzhBVnMzRoY1fkSEhK/HmwEVwlyULYgn/tMQYd6Obag3ylCjONle3gdErfXBW61SVTlR9QR7uWlgeIkg==",
+                    "requires": {
+                        "@babel/types": "^7.14.8",
+                        "jsesc": "^2.5.1",
+                        "source-map": "^0.5.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/generator/-/generator-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "@babel/helper-function-name": {
+                    "integrity": "sha512-Gjna0AsXWfFvrAuX+VKcN/aNNWonizBj39yGwUzVDVTlMYJMK2Wp6xdpy72mfArFq5uK+NOuexfzZlzI1z9+AQ==",
+                    "requires": {
+                        "@babel/helper-get-function-arity": "^7.14.5",
+                        "@babel/template": "^7.14.5",
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-function-name/-/helper-function-name-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-get-function-arity": {
+                    "integrity": "sha512-I1Db4Shst5lewOM4V+ZKJzQ0JGGaZ6VY1jYvMghRjqs6DWgxLCIyFt30GlnKkfUeFLpJt2vzbMVEXVSXlIFYUg==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-get-function-arity/-/helper-get-function-arity-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-split-export-declaration": {
+                    "integrity": "sha512-hprxVPu6e5Kdp2puZUmvOGjaLv9TCe58E/Fl6hRq4YiVQxIcNvuq6uTM2r1mT/oPskuS9CgR+I94sqAYv0NGKA==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-split-export-declaration/-/helper-split-export-declaration-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/highlight": {
+                    "integrity": "sha512-qf9u2WFWVV0MppaL877j2dBtQIDgmidgjGk5VIMw3OadXvYaXn66U1BFlH2t4+t3i+8PhedppRv+i40ABzd+gg==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.5",
+                        "chalk": "^2.0.0",
+                        "js-tokens": "^4.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/highlight/-/highlight-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/parser": {
+                    "integrity": "sha512-syoCQFOoo/fzkWDeM0dLEZi5xqurb5vuyzwIMNZRNun+N/9A4cUZeQaE7dTrB8jGaKuJRBtEOajtnmw0I5hvvA==",
+                    "resolved": "https://registry.npmjs.org/@babel/parser/-/parser-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "@babel/template": {
+                    "integrity": "sha512-6Z3Po85sfxRGachLULUhOmvAaOo7xCvqGQtxINai2mEGPFm6pQ4z5QInFnUrRpfoSV60BnjyF5F3c+15fxFV1g==",
+                    "requires": {
+                        "@babel/code-frame": "^7.14.5",
+                        "@babel/parser": "^7.14.5",
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/template/-/template-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/traverse": {
+                    "integrity": "sha512-kexHhzCljJcFNn1KYAQ6A5wxMRzq9ebYpEDV4+WdNyr3i7O44tanbDOR/xjiG2F3sllan+LgwK+7OMk0EmydHg==",
+                    "requires": {
+                        "@babel/code-frame": "^7.14.5",
+                        "@babel/generator": "^7.14.8",
+                        "@babel/helper-function-name": "^7.14.5",
+                        "@babel/helper-hoist-variables": "^7.14.5",
+                        "@babel/helper-split-export-declaration": "^7.14.5",
+                        "@babel/parser": "^7.14.8",
+                        "@babel/types": "^7.14.8",
+                        "debug": "^4.1.0",
+                        "globals": "^11.1.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/traverse/-/traverse-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "source-map": {
+                    "integrity": "sha1-igOdLRAh0i0eoUyA2OpGi6LvP8w=",
+                    "resolved": "https://registry.npmjs.org/source-map/-/source-map-0.5.7.tgz",
+                    "version": "0.5.7"
+                }
+            },
+            "integrity": "sha512-3i1Qe9/8x/hCHINujn+iuHy+mMRLoc77b2nI9TB0zjH1hvn9qGlXjWlggdwUcju36PkPCy/lpM7LLUdcTyH4Ow==",
             "requires": {
-                "@babel/helper-member-expression-to-functions": "^7.5.5",
-                "@babel/helper-optimise-call-expression": "^7.0.0",
-                "@babel/traverse": "^7.5.5",
-                "@babel/types": "^7.5.5"
+                "@babel/helper-member-expression-to-functions": "^7.14.5",
+                "@babel/helper-optimise-call-expression": "^7.14.5",
+                "@babel/traverse": "^7.14.5",
+                "@babel/types": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/helper-replace-supers/-/helper-replace-supers-7.5.5.tgz",
-            "version": "7.5.5"
+            "resolved": "https://registry.npmjs.org/@babel/helper-replace-supers/-/helper-replace-supers-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/helper-simple-access": {
-            "integrity": "sha512-Vk+78hNjRbsiu49zAPALxTb+JUQCz1aolpd8osOF16BGnLtseD21nbHgLPGUwrXEurZgiCOUmvs3ExTu4F5x6w==",
+            "dependencies": {
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                }
+            },
+            "integrity": "sha512-TrFN4RHh9gnWEU+s7JloIho2T76GPwRHhdzOWLqTrMnlas8T9O7ec+oEDNsRXndOmru9ymH9DFrEOxpzPoSbdg==",
             "requires": {
-                "@babel/template": "^7.1.0",
-                "@babel/types": "^7.0.0"
+                "@babel/types": "^7.14.8"
             },
-            "resolved": "https://registry.npmjs.org/@babel/helper-simple-access/-/helper-simple-access-7.1.0.tgz",
-            "version": "7.1.0"
+            "resolved": "https://registry.npmjs.org/@babel/helper-simple-access/-/helper-simple-access-7.14.8.tgz",
+            "version": "7.14.8"
+        },
+        "@babel/helper-skip-transparent-expression-wrappers": {
+            "dependencies": {
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                }
+            },
+            "integrity": "sha512-dmqZB7mrb94PZSAOYtr+ZN5qt5owZIAgqtoTuqiFbHFtxgEcmQlRJVI+bO++fciBunXtB6MK7HrzrfcAzIz2NQ==",
+            "requires": {
+                "@babel/types": "^7.14.5"
+            },
+            "resolved": "https://registry.npmjs.org/@babel/helper-skip-transparent-expression-wrappers/-/helper-skip-transparent-expression-wrappers-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/helper-split-export-declaration": {
             "integrity": "sha512-Ro/XkzLf3JFITkW6b+hNxzZ1n5OQ80NvIUdmHspih1XAhtN3vPTuUFT4eQnela+2MaZ5ulH+iyP513KJrxbN7Q==",
             "requires": {
                 "@babel/types": "^7.4.4"
             },
             "resolved": "https://registry.npmjs.org/@babel/helper-split-export-declaration/-/helper-split-export-declaration-7.4.4.tgz",
             "version": "7.4.4"
         },
+        "@babel/helper-validator-identifier": {
+            "integrity": "sha512-ZGy6/XQjllhYQrNw/3zfWRwZCTVSiBLZ9DHVZxn9n2gip/7ab8mv2TWlKPIBk26RwedCBoWdjLmn+t9na2Gcow==",
+            "resolved": "https://registry.npmjs.org/@babel/helper-validator-identifier/-/helper-validator-identifier-7.14.8.tgz",
+            "version": "7.14.8"
+        },
         "@babel/helper-wrap-function": {
-            "integrity": "sha512-o9fP1BZLLSrYlxYEYyl2aS+Flun5gtjTIG8iln+XuEzQTs0PLagAGSXUcqruJwD5fM48jzIEggCKpIfWTcR7pQ==",
+            "dependencies": {
+                "@babel/code-frame": {
+                    "integrity": "sha512-9pzDqyc6OLDaqe+zbACgFkb6fKMNG6CObKpnYXChRsvYGyEdc7CA2BaqeOM+vOtCS5ndmJicPJhKAwYRI6UfFw==",
+                    "requires": {
+                        "@babel/highlight": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/code-frame/-/code-frame-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/generator": {
+                    "integrity": "sha512-cYDUpvIzhBVnMzRoY1fkSEhK/HmwEVwlyULYgn/tMQYd6Obag3ylCjONle3gdErfXBW61SVTlR9QR7uWlgeIkg==",
+                    "requires": {
+                        "@babel/types": "^7.14.8",
+                        "jsesc": "^2.5.1",
+                        "source-map": "^0.5.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/generator/-/generator-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "@babel/helper-function-name": {
+                    "integrity": "sha512-Gjna0AsXWfFvrAuX+VKcN/aNNWonizBj39yGwUzVDVTlMYJMK2Wp6xdpy72mfArFq5uK+NOuexfzZlzI1z9+AQ==",
+                    "requires": {
+                        "@babel/helper-get-function-arity": "^7.14.5",
+                        "@babel/template": "^7.14.5",
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-function-name/-/helper-function-name-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-get-function-arity": {
+                    "integrity": "sha512-I1Db4Shst5lewOM4V+ZKJzQ0JGGaZ6VY1jYvMghRjqs6DWgxLCIyFt30GlnKkfUeFLpJt2vzbMVEXVSXlIFYUg==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-get-function-arity/-/helper-get-function-arity-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-split-export-declaration": {
+                    "integrity": "sha512-hprxVPu6e5Kdp2puZUmvOGjaLv9TCe58E/Fl6hRq4YiVQxIcNvuq6uTM2r1mT/oPskuS9CgR+I94sqAYv0NGKA==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-split-export-declaration/-/helper-split-export-declaration-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/highlight": {
+                    "integrity": "sha512-qf9u2WFWVV0MppaL877j2dBtQIDgmidgjGk5VIMw3OadXvYaXn66U1BFlH2t4+t3i+8PhedppRv+i40ABzd+gg==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.5",
+                        "chalk": "^2.0.0",
+                        "js-tokens": "^4.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/highlight/-/highlight-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/parser": {
+                    "integrity": "sha512-syoCQFOoo/fzkWDeM0dLEZi5xqurb5vuyzwIMNZRNun+N/9A4cUZeQaE7dTrB8jGaKuJRBtEOajtnmw0I5hvvA==",
+                    "resolved": "https://registry.npmjs.org/@babel/parser/-/parser-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "@babel/template": {
+                    "integrity": "sha512-6Z3Po85sfxRGachLULUhOmvAaOo7xCvqGQtxINai2mEGPFm6pQ4z5QInFnUrRpfoSV60BnjyF5F3c+15fxFV1g==",
+                    "requires": {
+                        "@babel/code-frame": "^7.14.5",
+                        "@babel/parser": "^7.14.5",
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/template/-/template-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/traverse": {
+                    "integrity": "sha512-kexHhzCljJcFNn1KYAQ6A5wxMRzq9ebYpEDV4+WdNyr3i7O44tanbDOR/xjiG2F3sllan+LgwK+7OMk0EmydHg==",
+                    "requires": {
+                        "@babel/code-frame": "^7.14.5",
+                        "@babel/generator": "^7.14.8",
+                        "@babel/helper-function-name": "^7.14.5",
+                        "@babel/helper-hoist-variables": "^7.14.5",
+                        "@babel/helper-split-export-declaration": "^7.14.5",
+                        "@babel/parser": "^7.14.8",
+                        "@babel/types": "^7.14.8",
+                        "debug": "^4.1.0",
+                        "globals": "^11.1.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/traverse/-/traverse-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "source-map": {
+                    "integrity": "sha1-igOdLRAh0i0eoUyA2OpGi6LvP8w=",
+                    "resolved": "https://registry.npmjs.org/source-map/-/source-map-0.5.7.tgz",
+                    "version": "0.5.7"
+                }
+            },
+            "integrity": "sha512-YEdjTCq+LNuNS1WfxsDCNpgXkJaIyqco6DAelTUjT4f2KIWC1nBcaCaSdHTBqQVLnTBexBcVcFhLSU1KnYuePQ==",
             "requires": {
-                "@babel/helper-function-name": "^7.1.0",
-                "@babel/template": "^7.1.0",
-                "@babel/traverse": "^7.1.0",
-                "@babel/types": "^7.2.0"
+                "@babel/helper-function-name": "^7.14.5",
+                "@babel/template": "^7.14.5",
+                "@babel/traverse": "^7.14.5",
+                "@babel/types": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/helper-wrap-function/-/helper-wrap-function-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/helper-wrap-function/-/helper-wrap-function-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/helpers": {
-            "integrity": "sha512-nRq2BUhxZFnfEn/ciJuhklHvFOqjJUD5wpx+1bxUF2axL9C+v4DE/dmp5sT2dKnpOs4orZWzpAZqlCy8QqE/7g==",
+            "dependencies": {
+                "@babel/code-frame": {
+                    "integrity": "sha512-9pzDqyc6OLDaqe+zbACgFkb6fKMNG6CObKpnYXChRsvYGyEdc7CA2BaqeOM+vOtCS5ndmJicPJhKAwYRI6UfFw==",
+                    "requires": {
+                        "@babel/highlight": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/code-frame/-/code-frame-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/generator": {
+                    "integrity": "sha512-cYDUpvIzhBVnMzRoY1fkSEhK/HmwEVwlyULYgn/tMQYd6Obag3ylCjONle3gdErfXBW61SVTlR9QR7uWlgeIkg==",
+                    "requires": {
+                        "@babel/types": "^7.14.8",
+                        "jsesc": "^2.5.1",
+                        "source-map": "^0.5.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/generator/-/generator-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "@babel/helper-function-name": {
+                    "integrity": "sha512-Gjna0AsXWfFvrAuX+VKcN/aNNWonizBj39yGwUzVDVTlMYJMK2Wp6xdpy72mfArFq5uK+NOuexfzZlzI1z9+AQ==",
+                    "requires": {
+                        "@babel/helper-get-function-arity": "^7.14.5",
+                        "@babel/template": "^7.14.5",
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-function-name/-/helper-function-name-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-get-function-arity": {
+                    "integrity": "sha512-I1Db4Shst5lewOM4V+ZKJzQ0JGGaZ6VY1jYvMghRjqs6DWgxLCIyFt30GlnKkfUeFLpJt2vzbMVEXVSXlIFYUg==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-get-function-arity/-/helper-get-function-arity-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-split-export-declaration": {
+                    "integrity": "sha512-hprxVPu6e5Kdp2puZUmvOGjaLv9TCe58E/Fl6hRq4YiVQxIcNvuq6uTM2r1mT/oPskuS9CgR+I94sqAYv0NGKA==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-split-export-declaration/-/helper-split-export-declaration-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/highlight": {
+                    "integrity": "sha512-qf9u2WFWVV0MppaL877j2dBtQIDgmidgjGk5VIMw3OadXvYaXn66U1BFlH2t4+t3i+8PhedppRv+i40ABzd+gg==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.5",
+                        "chalk": "^2.0.0",
+                        "js-tokens": "^4.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/highlight/-/highlight-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/parser": {
+                    "integrity": "sha512-syoCQFOoo/fzkWDeM0dLEZi5xqurb5vuyzwIMNZRNun+N/9A4cUZeQaE7dTrB8jGaKuJRBtEOajtnmw0I5hvvA==",
+                    "resolved": "https://registry.npmjs.org/@babel/parser/-/parser-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "@babel/template": {
+                    "integrity": "sha512-6Z3Po85sfxRGachLULUhOmvAaOo7xCvqGQtxINai2mEGPFm6pQ4z5QInFnUrRpfoSV60BnjyF5F3c+15fxFV1g==",
+                    "requires": {
+                        "@babel/code-frame": "^7.14.5",
+                        "@babel/parser": "^7.14.5",
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/template/-/template-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/traverse": {
+                    "integrity": "sha512-kexHhzCljJcFNn1KYAQ6A5wxMRzq9ebYpEDV4+WdNyr3i7O44tanbDOR/xjiG2F3sllan+LgwK+7OMk0EmydHg==",
+                    "requires": {
+                        "@babel/code-frame": "^7.14.5",
+                        "@babel/generator": "^7.14.8",
+                        "@babel/helper-function-name": "^7.14.5",
+                        "@babel/helper-hoist-variables": "^7.14.5",
+                        "@babel/helper-split-export-declaration": "^7.14.5",
+                        "@babel/parser": "^7.14.8",
+                        "@babel/types": "^7.14.8",
+                        "debug": "^4.1.0",
+                        "globals": "^11.1.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/traverse/-/traverse-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "source-map": {
+                    "integrity": "sha1-igOdLRAh0i0eoUyA2OpGi6LvP8w=",
+                    "resolved": "https://registry.npmjs.org/source-map/-/source-map-0.5.7.tgz",
+                    "version": "0.5.7"
+                }
+            },
+            "integrity": "sha512-ZRDmI56pnV+p1dH6d+UN6GINGz7Krps3+270qqI9UJ4wxYThfAIcI5i7j5vXC4FJ3Wap+S9qcebxeYiqn87DZw==",
             "requires": {
-                "@babel/template": "^7.4.4",
-                "@babel/traverse": "^7.5.5",
-                "@babel/types": "^7.5.5"
+                "@babel/template": "^7.14.5",
+                "@babel/traverse": "^7.14.8",
+                "@babel/types": "^7.14.8"
             },
-            "resolved": "https://registry.npmjs.org/@babel/helpers/-/helpers-7.5.5.tgz",
-            "version": "7.5.5"
+            "resolved": "https://registry.npmjs.org/@babel/helpers/-/helpers-7.14.8.tgz",
+            "version": "7.14.8"
         },
         "@babel/highlight": {
             "integrity": "sha512-7dV4eu9gBxoM0dAnj/BCFDW9LFU0zvTrkq0ugM7pnHEgguOEeOz1so2ZghEdzviYzQEED0r4EAgpsBChKy1TRQ==",
             "requires": {
                 "chalk": "^2.0.0",
                 "esutils": "^2.0.2",
                 "js-tokens": "^4.0.0"
@@ -300,550 +1035,842 @@
         },
         "@babel/parser": {
             "integrity": "sha512-E5BN68cqR7dhKan1SfqgPGhQ178bkVKpXTPEXnFJBrEt8/DKRZlybmy+IgYLTeN7tp1R5Ccmbm2rBk17sHYU3g==",
             "resolved": "https://registry.npmjs.org/@babel/parser/-/parser-7.5.5.tgz",
             "version": "7.5.5"
         },
         "@babel/plugin-proposal-async-generator-functions": {
-            "integrity": "sha512-+Dfo/SCQqrwx48ptLVGLdE39YtWRuKc/Y9I5Fy0P1DDBB9lsAHpjcEJQt+4IifuSOSTLBKJObJqMvaO1pIE8LQ==",
+            "integrity": "sha512-RK8Wj7lXLY3bqei69/cc25gwS5puEc3dknoFPFbqfy3XxYQBQFvu4ioWpafMBAB+L9NyptQK4nMOa5Xz16og8Q==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0",
-                "@babel/helper-remap-async-to-generator": "^7.1.0",
-                "@babel/plugin-syntax-async-generators": "^7.2.0"
+                "@babel/helper-plugin-utils": "^7.14.5",
+                "@babel/helper-remap-async-to-generator": "^7.14.5",
+                "@babel/plugin-syntax-async-generators": "^7.8.4"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-proposal-async-generator-functions/-/plugin-proposal-async-generator-functions-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-proposal-async-generator-functions/-/plugin-proposal-async-generator-functions-7.14.7.tgz",
+            "version": "7.14.7"
         },
         "@babel/plugin-proposal-class-properties": {
-            "integrity": "sha512-WjKTI8g8d5w1Bc9zgwSz2nfrsNQsXcCf9J9cdCvrJV6RF56yztwm4TmJC0MgJ9tvwO9gUA/mcYe89bLdGfiXFg==",
+            "integrity": "sha512-tufDcFA1Vj+eWvwHN+jvMN6QsV5o+vUlytNKrbMiCeDL0F2j92RURzUsUMWE5EJkLyWxjdUslCsMQa9FWth16A==",
             "requires": {
-                "@babel/helper-create-class-features-plugin": "^7.4.4",
+                "@babel/helper-create-class-features-plugin": "^7.7.0",
                 "@babel/helper-plugin-utils": "^7.0.0"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-proposal-class-properties/-/plugin-proposal-class-properties-7.4.4.tgz",
-            "version": "7.4.4"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-proposal-class-properties/-/plugin-proposal-class-properties-7.7.0.tgz",
+            "version": "7.7.0"
+        },
+        "@babel/plugin-proposal-dynamic-import": {
+            "dependencies": {
+                "@babel/plugin-syntax-dynamic-import": {
+                    "integrity": "sha512-5gdGbFon+PszYzqs83S3E5mpi7/y/8M9eC90MRTZfduQOYW76ig6SOSPNe41IG5LoP3FGBn2N0RjVDSQiS94kQ==",
+                    "requires": {
+                        "@babel/helper-plugin-utils": "^7.8.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-dynamic-import/-/plugin-syntax-dynamic-import-7.8.3.tgz",
+                    "version": "7.8.3"
+                }
+            },
+            "integrity": "sha512-ExjiNYc3HDN5PXJx+bwC50GIx/KKanX2HiggnIUAYedbARdImiCU4RhhHfdf0Kd7JNXGpsBBBCOm+bBVy3Gb0g==",
+            "requires": {
+                "@babel/helper-plugin-utils": "^7.14.5",
+                "@babel/plugin-syntax-dynamic-import": "^7.8.3"
+            },
+            "resolved": "https://registry.npmjs.org/@babel/plugin-proposal-dynamic-import/-/plugin-proposal-dynamic-import-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-proposal-json-strings": {
-            "integrity": "sha512-MAFV1CA/YVmYwZG0fBQyXhmj0BHCB5egZHCKWIFVv/XCxAeVGIHfos3SwDck4LvCllENIAg7xMKOG5kH0dzyUg==",
+            "integrity": "sha512-NSq2fczJYKVRIsUJyNxrVUMhB27zb7N7pOFGQOhBKJrChbGcgEAqyZrmZswkPk18VMurEeJAaICbfm57vUeTbQ==",
+            "requires": {
+                "@babel/helper-plugin-utils": "^7.14.5",
+                "@babel/plugin-syntax-json-strings": "^7.8.3"
+            },
+            "resolved": "https://registry.npmjs.org/@babel/plugin-proposal-json-strings/-/plugin-proposal-json-strings-7.14.5.tgz",
+            "version": "7.14.5"
+        },
+        "@babel/plugin-proposal-nullish-coalescing-operator": {
+            "integrity": "sha512-TbYHmr1Gl1UC7Vo2HVuj/Naci5BEGNZ0AJhzqD2Vpr6QPFWpUmBRLrIDjedzx7/CShq0bRDS2gI4FIs77VHLVQ==",
             "requires": {
                 "@babel/helper-plugin-utils": "^7.0.0",
-                "@babel/plugin-syntax-json-strings": "^7.2.0"
+                "@babel/plugin-syntax-nullish-coalescing-operator": "^7.7.4"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-proposal-json-strings/-/plugin-proposal-json-strings-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-proposal-nullish-coalescing-operator/-/plugin-proposal-nullish-coalescing-operator-7.7.4.tgz",
+            "version": "7.7.4"
+        },
+        "@babel/plugin-proposal-numeric-separator": {
+            "integrity": "sha512-jWioO1s6R/R+wEHizfaScNsAx+xKgwTLNXSh7tTC4Usj3ItsPEhYkEpU4h+lpnBwq7NBVOJXfO6cRFYcX69JUQ==",
+            "requires": {
+                "@babel/helper-plugin-utils": "^7.8.3",
+                "@babel/plugin-syntax-numeric-separator": "^7.8.3"
+            },
+            "resolved": "https://registry.npmjs.org/@babel/plugin-proposal-numeric-separator/-/plugin-proposal-numeric-separator-7.8.3.tgz",
+            "version": "7.8.3"
         },
         "@babel/plugin-proposal-object-rest-spread": {
-            "integrity": "sha512-dMBG6cSPBbHeEBdFXeQ2QLc5gUpg4Vkaz8octD4aoW/ISO+jBOcsuxYL7bsb5WSu8RLP6boxrBIALEHgoHtO9g==",
+            "integrity": "sha512-LDBXlmADCsMZV1Y9OQwMc0MyGZ8Ta/zlD9N67BfQT8uYwkRswiu2hU6nJKrjrt/58aH/vqfQlR/9yId/7A2gWw==",
             "requires": {
                 "@babel/helper-plugin-utils": "^7.0.0",
                 "@babel/plugin-syntax-object-rest-spread": "^7.2.0"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-proposal-object-rest-spread/-/plugin-proposal-object-rest-spread-7.4.4.tgz",
-            "version": "7.4.4"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-proposal-object-rest-spread/-/plugin-proposal-object-rest-spread-7.6.2.tgz",
+            "version": "7.6.2"
         },
         "@babel/plugin-proposal-optional-catch-binding": {
-            "integrity": "sha512-mgYj3jCcxug6KUcX4OBoOJz3CMrwRfQELPQ5560F70YQUBZB7uac9fqaWamKR1iWUzGiK2t0ygzjTScZnVz75g==",
+            "integrity": "sha512-3Oyiixm0ur7bzO5ybNcZFlmVsygSIQgdOa7cTfOYCMY+wEPAYhZAJxi3mixKFCTCKUhQXuCTtQ1MzrpL3WT8ZQ==",
+            "requires": {
+                "@babel/helper-plugin-utils": "^7.14.5",
+                "@babel/plugin-syntax-optional-catch-binding": "^7.8.3"
+            },
+            "resolved": "https://registry.npmjs.org/@babel/plugin-proposal-optional-catch-binding/-/plugin-proposal-optional-catch-binding-7.14.5.tgz",
+            "version": "7.14.5"
+        },
+        "@babel/plugin-proposal-optional-chaining": {
+            "integrity": "sha512-JmgaS+ygAWDR/STPe3/7y0lNlHgS+19qZ9aC06nYLwQ/XB7c0q5Xs+ksFU3EDnp9EiEsO0dnRAOKeyLHTZuW3A==",
             "requires": {
                 "@babel/helper-plugin-utils": "^7.0.0",
-                "@babel/plugin-syntax-optional-catch-binding": "^7.2.0"
+                "@babel/plugin-syntax-optional-chaining": "^7.7.4"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-proposal-optional-catch-binding/-/plugin-proposal-optional-catch-binding-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-proposal-optional-chaining/-/plugin-proposal-optional-chaining-7.7.4.tgz",
+            "version": "7.7.4"
         },
         "@babel/plugin-proposal-unicode-property-regex": {
-            "integrity": "sha512-j1NwnOqMG9mFUOH58JTFsA/+ZYzQLUZ/drqWUqxCYLGeu2JFZL8YrNC9hBxKmWtAuOCHPcRpgv7fhap09Fb4kA==",
+            "integrity": "sha512-6axIeOU5LnY471KenAB9vI8I5j7NQ2d652hIYwVyRfgaZT5UpiqFKCuVXCDMSrU+3VFafnu2c5m3lrWIlr6A5Q==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0",
-                "@babel/helper-regex": "^7.4.4",
-                "regexpu-core": "^4.5.4"
+                "@babel/helper-create-regexp-features-plugin": "^7.14.5",
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-proposal-unicode-property-regex/-/plugin-proposal-unicode-property-regex-7.4.4.tgz",
-            "version": "7.4.4"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-proposal-unicode-property-regex/-/plugin-proposal-unicode-property-regex-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-syntax-async-generators": {
-            "integrity": "sha512-1ZrIRBv2t0GSlcwVoQ6VgSLpLgiN/FVQUzt9znxo7v2Ov4jJrs8RY8tv0wvDmFN3qIdMKWrmMMW6yZ0G19MfGg==",
+            "integrity": "sha512-tycmZxkGfZaxhMRbXlPXuVFpdWlXpir2W4AMhSJgRKzk/eDlIXOhb2LHWoLpDF7TEHylV5zNhykX6KAgHJmTNw==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.8.0"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-async-generators/-/plugin-syntax-async-generators-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-async-generators/-/plugin-syntax-async-generators-7.8.4.tgz",
+            "version": "7.8.4"
+        },
+        "@babel/plugin-syntax-bigint": {
+            "integrity": "sha512-wnTnFlG+YxQm3vDxpGE57Pj0srRU4sHE/mDkt1qv2YJJSeUAec2ma4WLUnUPeKjyrfntVwe/N6dCXpU+zL3Npg==",
+            "requires": {
+                "@babel/helper-plugin-utils": "^7.8.0"
+            },
+            "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-bigint/-/plugin-syntax-bigint-7.8.3.tgz",
+            "version": "7.8.3"
         },
         "@babel/plugin-syntax-dynamic-import": {
             "integrity": "sha512-mVxuJ0YroI/h/tbFTPGZR8cv6ai+STMKNBq0f8hFxsxWjl94qqhsb+wXbpNMDPU3cfR1TIsVFzU3nXyZMqyK4w==",
             "requires": {
                 "@babel/helper-plugin-utils": "^7.0.0"
             },
             "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-dynamic-import/-/plugin-syntax-dynamic-import-7.2.0.tgz",
             "version": "7.2.0"
         },
         "@babel/plugin-syntax-json-strings": {
-            "integrity": "sha512-5UGYnMSLRE1dqqZwug+1LISpA403HzlSfsg6P9VXU6TBjcSHeNlw4DxDx7LgpF+iKZoOG/+uzqoRHTdcUpiZNg==",
+            "integrity": "sha512-lY6kdGpWHvjoe2vk4WrAapEuBR69EMxZl+RoGRhrFGNYVK8mOPAW8VfbT/ZgrFbXlDNiiaxQnAtgVCZ6jv30EA==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.8.0"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-json-strings/-/plugin-syntax-json-strings-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-json-strings/-/plugin-syntax-json-strings-7.8.3.tgz",
+            "version": "7.8.3"
         },
         "@babel/plugin-syntax-jsx": {
-            "integrity": "sha512-VyN4QANJkRW6lDBmENzRszvZf3/4AXaj9YR7GwrWeeN9tEBPuXbmDYVU9bYBN0D70zCWVwUy0HWq2553VCb6Hw==",
+            "integrity": "sha512-ohuFIsOMXJnbOMRfX7/w7LocdR6R7whhuRD4ax8IipLcLPlZGJKkBxgHp++U4N/vKyU16/YDQr2f5seajD3jIw==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-jsx/-/plugin-syntax-jsx-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-jsx/-/plugin-syntax-jsx-7.14.5.tgz",
+            "version": "7.14.5"
+        },
+        "@babel/plugin-syntax-nullish-coalescing-operator": {
+            "integrity": "sha512-aSff4zPII1u2QD7y+F8oDsz19ew4IGEJg9SVW+bqwpwtfFleiQDMdzA/R+UlWDzfnHFCxxleFT0PMIrR36XLNQ==",
+            "requires": {
+                "@babel/helper-plugin-utils": "^7.8.0"
+            },
+            "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-nullish-coalescing-operator/-/plugin-syntax-nullish-coalescing-operator-7.8.3.tgz",
+            "version": "7.8.3"
+        },
+        "@babel/plugin-syntax-numeric-separator": {
+            "integrity": "sha512-9H6YdfkcK/uOnY/K7/aA2xpzaAgkQn37yzWUMRK7OaPOqOpGS1+n0H5hxT9AUw9EsSjPW8SVyMJwYRtWs3X3ug==",
+            "requires": {
+                "@babel/helper-plugin-utils": "^7.10.4"
+            },
+            "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-numeric-separator/-/plugin-syntax-numeric-separator-7.10.4.tgz",
+            "version": "7.10.4"
         },
         "@babel/plugin-syntax-object-rest-spread": {
-            "integrity": "sha512-t0JKGgqk2We+9may3t0xDdmneaXmyxq0xieYcKHxIsrJO64n1OiMWNUtc5gQK1PA0NpdCRrtZp4z+IUaKugrSA==",
+            "integrity": "sha512-XoqMijGZb9y3y2XskN+P1wUGiVwWZ5JmoDRwx5+3GmEplNyVM2s2Dg8ILFQm8rWM48orGy5YpI5Bl8U1y7ydlA==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.8.0"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-object-rest-spread/-/plugin-syntax-object-rest-spread-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-object-rest-spread/-/plugin-syntax-object-rest-spread-7.8.3.tgz",
+            "version": "7.8.3"
         },
         "@babel/plugin-syntax-optional-catch-binding": {
-            "integrity": "sha512-bDe4xKNhb0LI7IvZHiA13kff0KEfaGX/Hv4lMA9+7TEc63hMNvfKo6ZFpXhKuEp+II/q35Gc4NoMeDZyaUbj9w==",
+            "integrity": "sha512-6VPD0Pc1lpTqw0aKoeRTMiB+kWhAoT24PA+ksWSBrFtl5SIRVpZlwN3NNPQjehA2E/91FV3RjLWoVTglWcSV3Q==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.8.0"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-optional-catch-binding/-/plugin-syntax-optional-catch-binding-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-optional-catch-binding/-/plugin-syntax-optional-catch-binding-7.8.3.tgz",
+            "version": "7.8.3"
+        },
+        "@babel/plugin-syntax-optional-chaining": {
+            "integrity": "sha512-KoK9ErH1MBlCPxV0VANkXW2/dw4vlbGDrFgz8bmUsBGYkFRcbRwMh6cIJubdPrkxRwuGdtCk0v/wPTKbQgBjkg==",
+            "requires": {
+                "@babel/helper-plugin-utils": "^7.8.0"
+            },
+            "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-optional-chaining/-/plugin-syntax-optional-chaining-7.8.3.tgz",
+            "version": "7.8.3"
+        },
+        "@babel/plugin-syntax-top-level-await": {
+            "integrity": "sha512-hx++upLv5U1rgYfwe1xBQUhRmU41NEvpUvrp8jkrSCdvGSnM5/qdRMtylJ6PG5OFkBaHkbTAKTnd3/YyESRHFw==",
+            "requires": {
+                "@babel/helper-plugin-utils": "^7.14.5"
+            },
+            "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-top-level-await/-/plugin-syntax-top-level-await-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-syntax-typescript": {
-            "integrity": "sha512-dGwbSMA1YhVS8+31CnPR7LB4pcbrzcV99wQzby4uAfrkZPYZlQ7ImwdpzLqi6Z6IL02b8IAL379CaMwo0x5Lag==",
+            "integrity": "sha512-u6OXzDaIXjEstBRRoBCQ/uKQKlbuaeE5in0RvWdA4pN6AhqxTIwUsnHPU1CFZA/amYObMsuWhYfRl3Ch90HD0Q==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-typescript/-/plugin-syntax-typescript-7.3.3.tgz",
-            "version": "7.3.3"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-syntax-typescript/-/plugin-syntax-typescript-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-arrow-functions": {
-            "integrity": "sha512-ER77Cax1+8/8jCB9fo4Ud161OZzWN5qawi4GusDuRLcDbDG+bIGYY20zb2dfAFdTRGzrfq2xZPvF0R64EHnimg==",
+            "integrity": "sha512-KOnO0l4+tD5IfOdi4x8C1XmEIRWUjNRV8wc6K2vz/3e8yAOoZZvsRXRRIF/yo/MAOFb4QjtAw9xSxMXbSMRy8A==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-arrow-functions/-/plugin-transform-arrow-functions-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-arrow-functions/-/plugin-transform-arrow-functions-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-async-to-generator": {
-            "integrity": "sha512-mqvkzwIGkq0bEF1zLRRiTdjfomZJDV33AH3oQzHVGkI2VzEmXLpKKOBvEVaFZBJdN0XTyH38s9j/Kiqr68dggg==",
+            "dependencies": {
+                "@babel/helper-module-imports": {
+                    "integrity": "sha512-SwrNHu5QWS84XlHwGYPDtCxcA0hrSlL2yhWYLgeOc0w7ccOl2qv4s/nARI0aYZW+bSwAL5CukeXA47B/1NKcnQ==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-module-imports/-/helper-module-imports-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                }
+            },
+            "integrity": "sha512-szkbzQ0mNk0rpu76fzDdqSyPu0MuvpXgC+6rz5rpMb5OIRxdmHfQxrktL8CYolL2d8luMCZTR0DpIMIdL27IjA==",
             "requires": {
-                "@babel/helper-module-imports": "^7.0.0",
-                "@babel/helper-plugin-utils": "^7.0.0",
-                "@babel/helper-remap-async-to-generator": "^7.1.0"
+                "@babel/helper-module-imports": "^7.14.5",
+                "@babel/helper-plugin-utils": "^7.14.5",
+                "@babel/helper-remap-async-to-generator": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-async-to-generator/-/plugin-transform-async-to-generator-7.5.0.tgz",
-            "version": "7.5.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-async-to-generator/-/plugin-transform-async-to-generator-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-block-scoped-functions": {
-            "integrity": "sha512-ntQPR6q1/NKuphly49+QiQiTN0O63uOwjdD6dhIjSWBI5xlrbUFh720TIpzBhpnrLfv2tNH/BXvLIab1+BAI0w==",
+            "integrity": "sha512-dtqWqdWZ5NqBX3KzsVCWfQI3A53Ft5pWFCT2eCVUftWZgjc5DpDponbIF1+c+7cSGk2wN0YK7HGL/ezfRbpKBQ==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-block-scoped-functions/-/plugin-transform-block-scoped-functions-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-block-scoped-functions/-/plugin-transform-block-scoped-functions-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-block-scoping": {
-            "integrity": "sha512-82A3CLRRdYubkG85lKwhZB0WZoHxLGsJdux/cOVaJCJpvYFl1LVzAIFyRsa7CvXqW8rBM4Zf3Bfn8PHt5DP0Sg==",
+            "integrity": "sha512-LBYm4ZocNgoCqyxMLoOnwpsmQ18HWTQvql64t3GvMUzLQrNoV1BDG0lNftC8QKYERkZgCCT/7J5xWGObGAyHDw==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0",
-                "lodash": "^4.17.13"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-block-scoping/-/plugin-transform-block-scoping-7.5.5.tgz",
-            "version": "7.5.5"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-block-scoping/-/plugin-transform-block-scoping-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-classes": {
-            "integrity": "sha512-U2htCNK/6e9K7jGyJ++1p5XRU+LJjrwtoiVn9SzRlDT2KubcZ11OOwy3s24TjHxPgxNwonCYP7U2K51uVYCMDg==",
+            "dependencies": {
+                "@babel/code-frame": {
+                    "integrity": "sha512-9pzDqyc6OLDaqe+zbACgFkb6fKMNG6CObKpnYXChRsvYGyEdc7CA2BaqeOM+vOtCS5ndmJicPJhKAwYRI6UfFw==",
+                    "requires": {
+                        "@babel/highlight": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/code-frame/-/code-frame-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-annotate-as-pure": {
+                    "integrity": "sha512-EivH9EgBIb+G8ij1B2jAwSH36WnGvkQSEC6CkX/6v6ZFlw5fVOHvsgGF4uiEHO2GzMvunZb6tDLQEQSdrdocrA==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-annotate-as-pure/-/helper-annotate-as-pure-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-function-name": {
+                    "integrity": "sha512-Gjna0AsXWfFvrAuX+VKcN/aNNWonizBj39yGwUzVDVTlMYJMK2Wp6xdpy72mfArFq5uK+NOuexfzZlzI1z9+AQ==",
+                    "requires": {
+                        "@babel/helper-get-function-arity": "^7.14.5",
+                        "@babel/template": "^7.14.5",
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-function-name/-/helper-function-name-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-get-function-arity": {
+                    "integrity": "sha512-I1Db4Shst5lewOM4V+ZKJzQ0JGGaZ6VY1jYvMghRjqs6DWgxLCIyFt30GlnKkfUeFLpJt2vzbMVEXVSXlIFYUg==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-get-function-arity/-/helper-get-function-arity-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-split-export-declaration": {
+                    "integrity": "sha512-hprxVPu6e5Kdp2puZUmvOGjaLv9TCe58E/Fl6hRq4YiVQxIcNvuq6uTM2r1mT/oPskuS9CgR+I94sqAYv0NGKA==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-split-export-declaration/-/helper-split-export-declaration-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/highlight": {
+                    "integrity": "sha512-qf9u2WFWVV0MppaL877j2dBtQIDgmidgjGk5VIMw3OadXvYaXn66U1BFlH2t4+t3i+8PhedppRv+i40ABzd+gg==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.5",
+                        "chalk": "^2.0.0",
+                        "js-tokens": "^4.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/highlight/-/highlight-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/parser": {
+                    "integrity": "sha512-syoCQFOoo/fzkWDeM0dLEZi5xqurb5vuyzwIMNZRNun+N/9A4cUZeQaE7dTrB8jGaKuJRBtEOajtnmw0I5hvvA==",
+                    "resolved": "https://registry.npmjs.org/@babel/parser/-/parser-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "@babel/template": {
+                    "integrity": "sha512-6Z3Po85sfxRGachLULUhOmvAaOo7xCvqGQtxINai2mEGPFm6pQ4z5QInFnUrRpfoSV60BnjyF5F3c+15fxFV1g==",
+                    "requires": {
+                        "@babel/code-frame": "^7.14.5",
+                        "@babel/parser": "^7.14.5",
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/template/-/template-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                }
+            },
+            "integrity": "sha512-J4VxKAMykM06K/64z9rwiL6xnBHgB1+FVspqvlgCdwD1KUbQNfszeKVVOMh59w3sztHYIZDgnhOC4WbdEfHFDA==",
             "requires": {
-                "@babel/helper-annotate-as-pure": "^7.0.0",
-                "@babel/helper-define-map": "^7.5.5",
-                "@babel/helper-function-name": "^7.1.0",
-                "@babel/helper-optimise-call-expression": "^7.0.0",
-                "@babel/helper-plugin-utils": "^7.0.0",
-                "@babel/helper-replace-supers": "^7.5.5",
-                "@babel/helper-split-export-declaration": "^7.4.4",
+                "@babel/helper-annotate-as-pure": "^7.14.5",
+                "@babel/helper-function-name": "^7.14.5",
+                "@babel/helper-optimise-call-expression": "^7.14.5",
+                "@babel/helper-plugin-utils": "^7.14.5",
+                "@babel/helper-replace-supers": "^7.14.5",
+                "@babel/helper-split-export-declaration": "^7.14.5",
                 "globals": "^11.1.0"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-classes/-/plugin-transform-classes-7.5.5.tgz",
-            "version": "7.5.5"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-classes/-/plugin-transform-classes-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-computed-properties": {
-            "integrity": "sha512-kP/drqTxY6Xt3NNpKiMomfgkNn4o7+vKxK2DDKcBG9sHj51vHqMBGy8wbDS/J4lMxnqs153/T3+DmCEAkC5cpA==",
+            "integrity": "sha512-pWM+E4283UxaVzLb8UBXv4EIxMovU4zxT1OPnpHJcmnvyY9QbPPTKZfEj31EUvG3/EQRbYAGaYEUZ4yWOBC2xg==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-computed-properties/-/plugin-transform-computed-properties-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-computed-properties/-/plugin-transform-computed-properties-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-destructuring": {
-            "integrity": "sha512-YbYgbd3TryYYLGyC7ZR+Tq8H/+bCmwoaxHfJHupom5ECstzbRLTch6gOQbhEY9Z4hiCNHEURgq06ykFv9JZ/QQ==",
+            "integrity": "sha512-0mDE99nK+kVh3xlc5vKwB6wnP9ecuSj+zQCa/n0voENtP/zymdT4HH6QEb65wjjcbqr1Jb/7z9Qp7TF5FtwYGw==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-destructuring/-/plugin-transform-destructuring-7.5.0.tgz",
-            "version": "7.5.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-destructuring/-/plugin-transform-destructuring-7.14.7.tgz",
+            "version": "7.14.7"
         },
         "@babel/plugin-transform-dotall-regex": {
-            "integrity": "sha512-P05YEhRc2h53lZDjRPk/OektxCVevFzZs2Gfjd545Wde3k+yFDbXORgl2e0xpbq8mLcKJ7Idss4fAg0zORN/zg==",
+            "integrity": "sha512-loGlnBdj02MDsFaHhAIJzh7euK89lBrGIdM9EAtHFo6xKygCUGuuWe07o1oZVk287amtW1n0808sQM99aZt3gw==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0",
-                "@babel/helper-regex": "^7.4.4",
-                "regexpu-core": "^4.5.4"
+                "@babel/helper-create-regexp-features-plugin": "^7.14.5",
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-dotall-regex/-/plugin-transform-dotall-regex-7.4.4.tgz",
-            "version": "7.4.4"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-dotall-regex/-/plugin-transform-dotall-regex-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-duplicate-keys": {
-            "integrity": "sha512-igcziksHizyQPlX9gfSjHkE2wmoCH3evvD2qR5w29/Dk0SMKE/eOI7f1HhBdNhR/zxJDqrgpoDTq5YSLH/XMsQ==",
+            "integrity": "sha512-iJjbI53huKbPDAsJ8EmVmvCKeeq21bAze4fu9GBQtSLqfvzj2oRuHVx4ZkDwEhg1htQ+5OBZh/Ab0XDf5iBZ7A==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-duplicate-keys/-/plugin-transform-duplicate-keys-7.5.0.tgz",
-            "version": "7.5.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-duplicate-keys/-/plugin-transform-duplicate-keys-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-exponentiation-operator": {
-            "integrity": "sha512-umh4hR6N7mu4Elq9GG8TOu9M0bakvlsREEC+ialrQN6ABS4oDQ69qJv1VtR3uxlKMCQMCvzk7vr17RHKcjx68A==",
+            "integrity": "sha512-jFazJhMBc9D27o9jDnIE5ZErI0R0m7PbKXVq77FFvqFbzvTMuv8jaAwLZ5PviOLSFttqKIW0/wxNSDbjLk0tYA==",
             "requires": {
-                "@babel/helper-builder-binary-assignment-operator-visitor": "^7.1.0",
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-builder-binary-assignment-operator-visitor": "^7.14.5",
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-exponentiation-operator/-/plugin-transform-exponentiation-operator-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-exponentiation-operator/-/plugin-transform-exponentiation-operator-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-for-of": {
-            "integrity": "sha512-9T/5Dlr14Z9TIEXLXkt8T1DU7F24cbhwhMNUziN3hB1AXoZcdzPcTiKGRn/6iOymDqtTKWnr/BtRKN9JwbKtdQ==",
+            "integrity": "sha512-CfmqxSUZzBl0rSjpoQSFoR9UEj3HzbGuGNL21/iFTmjb5gFggJp3ph0xR1YBhexmLoKRHzgxuFvty2xdSt6gTA==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-for-of/-/plugin-transform-for-of-7.4.4.tgz",
-            "version": "7.4.4"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-for-of/-/plugin-transform-for-of-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-function-name": {
-            "integrity": "sha512-iU9pv7U+2jC9ANQkKeNF6DrPy4GBa4NWQtl6dHB4Pb3izX2JOEvDTFarlNsBj/63ZEzNNIAMs3Qw4fNCcSOXJA==",
+            "dependencies": {
+                "@babel/code-frame": {
+                    "integrity": "sha512-9pzDqyc6OLDaqe+zbACgFkb6fKMNG6CObKpnYXChRsvYGyEdc7CA2BaqeOM+vOtCS5ndmJicPJhKAwYRI6UfFw==",
+                    "requires": {
+                        "@babel/highlight": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/code-frame/-/code-frame-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-function-name": {
+                    "integrity": "sha512-Gjna0AsXWfFvrAuX+VKcN/aNNWonizBj39yGwUzVDVTlMYJMK2Wp6xdpy72mfArFq5uK+NOuexfzZlzI1z9+AQ==",
+                    "requires": {
+                        "@babel/helper-get-function-arity": "^7.14.5",
+                        "@babel/template": "^7.14.5",
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-function-name/-/helper-function-name-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-get-function-arity": {
+                    "integrity": "sha512-I1Db4Shst5lewOM4V+ZKJzQ0JGGaZ6VY1jYvMghRjqs6DWgxLCIyFt30GlnKkfUeFLpJt2vzbMVEXVSXlIFYUg==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-get-function-arity/-/helper-get-function-arity-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/highlight": {
+                    "integrity": "sha512-qf9u2WFWVV0MppaL877j2dBtQIDgmidgjGk5VIMw3OadXvYaXn66U1BFlH2t4+t3i+8PhedppRv+i40ABzd+gg==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.5",
+                        "chalk": "^2.0.0",
+                        "js-tokens": "^4.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/highlight/-/highlight-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/parser": {
+                    "integrity": "sha512-syoCQFOoo/fzkWDeM0dLEZi5xqurb5vuyzwIMNZRNun+N/9A4cUZeQaE7dTrB8jGaKuJRBtEOajtnmw0I5hvvA==",
+                    "resolved": "https://registry.npmjs.org/@babel/parser/-/parser-7.14.8.tgz",
+                    "version": "7.14.8"
+                },
+                "@babel/template": {
+                    "integrity": "sha512-6Z3Po85sfxRGachLULUhOmvAaOo7xCvqGQtxINai2mEGPFm6pQ4z5QInFnUrRpfoSV60BnjyF5F3c+15fxFV1g==",
+                    "requires": {
+                        "@babel/code-frame": "^7.14.5",
+                        "@babel/parser": "^7.14.5",
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/template/-/template-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                }
+            },
+            "integrity": "sha512-vbO6kv0fIzZ1GpmGQuvbwwm+O4Cbm2NrPzwlup9+/3fdkuzo1YqOZcXw26+YUJB84Ja7j9yURWposEHLYwxUfQ==",
             "requires": {
-                "@babel/helper-function-name": "^7.1.0",
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-function-name": "^7.14.5",
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-function-name/-/plugin-transform-function-name-7.4.4.tgz",
-            "version": "7.4.4"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-function-name/-/plugin-transform-function-name-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-literals": {
-            "integrity": "sha512-2ThDhm4lI4oV7fVQ6pNNK+sx+c/GM5/SaML0w/r4ZB7sAneD/piDJtwdKlNckXeyGK7wlwg2E2w33C/Hh+VFCg==",
+            "integrity": "sha512-ql33+epql2F49bi8aHXxvLURHkxJbSmMKl9J5yHqg4PLtdE6Uc48CH1GS6TQvZ86eoB/ApZXwm7jlA+B3kra7A==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-literals/-/plugin-transform-literals-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-literals/-/plugin-transform-literals-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-member-expression-literals": {
-            "integrity": "sha512-HiU3zKkSU6scTidmnFJ0bMX8hz5ixC93b4MHMiYebmk2lUVNGOboPsqQvx5LzooihijUoLR/v7Nc1rbBtnc7FA==",
+            "integrity": "sha512-WkNXxH1VXVTKarWFqmso83xl+2V3Eo28YY5utIkbsmXoItO8Q3aZxN4BTS2k0hz9dGUloHK26mJMyQEYfkn/+Q==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-member-expression-literals/-/plugin-transform-member-expression-literals-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-member-expression-literals/-/plugin-transform-member-expression-literals-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-modules-amd": {
-            "integrity": "sha512-n20UsQMKnWrltocZZm24cRURxQnWIvsABPJlw/fvoy9c6AgHZzoelAIzajDHAQrDpuKFFPPcFGd7ChsYuIUMpg==",
+            "integrity": "sha512-3lpOU8Vxmp3roC4vzFpSdEpGUWSMsHFreTWOMMLzel2gNGfHE5UWIh/LN6ghHs2xurUp4jRFYMUIZhuFbody1g==",
             "requires": {
-                "@babel/helper-module-transforms": "^7.1.0",
-                "@babel/helper-plugin-utils": "^7.0.0",
-                "babel-plugin-dynamic-import-node": "^2.3.0"
+                "@babel/helper-module-transforms": "^7.14.5",
+                "@babel/helper-plugin-utils": "^7.14.5",
+                "babel-plugin-dynamic-import-node": "^2.3.3"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-modules-amd/-/plugin-transform-modules-amd-7.5.0.tgz",
-            "version": "7.5.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-modules-amd/-/plugin-transform-modules-amd-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-modules-commonjs": {
-            "integrity": "sha512-4sfBOJt58sEo9a2BQXnZq+Q3ZTSAUXyK3E30o36BOGnJ+tvJ6YSxF0PG6kERvbeISgProodWuI9UVG3/FMY6iw==",
+            "integrity": "sha512-KEMyWNNWnjOom8vR/1+d+Ocz/mILZG/eyHHO06OuBQ2aNhxT62fr4y6fGOplRx+CxCSp3IFwesL8WdINfY/3kg==",
             "requires": {
-                "@babel/helper-module-transforms": "^7.4.4",
+                "@babel/helper-module-transforms": "^7.7.0",
                 "@babel/helper-plugin-utils": "^7.0.0",
-                "@babel/helper-simple-access": "^7.1.0"
+                "@babel/helper-simple-access": "^7.7.0",
+                "babel-plugin-dynamic-import-node": "^2.3.0"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-modules-commonjs/-/plugin-transform-modules-commonjs-7.4.4.tgz",
-            "version": "7.4.4"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-modules-commonjs/-/plugin-transform-modules-commonjs-7.7.0.tgz",
+            "version": "7.7.0"
         },
         "@babel/plugin-transform-modules-systemjs": {
-            "integrity": "sha512-Q2m56tyoQWmuNGxEtUyeEkm6qJYFqs4c+XyXH5RAuYxObRNz9Zgj/1g2GMnjYp2EUyEy7YTrxliGCXzecl/vJg==",
+            "integrity": "sha512-mNMQdvBEE5DcMQaL5LbzXFMANrQjd2W7FPzg34Y4yEz7dBgdaC+9B84dSO+/1Wba98zoDbInctCDo4JGxz1VYA==",
             "requires": {
-                "@babel/helper-hoist-variables": "^7.4.4",
-                "@babel/helper-plugin-utils": "^7.0.0",
-                "babel-plugin-dynamic-import-node": "^2.3.0"
+                "@babel/helper-hoist-variables": "^7.14.5",
+                "@babel/helper-module-transforms": "^7.14.5",
+                "@babel/helper-plugin-utils": "^7.14.5",
+                "@babel/helper-validator-identifier": "^7.14.5",
+                "babel-plugin-dynamic-import-node": "^2.3.3"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-modules-systemjs/-/plugin-transform-modules-systemjs-7.5.0.tgz",
-            "version": "7.5.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-modules-systemjs/-/plugin-transform-modules-systemjs-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-modules-umd": {
-            "integrity": "sha512-BV3bw6MyUH1iIsGhXlOK6sXhmSarZjtJ/vMiD9dNmpY8QXFFQTj+6v92pcfy1iqa8DeAfJFwoxcrS/TUZda6sw==",
+            "integrity": "sha512-RfPGoagSngC06LsGUYyM9QWSXZ8MysEjDJTAea1lqRjNECE3y0qIJF/qbvJxc4oA4s99HumIMdXOrd+TdKaAAA==",
             "requires": {
-                "@babel/helper-module-transforms": "^7.1.0",
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-module-transforms": "^7.14.5",
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-modules-umd/-/plugin-transform-modules-umd-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-modules-umd/-/plugin-transform-modules-umd-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-named-capturing-groups-regex": {
-            "integrity": "sha512-z7+2IsWafTBbjNsOxU/Iv5CvTJlr5w4+HGu1HovKYTtgJ362f7kBcQglkfmlspKKZ3bgrbSGvLfNx++ZJgCWsg==",
+            "integrity": "sha512-DTNOTaS7TkW97xsDMrp7nycUVh6sn/eq22VaxWfEdzuEbRsiaOU0pqU7DlyUGHVsbQbSghvjKRpEl+nUCKGQSg==",
             "requires": {
-                "regexp-tree": "^0.1.6"
+                "@babel/helper-create-regexp-features-plugin": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-named-capturing-groups-regex/-/plugin-transform-named-capturing-groups-regex-7.4.5.tgz",
-            "version": "7.4.5"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-named-capturing-groups-regex/-/plugin-transform-named-capturing-groups-regex-7.14.7.tgz",
+            "version": "7.14.7"
         },
         "@babel/plugin-transform-new-target": {
-            "integrity": "sha512-r1z3T2DNGQwwe2vPGZMBNjioT2scgWzK9BCnDEh+46z8EEwXBq24uRzd65I7pjtugzPSj921aM15RpESgzsSuA==",
+            "integrity": "sha512-Nx054zovz6IIRWEB49RDRuXGI4Gy0GMgqG0cII9L3MxqgXz/+rgII+RU58qpo4g7tNEx1jG7rRVH4ihZoP4esQ==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-new-target/-/plugin-transform-new-target-7.4.4.tgz",
-            "version": "7.4.4"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-new-target/-/plugin-transform-new-target-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-object-super": {
-            "integrity": "sha512-un1zJQAhSosGFBduPgN/YFNvWVpRuHKU7IHBglLoLZsGmruJPOo6pbInneflUdmq7YvSVqhpPs5zdBvLnteltQ==",
+            "integrity": "sha512-MKfOBWzK0pZIrav9z/hkRqIk/2bTv9qvxHzPQc12RcVkMOzpIKnFCNYJip00ssKWYkd8Sf5g0Wr7pqJ+cmtuFg==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0",
-                "@babel/helper-replace-supers": "^7.5.5"
+                "@babel/helper-plugin-utils": "^7.14.5",
+                "@babel/helper-replace-supers": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-object-super/-/plugin-transform-object-super-7.5.5.tgz",
-            "version": "7.5.5"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-object-super/-/plugin-transform-object-super-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-parameters": {
-            "integrity": "sha512-oMh5DUO1V63nZcu/ZVLQFqiihBGo4OpxJxR1otF50GMeCLiRx5nUdtokd+u9SuVJrvvuIh9OosRFPP4pIPnwmw==",
+            "integrity": "sha512-Tl7LWdr6HUxTmzQtzuU14SqbgrSKmaR77M0OKyq4njZLQTPfOvzblNKyNkGwOfEFCEx7KeYHQHDI0P3F02IVkA==",
             "requires": {
-                "@babel/helper-call-delegate": "^7.4.4",
-                "@babel/helper-get-function-arity": "^7.0.0",
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-parameters/-/plugin-transform-parameters-7.4.4.tgz",
-            "version": "7.4.4"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-parameters/-/plugin-transform-parameters-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-property-literals": {
-            "integrity": "sha512-9q7Dbk4RhgcLp8ebduOpCbtjh7C0itoLYHXd9ueASKAG/is5PQtMR5VJGka9NKqGhYEGn5ITahd4h9QeBMylWQ==",
+            "integrity": "sha512-r1uilDthkgXW8Z1vJz2dKYLV1tuw2xsbrp3MrZmD99Wh9vsfKoob+JTgri5VUb/JqyKRXotlOtwgu4stIYCmnw==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-property-literals/-/plugin-transform-property-literals-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-property-literals/-/plugin-transform-property-literals-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-react-display-name": {
-            "integrity": "sha512-Htf/tPa5haZvRMiNSQSFifK12gtr/8vwfr+A9y69uF0QcU77AVu4K7MiHEkTxF7lQoHOL0F9ErqgfNEAKgXj7A==",
+            "integrity": "sha512-07aqY1ChoPgIxsuDviptRpVkWCSbXWmzQqcgy65C6YSFOfPFvb/DX3bBRHh7pCd/PMEEYHYWUTSVkCbkVainYQ==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-react-display-name/-/plugin-transform-react-display-name-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-react-display-name/-/plugin-transform-react-display-name-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-react-jsx": {
-            "integrity": "sha512-a/+aRb7R06WcKvQLOu4/TpjKOdvVEKRLWFpKcNuHhiREPgGRB4TQJxq07+EZLS8LFVYpfq1a5lDUnuMdcCpBKg==",
+            "dependencies": {
+                "@babel/helper-annotate-as-pure": {
+                    "integrity": "sha512-EivH9EgBIb+G8ij1B2jAwSH36WnGvkQSEC6CkX/6v6ZFlw5fVOHvsgGF4uiEHO2GzMvunZb6tDLQEQSdrdocrA==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-annotate-as-pure/-/helper-annotate-as-pure-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/helper-module-imports": {
+                    "integrity": "sha512-SwrNHu5QWS84XlHwGYPDtCxcA0hrSlL2yhWYLgeOc0w7ccOl2qv4s/nARI0aYZW+bSwAL5CukeXA47B/1NKcnQ==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-module-imports/-/helper-module-imports-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                }
+            },
+            "integrity": "sha512-7RylxNeDnxc1OleDm0F5Q/BSL+whYRbOAR+bwgCxIr0L32v7UFh/pz1DLMZideAUxKT6eMoS2zQH6fyODLEi8Q==",
             "requires": {
-                "@babel/helper-builder-react-jsx": "^7.3.0",
-                "@babel/helper-plugin-utils": "^7.0.0",
-                "@babel/plugin-syntax-jsx": "^7.2.0"
+                "@babel/helper-annotate-as-pure": "^7.14.5",
+                "@babel/helper-module-imports": "^7.14.5",
+                "@babel/helper-plugin-utils": "^7.14.5",
+                "@babel/plugin-syntax-jsx": "^7.14.5",
+                "@babel/types": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-react-jsx/-/plugin-transform-react-jsx-7.3.0.tgz",
-            "version": "7.3.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-react-jsx/-/plugin-transform-react-jsx-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-react-jsx-self": {
-            "integrity": "sha512-v6S5L/myicZEy+jr6ielB0OR8h+EH/1QFx/YJ7c7Ua+7lqsjj/vW6fD5FR9hB/6y7mGbfT4vAURn3xqBxsUcdg==",
+            "integrity": "sha512-M/fmDX6n0cfHK/NLTcPmrfVAORKDhK8tyjDhyxlUjYyPYYO8FRWwuxBA3WBx8kWN/uBUuwGa3s/0+hQ9JIN3Tg==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0",
-                "@babel/plugin-syntax-jsx": "^7.2.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-react-jsx-self/-/plugin-transform-react-jsx-self-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-react-jsx-self/-/plugin-transform-react-jsx-self-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-react-jsx-source": {
-            "integrity": "sha512-58Q+Jsy4IDCZx7kqEZuSDdam/1oW8OdDX8f+Loo6xyxdfg1yF0GE2XNJQSTZCaMol93+FBzpWiPEwtbMloAcPg==",
+            "integrity": "sha512-1TpSDnD9XR/rQ2tzunBVPThF5poaYT9GqP+of8fAtguYuI/dm2RkrMBDemsxtY0XBzvW7nXjYM0hRyKX9QYj7Q==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0",
-                "@babel/plugin-syntax-jsx": "^7.2.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-react-jsx-source/-/plugin-transform-react-jsx-source-7.5.0.tgz",
-            "version": "7.5.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-react-jsx-source/-/plugin-transform-react-jsx-source-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-regenerator": {
-            "integrity": "sha512-gBKRh5qAaCWntnd09S8QC7r3auLCqq5DI6O0DlfoyDjslSBVqBibrMdsqO+Uhmx3+BlOmE/Kw1HFxmGbv0N9dA==",
+            "integrity": "sha512-NVIY1W3ITDP5xQl50NgTKlZ0GrotKtLna08/uGY6ErQt6VEQZXla86x/CTddm5gZdcr+5GSsvMeTmWA5Ii6pkg==",
             "requires": {
-                "regenerator-transform": "^0.14.0"
+                "regenerator-transform": "^0.14.2"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-regenerator/-/plugin-transform-regenerator-7.4.5.tgz",
-            "version": "7.4.5"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-regenerator/-/plugin-transform-regenerator-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-reserved-words": {
-            "integrity": "sha512-fz43fqW8E1tAB3DKF19/vxbpib1fuyCwSPE418ge5ZxILnBhWyhtPgz8eh1RCGGJlwvksHkyxMxh0eenFi+kFw==",
+            "integrity": "sha512-cv4F2rv1nD4qdexOGsRQXJrOcyb5CrgjUH9PKrrtyhSDBNWGxd0UIitjyJiWagS+EbUGjG++22mGH1Pub8D6Vg==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-reserved-words/-/plugin-transform-reserved-words-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-reserved-words/-/plugin-transform-reserved-words-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-runtime": {
-            "integrity": "sha512-aMVojEjPszvau3NRg+TIH14ynZLvPewH4xhlCW1w6A3rkxTS1m4uwzRclYR9oS+rl/dr+kT+pzbfHuAWP/lc7Q==",
+            "integrity": "sha512-cqULw/QB4yl73cS5Y0TZlQSjDvNkzDbu0FurTZyHlJpWE5T3PCMdnyV+xXoH1opr1ldyHODe3QAX3OMAii5NxA==",
             "requires": {
                 "@babel/helper-module-imports": "^7.0.0",
                 "@babel/helper-plugin-utils": "^7.0.0",
                 "resolve": "^1.8.1",
                 "semver": "^5.5.1"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-runtime/-/plugin-transform-runtime-7.4.4.tgz",
-            "version": "7.4.4"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-runtime/-/plugin-transform-runtime-7.6.2.tgz",
+            "version": "7.6.2"
         },
         "@babel/plugin-transform-shorthand-properties": {
-            "integrity": "sha512-QP4eUM83ha9zmYtpbnyjTLAGKQritA5XW/iG9cjtuOI8s1RuL/3V6a3DeSHfKutJQ+ayUfeZJPcnCYEQzaPQqg==",
+            "integrity": "sha512-xLucks6T1VmGsTB+GWK5Pl9Jl5+nRXD1uoFdA5TSO6xtiNjtXTjKkmPdFXVLGlK5A2/or/wQMKfmQ2Y0XJfn5g==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-shorthand-properties/-/plugin-transform-shorthand-properties-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-shorthand-properties/-/plugin-transform-shorthand-properties-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-spread": {
-            "integrity": "sha512-KWfky/58vubwtS0hLqEnrWJjsMGaOeSBn90Ezn5Jeg9Z8KKHmELbP1yGylMlm5N6TPKeY9A2+UaSYLdxahg01w==",
+            "integrity": "sha512-Zr0x0YroFJku7n7+/HH3A2eIrGMjbmAIbJSVv0IZ+t3U2WUQUA64S/oeied2e+MaGSjmt4alzBCsK9E8gh+fag==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5",
+                "@babel/helper-skip-transparent-expression-wrappers": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-spread/-/plugin-transform-spread-7.2.2.tgz",
-            "version": "7.2.2"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-spread/-/plugin-transform-spread-7.14.6.tgz",
+            "version": "7.14.6"
         },
         "@babel/plugin-transform-sticky-regex": {
-            "integrity": "sha512-KKYCoGaRAf+ckH8gEL3JHUaFVyNHKe3ASNsZ+AlktgHevvxGigoIttrEJb8iKN03Q7Eazlv1s6cx2B2cQ3Jabw==",
+            "integrity": "sha512-Z7F7GyvEMzIIbwnziAZmnSNpdijdr4dWt+FJNBnBLz5mwDFkqIXU9wmBcWWad3QeJF5hMTkRe4dAq2sUZiG+8A==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0",
-                "@babel/helper-regex": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-sticky-regex/-/plugin-transform-sticky-regex-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-sticky-regex/-/plugin-transform-sticky-regex-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-template-literals": {
-            "integrity": "sha512-mQrEC4TWkhLN0z8ygIvEL9ZEToPhG5K7KDW3pzGqOfIGZ28Jb0POUkeWcoz8HnHvhFy6dwAT1j8OzqN8s804+g==",
+            "integrity": "sha512-22btZeURqiepOfuy/VkFr+zStqlujWaarpMErvay7goJS6BWwdd6BY9zQyDLDa4x2S3VugxFb162IZ4m/S/+Gg==",
             "requires": {
-                "@babel/helper-annotate-as-pure": "^7.0.0",
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-template-literals/-/plugin-transform-template-literals-7.4.4.tgz",
-            "version": "7.4.4"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-template-literals/-/plugin-transform-template-literals-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-typeof-symbol": {
-            "integrity": "sha512-2LNhETWYxiYysBtrBTqL8+La0jIoQQnIScUJc74OYvUGRmkskNY4EzLCnjHBzdmb38wqtTaixpo1NctEcvMDZw==",
+            "integrity": "sha512-lXzLD30ffCWseTbMQzrvDWqljvZlHkXU+CnseMhkMNqU1sASnCsz3tSzAaH3vCUXb9PHeUb90ZT1BdFTm1xxJw==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0"
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-typeof-symbol/-/plugin-transform-typeof-symbol-7.2.0.tgz",
-            "version": "7.2.0"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-typeof-symbol/-/plugin-transform-typeof-symbol-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/plugin-transform-typescript": {
-            "integrity": "sha512-pehKf4m640myZu5B2ZviLaiBlxMCjSZ1qTEO459AXKX5GnPueyulJeCqZFs1nz/Ya2dDzXQ1NxZ/kKNWyD4h6w==",
+            "integrity": "sha512-XlTdBq7Awr4FYIzqhmYY80WN0V0azF74DMPyFqVHBvf81ZUgc4X7ZOpx6O8eLDK6iM5cCQzeyJw0ynTaefixRA==",
             "requires": {
-                "@babel/helper-create-class-features-plugin": "^7.5.5",
-                "@babel/helper-plugin-utils": "^7.0.0",
-                "@babel/plugin-syntax-typescript": "^7.2.0"
+                "@babel/helper-create-class-features-plugin": "^7.14.6",
+                "@babel/helper-plugin-utils": "^7.14.5",
+                "@babel/plugin-syntax-typescript": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-typescript/-/plugin-transform-typescript-7.5.5.tgz",
-            "version": "7.5.5"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-typescript/-/plugin-transform-typescript-7.14.6.tgz",
+            "version": "7.14.6"
         },
         "@babel/plugin-transform-unicode-regex": {
-            "integrity": "sha512-il+/XdNw01i93+M9J9u4T7/e/Ue/vWfNZE4IRUQjplu2Mqb/AFTDimkw2tdEdSH50wuQXZAbXSql0UphQke+vA==",
+            "integrity": "sha512-UygduJpC5kHeCiRw/xDVzC+wj8VaYSoKl5JNVmbP7MadpNinAm3SvZCxZ42H37KZBKztz46YC73i9yV34d0Tzw==",
             "requires": {
-                "@babel/helper-plugin-utils": "^7.0.0",
-                "@babel/helper-regex": "^7.4.4",
-                "regexpu-core": "^4.5.4"
+                "@babel/helper-create-regexp-features-plugin": "^7.14.5",
+                "@babel/helper-plugin-utils": "^7.14.5"
             },
-            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-unicode-regex/-/plugin-transform-unicode-regex-7.4.4.tgz",
-            "version": "7.4.4"
+            "resolved": "https://registry.npmjs.org/@babel/plugin-transform-unicode-regex/-/plugin-transform-unicode-regex-7.14.5.tgz",
+            "version": "7.14.5"
         },
         "@babel/preset-env": {
-            "integrity": "sha512-f2yNVXM+FsR5V8UwcFeIHzHWgnhXg3NpRmy0ADvALpnhB0SLbCvrCRr4BLOUYbQNLS+Z0Yer46x9dJXpXewI7w==",
+            "dependencies": {
+                "@babel/helper-module-imports": {
+                    "integrity": "sha512-SwrNHu5QWS84XlHwGYPDtCxcA0hrSlL2yhWYLgeOc0w7ccOl2qv4s/nARI0aYZW+bSwAL5CukeXA47B/1NKcnQ==",
+                    "requires": {
+                        "@babel/types": "^7.14.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/helper-module-imports/-/helper-module-imports-7.14.5.tgz",
+                    "version": "7.14.5"
+                },
+                "@babel/types": {
+                    "integrity": "sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==",
+                    "requires": {
+                        "@babel/helper-validator-identifier": "^7.14.8",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.14.8.tgz",
+                    "version": "7.14.8"
+                }
+            },
+            "integrity": "sha512-/93SWhi3PxcVTDpSqC+Dp4YxUu3qZ4m7I76k0w73wYfn7bGVuRIO4QUz95aJksbS+AD1/mT1Ie7rbkT0wSplaA==",
             "requires": {
-                "@babel/helper-module-imports": "^7.0.0",
+                "@babel/helper-module-imports": "^7.7.0",
                 "@babel/helper-plugin-utils": "^7.0.0",
-                "@babel/plugin-proposal-async-generator-functions": "^7.2.0",
+                "@babel/plugin-proposal-async-generator-functions": "^7.7.0",
+                "@babel/plugin-proposal-dynamic-import": "^7.7.0",
                 "@babel/plugin-proposal-json-strings": "^7.2.0",
-                "@babel/plugin-proposal-object-rest-spread": "^7.4.4",
+                "@babel/plugin-proposal-object-rest-spread": "^7.6.2",
                 "@babel/plugin-proposal-optional-catch-binding": "^7.2.0",
-                "@babel/plugin-proposal-unicode-property-regex": "^7.4.4",
+                "@babel/plugin-proposal-unicode-property-regex": "^7.7.0",
                 "@babel/plugin-syntax-async-generators": "^7.2.0",
+                "@babel/plugin-syntax-dynamic-import": "^7.2.0",
                 "@babel/plugin-syntax-json-strings": "^7.2.0",
                 "@babel/plugin-syntax-object-rest-spread": "^7.2.0",
                 "@babel/plugin-syntax-optional-catch-binding": "^7.2.0",
+                "@babel/plugin-syntax-top-level-await": "^7.7.0",
                 "@babel/plugin-transform-arrow-functions": "^7.2.0",
-                "@babel/plugin-transform-async-to-generator": "^7.4.4",
+                "@babel/plugin-transform-async-to-generator": "^7.7.0",
                 "@babel/plugin-transform-block-scoped-functions": "^7.2.0",
-                "@babel/plugin-transform-block-scoping": "^7.4.4",
-                "@babel/plugin-transform-classes": "^7.4.4",
+                "@babel/plugin-transform-block-scoping": "^7.6.3",
+                "@babel/plugin-transform-classes": "^7.7.0",
                 "@babel/plugin-transform-computed-properties": "^7.2.0",
-                "@babel/plugin-transform-destructuring": "^7.4.4",
-                "@babel/plugin-transform-dotall-regex": "^7.4.4",
-                "@babel/plugin-transform-duplicate-keys": "^7.2.0",
+                "@babel/plugin-transform-destructuring": "^7.6.0",
+                "@babel/plugin-transform-dotall-regex": "^7.7.0",
+                "@babel/plugin-transform-duplicate-keys": "^7.5.0",
                 "@babel/plugin-transform-exponentiation-operator": "^7.2.0",
                 "@babel/plugin-transform-for-of": "^7.4.4",
-                "@babel/plugin-transform-function-name": "^7.4.4",
+                "@babel/plugin-transform-function-name": "^7.7.0",
                 "@babel/plugin-transform-literals": "^7.2.0",
                 "@babel/plugin-transform-member-expression-literals": "^7.2.0",
-                "@babel/plugin-transform-modules-amd": "^7.2.0",
-                "@babel/plugin-transform-modules-commonjs": "^7.4.4",
-                "@babel/plugin-transform-modules-systemjs": "^7.4.4",
-                "@babel/plugin-transform-modules-umd": "^7.2.0",
-                "@babel/plugin-transform-named-capturing-groups-regex": "^7.4.5",
+                "@babel/plugin-transform-modules-amd": "^7.5.0",
+                "@babel/plugin-transform-modules-commonjs": "^7.7.0",
+                "@babel/plugin-transform-modules-systemjs": "^7.7.0",
+                "@babel/plugin-transform-modules-umd": "^7.7.0",
+                "@babel/plugin-transform-named-capturing-groups-regex": "^7.7.0",
                 "@babel/plugin-transform-new-target": "^7.4.4",
-                "@babel/plugin-transform-object-super": "^7.2.0",
+                "@babel/plugin-transform-object-super": "^7.5.5",
                 "@babel/plugin-transform-parameters": "^7.4.4",
                 "@babel/plugin-transform-property-literals": "^7.2.0",
-                "@babel/plugin-transform-regenerator": "^7.4.5",
+                "@babel/plugin-transform-regenerator": "^7.7.0",
                 "@babel/plugin-transform-reserved-words": "^7.2.0",
                 "@babel/plugin-transform-shorthand-properties": "^7.2.0",
-                "@babel/plugin-transform-spread": "^7.2.0",
+                "@babel/plugin-transform-spread": "^7.6.2",
                 "@babel/plugin-transform-sticky-regex": "^7.2.0",
                 "@babel/plugin-transform-template-literals": "^7.4.4",
                 "@babel/plugin-transform-typeof-symbol": "^7.2.0",
-                "@babel/plugin-transform-unicode-regex": "^7.4.4",
-                "@babel/types": "^7.4.4",
+                "@babel/plugin-transform-unicode-regex": "^7.7.0",
+                "@babel/types": "^7.7.1",
                 "browserslist": "^4.6.0",
                 "core-js-compat": "^3.1.1",
                 "invariant": "^2.2.2",
                 "js-levenshtein": "^1.1.3",
                 "semver": "^5.5.0"
             },
-            "resolved": "https://registry.npmjs.org/@babel/preset-env/-/preset-env-7.4.5.tgz",
-            "version": "7.4.5"
+            "resolved": "https://registry.npmjs.org/@babel/preset-env/-/preset-env-7.7.1.tgz",
+            "version": "7.7.1"
+        },
+        "@babel/preset-modules": {
+            "integrity": "sha512-x/kt2aAZlgcFnP3P851fkkb2s4FmTiyGic58pkWMaRK9Am3u9KkH1ttHGjwlsKu7/TVJsLEBXZnjUxqsid3tww==",
+            "requires": {
+                "@babel/helper-plugin-utils": "^7.0.0",
+                "@babel/plugin-proposal-unicode-property-regex": "^7.4.4",
+                "@babel/plugin-transform-dotall-regex": "^7.4.4",
+                "@babel/types": "^7.4.4",
+                "esutils": "^2.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/@babel/preset-modules/-/preset-modules-0.1.1.tgz",
+            "version": "0.1.1"
         },
         "@babel/preset-react": {
-            "integrity": "sha512-oayxyPS4Zj+hF6Et11BwuBkmpgT/zMxyuZgFrMeZID6Hdh3dGlk4sHCAhdBCpuCKW2ppBfl2uCCetlrUIJRY3w==",
+            "integrity": "sha512-IXXgSUYBPHUGhUkH+89TR6faMcBtuMW0h5OHbMuVbL3/5wK2g6a2M2BBpkLa+Kw0sAHiZ9dNVgqJMDP/O4GRBA==",
             "requires": {
                 "@babel/helper-plugin-utils": "^7.0.0",
                 "@babel/plugin-transform-react-display-name": "^7.0.0",
-                "@babel/plugin-transform-react-jsx": "^7.0.0",
+                "@babel/plugin-transform-react-jsx": "^7.7.0",
                 "@babel/plugin-transform-react-jsx-self": "^7.0.0",
                 "@babel/plugin-transform-react-jsx-source": "^7.0.0"
             },
-            "resolved": "https://registry.npmjs.org/@babel/preset-react/-/preset-react-7.0.0.tgz",
-            "version": "7.0.0"
+            "resolved": "https://registry.npmjs.org/@babel/preset-react/-/preset-react-7.7.0.tgz",
+            "version": "7.7.0"
         },
         "@babel/preset-typescript": {
-            "integrity": "sha512-mzMVuIP4lqtn4du2ynEfdO0+RYcslwrZiJHXu4MGaC1ctJiW2fyaeDrtjJGs7R/KebZ1sgowcIoWf4uRpEfKEg==",
+            "integrity": "sha512-1B4HthAelaLGfNRyrWqJtBEjXX1ulThCrLQ5B2VOtEAznWFIFXFJahgXImqppy66lx/Oh+cOSCQdJzZqh2Jh5g==",
             "requires": {
                 "@babel/helper-plugin-utils": "^7.0.0",
-                "@babel/plugin-transform-typescript": "^7.3.2"
+                "@babel/plugin-transform-typescript": "^7.7.2"
             },
-            "resolved": "https://registry.npmjs.org/@babel/preset-typescript/-/preset-typescript-7.3.3.tgz",
-            "version": "7.3.3"
+            "resolved": "https://registry.npmjs.org/@babel/preset-typescript/-/preset-typescript-7.7.2.tgz",
+            "version": "7.7.2"
         },
         "@babel/runtime": {
-            "integrity": "sha512-TuI4qpWZP6lGOGIuGWtp9sPluqYICmbk8T/1vpSysqJxRPkudh/ofFWyqdcMsDf2s7KvDL4/YHgKyvcS3g9CJQ==",
-            "requires": {
-                "regenerator-runtime": "^0.13.2"
-            },
-            "resolved": "https://registry.npmjs.org/@babel/runtime/-/runtime-7.4.5.tgz",
-            "version": "7.4.5"
-        },
-        "@babel/runtime-corejs2": {
-            "integrity": "sha512-5yLuwzvIDecKwYMzJtiarky4Fb5643H3Ao5jwX0HrMR5oM5mn2iHH9wSZonxwNK0oAjAFUQAiOd4jT7/9Y2jMQ==",
+            "integrity": "sha512-JONRbXbTXc9WQE2mAZd1p0Z3DZ/6vaQIkgYMSTP3KjRCyd7rCZCcfhCyX+YjwcKxcZ82UrxbRD358bpExNgrjw==",
             "requires": {
-                "core-js": "^2.6.5",
                 "regenerator-runtime": "^0.13.2"
             },
-            "resolved": "https://registry.npmjs.org/@babel/runtime-corejs2/-/runtime-corejs2-7.4.5.tgz",
-            "version": "7.4.5"
+            "resolved": "https://registry.npmjs.org/@babel/runtime/-/runtime-7.7.2.tgz",
+            "version": "7.7.2"
         },
         "@babel/template": {
             "integrity": "sha512-CiGzLN9KgAvgZsnivND7rkA+AeJ9JB0ciPOD4U59GKbQP2iQl+olF1l76kJOupqidozfZ32ghwBEJDhnk9MEcw==",
             "requires": {
                 "@babel/code-frame": "^7.0.0",
                 "@babel/parser": "^7.4.4",
                 "@babel/types": "^7.4.4"
@@ -873,14 +1900,19 @@
                 "esutils": "^2.0.2",
                 "lodash": "^4.17.13",
                 "to-fast-properties": "^2.0.0"
             },
             "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.5.5.tgz",
             "version": "7.5.5"
         },
+        "@csstools/convert-colors": {
+            "integrity": "sha512-5a6wqoJV/xEdbRNKVo6I4hO3VjyDq//8q2f9I6PBAvMesJHFauXDorcNCsr9RzvsZnaWi5NYCcfyqP1QeFHFbw==",
+            "resolved": "https://registry.npmjs.org/@csstools/convert-colors/-/convert-colors-1.4.0.tgz",
+            "version": "1.4.0"
+        },
         "@emotion/is-prop-valid": {
             "integrity": "sha512-u5WtneEAr5IDG2Wv65yhunPSMLIpuKsbuOktRojfrEiEvRyC85LgPMZI63cr7NUqT8ZIGdSVg8ZKGxIug4lXcA==",
             "requires": {
                 "@emotion/memoize": "0.7.4"
             },
             "resolved": "https://registry.npmjs.org/@emotion/is-prop-valid/-/is-prop-valid-0.8.8.tgz",
             "version": "0.8.8"
@@ -891,14 +1923,24 @@
             "version": "0.7.4"
         },
         "@emotion/unitless": {
             "integrity": "sha512-OWORNpfjMsSSUBVrRBVGECkhWcULOAJz9ZW8uK9qgxD+87M7jHRcvh/A96XXNhXTLmKcoYSQtBEX7lHMO7YRwg==",
             "resolved": "https://registry.npmjs.org/@emotion/unitless/-/unitless-0.7.5.tgz",
             "version": "0.7.5"
         },
+        "@next/polyfill-nomodule": {
+            "integrity": "sha512-kEa7v3trZmW6iWeTJrhg+ZsE9njae7mLkgyZB5M1r975JHr5PQ69B5aX7hrEAj7aAJYvCKETgAczx4gGR8MOzQ==",
+            "resolved": "https://registry.npmjs.org/@next/polyfill-nomodule/-/polyfill-nomodule-9.3.2.tgz",
+            "version": "9.3.2"
+        },
+        "@types/json-schema": {
+            "integrity": "sha512-YSBPTLTVm2e2OoQIDYx8HaeWJ5tTToLH67kXR7zYNGupXMEHa2++G8k+DczX2cFVgalypqtyZIcU19AFcmOpmg==",
+            "resolved": "https://registry.npmjs.org/@types/json-schema/-/json-schema-7.0.8.tgz",
+            "version": "7.0.8"
+        },
         "@types/node": {
             "dev": true,
             "integrity": "sha512-51MYTDTyCziHb70wtGNFRwB4l+5JNvdqzFSkbDvpbftEgVUBEE+T5f7pROhWMp/fxp07oNIEQZd5bbfAH22ohQ==",
             "resolved": "https://registry.npmjs.org/@types/node/-/node-12.12.53.tgz",
             "version": "12.12.53"
         },
         "@types/prop-types": {
@@ -1091,48 +2133,97 @@
                 "mime-types": "~2.1.24",
                 "negotiator": "0.6.2"
             },
             "resolved": "https://registry.npmjs.org/accepts/-/accepts-1.3.7.tgz",
             "version": "1.3.7"
         },
         "acorn": {
-            "integrity": "sha512-ZVA9k326Nwrj3Cj9jlh3wGFutC2ZornPNARZwsNYqQYgN0EsV2d53w5RN/co65Ohn4sUAUtb1rSUAOD6XN9idA==",
-            "resolved": "https://registry.npmjs.org/acorn/-/acorn-6.4.1.tgz",
-            "version": "6.4.1"
+            "integrity": "sha512-XtGIhXwF8YM8bJhGxG5kXgjkEuNGLTkoYqVE+KMR+aspr4KGYmKYg7yUe3KghyQ9yheNwLnjmzh/7+gfDBmHCQ==",
+            "resolved": "https://registry.npmjs.org/acorn/-/acorn-6.4.2.tgz",
+            "version": "6.4.2"
+        },
+        "adjust-sourcemap-loader": {
+            "dependencies": {
+                "camelcase": {
+                    "integrity": "sha512-faqwZqnWxbxn+F1d399ygeamQNy3lPp/H9H6rNrqYh4FSVCtcY+3cub1MxA8o9mDd55mM8Aghuu/kuyYA6VTsA==",
+                    "resolved": "https://registry.npmjs.org/camelcase/-/camelcase-5.0.0.tgz",
+                    "version": "5.0.0"
+                },
+                "emojis-list": {
+                    "integrity": "sha1-TapNnbAPmBmIDHn6RXrlsJof04k=",
+                    "resolved": "https://registry.npmjs.org/emojis-list/-/emojis-list-2.1.0.tgz",
+                    "version": "2.1.0"
+                },
+                "json5": {
+                    "integrity": "sha512-aKS4WQjPenRxiQsC93MNfjx+nbF4PAdYzmd/1JIj8HYzqfbu86beTuNgXDzPknWk0n0uARlyewZo4s++ES36Ow==",
+                    "requires": {
+                        "minimist": "^1.2.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/json5/-/json5-1.0.1.tgz",
+                    "version": "1.0.1"
+                },
+                "loader-utils": {
+                    "integrity": "sha512-fkpz8ejdnEMG3s37wGL07iSBDg99O9D5yflE9RGNH3hRdx9SOwYfnGYdZOUIZitN8E+E2vkq3MUMYMvPYl5ZZA==",
+                    "requires": {
+                        "big.js": "^5.2.2",
+                        "emojis-list": "^2.0.0",
+                        "json5": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/loader-utils/-/loader-utils-1.2.3.tgz",
+                    "version": "1.2.3"
+                }
+            },
+            "integrity": "sha512-4hFsTsn58+YjrU9qKzML2JSSDqKvN8mUGQ0nNIrfPi8hmIONT4L3uUaT6MKdMsZ9AjsU6D2xDkZxCkbQPxChrA==",
+            "requires": {
+                "assert": "1.4.1",
+                "camelcase": "5.0.0",
+                "loader-utils": "1.2.3",
+                "object-path": "0.11.4",
+                "regex-parser": "2.2.10"
+            },
+            "resolved": "https://registry.npmjs.org/adjust-sourcemap-loader/-/adjust-sourcemap-loader-2.0.0.tgz",
+            "version": "2.0.0"
         },
         "ajv": {
-            "integrity": "sha512-TXtUUEYHuaTEbLZWIKUr5pmBuhDLy+8KYtPYdcV8qC+pOZL+NKqYwvWSRrVXHn+ZmRRAu8vJTAznH7Oag6RVRw==",
+            "integrity": "sha512-j3fVLgvTo527anyYyJOGTYJbG+vnnQYvE0m5mmkc1TK+nxAppkCLMIL0aZ4dblVCNoGShhm+kzE4ZUykBoMg4g==",
             "requires": {
-                "fast-deep-equal": "^2.0.1",
+                "fast-deep-equal": "^3.1.1",
                 "fast-json-stable-stringify": "^2.0.0",
                 "json-schema-traverse": "^0.4.1",
                 "uri-js": "^4.2.2"
             },
-            "resolved": "https://registry.npmjs.org/ajv/-/ajv-6.10.2.tgz",
-            "version": "6.10.2"
+            "resolved": "https://registry.npmjs.org/ajv/-/ajv-6.12.6.tgz",
+            "version": "6.12.6"
         },
         "ajv-errors": {
             "integrity": "sha512-DCRfO/4nQ+89p/RK43i8Ezd41EqdGIU4ld7nGF8OQ14oc/we5rEntLCUa7+jrn3nn83BosfwZA0wb4pon2o8iQ==",
             "resolved": "https://registry.npmjs.org/ajv-errors/-/ajv-errors-1.0.1.tgz",
             "version": "1.0.1"
         },
         "ajv-keywords": {
-            "integrity": "sha512-RO1ibKvd27e6FEShVFfPALuHI3WjSVNeK5FIsmme/LYRNxjKuNj+Dt7bucLa6NdSv3JcVTyMlm9kGR84z1XpaQ==",
-            "resolved": "https://registry.npmjs.org/ajv-keywords/-/ajv-keywords-3.4.1.tgz",
-            "version": "3.4.1"
+            "integrity": "sha512-5p6WTN0DdTGVQk6VjcEju19IgaHudalcfabD7yhDGeA6bcQnmL+CpveLJq/3hvfwd1aof6L386Ougkx6RfyMIQ==",
+            "resolved": "https://registry.npmjs.org/ajv-keywords/-/ajv-keywords-3.5.2.tgz",
+            "version": "3.5.2"
         },
         "amphtml-validator": {
-            "integrity": "sha1-26DDhUKJVjwK2qwpLNTWCW7k18g=",
+            "dependencies": {
+                "commander": {
+                    "integrity": "sha512-VlfT9F3V0v+jr4yxPc5gg9s62/fIVWsd2Bk2iD435um1NlGMYdVCq+MjcXnhYq2icNOizHr1kK+5TI6H0Hy0ag==",
+                    "resolved": "https://registry.npmjs.org/commander/-/commander-2.15.1.tgz",
+                    "version": "2.15.1"
+                }
+            },
+            "integrity": "sha512-CaEm2ivIi4M4QTiFnCE9t4MRgawCf88iAV/+VsS0zEw6T4VBU6zoXcgn4L+dt6/WZ/NYxKpc38duSoRLqZJhNQ==",
             "requires": {
-                "colors": "1.1.2",
-                "commander": "2.9.0",
-                "promise": "7.1.1"
+                "colors": "1.2.5",
+                "commander": "2.15.1",
+                "promise": "8.0.1"
             },
-            "resolved": "https://registry.npmjs.org/amphtml-validator/-/amphtml-validator-1.0.23.tgz",
-            "version": "1.0.23"
+            "resolved": "https://registry.npmjs.org/amphtml-validator/-/amphtml-validator-1.0.30.tgz",
+            "version": "1.0.30"
         },
         "ansi-colors": {
             "integrity": "sha512-hHUXGagefjN2iRrID63xckIvotOXOojhQKWIPUZ4mNUZ9nLZW+7FMNoE1lOkEhNWYsx/7ysGIuJYCiMAA9FnrA==",
             "resolved": "https://registry.npmjs.org/ansi-colors/-/ansi-colors-3.2.4.tgz",
             "version": "3.2.4"
         },
         "ansi-html": {
@@ -1150,37 +2241,40 @@
             "requires": {
                 "color-convert": "^1.9.0"
             },
             "resolved": "https://registry.npmjs.org/ansi-styles/-/ansi-styles-3.2.1.tgz",
             "version": "3.2.1"
         },
         "anymatch": {
-            "dependencies": {
-                "normalize-path": {
-                    "integrity": "sha1-GrKLVW4Zg2Oowab35vogE3/mrtk=",
-                    "requires": {
-                        "remove-trailing-separator": "^1.0.1"
-                    },
-                    "resolved": "https://registry.npmjs.org/normalize-path/-/normalize-path-2.1.1.tgz",
-                    "version": "2.1.1"
-                }
-            },
-            "integrity": "sha512-5teOsQWABXHHBFP9y3skS5P3d/WfWXpv3FUpy+LorMrNYaT9pI4oLMQX7jzQ2KklNpGpWHzdCXTDT2Y3XGlZBw==",
+            "integrity": "sha512-P43ePfOAIupkguHUycrc4qJ9kz8ZiuOUijaETwX7THt0Y/GNK7v0aa8rY816xWjZ7rJdA5XdMcpVFTKMq+RvWg==",
             "requires": {
-                "micromatch": "^3.1.4",
-                "normalize-path": "^2.1.1"
+                "normalize-path": "^3.0.0",
+                "picomatch": "^2.0.4"
             },
-            "resolved": "https://registry.npmjs.org/anymatch/-/anymatch-2.0.0.tgz",
-            "version": "2.0.0"
+            "resolved": "https://registry.npmjs.org/anymatch/-/anymatch-3.1.2.tgz",
+            "version": "3.1.2"
         },
         "aproba": {
             "integrity": "sha512-Y9J6ZjXtoYh8RnXVCMOU/ttDmk1aBjunq9vO0ta5x85WDQiQfUF9sIPBITdbiiIVcBo03Hi3jMxigBtsddlXRw==",
             "resolved": "https://registry.npmjs.org/aproba/-/aproba-1.2.0.tgz",
             "version": "1.2.0"
         },
+        "argparse": {
+            "integrity": "sha512-o5Roy6tNG4SL/FOkCAN6RzjiakZS25RLYFrcMttJqbdd8BWrnA+fGz57iN5Pb06pvBGvl5gQ0B48dJlslXvoTg==",
+            "requires": {
+                "sprintf-js": "~1.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/argparse/-/argparse-1.0.10.tgz",
+            "version": "1.0.10"
+        },
+        "arity-n": {
+            "integrity": "sha1-2edrEXM+CFacCEeuezmyhgswt0U=",
+            "resolved": "https://registry.npmjs.org/arity-n/-/arity-n-1.0.4.tgz",
+            "version": "1.0.4"
+        },
         "arr-diff": {
             "integrity": "sha1-1kYQdP6/7HHn4VI1dhoyml3HxSA=",
             "resolved": "https://registry.npmjs.org/arr-diff/-/arr-diff-4.0.0.tgz",
             "version": "4.0.0"
         },
         "arr-flatten": {
             "integrity": "sha512-L3hKV5R/p5o81R7O02IGnwpDmkp6E982XhtbuwSe3O4qOtMMMtodicASA1Cny2U+aCXcNpml+m4dPsvsJ3jatg==",
@@ -1212,57 +2306,63 @@
         },
         "asap": {
             "integrity": "sha1-5QNHYR1+aQlDIIu9r+vLwvuGbUY=",
             "resolved": "https://registry.npmjs.org/asap/-/asap-2.0.6.tgz",
             "version": "2.0.6"
         },
         "asn1.js": {
-            "integrity": "sha512-p32cOF5q0Zqs9uBiONKYLm6BClCoBCM5O9JfeUSlnQLBTxYdTK+pW+nXflm8UkKd2UYlEbYz5qEi0JuZR9ckSw==",
+            "dependencies": {
+                "bn.js": {
+                    "integrity": "sha512-c98Bf3tPniI+scsdk237ku1Dc3ujXQTSgyiPUDEOe7tRkhrqridvh8klBv0HCEso1OLOYcHuCv/cS6DNxKH+ZA==",
+                    "resolved": "https://registry.npmjs.org/bn.js/-/bn.js-4.12.0.tgz",
+                    "version": "4.12.0"
+                }
+            },
+            "integrity": "sha512-+I//4cYPccV8LdmBLiX8CYvf9Sp3vQsrqu2QNXRcrbiWvcx/UdlFiqUJJzxRQxgsZmvhXhn4cSKeSmoFjVdupA==",
             "requires": {
                 "bn.js": "^4.0.0",
                 "inherits": "^2.0.1",
-                "minimalistic-assert": "^1.0.0"
+                "minimalistic-assert": "^1.0.0",
+                "safer-buffer": "^2.1.0"
             },
-            "resolved": "https://registry.npmjs.org/asn1.js/-/asn1.js-4.10.1.tgz",
-            "version": "4.10.1"
+            "resolved": "https://registry.npmjs.org/asn1.js/-/asn1.js-5.4.1.tgz",
+            "version": "5.4.1"
         },
         "assert": {
-            "dependencies": {
-                "inherits": {
-                    "integrity": "sha1-sX0I0ya0Qj5Wjv9xn5GwscvfafE=",
-                    "resolved": "https://registry.npmjs.org/inherits/-/inherits-2.0.1.tgz",
-                    "version": "2.0.1"
-                },
-                "util": {
-                    "integrity": "sha1-evsa/lCAUkZInj23/g7TeTNqwPk=",
-                    "requires": {
-                        "inherits": "2.0.1"
-                    },
-                    "resolved": "https://registry.npmjs.org/util/-/util-0.10.3.tgz",
-                    "version": "0.10.3"
-                }
-            },
-            "integrity": "sha512-EDsgawzwoun2CZkCgtxJbv392v4nbk9XDD06zI+kQYoBM/3RBWLlEyJARDOmhAAosBjWACEkKL6S+lIZtcAubA==",
+            "integrity": "sha1-mZEtWRg2tab1s0XA8H7vwI/GXZE=",
             "requires": {
-                "object-assign": "^4.1.1",
                 "util": "0.10.3"
             },
-            "resolved": "https://registry.npmjs.org/assert/-/assert-1.5.0.tgz",
-            "version": "1.5.0"
+            "resolved": "https://registry.npmjs.org/assert/-/assert-1.4.1.tgz",
+            "version": "1.4.1"
         },
         "assign-symbols": {
             "integrity": "sha1-WWZ/QfrdTyDMvCu5a41Pf3jsA2c=",
             "resolved": "https://registry.npmjs.org/assign-symbols/-/assign-symbols-1.0.0.tgz",
             "version": "1.0.0"
         },
+        "ast-types": {
+            "integrity": "sha512-uWMHxJxtfj/1oZClOxDEV1sQ1HCDkA4MG8Gr69KKeBjEVH0R84WlejZ0y2DcwyBlpAEMltmVYkVgqfLFb2oyiA==",
+            "resolved": "https://registry.npmjs.org/ast-types/-/ast-types-0.13.2.tgz",
+            "version": "0.13.2"
+        },
         "async-each": {
             "integrity": "sha512-z/WhQ5FPySLdvREByI2vZiTWwCnF0moMJ1hK9YQwDTHKh6I7/uSckMetoRGb5UBZPC1z0jlw+n/XCgjeH7y1AQ==",
+            "optional": true,
             "resolved": "https://registry.npmjs.org/async-each/-/async-each-1.0.3.tgz",
             "version": "1.0.3"
         },
+        "async-retry": {
+            "integrity": "sha512-tfDb02Th6CE6pJUF2gjW5ZVjsgwlucVXOEQMvEX9JgSJMs9gAX+Nz3xRuJBKuUYjTSYORqvDBORdAQ3LU59g7Q==",
+            "requires": {
+                "retry": "0.12.0"
+            },
+            "resolved": "https://registry.npmjs.org/async-retry/-/async-retry-1.2.3.tgz",
+            "version": "1.2.3"
+        },
         "async-sema": {
             "integrity": "sha512-zyCMBDl4m71feawrxYcVbHxv/UUkqm4nKJiLu3+l9lfiQha6jQ/9dxhrXLnzzBXVFqCTDwiUkZOz9XFbdEGQsg==",
             "resolved": "https://registry.npmjs.org/async-sema/-/async-sema-3.0.0.tgz",
             "version": "3.0.0"
         },
         "atob": {
             "integrity": "sha512-Wm6ukoaOGJi/73p/cl2GvLjTI5JM1k/O14isD73YML8StrH/7/lRFgmg8nICZgD3bZZvjwCGxtMOD3wWNAu8cg==",
@@ -1282,14 +2382,47 @@
                 "tapable": "^1.0.0",
                 "webpack-merge": "^4.1.0",
                 "webpack-sources": "^1.0.1"
             },
             "resolved": "https://registry.npmjs.org/autodll-webpack-plugin/-/autodll-webpack-plugin-0.4.2.tgz",
             "version": "0.4.2"
         },
+        "autoprefixer": {
+            "dependencies": {
+                "browserslist": {
+                    "integrity": "sha512-Wspk/PqO+4W9qp5iUTJsa1B/QrYn1keNCcEP5OvP7WBwT4KaDly0uONYmC6Xa3Z5IqnUgS0KcgLYu1l74x0ZXQ==",
+                    "requires": {
+                        "caniuse-lite": "^1.0.30001219",
+                        "colorette": "^1.2.2",
+                        "electron-to-chromium": "^1.3.723",
+                        "escalade": "^3.1.1",
+                        "node-releases": "^1.1.71"
+                    },
+                    "resolved": "https://registry.npmjs.org/browserslist/-/browserslist-4.16.6.tgz",
+                    "version": "4.16.6"
+                },
+                "postcss-value-parser": {
+                    "integrity": "sha512-97DXOFbQJhk71ne5/Mt6cOu6yxsSfM0QGQyl0L25Gca4yGWEGJaig7l7gbCX623VqTBNGLRLaVUCnNkcedlRSQ==",
+                    "resolved": "https://registry.npmjs.org/postcss-value-parser/-/postcss-value-parser-4.1.0.tgz",
+                    "version": "4.1.0"
+                }
+            },
+            "integrity": "sha512-XrvP4VVHdRBCdX1S3WXVD8+RyG9qeb1D5Sn1DeLiG2xfSpzellk5k54xbUERJ3M5DggQxes39UGOTP8CFrEGbg==",
+            "requires": {
+                "browserslist": "^4.12.0",
+                "caniuse-lite": "^1.0.30001109",
+                "colorette": "^1.2.1",
+                "normalize-range": "^0.1.2",
+                "num2fraction": "^1.2.2",
+                "postcss": "^7.0.32",
+                "postcss-value-parser": "^4.1.0"
+            },
+            "resolved": "https://registry.npmjs.org/autoprefixer/-/autoprefixer-9.8.6.tgz",
+            "version": "9.8.6"
+        },
         "babel-code-frame": {
             "dependencies": {
                 "ansi-styles": {
                     "integrity": "sha1-tDLdM1i2NM914eRmQ2gkBTPB3b4=",
                     "resolved": "https://registry.npmjs.org/ansi-styles/-/ansi-styles-2.2.1.tgz",
                     "version": "2.2.1"
                 },
@@ -1354,14 +2487,32 @@
                     "integrity": "sha512-1yD6RmLI1XBfxugvORwlck6f75tYL+iR0jqwsOrOxMZyGYqUuDhJ0l4AXdO1iX/FTs9cBAMEk1gWSEx1kSbylg==",
                     "requires": {
                         "locate-path": "^3.0.0"
                     },
                     "resolved": "https://registry.npmjs.org/find-up/-/find-up-3.0.0.tgz",
                     "version": "3.0.0"
                 },
+                "json5": {
+                    "integrity": "sha512-aKS4WQjPenRxiQsC93MNfjx+nbF4PAdYzmd/1JIj8HYzqfbu86beTuNgXDzPknWk0n0uARlyewZo4s++ES36Ow==",
+                    "requires": {
+                        "minimist": "^1.2.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/json5/-/json5-1.0.1.tgz",
+                    "version": "1.0.1"
+                },
+                "loader-utils": {
+                    "integrity": "sha512-qH0WSMBtn/oHuwjy/NucEgbx5dbxxnxup9s4PVXJUDHZBQY+s0NWA9rJf53RBnQZxfch7euUui7hpoAPvALZdA==",
+                    "requires": {
+                        "big.js": "^5.2.2",
+                        "emojis-list": "^3.0.0",
+                        "json5": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/loader-utils/-/loader-utils-1.4.0.tgz",
+                    "version": "1.4.0"
+                },
                 "locate-path": {
                     "integrity": "sha512-7AO748wWnIhNqAuaty2ZWHkQHRSNfPVIsPIfwEOWO22AmaoVrWavlOcMR5nzTLNYvp36X220/maaRsrec1G65A==",
                     "requires": {
                         "p-locate": "^3.0.0",
                         "path-exists": "^3.0.0"
                     },
                     "resolved": "https://registry.npmjs.org/locate-path/-/locate-path-3.0.0.tgz",
@@ -1373,20 +2524,20 @@
                         "pify": "^4.0.1",
                         "semver": "^5.6.0"
                     },
                     "resolved": "https://registry.npmjs.org/make-dir/-/make-dir-2.1.0.tgz",
                     "version": "2.1.0"
                 },
                 "p-limit": {
-                    "integrity": "sha512-85Tk+90UCVWvbDavCLKPOLC9vvY8OwEX/RtKF+/1OADJMVlFfEHOiMTPVyxg7mk/dKa+ipdHm0OUkTvCpMTuwg==",
+                    "integrity": "sha512-//88mFWSJx8lxCzwdAABTJL2MyWB12+eIY7MDL2SqLmAkeKU9qxRvWuSyTjm3FUmpBEMuFfckAIqEaVGUDxb6w==",
                     "requires": {
                         "p-try": "^2.0.0"
                     },
-                    "resolved": "https://registry.npmjs.org/p-limit/-/p-limit-2.2.1.tgz",
-                    "version": "2.2.1"
+                    "resolved": "https://registry.npmjs.org/p-limit/-/p-limit-2.3.0.tgz",
+                    "version": "2.3.0"
                 },
                 "p-locate": {
                     "integrity": "sha512-x+12w/To+4GFfgJhBEpiDcLozRJGegY+Ei7/z0tSLkMmxGZNybVMSfWj9aJn8Z5Fc7dBUNJOOVgPv2H7IwulSQ==",
                     "requires": {
                         "p-limit": "^2.0.0"
                     },
                     "resolved": "https://registry.npmjs.org/p-locate/-/p-locate-3.0.0.tgz",
@@ -1418,20 +2569,20 @@
                 "mkdirp": "^0.5.1",
                 "pify": "^4.0.1"
             },
             "resolved": "https://registry.npmjs.org/babel-loader/-/babel-loader-8.0.6.tgz",
             "version": "8.0.6"
         },
         "babel-plugin-dynamic-import-node": {
-            "integrity": "sha512-o6qFkpeQEBxcqt0XYlWzAVxNCSCZdUgcR8IRlhD/8DylxjjO4foPcvTW0GGKa/cVt3rvxZ7o5ippJ+/0nvLhlQ==",
+            "integrity": "sha512-jZVI+s9Zg3IqA/kdi0i6UDCybUI3aSBLnglhYbSSjKlV7yF1F/5LWv8MakQmvYpnbJDS6fcBL2KzHSxNCMtWSQ==",
             "requires": {
                 "object.assign": "^4.1.0"
             },
-            "resolved": "https://registry.npmjs.org/babel-plugin-dynamic-import-node/-/babel-plugin-dynamic-import-node-2.3.0.tgz",
-            "version": "2.3.0"
+            "resolved": "https://registry.npmjs.org/babel-plugin-dynamic-import-node/-/babel-plugin-dynamic-import-node-2.3.3.tgz",
+            "version": "2.3.3"
         },
         "babel-plugin-styled-components": {
             "integrity": "sha512-YwrInHyKUk1PU3avIRdiLyCpM++18Rs1NgyMXEAQC33rIXs/vro0A+stf4sT0Gf22Got+xRWB8Cm0tw+qkRzBA==",
             "requires": {
                 "@babel/helper-annotate-as-pure": "^7.0.0",
                 "@babel/helper-module-imports": "^7.0.0",
                 "babel-plugin-syntax-jsx": "^6.18.0",
@@ -1442,65 +2593,31 @@
         },
         "babel-plugin-syntax-jsx": {
             "integrity": "sha1-CvMqmm4Tyno/1QaeYtew9Y0NiUY=",
             "resolved": "https://registry.npmjs.org/babel-plugin-syntax-jsx/-/babel-plugin-syntax-jsx-6.18.0.tgz",
             "version": "6.18.0"
         },
         "babel-plugin-transform-define": {
-            "integrity": "sha512-JXZ1xE9jIbKCGYZ4wbSMPSI5mdS4DRLi5+SkTHgZqWn5YIf/EucykkzUsPmzJlpkX8fsMVdLnA5vt/LvT97Zbg==",
+            "integrity": "sha512-0dv5RNRUlUKxGYIIErl01lpvi8b7W2R04Qcl1mCj70ahwZcgiklfXnFlh4FGnRh6aayCfSZKdhiMryVzcq5Dmg==",
             "requires": {
                 "lodash": "^4.17.11",
                 "traverse": "0.6.6"
             },
-            "resolved": "https://registry.npmjs.org/babel-plugin-transform-define/-/babel-plugin-transform-define-1.3.1.tgz",
-            "version": "1.3.1"
+            "resolved": "https://registry.npmjs.org/babel-plugin-transform-define/-/babel-plugin-transform-define-2.0.0.tgz",
+            "version": "2.0.0"
         },
         "babel-plugin-transform-react-remove-prop-types": {
             "integrity": "sha512-eqj0hVcJUR57/Ug2zE1Yswsw4LhuqqHhD+8v120T1cl3kjg76QwtyBrdIk4WVwK+lAhBJVYCd/v+4nc4y+8JsA==",
             "resolved": "https://registry.npmjs.org/babel-plugin-transform-react-remove-prop-types/-/babel-plugin-transform-react-remove-prop-types-0.4.24.tgz",
             "version": "0.4.24"
         },
-        "babel-runtime": {
-            "dependencies": {
-                "regenerator-runtime": {
-                    "integrity": "sha512-MguG95oij0fC3QV3URf4V2SDYGJhJnJGqvIIgdECeODCT98wSWDAJ94SSuVpYQUoTcGUIL6L4yNB7j1DFFHSBg==",
-                    "resolved": "https://registry.npmjs.org/regenerator-runtime/-/regenerator-runtime-0.11.1.tgz",
-                    "version": "0.11.1"
-                }
-            },
-            "integrity": "sha1-llxwWGaOgrVde/4E/yM3vItWR/4=",
-            "requires": {
-                "core-js": "^2.4.0",
-                "regenerator-runtime": "^0.11.0"
-            },
-            "resolved": "https://registry.npmjs.org/babel-runtime/-/babel-runtime-6.26.0.tgz",
-            "version": "6.26.0"
-        },
-        "babel-types": {
-            "dependencies": {
-                "to-fast-properties": {
-                    "integrity": "sha1-uDVx+k2MJbguIxsG46MFXeTKGkc=",
-                    "resolved": "https://registry.npmjs.org/to-fast-properties/-/to-fast-properties-1.0.3.tgz",
-                    "version": "1.0.3"
-                }
-            },
-            "integrity": "sha1-o7Bz+Uq0nrb6Vc1lInozQ4BjJJc=",
-            "requires": {
-                "babel-runtime": "^6.26.0",
-                "esutils": "^2.0.2",
-                "lodash": "^4.17.4",
-                "to-fast-properties": "^1.0.3"
-            },
-            "resolved": "https://registry.npmjs.org/babel-types/-/babel-types-6.26.0.tgz",
-            "version": "6.26.0"
-        },
         "balanced-match": {
-            "integrity": "sha1-ibTRmasr7kneFk6gK4nORi1xt2c=",
-            "resolved": "https://registry.npmjs.org/balanced-match/-/balanced-match-1.0.0.tgz",
-            "version": "1.0.0"
+            "integrity": "sha512-3oSeUO0TMV67hN1AmbXsK4yaqU7tjiHlbxRDZOpH0KW9+CeX4bRAaX0Anxt0tx2MrpRpWwQaPwIlISEJhYU5Pw==",
+            "resolved": "https://registry.npmjs.org/balanced-match/-/balanced-match-1.0.2.tgz",
+            "version": "1.0.2"
         },
         "base": {
             "dependencies": {
                 "define-property": {
                     "integrity": "sha1-dp66rz9KY6rTr56NMEybvnm/sOY=",
                     "requires": {
                         "is-descriptor": "^1.0.0"
@@ -1545,73 +2662,63 @@
                 "mixin-deep": "^1.2.0",
                 "pascalcase": "^0.1.1"
             },
             "resolved": "https://registry.npmjs.org/base/-/base-0.11.2.tgz",
             "version": "0.11.2"
         },
         "base64-js": {
-            "integrity": "sha512-mLQ4i2QO1ytvGWFWmcngKO//JXAQueZvwEKtjgQFM4jIK0kU+ytMfplL8j+n5mspOfjHwoAg+9yhb7BwAHm36g==",
-            "resolved": "https://registry.npmjs.org/base64-js/-/base64-js-1.3.1.tgz",
-            "version": "1.3.1"
+            "integrity": "sha512-AKpaYlHn8t4SVbOHCy+b5+KKgvR4vrsD8vbvrbiQJps7fKDTkjkDry6ji0rUJjC0kzbNePLwzxq8iypo41qeWA==",
+            "resolved": "https://registry.npmjs.org/base64-js/-/base64-js-1.5.1.tgz",
+            "version": "1.5.1"
         },
         "big.js": {
             "integrity": "sha512-vyL2OymJxmarO8gxMr0mhChsO9QGwhynfuu4+MHTAW6czfq9humCB7rKpUjDd9YUiDPU4mzpyupFSvOClAwbmQ==",
             "resolved": "https://registry.npmjs.org/big.js/-/big.js-5.2.2.tgz",
             "version": "5.2.2"
         },
         "binary-extensions": {
-            "integrity": "sha512-Un7MIEDdUC5gNpcGDV97op1Ywk748MpHcFTHoYs6qnj1Z3j7I53VG3nwZhKzoBZmbdRNnb6WRdFlwl7tSDuZGw==",
-            "resolved": "https://registry.npmjs.org/binary-extensions/-/binary-extensions-1.13.1.tgz",
-            "version": "1.13.1"
+            "integrity": "sha512-jDctJ/IVQbZoJykoeHbhXpOlNBqGNcwXJKJog42E5HDPUwQTSdjCHdihjj0DlnheQ7blbT6dHOafNAiS8ooQKA==",
+            "resolved": "https://registry.npmjs.org/binary-extensions/-/binary-extensions-2.2.0.tgz",
+            "version": "2.2.0"
+        },
+        "bindings": {
+            "integrity": "sha512-p2q/t/mhvuOj/UeLlV6566GD/guowlr0hHxClI0W9m7MWYkL1F0hLo+0Aexs9HSPCtR1SXQ0TD3MMKrXZajbiQ==",
+            "optional": true,
+            "requires": {
+                "file-uri-to-path": "1.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/bindings/-/bindings-1.5.0.tgz",
+            "version": "1.5.0"
         },
         "bluebird": {
-            "integrity": "sha512-5am6HnnfN+urzt4yfg7IgTbotDjIT/u8AJpEt0sIU9FtXfVeezXAPKswrG+xKUCOYAINpSdgZVDU6QFh+cuH3w==",
-            "resolved": "https://registry.npmjs.org/bluebird/-/bluebird-3.5.5.tgz",
-            "version": "3.5.5"
+            "integrity": "sha512-XpNj6GDQzdfW+r2Wnn7xiSAd7TM3jzkxGXBGTtWKuSXv1xUV+azxAm8jdWZN06QTQk+2N2XB9jRDkvbmQmcRtg==",
+            "resolved": "https://registry.npmjs.org/bluebird/-/bluebird-3.7.2.tgz",
+            "version": "3.7.2"
         },
         "bn.js": {
-            "integrity": "sha512-ItfYfPLkWHUjckQCk8xC+LwxgK8NYcXywGigJgSwOP8Y2iyWT4f2vsZnoOXTTbo+o5yXmIUJ4gn5538SO5S3gA==",
-            "resolved": "https://registry.npmjs.org/bn.js/-/bn.js-4.11.8.tgz",
-            "version": "4.11.8"
+            "integrity": "sha512-D7iWRBvnZE8ecXiLj/9wbxH7Tk79fAh8IHaTNq1RWRixsS02W+5qS+iE9yq6RYl0asXx5tw0bLhmT5pIfbSquw==",
+            "resolved": "https://registry.npmjs.org/bn.js/-/bn.js-5.2.0.tgz",
+            "version": "5.2.0"
         },
         "brace-expansion": {
             "integrity": "sha512-iCuPHDFgrHX7H2vEI/5xpz07zSHB00TpugqhmYtVmMO6518mCuRMoOYFldEBl0g187ufozdaHgWKcYFb61qGiA==",
             "requires": {
                 "balanced-match": "^1.0.0",
                 "concat-map": "0.0.1"
             },
             "resolved": "https://registry.npmjs.org/brace-expansion/-/brace-expansion-1.1.11.tgz",
             "version": "1.1.11"
         },
         "braces": {
-            "dependencies": {
-                "extend-shallow": {
-                    "integrity": "sha1-Ua99YUrZqfYQ6huvu5idaxxWiQ8=",
-                    "requires": {
-                        "is-extendable": "^0.1.0"
-                    },
-                    "resolved": "https://registry.npmjs.org/extend-shallow/-/extend-shallow-2.0.1.tgz",
-                    "version": "2.0.1"
-                }
-            },
-            "integrity": "sha512-aNdbnj9P8PjdXU4ybaWLK2IF3jc/EoDYbC7AazW6to3TRsfXxscC9UXOB5iDiEQrkyIbWp2SLQda4+QAa7nc3w==",
+            "integrity": "sha512-b8um+L1RzM3WDSzvhm6gIz1yfTbBt6YTlcEKAvsmqCZZFw46z626lVj9j1yEPW33H5H+lBQpZMP1k8l+78Ha0A==",
             "requires": {
-                "arr-flatten": "^1.1.0",
-                "array-unique": "^0.3.2",
-                "extend-shallow": "^2.0.1",
-                "fill-range": "^4.0.0",
-                "isobject": "^3.0.1",
-                "repeat-element": "^1.1.2",
-                "snapdragon": "^0.8.1",
-                "snapdragon-node": "^2.0.1",
-                "split-string": "^3.0.2",
-                "to-regex": "^3.0.1"
+                "fill-range": "^7.0.1"
             },
-            "resolved": "https://registry.npmjs.org/braces/-/braces-2.3.2.tgz",
-            "version": "2.3.2"
+            "resolved": "https://registry.npmjs.org/braces/-/braces-3.0.2.tgz",
+            "version": "3.0.2"
         },
         "brorand": {
             "integrity": "sha1-EsJe/kCkXjwyPrhnWgoM5XsiNx8=",
             "resolved": "https://registry.npmjs.org/brorand/-/brorand-1.1.0.tgz",
             "version": "1.1.0"
         },
         "browserify-aes": {
@@ -1645,69 +2752,98 @@
                 "inherits": "^2.0.1",
                 "safe-buffer": "^5.1.2"
             },
             "resolved": "https://registry.npmjs.org/browserify-des/-/browserify-des-1.0.2.tgz",
             "version": "1.0.2"
         },
         "browserify-rsa": {
-            "integrity": "sha1-IeCr+vbyApzy+vsTNWenAdQTVSQ=",
+            "integrity": "sha512-AdEER0Hkspgno2aR97SAf6vi0y0k8NuOpGnVH3O99rcA5Q6sh8QxcngtHuJ6uXwnfAXNM4Gn1Gb7/MV1+Ymbog==",
             "requires": {
-                "bn.js": "^4.1.0",
+                "bn.js": "^5.0.0",
                 "randombytes": "^2.0.1"
             },
-            "resolved": "https://registry.npmjs.org/browserify-rsa/-/browserify-rsa-4.0.1.tgz",
-            "version": "4.0.1"
+            "resolved": "https://registry.npmjs.org/browserify-rsa/-/browserify-rsa-4.1.0.tgz",
+            "version": "4.1.0"
         },
         "browserify-sign": {
-            "integrity": "sha1-qk62jl17ZYuqa/alfmMMvXqT0pg=",
-            "requires": {
-                "bn.js": "^4.1.1",
-                "browserify-rsa": "^4.0.0",
-                "create-hash": "^1.1.0",
-                "create-hmac": "^1.1.2",
-                "elliptic": "^6.0.0",
-                "inherits": "^2.0.1",
-                "parse-asn1": "^5.0.0"
+            "dependencies": {
+                "readable-stream": {
+                    "integrity": "sha512-BViHy7LKeTz4oNnkcLJ+lVSL6vpiFeX6/d3oSH8zCW7UxP2onchk+vTGB143xuFjHS3deTgkKoXXymXqymiIdA==",
+                    "requires": {
+                        "inherits": "^2.0.3",
+                        "string_decoder": "^1.1.1",
+                        "util-deprecate": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/readable-stream/-/readable-stream-3.6.0.tgz",
+                    "version": "3.6.0"
+                },
+                "safe-buffer": {
+                    "integrity": "sha512-rp3So07KcdmmKbGvgaNxQSJr7bGVSVk5S9Eq1F+ppbRo70+YeaDxkw5Dd8NPN+GD6bjnYm2VuPuCXmpuYvmCXQ==",
+                    "resolved": "https://registry.npmjs.org/safe-buffer/-/safe-buffer-5.2.1.tgz",
+                    "version": "5.2.1"
+                }
+            },
+            "integrity": "sha512-/vrA5fguVAKKAVTNJjgSm1tRQDHUU6DbwO9IROu/0WAzC8PKhucDSh18J0RMvVeHAn5puMd+QHC2erPRNf8lmg==",
+            "requires": {
+                "bn.js": "^5.1.1",
+                "browserify-rsa": "^4.0.1",
+                "create-hash": "^1.2.0",
+                "create-hmac": "^1.1.7",
+                "elliptic": "^6.5.3",
+                "inherits": "^2.0.4",
+                "parse-asn1": "^5.1.5",
+                "readable-stream": "^3.6.0",
+                "safe-buffer": "^5.2.0"
             },
-            "resolved": "https://registry.npmjs.org/browserify-sign/-/browserify-sign-4.0.4.tgz",
-            "version": "4.0.4"
+            "resolved": "https://registry.npmjs.org/browserify-sign/-/browserify-sign-4.2.1.tgz",
+            "version": "4.2.1"
         },
         "browserify-zlib": {
             "integrity": "sha512-Z942RysHXmJrhqk88FmKBVq/v5tqmSkDz7p54G/MGyjMnCFFnC79XWNbg+Vta8W6Wb2qtSZTSxIGkJrRpCFEiA==",
             "requires": {
                 "pako": "~1.0.5"
             },
             "resolved": "https://registry.npmjs.org/browserify-zlib/-/browserify-zlib-0.2.0.tgz",
             "version": "0.2.0"
         },
         "browserslist": {
-            "integrity": "sha512-D2Nk3W9JL9Fp/gIcWei8LrERCS+eXu9AM5cfXA8WEZ84lFks+ARnZ0q/R69m2SV3Wjma83QDDPxsNKXUwdIsyA==",
+            "integrity": "sha512-iU43cMMknxG1ClEZ2MDKeonKE1CCrFVkQK2AqO2YWFmvIrx4JWrvQ4w4hQez6EpVI8rHTtqh/ruHHDHSOKxvUg==",
             "requires": {
-                "caniuse-lite": "^1.0.30000984",
-                "electron-to-chromium": "^1.3.191",
-                "node-releases": "^1.1.25"
+                "caniuse-lite": "^1.0.30001017",
+                "electron-to-chromium": "^1.3.322",
+                "node-releases": "^1.1.44"
             },
-            "resolved": "https://registry.npmjs.org/browserslist/-/browserslist-4.6.6.tgz",
-            "version": "4.6.6"
+            "resolved": "https://registry.npmjs.org/browserslist/-/browserslist-4.8.3.tgz",
+            "version": "4.8.3"
         },
         "buffer": {
-            "integrity": "sha1-bRu2AbB6TvztlwlBMgkwJ8lbwpg=",
+            "integrity": "sha512-xq+q3SRMOxGivLhBNaUdC64hDTQwejJ+H0T/NB1XMtTVEwNTrfFF3gAxiyW0Bu/xWEGhjVKgUcMhCrUy2+uCWg==",
             "requires": {
                 "base64-js": "^1.0.2",
                 "ieee754": "^1.1.4",
                 "isarray": "^1.0.0"
             },
-            "resolved": "https://registry.npmjs.org/buffer/-/buffer-4.9.1.tgz",
-            "version": "4.9.1"
+            "resolved": "https://registry.npmjs.org/buffer/-/buffer-4.9.2.tgz",
+            "version": "4.9.2"
+        },
+        "buffer-equal-constant-time": {
+            "integrity": "sha1-+OcRMvf/5uAaXJaXpMbz5I1cyBk=",
+            "resolved": "https://registry.npmjs.org/buffer-equal-constant-time/-/buffer-equal-constant-time-1.0.1.tgz",
+            "version": "1.0.1"
         },
         "buffer-from": {
             "integrity": "sha512-MQcXEUbCKtEo7bhqEs6560Hyd4XaovZlO/k9V3hjVUF/zwW7KBVdSK4gIt/bzwS9MbR5qob+F5jusZsb0YQK2A==",
             "resolved": "https://registry.npmjs.org/buffer-from/-/buffer-from-1.1.1.tgz",
             "version": "1.1.1"
         },
+        "buffer-json": {
+            "integrity": "sha512-+jjPFVqyfF1esi9fvfUs3NqM0pH1ziZ36VP4hmA/y/Ssfo/5w5xHKfTw9BwQjoJ1w/oVtpLomqwUHKdefGyuHw==",
+            "resolved": "https://registry.npmjs.org/buffer-json/-/buffer-json-2.0.0.tgz",
+            "version": "2.0.0"
+        },
         "buffer-xor": {
             "integrity": "sha1-JuYe0UIvtw3ULm42cp7VHYVf6Nk=",
             "resolved": "https://registry.npmjs.org/buffer-xor/-/buffer-xor-1.0.3.tgz",
             "version": "1.0.3"
         },
         "builtin-status-codes": {
             "integrity": "sha1-hZgoeOIbmOHGZCXgPQF0eI9Wnug=",
@@ -1716,15 +2852,15 @@
         },
         "bytes": {
             "integrity": "sha1-0ygVQE1olpn4Wk6k+odV3ROpYEg=",
             "resolved": "https://registry.npmjs.org/bytes/-/bytes-3.0.0.tgz",
             "version": "3.0.0"
         },
         "cacache": {
-            "integrity": "sha512-kqdmfXEGFepesTuROHMs3MpFLWrPkSSpRqOw80RCflZXy/khxaArvFrQ7uJxSUduzAufc6G0g1VUCOZXxWavPw==",
+            "integrity": "sha512-a0tMB40oefvuInr4Cwb3GerbL9xTj1D5yg0T5xrjGCGyfvbxseIXX7BAO/u/hIXdafzOI5JC3wDwHyf24buOAQ==",
             "requires": {
                 "bluebird": "^3.5.5",
                 "chownr": "^1.1.1",
                 "figgy-pudding": "^3.5.1",
                 "glob": "^7.1.4",
                 "graceful-fs": "^4.1.15",
                 "infer-owner": "^1.0.3",
@@ -1734,16 +2870,16 @@
                 "move-concurrently": "^1.0.1",
                 "promise-inflight": "^1.0.1",
                 "rimraf": "^2.6.3",
                 "ssri": "^6.0.1",
                 "unique-filename": "^1.1.1",
                 "y18n": "^4.0.0"
             },
-            "resolved": "https://registry.npmjs.org/cacache/-/cacache-12.0.3.tgz",
-            "version": "12.0.3"
+            "resolved": "https://registry.npmjs.org/cacache/-/cacache-12.0.4.tgz",
+            "version": "12.0.4"
         },
         "cache-base": {
             "integrity": "sha512-AKcdTnFSWATd5/GCPRxr2ChwIJ85CeyrEyjRHlKxQ56d4XJMGym0uAiKn0xbLOGOl3+yRpOTi484dVCEc5AUzQ==",
             "requires": {
                 "collection-visit": "^1.0.0",
                 "component-emitter": "^1.2.1",
                 "get-value": "^2.0.6",
@@ -1753,65 +2889,162 @@
                 "to-object-path": "^0.3.0",
                 "union-value": "^1.0.0",
                 "unset-value": "^1.0.0"
             },
             "resolved": "https://registry.npmjs.org/cache-base/-/cache-base-1.0.1.tgz",
             "version": "1.0.1"
         },
+        "cache-loader": {
+            "dependencies": {
+                "find-cache-dir": {
+                    "integrity": "sha512-t2GDMt3oGC/v+BMwzmllWDuJF/xcDtE5j/fCGbqDD7OLuJkj0cfh1YSA5VKPvwMeLFLNDBkwOKZ2X85jGLVftQ==",
+                    "requires": {
+                        "commondir": "^1.0.1",
+                        "make-dir": "^3.0.2",
+                        "pkg-dir": "^4.1.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/find-cache-dir/-/find-cache-dir-3.3.1.tgz",
+                    "version": "3.3.1"
+                },
+                "json5": {
+                    "integrity": "sha512-aKS4WQjPenRxiQsC93MNfjx+nbF4PAdYzmd/1JIj8HYzqfbu86beTuNgXDzPknWk0n0uARlyewZo4s++ES36Ow==",
+                    "requires": {
+                        "minimist": "^1.2.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/json5/-/json5-1.0.1.tgz",
+                    "version": "1.0.1"
+                },
+                "loader-utils": {
+                    "integrity": "sha512-qH0WSMBtn/oHuwjy/NucEgbx5dbxxnxup9s4PVXJUDHZBQY+s0NWA9rJf53RBnQZxfch7euUui7hpoAPvALZdA==",
+                    "requires": {
+                        "big.js": "^5.2.2",
+                        "emojis-list": "^3.0.0",
+                        "json5": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/loader-utils/-/loader-utils-1.4.0.tgz",
+                    "version": "1.4.0"
+                },
+                "make-dir": {
+                    "integrity": "sha512-g3FeP20LNwhALb/6Cz6Dd4F2ngze0jz7tbzrD2wAV+o9FeNHe4rL+yK2md0J/fiSf1sa1ADhXqi5+oVwOM/eGw==",
+                    "requires": {
+                        "semver": "^6.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/make-dir/-/make-dir-3.1.0.tgz",
+                    "version": "3.1.0"
+                },
+                "pkg-dir": {
+                    "integrity": "sha512-HRDzbaKjC+AOWVXxAU/x54COGeIv9eb+6CkDSQoNTt4XyWoIJvuPsXizxu/Fr23EiekbtZwmh1IcIG/l/a10GQ==",
+                    "requires": {
+                        "find-up": "^4.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/pkg-dir/-/pkg-dir-4.2.0.tgz",
+                    "version": "4.2.0"
+                },
+                "semver": {
+                    "integrity": "sha512-b39TBaTSfV6yBrapU89p5fKekE2m/NwnDocOVruQFS1/veMgdzuPcnOM34M6CwxW8jH/lxEa5rBoDeUwu5HHTw==",
+                    "resolved": "https://registry.npmjs.org/semver/-/semver-6.3.0.tgz",
+                    "version": "6.3.0"
+                }
+            },
+            "integrity": "sha512-ftOayxve0PwKzBF/GLsZNC9fJBXl8lkZE3TOsjkboHfVHVkL39iUEs1FO07A33mizmci5Dudt38UZrrYXDtbhw==",
+            "requires": {
+                "buffer-json": "^2.0.0",
+                "find-cache-dir": "^3.0.0",
+                "loader-utils": "^1.2.3",
+                "mkdirp": "^0.5.1",
+                "neo-async": "^2.6.1",
+                "schema-utils": "^2.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/cache-loader/-/cache-loader-4.1.0.tgz",
+            "version": "4.1.0"
+        },
+        "call-bind": {
+            "integrity": "sha512-7O+FbCihrB5WGbFYesctwmTKae6rOiIzmz1icreWJ+0aA7LJfuqhEso2T9ncpcFtzMQtzXf2QGGueWJGTYsqrA==",
+            "requires": {
+                "function-bind": "^1.1.1",
+                "get-intrinsic": "^1.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/call-bind/-/call-bind-1.0.2.tgz",
+            "version": "1.0.2"
+        },
+        "caller-callsite": {
+            "integrity": "sha1-hH4PzgoiN1CpoCfFSzNzGtMVQTQ=",
+            "requires": {
+                "callsites": "^2.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/caller-callsite/-/caller-callsite-2.0.0.tgz",
+            "version": "2.0.0"
+        },
+        "caller-path": {
+            "integrity": "sha1-Ro+DBE42mrIBD6xfBs7uFbsssfQ=",
+            "requires": {
+                "caller-callsite": "^2.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/caller-path/-/caller-path-2.0.0.tgz",
+            "version": "2.0.0"
+        },
+        "callsites": {
+            "integrity": "sha1-BuuE8A7qQT2oav/vrL/7Ngk7PFA=",
+            "resolved": "https://registry.npmjs.org/callsites/-/callsites-2.0.0.tgz",
+            "version": "2.0.0"
+        },
+        "camelcase": {
+            "integrity": "sha512-L28STB170nwWS63UjtlEOE3dldQApaJXZkOI1uMFfzf3rRuPegHaHesyee+YxQ+W6SvRDQV6UrdOdRiR153wJg==",
+            "resolved": "https://registry.npmjs.org/camelcase/-/camelcase-5.3.1.tgz",
+            "version": "5.3.1"
+        },
         "camelize": {
             "integrity": "sha1-FkpUg+Yw+kMh5a8HAg5TGDGyYJs=",
             "resolved": "https://registry.npmjs.org/camelize/-/camelize-1.0.0.tgz",
             "version": "1.0.0"
         },
         "caniuse-lite": {
-            "integrity": "sha512-vrMcvSuMz16YY6GSVZ0dWDTJP8jqk3iFQ/Aq5iqblPwxSVVZI+zxDyTX0VPqtQsDnfdrBDcsmhgTEOh5R8Lbpw==",
-            "resolved": "https://registry.npmjs.org/caniuse-lite/-/caniuse-lite-1.0.30000989.tgz",
-            "version": "1.0.30000989"
+            "integrity": "sha512-Tc+ff0Co/nFNbLOrziBXmMVtpt9S2c2Y+Z9Nk9Khj09J+0zR9ejvIW5qkZAErCbOrVODCx/MN+GpB5FNBs5GFA==",
+            "resolved": "https://registry.npmjs.org/caniuse-lite/-/caniuse-lite-1.0.30001246.tgz",
+            "version": "1.0.30001246"
         },
         "chalk": {
             "integrity": "sha512-Mti+f9lpJNcwF4tWV8/OrTTtF1gZi+f8FqlyAdouralcFWFQWF2+NgCHShjkCb+IFBLq9buZwE1xckQU4peSuQ==",
             "requires": {
                 "ansi-styles": "^3.2.1",
                 "escape-string-regexp": "^1.0.5",
                 "supports-color": "^5.3.0"
             },
             "resolved": "https://registry.npmjs.org/chalk/-/chalk-2.4.2.tgz",
             "version": "2.4.2"
         },
         "chokidar": {
-            "integrity": "sha512-ZmZUazfOzf0Nve7duiCKD23PFSCs4JPoYyccjUFF3aQkQadqBhfzhjkwBH2mNOG9cTBwhamM37EIsIkZw3nRgg==",
+            "integrity": "sha512-ekGhOnNVPgT77r4K/U3GDhu+FQ2S8TnK/s2KbIGXi0SZWuwkZ2QNyfWdZW+TVfn84DpEP7rLeCt2UI6bJ8GwbQ==",
             "requires": {
-                "anymatch": "^2.0.0",
-                "async-each": "^1.0.1",
-                "braces": "^2.3.2",
-                "fsevents": "^1.2.7",
-                "glob-parent": "^3.1.0",
-                "inherits": "^2.0.3",
-                "is-binary-path": "^1.0.0",
-                "is-glob": "^4.0.0",
-                "normalize-path": "^3.0.0",
-                "path-is-absolute": "^1.0.0",
-                "readdirp": "^2.2.1",
-                "upath": "^1.1.1"
+                "anymatch": "~3.1.2",
+                "braces": "~3.0.2",
+                "fsevents": "~2.3.2",
+                "glob-parent": "~5.1.2",
+                "is-binary-path": "~2.1.0",
+                "is-glob": "~4.0.1",
+                "normalize-path": "~3.0.0",
+                "readdirp": "~3.6.0"
             },
-            "resolved": "https://registry.npmjs.org/chokidar/-/chokidar-2.1.8.tgz",
-            "version": "2.1.8"
+            "resolved": "https://registry.npmjs.org/chokidar/-/chokidar-3.5.2.tgz",
+            "version": "3.5.2"
         },
         "chownr": {
-            "integrity": "sha512-GkfeAQh+QNy3wquu9oIZr6SS5x7wGdSgNQvD10X3r+AZr1Oys22HW8kAmDMvNg2+Dm0TeGaEuO8gFwdBXxwO8A==",
-            "resolved": "https://registry.npmjs.org/chownr/-/chownr-1.1.2.tgz",
-            "version": "1.1.2"
+            "integrity": "sha512-jJ0bqzaylmJtVnNgzTeSOs8DPavpbYgEr/b0YL8/2GO3xJEhInFmhKMUnEJQjZumK7KXGFhUy89PrsJWlakBVg==",
+            "resolved": "https://registry.npmjs.org/chownr/-/chownr-1.1.4.tgz",
+            "version": "1.1.4"
         },
         "chrome-trace-event": {
-            "integrity": "sha512-9e/zx1jw7B4CO+c/RXoCsfg/x1AfUBioy4owYH0bJprEYAx5hRFLRhWBqHAG57D0ZM4H7vxbP7bPe0VwhQRYDQ==",
-            "requires": {
-                "tslib": "^1.9.0"
-            },
-            "resolved": "https://registry.npmjs.org/chrome-trace-event/-/chrome-trace-event-1.0.2.tgz",
-            "version": "1.0.2"
+            "integrity": "sha512-p3KULyQg4S7NIHixdwbGX+nFHkoBiA4YQmyWtjb8XngSKV124nJmRysgAeujbUVb15vh+RvFUfCPqU7rXk+hZg==",
+            "resolved": "https://registry.npmjs.org/chrome-trace-event/-/chrome-trace-event-1.0.3.tgz",
+            "version": "1.0.3"
+        },
+        "ci-info": {
+            "integrity": "sha512-5tK7EtrZ0N+OLFMthtqOj4fI2Jeb88C4CAZPu25LDVUgXJ0A3Js4PMGqrn0JU1W0Mh1/Z8wZzYPxqUrXeBboCQ==",
+            "resolved": "https://registry.npmjs.org/ci-info/-/ci-info-2.0.0.tgz",
+            "version": "2.0.0"
         },
         "cipher-base": {
             "integrity": "sha512-Kkht5ye6ZGmwv40uUDZztayT2ThLQGfnj/T71N/XzeZeo3nf8foyW7zGTsPYkEya3m5f3cAypH+qe7YOrM1U2Q==",
             "requires": {
                 "inherits": "^2.0.1",
                 "safe-buffer": "^5.0.1"
             },
@@ -1835,14 +3068,42 @@
                 "define-property": "^0.2.5",
                 "isobject": "^3.0.0",
                 "static-extend": "^0.1.1"
             },
             "resolved": "https://registry.npmjs.org/class-utils/-/class-utils-0.3.6.tgz",
             "version": "0.3.6"
         },
+        "cli-cursor": {
+            "integrity": "sha1-s12sN2R5+sw+lHR9QdDQ9SOP/LU=",
+            "requires": {
+                "restore-cursor": "^2.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/cli-cursor/-/cli-cursor-2.1.0.tgz",
+            "version": "2.1.0"
+        },
+        "cli-spinners": {
+            "integrity": "sha512-t+4/y50K/+4xcCRosKkA7W4gTr1MySvLV0q+PxmG7FJ5g+66ChKurYjxBCjHggHH3HA5Hh9cy+lcUGWDqVH+4Q==",
+            "resolved": "https://registry.npmjs.org/cli-spinners/-/cli-spinners-2.6.0.tgz",
+            "version": "2.6.0"
+        },
+        "clone": {
+            "integrity": "sha1-2jCcwmPfFZlMaIypAheco8fNfH4=",
+            "resolved": "https://registry.npmjs.org/clone/-/clone-1.0.4.tgz",
+            "version": "1.0.4"
+        },
+        "clone-deep": {
+            "integrity": "sha512-neHB9xuzh/wk0dIHweyAXv2aPGZIVk3pLMe+/RNzINf17fe0OG96QroktYAUm7SM1PBnzTabaLboqqxDyMU+SQ==",
+            "requires": {
+                "is-plain-object": "^2.0.4",
+                "kind-of": "^6.0.2",
+                "shallow-clone": "^3.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/clone-deep/-/clone-deep-4.0.1.tgz",
+            "version": "4.0.1"
+        },
         "collection-visit": {
             "integrity": "sha1-S8A3PBZLwykbTTaMgpzxqApZ3KA=",
             "requires": {
                 "map-visit": "^1.0.0",
                 "object-visit": "^1.0.0"
             },
             "resolved": "https://registry.npmjs.org/collection-visit/-/collection-visit-1.0.0.tgz",
@@ -1857,44 +3118,54 @@
             "version": "1.9.3"
         },
         "color-name": {
             "integrity": "sha1-p9BVi9icQveV3UIyj3QIMcpTvCU=",
             "resolved": "https://registry.npmjs.org/color-name/-/color-name-1.1.3.tgz",
             "version": "1.1.3"
         },
+        "colorette": {
+            "integrity": "sha512-MKGMzyfeuutC/ZJ1cba9NqcNpfeqMUcYmyF1ZFY6/Cn7CNSAKx6a+s48sqLqyAiZuaP2TcqMhoo+dlwFnVxT9w==",
+            "resolved": "https://registry.npmjs.org/colorette/-/colorette-1.2.2.tgz",
+            "version": "1.2.2"
+        },
         "colors": {
-            "integrity": "sha1-FopHAXVran9RoSzgyXv6KMCE7WM=",
-            "resolved": "https://registry.npmjs.org/colors/-/colors-1.1.2.tgz",
-            "version": "1.1.2"
+            "integrity": "sha512-erNRLao/Y3Fv54qUa0LBB+//Uf3YwMUmdJinN20yMXm9zdKKqH9wt7R9IIVZ+K7ShzfpLV/Zg8+VyrBJYB4lpg==",
+            "resolved": "https://registry.npmjs.org/colors/-/colors-1.2.5.tgz",
+            "version": "1.2.5"
         },
         "commander": {
-            "integrity": "sha1-nJkJQXbhIkDLItbFFGCYQA/g99Q=",
-            "requires": {
-                "graceful-readlink": ">= 1.0.0"
-            },
-            "resolved": "https://registry.npmjs.org/commander/-/commander-2.9.0.tgz",
-            "version": "2.9.0"
+            "integrity": "sha512-GpVkmM8vF2vQUkj2LvZmD35JxeJOLCwJ9cUkugyk2nuhbv3+mJvpLYYt+0+USMxE+oj+ey/lJEnhZw75x/OMcQ==",
+            "resolved": "https://registry.npmjs.org/commander/-/commander-2.20.3.tgz",
+            "version": "2.20.3"
         },
         "commondir": {
             "integrity": "sha1-3dgA2gxmEnOTzKWVDqloo6rxJTs=",
             "resolved": "https://registry.npmjs.org/commondir/-/commondir-1.0.1.tgz",
             "version": "1.0.1"
         },
         "component-emitter": {
             "integrity": "sha512-Rd3se6QB+sO1TwqZjscQrurpEPIfO0/yYnSin6Q/rD3mOutHvUrCAhJub3r90uNb+SESBuE0QYoB90YdfatsRg==",
             "resolved": "https://registry.npmjs.org/component-emitter/-/component-emitter-1.3.0.tgz",
             "version": "1.3.0"
         },
+        "compose-function": {
+            "integrity": "sha1-ntZ18TzFRQHTCVCkhv9qe6OrGF8=",
+            "requires": {
+                "arity-n": "^1.0.4"
+            },
+            "resolved": "https://registry.npmjs.org/compose-function/-/compose-function-3.0.3.tgz",
+            "version": "3.0.3"
+        },
         "compressible": {
-            "integrity": "sha512-BGHeLCK1GV7j1bSmQQAi26X+GgWcTjLr/0tzSvMCl3LH1w1IJ4PFSPoV5316b30cneTziC+B1a+3OjoSUcQYmw==",
+            "integrity": "sha512-AF3r7P5dWxL8MxyITRMlORQNaOA2IkAFaTr4k7BUumjPtRpGDTZpl0Pb1XCO6JeDCBdp126Cgs9sMxqSjgYyRg==",
             "requires": {
-                "mime-db": ">= 1.40.0 < 2"
+                "mime-db": ">= 1.43.0 < 2"
             },
-            "resolved": "https://registry.npmjs.org/compressible/-/compressible-2.0.17.tgz",
-            "version": "2.0.17"
+            "resolved": "https://registry.npmjs.org/compressible/-/compressible-2.0.18.tgz",
+            "version": "2.0.18"
         },
         "compression": {
             "dependencies": {
                 "debug": {
                     "integrity": "sha512-bC7ElrdJaJnPbAP+1EotYvqZsb3ecl5wi6Bfi6BJTUcNowp6cvspg0jXznRTKDjm/E7AdgFBVeAPVMNcKGsHMA==",
                     "requires": {
                         "ms": "2.0.0"
@@ -1933,39 +3204,65 @@
                 "inherits": "^2.0.3",
                 "readable-stream": "^2.2.2",
                 "typedarray": "^0.0.6"
             },
             "resolved": "https://registry.npmjs.org/concat-stream/-/concat-stream-1.6.2.tgz",
             "version": "1.6.2"
         },
-        "console-browserify": {
-            "integrity": "sha1-8CQcRXMKn8YyOyBtvzjtx0HQuxA=",
+        "conf": {
+            "dependencies": {
+                "make-dir": {
+                    "integrity": "sha512-g3FeP20LNwhALb/6Cz6Dd4F2ngze0jz7tbzrD2wAV+o9FeNHe4rL+yK2md0J/fiSf1sa1ADhXqi5+oVwOM/eGw==",
+                    "requires": {
+                        "semver": "^6.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/make-dir/-/make-dir-3.1.0.tgz",
+                    "version": "3.1.0"
+                },
+                "semver": {
+                    "integrity": "sha512-b39TBaTSfV6yBrapU89p5fKekE2m/NwnDocOVruQFS1/veMgdzuPcnOM34M6CwxW8jH/lxEa5rBoDeUwu5HHTw==",
+                    "resolved": "https://registry.npmjs.org/semver/-/semver-6.3.0.tgz",
+                    "version": "6.3.0"
+                }
+            },
+            "integrity": "sha512-lRNyt+iRD4plYaOSVTxu1zPWpaH0EOxgFIR1l3mpC/DGZ7XzhoGFMKmbl54LAgXcSu6knqWgOwdINkqm58N85A==",
             "requires": {
-                "date-now": "^0.1.4"
+                "ajv": "^6.10.0",
+                "dot-prop": "^5.0.0",
+                "env-paths": "^2.2.0",
+                "json-schema-typed": "^7.0.0",
+                "make-dir": "^3.0.0",
+                "pkg-up": "^3.0.1",
+                "write-file-atomic": "^3.0.0"
             },
-            "resolved": "https://registry.npmjs.org/console-browserify/-/console-browserify-1.1.0.tgz",
-            "version": "1.1.0"
+            "resolved": "https://registry.npmjs.org/conf/-/conf-5.0.0.tgz",
+            "version": "5.0.0"
+        },
+        "console-browserify": {
+            "integrity": "sha512-ZMkYO/LkF17QvCPqM0gxw8yUzigAOZOSWSHg91FH6orS7vcEj5dVZTidN2fQ14yBSdg97RqhSNwLUXInd52OTA==",
+            "resolved": "https://registry.npmjs.org/console-browserify/-/console-browserify-1.2.0.tgz",
+            "version": "1.2.0"
         },
         "constants-browserify": {
             "integrity": "sha1-wguW2MYXdIqvHBYCF2DNJ/y4y3U=",
             "resolved": "https://registry.npmjs.org/constants-browserify/-/constants-browserify-1.0.0.tgz",
             "version": "1.0.0"
         },
         "content-type": {
             "integrity": "sha512-hIP3EEPs8tB9AT1L+NUqtwOAps4mk2Zob89MWXMHjHWg9milF/j4osnnQLXBCBFBk/tvIG/tUc9mOUJiPBhPXA==",
             "resolved": "https://registry.npmjs.org/content-type/-/content-type-1.0.4.tgz",
             "version": "1.0.4"
         },
         "convert-source-map": {
-            "integrity": "sha512-eFu7XigvxdZ1ETfbgPBohgyQ/Z++C0eEhTor0qRwBw9unw+L0/6V8wkSuGgzdThkiS5lSpdptOQPD8Ak40a+7A==",
+            "integrity": "sha512-+OQdjP49zViI/6i7nIJpA8rAl4sV/JdPfU9nZs3VqOwGIgizICvuN2ru6fMd+4llL0tar18UYJXfZ/TWtmhUjA==",
             "requires": {
                 "safe-buffer": "~5.1.1"
             },
-            "resolved": "https://registry.npmjs.org/convert-source-map/-/convert-source-map-1.6.0.tgz",
-            "version": "1.6.0"
+            "resolved": "https://registry.npmjs.org/convert-source-map/-/convert-source-map-1.8.0.tgz",
+            "version": "1.8.0"
         },
         "cookie": {
             "integrity": "sha512-+Hp8fLp57wnUSt0tY0tHEXh4voZRDnoIrZPqlo3DPiI4y9lwg/jqx+1Om94/W6ZaPDOUbnjOt/99w66zk+l1Xg==",
             "resolved": "https://registry.npmjs.org/cookie/-/cookie-0.4.0.tgz",
             "version": "0.4.0"
         },
         "copy-concurrently": {
@@ -1982,48 +3279,84 @@
             "version": "1.0.5"
         },
         "copy-descriptor": {
             "integrity": "sha1-Z29us8OZl8LuGsOpJP1hJHSPV40=",
             "resolved": "https://registry.npmjs.org/copy-descriptor/-/copy-descriptor-0.1.1.tgz",
             "version": "0.1.1"
         },
-        "core-js": {
-            "integrity": "sha512-HOpZf6eXmnl7la+cUdMnLvUxKNqLUzJvgIziQ0DiF3JwSImNphIqdGqzj6hIKyX04MmV0poclQ7+wjWvxQyR2A==",
-            "resolved": "https://registry.npmjs.org/core-js/-/core-js-2.6.9.tgz",
-            "version": "2.6.9"
-        },
         "core-js-compat": {
             "dependencies": {
+                "browserslist": {
+                    "integrity": "sha512-Wspk/PqO+4W9qp5iUTJsa1B/QrYn1keNCcEP5OvP7WBwT4KaDly0uONYmC6Xa3Z5IqnUgS0KcgLYu1l74x0ZXQ==",
+                    "requires": {
+                        "caniuse-lite": "^1.0.30001219",
+                        "colorette": "^1.2.2",
+                        "electron-to-chromium": "^1.3.723",
+                        "escalade": "^3.1.1",
+                        "node-releases": "^1.1.71"
+                    },
+                    "resolved": "https://registry.npmjs.org/browserslist/-/browserslist-4.16.6.tgz",
+                    "version": "4.16.6"
+                },
                 "semver": {
-                    "integrity": "sha512-b39TBaTSfV6yBrapU89p5fKekE2m/NwnDocOVruQFS1/veMgdzuPcnOM34M6CwxW8jH/lxEa5rBoDeUwu5HHTw==",
-                    "resolved": "https://registry.npmjs.org/semver/-/semver-6.3.0.tgz",
-                    "version": "6.3.0"
+                    "integrity": "sha512-+GB6zVA9LWh6zovYQLALHwv5rb2PHGlJi3lfiqIHxR0uuwCgefcOJc59v9fv1w8GbStwxuuqqAjI9NMAOOgq1A==",
+                    "resolved": "https://registry.npmjs.org/semver/-/semver-7.0.0.tgz",
+                    "version": "7.0.0"
                 }
             },
-            "integrity": "sha512-MwPZle5CF9dEaMYdDeWm73ao/IflDH+FjeJCWEADcEgFSE9TLimFKwJsfmkwzI8eC0Aj0mgvMDjeQjrElkz4/A==",
+            "integrity": "sha512-Wp+BJVvwopjI+A1EFqm2dwUmWYXrvucmtIB2LgXn/Rb+gWPKYxtmb4GKHGKG/KGF1eK9jfjzT38DITbTOCX/SQ==",
             "requires": {
-                "browserslist": "^4.6.6",
-                "semver": "^6.3.0"
+                "browserslist": "^4.16.6",
+                "semver": "7.0.0"
             },
-            "resolved": "https://registry.npmjs.org/core-js-compat/-/core-js-compat-3.2.1.tgz",
-            "version": "3.2.1"
+            "resolved": "https://registry.npmjs.org/core-js-compat/-/core-js-compat-3.15.2.tgz",
+            "version": "3.15.2"
         },
         "core-util-is": {
             "integrity": "sha1-tf1UIgqivFq1eqtxQMlAdUUDwac=",
             "resolved": "https://registry.npmjs.org/core-util-is/-/core-util-is-1.0.2.tgz",
             "version": "1.0.2"
         },
+        "cosmiconfig": {
+            "dependencies": {
+                "parse-json": {
+                    "integrity": "sha1-vjX1Qlvh9/bHRxhPmKeIy5lHfuA=",
+                    "requires": {
+                        "error-ex": "^1.3.1",
+                        "json-parse-better-errors": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/parse-json/-/parse-json-4.0.0.tgz",
+                    "version": "4.0.0"
+                }
+            },
+            "integrity": "sha512-H65gsXo1SKjf8zmrJ67eJk8aIRKV5ff2D4uKZIBZShbhGSpEmsQOPW/SKMKYhSTrqR7ufy6RP69rPogdaPh/kA==",
+            "requires": {
+                "import-fresh": "^2.0.0",
+                "is-directory": "^0.3.1",
+                "js-yaml": "^3.13.1",
+                "parse-json": "^4.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/cosmiconfig/-/cosmiconfig-5.2.1.tgz",
+            "version": "5.2.1"
+        },
         "create-ecdh": {
-            "integrity": "sha512-GbEHQPMOswGpKXM9kCWVrremUcBmjteUaQ01T9rkKCPDXfUHX0IoP9LpHYo2NPFampa4e+/pFDc3jQdxrxQLaw==",
+            "dependencies": {
+                "bn.js": {
+                    "integrity": "sha512-c98Bf3tPniI+scsdk237ku1Dc3ujXQTSgyiPUDEOe7tRkhrqridvh8klBv0HCEso1OLOYcHuCv/cS6DNxKH+ZA==",
+                    "resolved": "https://registry.npmjs.org/bn.js/-/bn.js-4.12.0.tgz",
+                    "version": "4.12.0"
+                }
+            },
+            "integrity": "sha512-mf+TCx8wWc9VpuxfP2ht0iSISLZnt0JgWlrOKZiNqyUZWnjIaCIVNQArMHnCZKfEYRg6IM7A+NeJoN8gf/Ws0A==",
             "requires": {
                 "bn.js": "^4.1.0",
-                "elliptic": "^6.0.0"
+                "elliptic": "^6.5.3"
             },
-            "resolved": "https://registry.npmjs.org/create-ecdh/-/create-ecdh-4.0.3.tgz",
-            "version": "4.0.3"
+            "resolved": "https://registry.npmjs.org/create-ecdh/-/create-ecdh-4.0.4.tgz",
+            "version": "4.0.4"
         },
         "create-hash": {
             "integrity": "sha512-z00bCGNHDG8mHAkP7CtT1qVu+bFQUPjYq/4Iv3C3kWjTFV10zIjfSoeqXo9Asws8gwSHDGj/hl2u4OGIjapeCg==",
             "requires": {
                 "cipher-base": "^1.0.1",
                 "inherits": "^2.0.1",
                 "md5.js": "^1.3.4",
@@ -2042,14 +3375,29 @@
                 "ripemd160": "^2.0.0",
                 "safe-buffer": "^5.0.1",
                 "sha.js": "^2.4.8"
             },
             "resolved": "https://registry.npmjs.org/create-hmac/-/create-hmac-1.1.7.tgz",
             "version": "1.1.7"
         },
+        "cross-fetch": {
+            "dependencies": {
+                "node-fetch": {
+                    "integrity": "sha512-V4aYg89jEoVRxRb2fJdAg8FHvI7cEyYdVAh94HH0UIK8oJxUfkjlDQN9RbMx+bEjP7+ggMiFRprSti032Oipxw==",
+                    "resolved": "https://registry.npmjs.org/node-fetch/-/node-fetch-2.6.1.tgz",
+                    "version": "2.6.1"
+                }
+            },
+            "integrity": "sha512-+JhD65rDNqLbGmB3Gzs3HrEKC0aQnD+XA3SY6RjgkF88jV2q5cTc5+CwxlS3sdmLk98gpPt5CF9XRnPdlxZe6w==",
+            "requires": {
+                "node-fetch": "2.6.1"
+            },
+            "resolved": "https://registry.npmjs.org/cross-fetch/-/cross-fetch-3.1.2.tgz",
+            "version": "3.1.2"
+        },
         "crypto-browserify": {
             "integrity": "sha512-fz4spIh+znjO2VjL+IdhEpRJ3YN6sMzITSBijk6FK2UvTqruSQW+/cCZTSNsMiZNvUeq0CqurF+dAbyiGOY6Wg==",
             "requires": {
                 "browserify-cipher": "^1.0.0",
                 "browserify-sign": "^4.0.0",
                 "create-ecdh": "^4.0.0",
                 "create-hash": "^1.1.0",
@@ -2071,58 +3419,184 @@
                 "source-map": "^0.6.1",
                 "source-map-resolve": "^0.5.2",
                 "urix": "^0.1.0"
             },
             "resolved": "https://registry.npmjs.org/css/-/css-2.2.4.tgz",
             "version": "2.2.4"
         },
+        "css-blank-pseudo": {
+            "integrity": "sha512-LHz35Hr83dnFeipc7oqFDmsjHdljj3TQtxGGiNWSOsTLIAubSm4TEz8qCaKFpk7idaQ1GfWscF4E6mgpBysA1w==",
+            "requires": {
+                "postcss": "^7.0.5"
+            },
+            "resolved": "https://registry.npmjs.org/css-blank-pseudo/-/css-blank-pseudo-0.1.4.tgz",
+            "version": "0.1.4"
+        },
         "css-color-keywords": {
             "integrity": "sha1-/qJhbcZ2spYmhrOvjb2+GAskTgU=",
             "resolved": "https://registry.npmjs.org/css-color-keywords/-/css-color-keywords-1.0.0.tgz",
             "version": "1.0.0"
         },
+        "css-has-pseudo": {
+            "dependencies": {
+                "cssesc": {
+                    "integrity": "sha512-MsCAG1z9lPdoO/IUMLSBWBSVxVtJ1395VGIQ+Fc2gNdkQ1hNDnQdw3YhA71WJCBW1vdwA0cAnk/DnW6bqoEUYg==",
+                    "resolved": "https://registry.npmjs.org/cssesc/-/cssesc-2.0.0.tgz",
+                    "version": "2.0.0"
+                },
+                "postcss-selector-parser": {
+                    "integrity": "sha512-w+zLE5Jhg6Liz8+rQOWEAwtwkyqpfnmsinXjXg6cY7YIONZZtgvE0v2O0uhQBs0peNomOJwWRKt6JBfTdTd3OQ==",
+                    "requires": {
+                        "cssesc": "^2.0.0",
+                        "indexes-of": "^1.0.1",
+                        "uniq": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/postcss-selector-parser/-/postcss-selector-parser-5.0.0.tgz",
+                    "version": "5.0.0"
+                }
+            },
+            "integrity": "sha512-Z8hnfsZu4o/kt+AuFzeGpLVhFOGO9mluyHBaA2bA8aCGTwah5sT3WV/fTHH8UNZUytOIImuGPrl/prlb4oX4qQ==",
+            "requires": {
+                "postcss": "^7.0.6",
+                "postcss-selector-parser": "^5.0.0-rc.4"
+            },
+            "resolved": "https://registry.npmjs.org/css-has-pseudo/-/css-has-pseudo-0.10.0.tgz",
+            "version": "0.10.0"
+        },
+        "css-loader": {
+            "dependencies": {
+                "json5": {
+                    "integrity": "sha512-aKS4WQjPenRxiQsC93MNfjx+nbF4PAdYzmd/1JIj8HYzqfbu86beTuNgXDzPknWk0n0uARlyewZo4s++ES36Ow==",
+                    "requires": {
+                        "minimist": "^1.2.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/json5/-/json5-1.0.1.tgz",
+                    "version": "1.0.1"
+                },
+                "loader-utils": {
+                    "integrity": "sha512-qH0WSMBtn/oHuwjy/NucEgbx5dbxxnxup9s4PVXJUDHZBQY+s0NWA9rJf53RBnQZxfch7euUui7hpoAPvALZdA==",
+                    "requires": {
+                        "big.js": "^5.2.2",
+                        "emojis-list": "^3.0.0",
+                        "json5": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/loader-utils/-/loader-utils-1.4.0.tgz",
+                    "version": "1.4.0"
+                },
+                "postcss-value-parser": {
+                    "integrity": "sha512-97DXOFbQJhk71ne5/Mt6cOu6yxsSfM0QGQyl0L25Gca4yGWEGJaig7l7gbCX623VqTBNGLRLaVUCnNkcedlRSQ==",
+                    "resolved": "https://registry.npmjs.org/postcss-value-parser/-/postcss-value-parser-4.1.0.tgz",
+                    "version": "4.1.0"
+                }
+            },
+            "integrity": "sha512-x9Y1vvHe5RR+4tzwFdWExPueK00uqFTCw7mZy+9aE/X1SKWOArm5luaOrtJ4d05IpOwJ6S86b/tVcIdhw1Bu4A==",
+            "requires": {
+                "camelcase": "^5.3.1",
+                "cssesc": "^3.0.0",
+                "icss-utils": "^4.1.1",
+                "loader-utils": "^1.2.3",
+                "normalize-path": "^3.0.0",
+                "postcss": "^7.0.23",
+                "postcss-modules-extract-imports": "^2.0.0",
+                "postcss-modules-local-by-default": "^3.0.2",
+                "postcss-modules-scope": "^2.1.1",
+                "postcss-modules-values": "^3.0.0",
+                "postcss-value-parser": "^4.0.2",
+                "schema-utils": "^2.6.0"
+            },
+            "resolved": "https://registry.npmjs.org/css-loader/-/css-loader-3.3.0.tgz",
+            "version": "3.3.0"
+        },
+        "css-prefers-color-scheme": {
+            "integrity": "sha512-MTu6+tMs9S3EUqzmqLXEcgNRbNkkD/TGFvowpeoWJn5Vfq7FMgsmRQs9X5NXAURiOBmOxm/lLjsDNXDE6k9bhg==",
+            "requires": {
+                "postcss": "^7.0.5"
+            },
+            "resolved": "https://registry.npmjs.org/css-prefers-color-scheme/-/css-prefers-color-scheme-3.1.1.tgz",
+            "version": "3.1.1"
+        },
         "css-to-react-native": {
             "integrity": "sha512-VOFaeZA053BqvvvqIA8c9n0+9vFppVBAHCp6JgFTtTMU3Mzi+XnelJ9XC9ul3BqFzZyQ5N+H0SnwsWT2Ebchxw==",
             "requires": {
                 "camelize": "^1.0.0",
                 "css-color-keywords": "^1.0.0",
                 "postcss-value-parser": "^3.3.0"
             },
             "resolved": "https://registry.npmjs.org/css-to-react-native/-/css-to-react-native-2.3.2.tgz",
             "version": "2.3.2"
         },
+        "cssdb": {
+            "integrity": "sha512-LsTAR1JPEM9TpGhl/0p3nQecC2LJ0kD8X5YARu1hk/9I1gril5vDtMZyNxcEpxxDj34YNck/ucjuoUd66K03oQ==",
+            "resolved": "https://registry.npmjs.org/cssdb/-/cssdb-4.4.0.tgz",
+            "version": "4.4.0"
+        },
+        "cssesc": {
+            "integrity": "sha512-/Tb/JcjK111nNScGob5MNtsntNM1aCNUDipB/TkwZFhyDrrE47SOx/18wF2bbjgc3ZzCSKW1T5nt5EbFoAz/Vg==",
+            "resolved": "https://registry.npmjs.org/cssesc/-/cssesc-3.0.0.tgz",
+            "version": "3.0.0"
+        },
+        "cssnano-preset-simple": {
+            "integrity": "sha512-T3a4FACX9h9WRw3Gmiut9T+Pjrj/ScEPk5EclCzHz6Ya6WjMu8oZ3IOtIB32bQOJmkefa1LAI9G7ZDy5psvw2A==",
+            "requires": {
+                "caniuse-lite": "^1.0.30001179",
+                "postcss": "^7.0.32"
+            },
+            "resolved": "https://registry.npmjs.org/cssnano-preset-simple/-/cssnano-preset-simple-1.3.1.tgz",
+            "version": "1.3.1"
+        },
+        "cssnano-simple": {
+            "integrity": "sha512-B7u9vvtXEqeU2rzdt+Kfw5O9Nd46R7KNjJoP7Y5lGQs6c7n1Et5Ilofh2W9OjBV/ZiJV5+7j9ShWgiYNtH/57A==",
+            "requires": {
+                "cssnano-preset-simple": "^1.0.0",
+                "postcss": "^7.0.18"
+            },
+            "resolved": "https://registry.npmjs.org/cssnano-simple/-/cssnano-simple-1.0.0.tgz",
+            "version": "1.0.0"
+        },
         "csstype": {
             "dev": true,
             "integrity": "sha512-ofovWglpqoqbfLNOTBNZLSbMuGrblAf1efvvArGKOZMBrIoJeu5UsAipQolkijtyQx5MtAzT/J9IHj/CEY1mJw==",
             "resolved": "https://registry.npmjs.org/csstype/-/csstype-3.0.2.tgz",
             "version": "3.0.2"
         },
         "cyclist": {
-            "integrity": "sha1-GzN5LhHpFKL9bW7WRHRkRE5fpkA=",
-            "resolved": "https://registry.npmjs.org/cyclist/-/cyclist-0.2.2.tgz",
-            "version": "0.2.2"
+            "integrity": "sha1-WW6WmP0MgOEgOMK4LW6xs1tiJNk=",
+            "resolved": "https://registry.npmjs.org/cyclist/-/cyclist-1.0.1.tgz",
+            "version": "1.0.1"
         },
-        "date-now": {
-            "integrity": "sha1-6vQ5/U1ISK105cx9vvIAZyueNFs=",
-            "resolved": "https://registry.npmjs.org/date-now/-/date-now-0.1.4.tgz",
-            "version": "0.1.4"
+        "d": {
+            "integrity": "sha512-m62ShEObQ39CfralilEQRjH6oAMtNCV1xJyEx5LpRYUVN+EviphDgUc/F3hnYbADmkiNs67Y+3ylmlG7Lnu+FA==",
+            "requires": {
+                "es5-ext": "^0.10.50",
+                "type": "^1.0.1"
+            },
+            "resolved": "https://registry.npmjs.org/d/-/d-1.0.1.tgz",
+            "version": "1.0.1"
         },
         "debug": {
             "integrity": "sha512-pYAIzeRo8J6KPEaJ0VWOh5Pzkbw/RetuzehGM7QRRX5he4fPHx2rdKMB256ehJCkX+XRQm16eZLqLNS8RSZXZw==",
             "requires": {
                 "ms": "^2.1.1"
             },
             "resolved": "https://registry.npmjs.org/debug/-/debug-4.1.1.tgz",
             "version": "4.1.1"
         },
         "decode-uri-component": {
             "integrity": "sha1-6zkTMzRYd1y4TNGh+uBiEGu4dUU=",
             "resolved": "https://registry.npmjs.org/decode-uri-component/-/decode-uri-component-0.2.0.tgz",
             "version": "0.2.0"
         },
+        "defaults": {
+            "integrity": "sha1-xlYFHpgX2f8I7YgUd/P+QBnz730=",
+            "requires": {
+                "clone": "^1.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/defaults/-/defaults-1.0.3.tgz",
+            "version": "1.0.3"
+        },
         "define-properties": {
             "integrity": "sha512-3MqfYKj2lLzdMSf8ZIZE/V+Zuy+BgD6f164e8K2w7dgnpKArBDerGYpM46IYYcjnkdPNMjPk9A6VFB8+3SKlXQ==",
             "requires": {
                 "object-keys": "^1.0.12"
             },
             "resolved": "https://registry.npmjs.org/define-properties/-/define-properties-1.1.3.tgz",
             "version": "1.1.3"
@@ -2179,121 +3653,248 @@
         },
         "depd": {
             "integrity": "sha1-m81S4UwJd2PnSbJ0xDRu0uVgtak=",
             "resolved": "https://registry.npmjs.org/depd/-/depd-1.1.2.tgz",
             "version": "1.1.2"
         },
         "des.js": {
-            "integrity": "sha1-wHTS4qpqipoH29YfmhXCzYPsjsw=",
+            "integrity": "sha512-Q0I4pfFrv2VPd34/vfLrFOoRmlYj3OV50i7fskps1jZWK1kApMWWT9G6RRUeYedLcBDIhnSDaUvJMb3AhUlaEA==",
             "requires": {
                 "inherits": "^2.0.1",
                 "minimalistic-assert": "^1.0.0"
             },
-            "resolved": "https://registry.npmjs.org/des.js/-/des.js-1.0.0.tgz",
-            "version": "1.0.0"
+            "resolved": "https://registry.npmjs.org/des.js/-/des.js-1.0.1.tgz",
+            "version": "1.0.1"
         },
         "destroy": {
             "integrity": "sha1-l4hXRCxEdJ5CBmE+N5RiBYJqvYA=",
             "resolved": "https://registry.npmjs.org/destroy/-/destroy-1.0.4.tgz",
             "version": "1.0.4"
         },
+        "devalue": {
+            "integrity": "sha512-I2TiqT5iWBEyB8GRfTDP0hiLZ0YeDJZ+upDxjBfOC2lebO5LezQMv7QvIUTzdb64jQyAKLf1AHADtGN+jw6v8Q==",
+            "resolved": "https://registry.npmjs.org/devalue/-/devalue-2.0.1.tgz",
+            "version": "2.0.1"
+        },
         "diffie-hellman": {
+            "dependencies": {
+                "bn.js": {
+                    "integrity": "sha512-c98Bf3tPniI+scsdk237ku1Dc3ujXQTSgyiPUDEOe7tRkhrqridvh8klBv0HCEso1OLOYcHuCv/cS6DNxKH+ZA==",
+                    "resolved": "https://registry.npmjs.org/bn.js/-/bn.js-4.12.0.tgz",
+                    "version": "4.12.0"
+                }
+            },
             "integrity": "sha512-kqag/Nl+f3GwyK25fhUMYj81BUOrZ9IuJsjIcDE5icNM9FJHAVm3VcUDxdLPoQtTuUylWm6ZIknYJwwaPxsUzg==",
             "requires": {
                 "bn.js": "^4.1.0",
                 "miller-rabin": "^4.0.0",
                 "randombytes": "^2.0.0"
             },
             "resolved": "https://registry.npmjs.org/diffie-hellman/-/diffie-hellman-5.0.3.tgz",
             "version": "5.0.3"
         },
+        "dom-serializer": {
+            "integrity": "sha512-2/xPb3ORsQ42nHYiSunXkDjPLBaEj/xTwUO4B7XCZQTRk7EBtTOPaygh10YAAh2OI1Qrp6NWfpAhzswj0ydt9g==",
+            "requires": {
+                "domelementtype": "^2.0.1",
+                "entities": "^2.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/dom-serializer/-/dom-serializer-0.2.2.tgz",
+            "version": "0.2.2"
+        },
         "domain-browser": {
             "integrity": "sha512-jnjyiM6eRyZl2H+W8Q/zLMA481hzi0eszAaBUzIVnmYVDBbnLxVNnfu1HgEBvCbL+71FrxMl3E6lpKH7Ge3OXA==",
             "resolved": "https://registry.npmjs.org/domain-browser/-/domain-browser-1.2.0.tgz",
             "version": "1.2.0"
         },
+        "domelementtype": {
+            "integrity": "sha512-DtBMo82pv1dFtUmHyr48beiuq792Sxohr+8Hm9zoxklYPfa6n0Z3Byjj2IV7bmr2IyqClnqEQhfgHJJ5QF0R5A==",
+            "resolved": "https://registry.npmjs.org/domelementtype/-/domelementtype-2.2.0.tgz",
+            "version": "2.2.0"
+        },
+        "domhandler": {
+            "integrity": "sha512-eKLdI5v9m67kbXQbJSNn1zjh0SDzvzWVWtX+qEI3eMjZw8daH9k8rlj1FZY9memPwjiskQFbe7vHVVJIAqoEhw==",
+            "requires": {
+                "domelementtype": "^2.0.1"
+            },
+            "resolved": "https://registry.npmjs.org/domhandler/-/domhandler-3.0.0.tgz",
+            "version": "3.0.0"
+        },
+        "domutils": {
+            "integrity": "sha512-n5SelJ1axbO636c2yUtOGia/IcJtVtlhQbFiVDBZHKV5ReJO1ViX7sFEemtuyoAnBxk5meNSYgA8V4s0271efg==",
+            "requires": {
+                "dom-serializer": "^0.2.1",
+                "domelementtype": "^2.0.1",
+                "domhandler": "^3.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/domutils/-/domutils-2.0.0.tgz",
+            "version": "2.0.0"
+        },
+        "dot-prop": {
+            "integrity": "sha512-QM8q3zDe58hqUqjraQOmzZ1LIH9SWQJTlEKCH4kJ2oQvLZk7RbQXvtDM2XEq3fwkV9CCvvH4LA0AV+ogFsBM2Q==",
+            "requires": {
+                "is-obj": "^2.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/dot-prop/-/dot-prop-5.3.0.tgz",
+            "version": "5.3.0"
+        },
+        "duplexer": {
+            "integrity": "sha512-jtD6YG370ZCIi/9GTaJKQxWTZD045+4R4hTk/x1UyoqadyJ9x9CgSi1RlVDQF8U2sxLLSnFkCaMihqljHIWgMg==",
+            "resolved": "https://registry.npmjs.org/duplexer/-/duplexer-0.1.2.tgz",
+            "version": "0.1.2"
+        },
         "duplexify": {
             "integrity": "sha512-07z8uv2wMyS51kKhD1KsdXJg5WQ6t93RneqRxUHnskXVtlYYkLqM0gqStQZ3pj073g687jPCHrqNfCzawLYh5g==",
             "requires": {
                 "end-of-stream": "^1.0.0",
                 "inherits": "^2.0.1",
                 "readable-stream": "^2.0.0",
                 "stream-shift": "^1.0.0"
             },
             "resolved": "https://registry.npmjs.org/duplexify/-/duplexify-3.7.1.tgz",
             "version": "3.7.1"
         },
+        "ecdsa-sig-formatter": {
+            "integrity": "sha512-nagl3RYrbNv6kQkeJIpt6NJZy8twLB/2vtz6yN9Z4vRKHN4/QZJIEbqohALSgwKdnksuY3k5Addp5lg8sVoVcQ==",
+            "requires": {
+                "safe-buffer": "^5.0.1"
+            },
+            "resolved": "https://registry.npmjs.org/ecdsa-sig-formatter/-/ecdsa-sig-formatter-1.0.11.tgz",
+            "version": "1.0.11"
+        },
         "ee-first": {
             "integrity": "sha1-WQxhFWsK4vTwJVcyoViyZrxWsh0=",
             "resolved": "https://registry.npmjs.org/ee-first/-/ee-first-1.1.1.tgz",
             "version": "1.1.1"
         },
         "electron-to-chromium": {
-            "integrity": "sha512-SPAFjDr/7iiVK2kgTluwxela6eaWjjFkS9rO/iYpB/KGXgccUom5YC7OIf19c8m8GGptWxLU0Em8xM64A/N7Fg==",
-            "resolved": "https://registry.npmjs.org/electron-to-chromium/-/electron-to-chromium-1.3.237.tgz",
-            "version": "1.3.237"
+            "integrity": "sha512-WmCgAeURsMFiyoJ646eUaJQ7GNfvMRLXo+GamUyKVNEM4MqTAsXyC0f38JEB4N3BtbD0tlAKozGP5E2T9K3YGg==",
+            "resolved": "https://registry.npmjs.org/electron-to-chromium/-/electron-to-chromium-1.3.785.tgz",
+            "version": "1.3.785"
         },
         "elliptic": {
-            "integrity": "sha512-IMqzv5wNQf+E6aHeIqATs0tOLeOTwj1QKbRcS3jBbYkl5oLAserA8yJTT7/VyHUYG91PRmPyeQDObKLPpeS4dw==",
+            "dependencies": {
+                "bn.js": {
+                    "integrity": "sha512-c98Bf3tPniI+scsdk237ku1Dc3ujXQTSgyiPUDEOe7tRkhrqridvh8klBv0HCEso1OLOYcHuCv/cS6DNxKH+ZA==",
+                    "resolved": "https://registry.npmjs.org/bn.js/-/bn.js-4.12.0.tgz",
+                    "version": "4.12.0"
+                }
+            },
+            "integrity": "sha512-iLhC6ULemrljPZb+QutR5TQGB+pdW6KGD5RSegS+8sorOZT+rdQFbsQFJgvN3eRqNALqJer4oQ16YvJHlU8hzQ==",
             "requires": {
-                "bn.js": "^4.4.0",
-                "brorand": "^1.0.1",
+                "bn.js": "^4.11.9",
+                "brorand": "^1.1.0",
                 "hash.js": "^1.0.0",
-                "hmac-drbg": "^1.0.0",
-                "inherits": "^2.0.1",
-                "minimalistic-assert": "^1.0.0",
-                "minimalistic-crypto-utils": "^1.0.0"
+                "hmac-drbg": "^1.0.1",
+                "inherits": "^2.0.4",
+                "minimalistic-assert": "^1.0.1",
+                "minimalistic-crypto-utils": "^1.0.1"
             },
-            "resolved": "https://registry.npmjs.org/elliptic/-/elliptic-6.5.3.tgz",
-            "version": "6.5.3"
+            "resolved": "https://registry.npmjs.org/elliptic/-/elliptic-6.5.4.tgz",
+            "version": "6.5.4"
         },
         "emojis-list": {
-            "integrity": "sha1-TapNnbAPmBmIDHn6RXrlsJof04k=",
-            "resolved": "https://registry.npmjs.org/emojis-list/-/emojis-list-2.1.0.tgz",
-            "version": "2.1.0"
+            "integrity": "sha512-/kyM18EfinwXZbno9FyUGeFh87KC8HRQBQGildHZbEuRyWFOmv1U10o9BBp8XVZDVNNuQKyIGIu5ZYAAXJ0V2Q==",
+            "resolved": "https://registry.npmjs.org/emojis-list/-/emojis-list-3.0.0.tgz",
+            "version": "3.0.0"
         },
         "encodeurl": {
             "integrity": "sha1-rT/0yG7C0CkyL1oCw6mmBslbP1k=",
             "resolved": "https://registry.npmjs.org/encodeurl/-/encodeurl-1.0.2.tgz",
             "version": "1.0.2"
         },
         "end-of-stream": {
-            "integrity": "sha512-1MkrZNvWTKCaigbn+W15elq2BB/L22nqrSY5DKlo3X6+vclJm8Bb5djXJBmEX6fS3+zCh/F4VBK5Z2KxJt4s2Q==",
+            "integrity": "sha512-+uw1inIHVPQoaVuHzRyXd21icM+cnt4CzD5rW+NC1wjOUSTOs+Te7FOv7AhN7vS9x/oIyhLP5PR1H+phQAHu5Q==",
             "requires": {
                 "once": "^1.4.0"
             },
-            "resolved": "https://registry.npmjs.org/end-of-stream/-/end-of-stream-1.4.1.tgz",
-            "version": "1.4.1"
+            "resolved": "https://registry.npmjs.org/end-of-stream/-/end-of-stream-1.4.4.tgz",
+            "version": "1.4.4"
         },
         "enhanced-resolve": {
-            "integrity": "sha512-F/7vkyTtyc/llOIn8oWclcB25KdRaiPBpZYDgJHgh/UHtpgT2p2eldQgtQnLtUvfMKPKxbRaQM/hHkvLHt1Vng==",
+            "dependencies": {
+                "memory-fs": {
+                    "integrity": "sha512-jA0rdU5KoQMC0e6ppoNRtpp6vjFq6+NY7r8hywnC7V+1Xj/MtHwGIbB1QaK/dunyjWteJzmkpd7ooeWg10T7GA==",
+                    "requires": {
+                        "errno": "^0.1.3",
+                        "readable-stream": "^2.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/memory-fs/-/memory-fs-0.5.0.tgz",
+                    "version": "0.5.0"
+                }
+            },
+            "integrity": "sha512-Nv9m36S/vxpsI+Hc4/ZGRs0n9mXqSWGGq49zxb/cJfPAQMbUtttJAlNPS4AQzaBdw/pKskw5bMbekT/Y7W/Wlg==",
             "requires": {
                 "graceful-fs": "^4.1.2",
-                "memory-fs": "^0.4.0",
+                "memory-fs": "^0.5.0",
                 "tapable": "^1.0.0"
             },
-            "resolved": "https://registry.npmjs.org/enhanced-resolve/-/enhanced-resolve-4.1.0.tgz",
-            "version": "4.1.0"
+            "resolved": "https://registry.npmjs.org/enhanced-resolve/-/enhanced-resolve-4.5.0.tgz",
+            "version": "4.5.0"
+        },
+        "entities": {
+            "integrity": "sha512-p92if5Nz619I0w+akJrLZH0MX0Pb5DX39XOwQTtXSdQQOaYH03S1uIQp4mhOZtAXrxq4ViO67YTiLBo2638o9A==",
+            "resolved": "https://registry.npmjs.org/entities/-/entities-2.2.0.tgz",
+            "version": "2.2.0"
+        },
+        "env-paths": {
+            "integrity": "sha512-+h1lkLKhZMTYjog1VEpJNG7NZJWcuc2DDk/qsqSTRRCOXiLjeQ1d1/udrUGhqMxUgAlwKNZ0cf2uqan5GLuS2A==",
+            "resolved": "https://registry.npmjs.org/env-paths/-/env-paths-2.2.1.tgz",
+            "version": "2.2.1"
         },
         "errno": {
-            "integrity": "sha512-MfrRBDWzIWifgq6tJj60gkAwtLNb6sQPlcFrSOflcP1aFmmruKQ2wRnze/8V6kgyz7H3FF8Npzv78mZ7XLLflg==",
+            "integrity": "sha512-dJ6oBr5SQ1VSd9qkk7ByRgb/1SH4JZjCHSW/mr63/QcXO9zLVxvJ6Oy13nio03rxpSnVDDjFor75SjVeZWPW/A==",
             "requires": {
                 "prr": "~1.0.1"
             },
-            "resolved": "https://registry.npmjs.org/errno/-/errno-0.1.7.tgz",
-            "version": "0.1.7"
+            "resolved": "https://registry.npmjs.org/errno/-/errno-0.1.8.tgz",
+            "version": "0.1.8"
         },
         "error-ex": {
             "integrity": "sha512-7dFHNmqeFSEt2ZBsCriorKnn3Z2pj+fd9kmI6QoWw4//DL+icEBfc0U7qJCisqrTsKTjw4fNFy2pW9OqStD84g==",
             "requires": {
                 "is-arrayish": "^0.2.1"
             },
             "resolved": "https://registry.npmjs.org/error-ex/-/error-ex-1.3.2.tgz",
             "version": "1.3.2"
         },
+        "es5-ext": {
+            "integrity": "sha512-Xs2Stw6NiNHWypzRTY1MtaG/uJlwCk8kH81920ma8mvN8Xq1gsfhZvpkImLQArw8AHnv8MT2I45J3c0R8slE+Q==",
+            "requires": {
+                "es6-iterator": "~2.0.3",
+                "es6-symbol": "~3.1.3",
+                "next-tick": "~1.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/es5-ext/-/es5-ext-0.10.53.tgz",
+            "version": "0.10.53"
+        },
+        "es6-iterator": {
+            "integrity": "sha1-p96IkUGgWpSwhUQDstCg+/qY87c=",
+            "requires": {
+                "d": "1",
+                "es5-ext": "^0.10.35",
+                "es6-symbol": "^3.1.1"
+            },
+            "resolved": "https://registry.npmjs.org/es6-iterator/-/es6-iterator-2.0.3.tgz",
+            "version": "2.0.3"
+        },
+        "es6-symbol": {
+            "integrity": "sha512-NJ6Yn3FuDinBaBRWl/q5X/s4koRHBrgKAu+yGI6JCBeiu3qrcbJhwT2GeR/EXVfylRk8dpQVJoLEFhK+Mu31NA==",
+            "requires": {
+                "d": "^1.0.1",
+                "ext": "^1.1.2"
+            },
+            "resolved": "https://registry.npmjs.org/es6-symbol/-/es6-symbol-3.1.3.tgz",
+            "version": "3.1.3"
+        },
+        "escalade": {
+            "integrity": "sha512-k0er2gUkLf8O0zKJiAhmkTnJlTvINGv7ygDNPbeIsX/TJjGJZHuh9B2UxbsaEkmlEo9MfhrSzmhIlhRlI2GXnw==",
+            "resolved": "https://registry.npmjs.org/escalade/-/escalade-3.1.1.tgz",
+            "version": "3.1.1"
+        },
         "escape-html": {
             "integrity": "sha1-Aljq5NPQwJdN4cFpGI7wBR0dGYg=",
             "resolved": "https://registry.npmjs.org/escape-html/-/escape-html-1.0.3.tgz",
             "version": "1.0.3"
         },
         "escape-string-regexp": {
             "integrity": "sha1-G2HAViGQqN/2rjuyzwIAyhMLhtQ=",
@@ -2305,21 +3906,33 @@
             "requires": {
                 "esrecurse": "^4.1.0",
                 "estraverse": "^4.1.1"
             },
             "resolved": "https://registry.npmjs.org/eslint-scope/-/eslint-scope-4.0.3.tgz",
             "version": "4.0.3"
         },
+        "esprima": {
+            "integrity": "sha512-eGuFFw7Upda+g4p+QHvnW0RyTX/SVeJBDM/gCtMARO0cLuT2HcEKnTPvhjV6aGeqrCB/sbNop0Kszm0jsaWU4A==",
+            "resolved": "https://registry.npmjs.org/esprima/-/esprima-4.0.1.tgz",
+            "version": "4.0.1"
+        },
         "esrecurse": {
-            "integrity": "sha512-64RBB++fIOAXPw3P9cy89qfMlvZEXZkqqJkjqqXIvzP5ezRZjW+lPWjw35UX/3EhUPFYbg5ER4JYgDw4007/DQ==",
+            "dependencies": {
+                "estraverse": {
+                    "integrity": "sha512-BxbNGGNm0RyRYvUdHpIwv9IWzeM9XClbOxwoATuFdOE7ZE6wHL+HQ5T8hoPM+zHvmKzzsEqhgy0GrQ5X13afiQ==",
+                    "resolved": "https://registry.npmjs.org/estraverse/-/estraverse-5.2.0.tgz",
+                    "version": "5.2.0"
+                }
+            },
+            "integrity": "sha512-KmfKL3b6G+RXvP8N1vr3Tq1kL/oCFgn2NYXEtqP8/L3pKapUA4G8cFVaoF3SU323CD4XypR/ffioHmkti6/Tag==",
             "requires": {
-                "estraverse": "^4.1.0"
+                "estraverse": "^5.2.0"
             },
-            "resolved": "https://registry.npmjs.org/esrecurse/-/esrecurse-4.2.1.tgz",
-            "version": "4.2.1"
+            "resolved": "https://registry.npmjs.org/esrecurse/-/esrecurse-4.3.0.tgz",
+            "version": "4.3.0"
         },
         "estraverse": {
             "integrity": "sha512-39nnKffWz8xN1BU/2c79n9nB9HDzo0niYUqx6xyqUnyoAnQyyWpOTdZEeiCch8BBu515t4wp9ZmgVfVhn9EBpw==",
             "resolved": "https://registry.npmjs.org/estraverse/-/estraverse-4.3.0.tgz",
             "version": "4.3.0"
         },
         "esutils": {
@@ -2328,18 +3941,23 @@
             "version": "2.0.3"
         },
         "etag": {
             "integrity": "sha1-Qa4u62XvpiJorr/qg6x9eSmbCIc=",
             "resolved": "https://registry.npmjs.org/etag/-/etag-1.8.1.tgz",
             "version": "1.8.1"
         },
+        "eventemitter3": {
+            "integrity": "sha512-8guHBZCwKnFhYdHr2ysuRWErTwhoN2X8XELRlrRwpmfeY2jjuUN4taQMsULKUVo1K4DvZl+0pgfyoysHxvmvEw==",
+            "resolved": "https://registry.npmjs.org/eventemitter3/-/eventemitter3-4.0.7.tgz",
+            "version": "4.0.7"
+        },
         "events": {
-            "integrity": "sha512-Dc381HFWJzEOhQ+d8pkNon++bk9h6cdAoAj4iE6Q4y6xgTzySWXlKn05/TVNpjnfRqi/X0EpJEJohPjNI3zpVA==",
-            "resolved": "https://registry.npmjs.org/events/-/events-3.0.0.tgz",
-            "version": "3.0.0"
+            "integrity": "sha512-mQw+2fkQbALzQ7V0MY0IqdnXNOeTtP4r0lN9z7AAawCXgqea7bDii20AYrIBrFd/Hx0M2Ocz6S111CaFkUcb0Q==",
+            "resolved": "https://registry.npmjs.org/events/-/events-3.3.0.tgz",
+            "version": "3.3.0"
         },
         "evp_bytestokey": {
             "integrity": "sha512-/f2Go4TognH/KvCISP7OUsHn85hT9nUkxxA9BEWxFn+Oj9o8ZNLm/40hdlgSLyuOimsrTKLUMEorQexp/aPQeA==",
             "requires": {
                 "md5.js": "^1.3.4",
                 "safe-buffer": "^5.1.1"
             },
@@ -2387,14 +4005,29 @@
                 "regex-not": "^1.0.0",
                 "snapdragon": "^0.8.1",
                 "to-regex": "^3.0.1"
             },
             "resolved": "https://registry.npmjs.org/expand-brackets/-/expand-brackets-2.1.4.tgz",
             "version": "2.1.4"
         },
+        "ext": {
+            "dependencies": {
+                "type": {
+                    "integrity": "sha512-180WMDQaIMm3+7hGXWf12GtdniDEy7nYcyFMKJn/eZz/6tSLXrUN9V0wKSbMjej0I1WHWbpREDEKHtqPQa9NNw==",
+                    "resolved": "https://registry.npmjs.org/type/-/type-2.5.0.tgz",
+                    "version": "2.5.0"
+                }
+            },
+            "integrity": "sha512-Key5NIsUxdqKg3vIsdw9dSuXpPCQ297y6wBjL30edxwPgt2E44WcWBZey/ZvUc6sERLTxKdyCu4gZFmUbk1Q7A==",
+            "requires": {
+                "type": "^2.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/ext/-/ext-1.4.0.tgz",
+            "version": "1.4.0"
+        },
         "extend-shallow": {
             "dependencies": {
                 "is-extendable": {
                     "integrity": "sha512-arnXMxT1hhoKo9k1LZdmlNyJdDDfy2v0fXjFlmok4+i8ul/6WlbVge9bhM74OpNPQPMGUToDtz+KXa1PneJxOA==",
                     "requires": {
                         "is-plain-object": "^2.0.4"
                     },
@@ -2466,48 +4099,75 @@
                 "snapdragon": "^0.8.1",
                 "to-regex": "^3.0.1"
             },
             "resolved": "https://registry.npmjs.org/extglob/-/extglob-2.0.4.tgz",
             "version": "2.0.4"
         },
         "fast-deep-equal": {
-            "integrity": "sha1-ewUhjd+WZ79/Nwv3/bLLFf3Qqkk=",
-            "resolved": "https://registry.npmjs.org/fast-deep-equal/-/fast-deep-equal-2.0.1.tgz",
-            "version": "2.0.1"
+            "integrity": "sha512-f3qQ9oQy9j2AhBe/H9VC91wLmKBCCU/gDOnKNAYG5hswO7BLKj09Hc5HYNz9cGI++xlpDCIgDaitVs03ATR84Q==",
+            "resolved": "https://registry.npmjs.org/fast-deep-equal/-/fast-deep-equal-3.1.3.tgz",
+            "version": "3.1.3"
         },
         "fast-json-stable-stringify": {
-            "integrity": "sha1-1RQsDK7msRifh9OnYREGT4bIu/I=",
-            "resolved": "https://registry.npmjs.org/fast-json-stable-stringify/-/fast-json-stable-stringify-2.0.0.tgz",
-            "version": "2.0.0"
+            "integrity": "sha512-lhd/wF+Lk98HZoTCtlVraHtfh5XYijIjalXck7saUtuanSDyLMxnHhSXEDJqHxD7msR8D0uCmqlkwjCV8xvwHw==",
+            "resolved": "https://registry.npmjs.org/fast-json-stable-stringify/-/fast-json-stable-stringify-2.1.0.tgz",
+            "version": "2.1.0"
         },
         "figgy-pudding": {
-            "integrity": "sha512-vNKxJHTEKNThjfrdJwHc7brvM6eVevuO5nTj6ez8ZQ1qbXTvGthucRF7S4vf2cr71QVnT70V34v0S1DyQsti0w==",
-            "resolved": "https://registry.npmjs.org/figgy-pudding/-/figgy-pudding-3.5.1.tgz",
-            "version": "3.5.1"
+            "integrity": "sha512-0btnI/H8f2pavGMN8w40mlSKOfTK2SVJmBfBeVIj3kNw0swwgzyRq0d5TJVOwodFmtvpPeWPN/MCcfuWF0Ezbw==",
+            "resolved": "https://registry.npmjs.org/figgy-pudding/-/figgy-pudding-3.5.2.tgz",
+            "version": "3.5.2"
         },
-        "fill-range": {
+        "file-loader": {
             "dependencies": {
-                "extend-shallow": {
-                    "integrity": "sha1-Ua99YUrZqfYQ6huvu5idaxxWiQ8=",
+                "json5": {
+                    "integrity": "sha512-aKS4WQjPenRxiQsC93MNfjx+nbF4PAdYzmd/1JIj8HYzqfbu86beTuNgXDzPknWk0n0uARlyewZo4s++ES36Ow==",
                     "requires": {
-                        "is-extendable": "^0.1.0"
+                        "minimist": "^1.2.0"
                     },
-                    "resolved": "https://registry.npmjs.org/extend-shallow/-/extend-shallow-2.0.1.tgz",
-                    "version": "2.0.1"
+                    "resolved": "https://registry.npmjs.org/json5/-/json5-1.0.1.tgz",
+                    "version": "1.0.1"
+                },
+                "loader-utils": {
+                    "integrity": "sha512-qH0WSMBtn/oHuwjy/NucEgbx5dbxxnxup9s4PVXJUDHZBQY+s0NWA9rJf53RBnQZxfch7euUui7hpoAPvALZdA==",
+                    "requires": {
+                        "big.js": "^5.2.2",
+                        "emojis-list": "^3.0.0",
+                        "json5": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/loader-utils/-/loader-utils-1.4.0.tgz",
+                    "version": "1.4.0"
                 }
             },
-            "integrity": "sha1-1USBHUKPmOsGpj3EAtJAPDKMOPc=",
+            "integrity": "sha512-+xZnaK5R8kBJrHK0/6HRlrKNamvVS5rjyuju+rnyxRGuwUJwpAMsVzUl5dz6rK8brkzjV6JpcFNjp6NqV0g1OQ==",
             "requires": {
-                "extend-shallow": "^2.0.1",
-                "is-number": "^3.0.0",
-                "repeat-string": "^1.6.1",
-                "to-regex-range": "^2.1.0"
+                "loader-utils": "^1.2.3",
+                "schema-utils": "^2.0.0"
             },
-            "resolved": "https://registry.npmjs.org/fill-range/-/fill-range-4.0.0.tgz",
-            "version": "4.0.0"
+            "resolved": "https://registry.npmjs.org/file-loader/-/file-loader-4.2.0.tgz",
+            "version": "4.2.0"
+        },
+        "file-uri-to-path": {
+            "integrity": "sha512-0Zt+s3L7Vf1biwWZ29aARiVYLx7iMGnEUl9x33fbB/j3jR81u/O2LbqK+Bm1CDSNDKVtJ/YjwY7TUd5SkeLQLw==",
+            "optional": true,
+            "resolved": "https://registry.npmjs.org/file-uri-to-path/-/file-uri-to-path-1.0.0.tgz",
+            "version": "1.0.0"
+        },
+        "fill-range": {
+            "integrity": "sha512-qOo9F+dMUmC2Lcb4BbVvnKJxTPjCm+RRpe4gDuGrzkL7mEVl/djYSu2OdQ2Pa302N4oqkSg9ir6jaLWJ2USVpQ==",
+            "requires": {
+                "to-regex-range": "^5.0.1"
+            },
+            "resolved": "https://registry.npmjs.org/fill-range/-/fill-range-7.0.1.tgz",
+            "version": "7.0.1"
+        },
+        "finally-polyfill": {
+            "integrity": "sha512-J1LEcZ5VXe1l3sEO+S//WqL5wcJ/ep7QeKJA6HhNZrcEEFj0eyC8IW3DEZhxySI2bx3r85dwAXz+vYPGuHx5UA==",
+            "resolved": "https://registry.npmjs.org/finally-polyfill/-/finally-polyfill-0.1.0.tgz",
+            "version": "0.1.0"
         },
         "find-cache-dir": {
             "integrity": "sha1-kojj6ePMN0hxfTnq3hfPcfww7m8=",
             "requires": {
                 "commondir": "^1.0.1",
                 "make-dir": "^1.0.0",
                 "pkg-dir": "^2.0.0"
@@ -2522,20 +4182,20 @@
                     "requires": {
                         "p-locate": "^4.1.0"
                     },
                     "resolved": "https://registry.npmjs.org/locate-path/-/locate-path-5.0.0.tgz",
                     "version": "5.0.0"
                 },
                 "p-limit": {
-                    "integrity": "sha512-85Tk+90UCVWvbDavCLKPOLC9vvY8OwEX/RtKF+/1OADJMVlFfEHOiMTPVyxg7mk/dKa+ipdHm0OUkTvCpMTuwg==",
+                    "integrity": "sha512-//88mFWSJx8lxCzwdAABTJL2MyWB12+eIY7MDL2SqLmAkeKU9qxRvWuSyTjm3FUmpBEMuFfckAIqEaVGUDxb6w==",
                     "requires": {
                         "p-try": "^2.0.0"
                     },
-                    "resolved": "https://registry.npmjs.org/p-limit/-/p-limit-2.2.1.tgz",
-                    "version": "2.2.1"
+                    "resolved": "https://registry.npmjs.org/p-limit/-/p-limit-2.3.0.tgz",
+                    "version": "2.3.0"
                 },
                 "p-locate": {
                     "integrity": "sha512-R79ZZ/0wAxKGu3oYMlz8jy/kbhsNrS7SKZ7PxEHBgJ5+F2mtFW2fK2cOtBh1cHYkQsbzFV7I+EoRKe6Yt0oK7A==",
                     "requires": {
                         "p-limit": "^2.2.0"
                     },
                     "resolved": "https://registry.npmjs.org/p-locate/-/p-locate-4.1.0.tgz",
@@ -2550,42 +4210,52 @@
             "integrity": "sha512-zoH7ZWPkRdgwYCDVoQTzqjG8JSPANhtvLhh4KVUHyKnaUJJrNeFmWIkTcNuJmR3GLMEmGYEf2S2bjgx26JTF+Q==",
             "requires": {
                 "locate-path": "^5.0.0"
             },
             "resolved": "https://registry.npmjs.org/find-up/-/find-up-4.0.0.tgz",
             "version": "4.0.0"
         },
+        "flatten": {
+            "integrity": "sha512-dVsPA/UwQ8+2uoFe5GHtiBMu48dWLTdsuEd7CKGlZlD78r1TTWBvDuFaFGKCo/ZfEr95Uk56vZoX86OsHkUeIg==",
+            "resolved": "https://registry.npmjs.org/flatten/-/flatten-1.0.3.tgz",
+            "version": "1.0.3"
+        },
         "flush-write-stream": {
             "integrity": "sha512-3Z4XhFZ3992uIq0XOqb9AreonueSYphE6oYbpt5+3u06JWklbsPkNv3ZKkP9Bz/r+1MWCaMoSQ28P85+1Yc77w==",
             "requires": {
                 "inherits": "^2.0.3",
                 "readable-stream": "^2.3.6"
             },
             "resolved": "https://registry.npmjs.org/flush-write-stream/-/flush-write-stream-1.1.1.tgz",
             "version": "1.1.1"
         },
+        "follow-redirects": {
+            "integrity": "sha512-HWqDgT7ZEkqRzBvc2s64vSZ/hfOceEol3ac/7tKwzuvEyWx3/4UegXh5oBOIotkGsObyk3xznnSRVADBgWSQVg==",
+            "resolved": "https://registry.npmjs.org/follow-redirects/-/follow-redirects-1.14.1.tgz",
+            "version": "1.14.1"
+        },
         "for-in": {
             "integrity": "sha1-gQaNKVqBQuwKxybG4iAMMPttXoA=",
             "resolved": "https://registry.npmjs.org/for-in/-/for-in-1.0.2.tgz",
             "version": "1.0.2"
         },
         "fork-ts-checker-webpack-plugin": {
-            "integrity": "sha512-2QDXnI2mbbly/OHx/ivtspi2l4K2g+IB0LTQ3AwsBfxyHtMFXtojlsJqGyhUggX08BC+F02CoCG0hRSPOLU2dQ==",
+            "integrity": "sha512-DuVkPNrM12jR41KM2e+N+styka0EgLkTnXmNcXdgOM37vtGeY+oCBK/Jx0hzSeEU6memFCtWb4htrHPMDfwwUQ==",
             "requires": {
                 "babel-code-frame": "^6.22.0",
                 "chalk": "^2.4.1",
-                "chokidar": "^2.0.4",
+                "chokidar": "^3.3.0",
                 "micromatch": "^3.1.10",
                 "minimatch": "^3.0.4",
                 "semver": "^5.6.0",
                 "tapable": "^1.0.0",
                 "worker-rpc": "^0.1.0"
             },
-            "resolved": "https://registry.npmjs.org/fork-ts-checker-webpack-plugin/-/fork-ts-checker-webpack-plugin-1.3.4.tgz",
-            "version": "1.3.4"
+            "resolved": "https://registry.npmjs.org/fork-ts-checker-webpack-plugin/-/fork-ts-checker-webpack-plugin-3.1.1.tgz",
+            "version": "3.1.1"
         },
         "fragment-cache": {
             "integrity": "sha1-QpD60n8T6Jvn8zeZxrxaCr//DRk=",
             "requires": {
                 "map-cache": "^0.2.2"
             },
             "resolved": "https://registry.npmjs.org/fragment-cache/-/fragment-cache-0.2.1.tgz",
@@ -2618,535 +4288,59 @@
         },
         "fs.realpath": {
             "integrity": "sha1-FQStJSMVjKpA20onh8sBQRmU6k8=",
             "resolved": "https://registry.npmjs.org/fs.realpath/-/fs.realpath-1.0.0.tgz",
             "version": "1.0.0"
         },
         "fsevents": {
-            "dependencies": {
-                "abbrev": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.1.1"
-                },
-                "ansi-regex": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "2.1.1"
-                },
-                "aproba": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.2.0"
-                },
-                "are-we-there-yet": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "delegates": "^1.0.0",
-                        "readable-stream": "^2.0.6"
-                    },
-                    "version": "1.1.5"
-                },
-                "balanced-match": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.0.0"
-                },
-                "brace-expansion": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "balanced-match": "^1.0.0",
-                        "concat-map": "0.0.1"
-                    },
-                    "version": "1.1.11"
-                },
-                "chownr": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.1.1"
-                },
-                "code-point-at": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.1.0"
-                },
-                "concat-map": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "0.0.1"
-                },
-                "console-control-strings": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.1.0"
-                },
-                "core-util-is": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.0.2"
-                },
-                "debug": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "ms": "^2.1.1"
-                    },
-                    "version": "4.1.1"
-                },
-                "deep-extend": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "0.6.0"
-                },
-                "delegates": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.0.0"
-                },
-                "detect-libc": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.0.3"
-                },
-                "fs-minipass": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "minipass": "^2.2.1"
-                    },
-                    "version": "1.2.5"
-                },
-                "fs.realpath": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.0.0"
-                },
-                "gauge": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "aproba": "^1.0.3",
-                        "console-control-strings": "^1.0.0",
-                        "has-unicode": "^2.0.0",
-                        "object-assign": "^4.1.0",
-                        "signal-exit": "^3.0.0",
-                        "string-width": "^1.0.1",
-                        "strip-ansi": "^3.0.1",
-                        "wide-align": "^1.1.0"
-                    },
-                    "version": "2.7.4"
-                },
-                "glob": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "fs.realpath": "^1.0.0",
-                        "inflight": "^1.0.4",
-                        "inherits": "2",
-                        "minimatch": "^3.0.4",
-                        "once": "^1.3.0",
-                        "path-is-absolute": "^1.0.0"
-                    },
-                    "version": "7.1.3"
-                },
-                "has-unicode": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "2.0.1"
-                },
-                "iconv-lite": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "safer-buffer": ">= 2.1.2 < 3"
-                    },
-                    "version": "0.4.24"
-                },
-                "ignore-walk": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "minimatch": "^3.0.4"
-                    },
-                    "version": "3.0.1"
-                },
-                "inflight": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "once": "^1.3.0",
-                        "wrappy": "1"
-                    },
-                    "version": "1.0.6"
-                },
-                "inherits": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "2.0.3"
-                },
-                "ini": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.3.5"
-                },
-                "is-fullwidth-code-point": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "number-is-nan": "^1.0.0"
-                    },
-                    "version": "1.0.0"
-                },
-                "isarray": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.0.0"
-                },
-                "minimatch": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "brace-expansion": "^1.1.7"
-                    },
-                    "version": "3.0.4"
-                },
-                "minimist": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "0.0.8"
-                },
-                "minipass": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "safe-buffer": "^5.1.2",
-                        "yallist": "^3.0.0"
-                    },
-                    "version": "2.3.5"
-                },
-                "minizlib": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "minipass": "^2.2.1"
-                    },
-                    "version": "1.2.1"
-                },
-                "mkdirp": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "minimist": "0.0.8"
-                    },
-                    "version": "0.5.1"
-                },
-                "ms": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "2.1.1"
-                },
-                "needle": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "debug": "^4.1.0",
-                        "iconv-lite": "^0.4.4",
-                        "sax": "^1.2.4"
-                    },
-                    "version": "2.3.0"
-                },
-                "node-pre-gyp": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "detect-libc": "^1.0.2",
-                        "mkdirp": "^0.5.1",
-                        "needle": "^2.2.1",
-                        "nopt": "^4.0.1",
-                        "npm-packlist": "^1.1.6",
-                        "npmlog": "^4.0.2",
-                        "rc": "^1.2.7",
-                        "rimraf": "^2.6.1",
-                        "semver": "^5.3.0",
-                        "tar": "^4"
-                    },
-                    "version": "0.12.0"
-                },
-                "nopt": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "abbrev": "1",
-                        "osenv": "^0.1.4"
-                    },
-                    "version": "4.0.1"
-                },
-                "npm-bundled": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.0.6"
-                },
-                "npm-packlist": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "ignore-walk": "^3.0.1",
-                        "npm-bundled": "^1.0.1"
-                    },
-                    "version": "1.4.1"
-                },
-                "npmlog": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "are-we-there-yet": "~1.1.2",
-                        "console-control-strings": "~1.1.0",
-                        "gauge": "~2.7.3",
-                        "set-blocking": "~2.0.0"
-                    },
-                    "version": "4.1.2"
-                },
-                "number-is-nan": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.0.1"
-                },
-                "object-assign": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "4.1.1"
-                },
-                "once": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "wrappy": "1"
-                    },
-                    "version": "1.4.0"
-                },
-                "os-homedir": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.0.2"
-                },
-                "os-tmpdir": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.0.2"
-                },
-                "osenv": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "os-homedir": "^1.0.0",
-                        "os-tmpdir": "^1.0.0"
-                    },
-                    "version": "0.1.5"
-                },
-                "path-is-absolute": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.0.1"
-                },
-                "process-nextick-args": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "2.0.0"
-                },
-                "rc": {
-                    "bundled": true,
-                    "dependencies": {
-                        "minimist": {
-                            "bundled": true,
-                            "optional": true,
-                            "version": "1.2.0"
-                        }
-                    },
-                    "optional": true,
-                    "requires": {
-                        "deep-extend": "^0.6.0",
-                        "ini": "~1.3.0",
-                        "minimist": "^1.2.0",
-                        "strip-json-comments": "~2.0.1"
-                    },
-                    "version": "1.2.8"
-                },
-                "readable-stream": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "core-util-is": "~1.0.0",
-                        "inherits": "~2.0.3",
-                        "isarray": "~1.0.0",
-                        "process-nextick-args": "~2.0.0",
-                        "safe-buffer": "~5.1.1",
-                        "string_decoder": "~1.1.1",
-                        "util-deprecate": "~1.0.1"
-                    },
-                    "version": "2.3.6"
-                },
-                "rimraf": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "glob": "^7.1.3"
-                    },
-                    "version": "2.6.3"
-                },
-                "safe-buffer": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "5.1.2"
-                },
-                "safer-buffer": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "2.1.2"
-                },
-                "sax": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.2.4"
-                },
-                "semver": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "5.7.0"
-                },
-                "set-blocking": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "2.0.0"
-                },
-                "signal-exit": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "3.0.2"
-                },
-                "string-width": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "code-point-at": "^1.0.0",
-                        "is-fullwidth-code-point": "^1.0.0",
-                        "strip-ansi": "^3.0.0"
-                    },
-                    "version": "1.0.2"
-                },
-                "string_decoder": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "safe-buffer": "~5.1.0"
-                    },
-                    "version": "1.1.1"
-                },
-                "strip-ansi": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "ansi-regex": "^2.0.0"
-                    },
-                    "version": "3.0.1"
-                },
-                "strip-json-comments": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "2.0.1"
-                },
-                "tar": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "chownr": "^1.1.1",
-                        "fs-minipass": "^1.2.5",
-                        "minipass": "^2.3.4",
-                        "minizlib": "^1.1.1",
-                        "mkdirp": "^0.5.0",
-                        "safe-buffer": "^5.1.2",
-                        "yallist": "^3.0.2"
-                    },
-                    "version": "4.4.8"
-                },
-                "util-deprecate": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.0.2"
-                },
-                "wide-align": {
-                    "bundled": true,
-                    "optional": true,
-                    "requires": {
-                        "string-width": "^1.0.2 || 2"
-                    },
-                    "version": "1.1.3"
-                },
-                "wrappy": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "1.0.2"
-                },
-                "yallist": {
-                    "bundled": true,
-                    "optional": true,
-                    "version": "3.0.3"
-                }
-            },
-            "integrity": "sha512-oeyj2H3EjjonWcFjD5NvZNE9Rqe4UW+nQBU2HNeKw0koVLEFIhtyETyAakeAM3de7Z/SW5kcA+fZUait9EApnw==",
+            "integrity": "sha512-xiqMQR4xAeHTuB9uWm+fFRcIOgKBMiOBP+eXiyT7jsgVCq1bkVygt00oASowB7EdtpOHaaPgKt812P9ab+DDKA==",
             "optional": true,
-            "requires": {
-                "nan": "^2.12.1",
-                "node-pre-gyp": "^0.12.0"
-            },
-            "resolved": "https://registry.npmjs.org/fsevents/-/fsevents-1.2.9.tgz",
-            "version": "1.2.9"
+            "resolved": "https://registry.npmjs.org/fsevents/-/fsevents-2.3.2.tgz",
+            "version": "2.3.2"
         },
         "function-bind": {
             "integrity": "sha512-yIovAzMX49sF8Yl58fSCWJ5svSLuaibPxXQJFLmBObTuCr0Mf1KiPopGM9NiFjiYBCbfaa2Fh6breQ6ANVTI0A==",
             "resolved": "https://registry.npmjs.org/function-bind/-/function-bind-1.1.1.tgz",
             "version": "1.1.1"
         },
+        "get-intrinsic": {
+            "integrity": "sha512-kWZrnVM42QCiEA2Ig1bG8zjoIMOgxWwYCEeNdwY6Tv/cOSeGpcoX4pXHfKUxNKVoArnrEr2e9srnAxxGIraS9Q==",
+            "requires": {
+                "function-bind": "^1.1.1",
+                "has": "^1.0.3",
+                "has-symbols": "^1.0.1"
+            },
+            "resolved": "https://registry.npmjs.org/get-intrinsic/-/get-intrinsic-1.1.1.tgz",
+            "version": "1.1.1"
+        },
         "get-value": {
             "integrity": "sha1-3BXKHGcjh8p2vTesCjlbogQqLCg=",
             "resolved": "https://registry.npmjs.org/get-value/-/get-value-2.0.6.tgz",
             "version": "2.0.6"
         },
         "glob": {
-            "integrity": "sha512-hkLPepehmnKk41pUGm3sYxoFs/umurYfYJCerbXEyFIWcAzvpipAgVkBqqT9RBKMGjnq6kMuyYwha6csxbiM1A==",
+            "integrity": "sha512-OvD9ENzPLbegENnYP5UUfJIirTg4+XwMWGaQfQTY0JenxNvvIKP3U3/tAQSPIu/lHxXYSZmpXlUHeqAIdKzBLQ==",
             "requires": {
                 "fs.realpath": "^1.0.0",
                 "inflight": "^1.0.4",
                 "inherits": "2",
                 "minimatch": "^3.0.4",
                 "once": "^1.3.0",
                 "path-is-absolute": "^1.0.0"
             },
-            "resolved": "https://registry.npmjs.org/glob/-/glob-7.1.4.tgz",
-            "version": "7.1.4"
+            "resolved": "https://registry.npmjs.org/glob/-/glob-7.1.7.tgz",
+            "version": "7.1.7"
         },
         "glob-parent": {
-            "dependencies": {
-                "is-glob": {
-                    "integrity": "sha1-e6WuJCF4BKxwcHuWkiVnSGzD6Eo=",
-                    "requires": {
-                        "is-extglob": "^2.1.0"
-                    },
-                    "resolved": "https://registry.npmjs.org/is-glob/-/is-glob-3.1.0.tgz",
-                    "version": "3.1.0"
-                }
-            },
-            "integrity": "sha1-nmr2KZ2NO9K9QEMIMr0RPfkGxa4=",
+            "integrity": "sha512-AOIgSQCepiJYwP3ARnGx+5VnTu2HBYdzbGP45eLw1vr3zB3vZLeyed1sC9hnbcOc9/SrMyM5RPQrkGz4aS9Zow==",
             "requires": {
-                "is-glob": "^3.1.0",
-                "path-dirname": "^1.0.0"
+                "is-glob": "^4.0.1"
             },
-            "resolved": "https://registry.npmjs.org/glob-parent/-/glob-parent-3.1.0.tgz",
-            "version": "3.1.0"
+            "resolved": "https://registry.npmjs.org/glob-parent/-/glob-parent-5.1.2.tgz",
+            "version": "5.1.2"
         },
         "glob-to-regexp": {
             "integrity": "sha512-lkX1HJXwyMcprw/5YUZc2s7DrpAiHB21/V+E1rHUrVNokkvB6bqMzT0VfV6/86ZNabt1k14YOIaT7nDvOX3Iiw==",
             "resolved": "https://registry.npmjs.org/glob-to-regexp/-/glob-to-regexp-0.4.1.tgz",
             "version": "0.4.1"
         },
         "globals": {
@@ -3170,22 +4364,33 @@
                 "pify": "^2.0.0",
                 "pinkie-promise": "^2.0.0"
             },
             "resolved": "https://registry.npmjs.org/globby/-/globby-6.1.0.tgz",
             "version": "6.1.0"
         },
         "graceful-fs": {
-            "integrity": "sha512-IItsdsea19BoLC7ELy13q1iJFNmd7ofZH5+X/pJr90/nRoPEX0DJo1dHDbgtYWOhJhcCgMDTOw84RZ72q6lB+Q==",
-            "resolved": "https://registry.npmjs.org/graceful-fs/-/graceful-fs-4.2.2.tgz",
-            "version": "4.2.2"
+            "integrity": "sha512-nTnJ528pbqxYanhpDYsi4Rd8MAeaBA67+RZ10CM1m3bTAVFEDcd5AuA4a6W5YkGZ1iNXHzZz8T6TBKLeBuNriQ==",
+            "resolved": "https://registry.npmjs.org/graceful-fs/-/graceful-fs-4.2.6.tgz",
+            "version": "4.2.6"
         },
-        "graceful-readlink": {
-            "integrity": "sha1-TK+tdrxi8C+gObL5Tpo906ORpyU=",
-            "resolved": "https://registry.npmjs.org/graceful-readlink/-/graceful-readlink-1.0.1.tgz",
-            "version": "1.0.1"
+        "gzip-size": {
+            "dependencies": {
+                "pify": {
+                    "integrity": "sha512-uB80kBFb/tfd68bVleG9T5GGsGPjJrLAUpR5PZIrhBnIaRTQRjqdJSsIKkOP6OAIFbj7GOrcudc5pNjZ+geV2g==",
+                    "resolved": "https://registry.npmjs.org/pify/-/pify-4.0.1.tgz",
+                    "version": "4.0.1"
+                }
+            },
+            "integrity": "sha512-FNHi6mmoHvs1mxZAds4PpdCS6QG8B4C1krxJsMutgxl5t3+GlRTzzI3NEkifXx2pVsOvJdOGSmIgDhQ55FwdPA==",
+            "requires": {
+                "duplexer": "^0.1.1",
+                "pify": "^4.0.1"
+            },
+            "resolved": "https://registry.npmjs.org/gzip-size/-/gzip-size-5.1.1.tgz",
+            "version": "5.1.1"
         },
         "has": {
             "integrity": "sha512-f2dvO0VU6Oej7RkWJGrehjbzMAjFp5/VKPp5tTpWIV4JHHZK1/BxbFRtf/siA2SWTe09caDmVtYYzWEIbBS4zw==",
             "requires": {
                 "function-bind": "^1.1.1"
             },
             "resolved": "https://registry.npmjs.org/has/-/has-1.0.3.tgz",
@@ -3201,30 +4406,48 @@
         },
         "has-flag": {
             "integrity": "sha1-tdRU3CGZriJWmfNGfloH87lVuv0=",
             "resolved": "https://registry.npmjs.org/has-flag/-/has-flag-3.0.0.tgz",
             "version": "3.0.0"
         },
         "has-symbols": {
-            "integrity": "sha1-uhqPGvKg/DllD1yFA2dwQSIGO0Q=",
-            "resolved": "https://registry.npmjs.org/has-symbols/-/has-symbols-1.0.0.tgz",
-            "version": "1.0.0"
+            "integrity": "sha512-chXa79rL/UC2KlX17jo3vRGz0azaWEx5tGqZg5pO3NUyEJVB17dMruQlzCCOfUvElghKcm5194+BCRvi2Rv/Gw==",
+            "resolved": "https://registry.npmjs.org/has-symbols/-/has-symbols-1.0.2.tgz",
+            "version": "1.0.2"
         },
         "has-value": {
             "integrity": "sha1-GLKB2lhbHFxR3vJMkw7SmgvmsXc=",
             "requires": {
                 "get-value": "^2.0.6",
                 "has-values": "^1.0.0",
                 "isobject": "^3.0.0"
             },
             "resolved": "https://registry.npmjs.org/has-value/-/has-value-1.0.0.tgz",
             "version": "1.0.0"
         },
         "has-values": {
             "dependencies": {
+                "is-number": {
+                    "dependencies": {
+                        "kind-of": {
+                            "integrity": "sha1-MeohpzS6ubuw8yRm2JOupR5KPGQ=",
+                            "requires": {
+                                "is-buffer": "^1.1.5"
+                            },
+                            "resolved": "https://registry.npmjs.org/kind-of/-/kind-of-3.2.2.tgz",
+                            "version": "3.2.2"
+                        }
+                    },
+                    "integrity": "sha1-JP1iAaR4LPUFYcgQJ2r8fRLXEZU=",
+                    "requires": {
+                        "kind-of": "^3.0.2"
+                    },
+                    "resolved": "https://registry.npmjs.org/is-number/-/is-number-3.0.0.tgz",
+                    "version": "3.0.0"
+                },
                 "kind-of": {
                     "integrity": "sha1-IIE989cSkosgc3hpGkUGb65y3Vc=",
                     "requires": {
                         "is-buffer": "^1.1.5"
                     },
                     "resolved": "https://registry.npmjs.org/kind-of/-/kind-of-4.0.0.tgz",
                     "version": "4.0.0"
@@ -3235,21 +4458,39 @@
                 "is-number": "^3.0.0",
                 "kind-of": "^4.0.0"
             },
             "resolved": "https://registry.npmjs.org/has-values/-/has-values-1.0.0.tgz",
             "version": "1.0.0"
         },
         "hash-base": {
-            "integrity": "sha1-X8hoaEfs1zSZQDMZprCj8/auSRg=",
+            "dependencies": {
+                "readable-stream": {
+                    "integrity": "sha512-BViHy7LKeTz4oNnkcLJ+lVSL6vpiFeX6/d3oSH8zCW7UxP2onchk+vTGB143xuFjHS3deTgkKoXXymXqymiIdA==",
+                    "requires": {
+                        "inherits": "^2.0.3",
+                        "string_decoder": "^1.1.1",
+                        "util-deprecate": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/readable-stream/-/readable-stream-3.6.0.tgz",
+                    "version": "3.6.0"
+                },
+                "safe-buffer": {
+                    "integrity": "sha512-rp3So07KcdmmKbGvgaNxQSJr7bGVSVk5S9Eq1F+ppbRo70+YeaDxkw5Dd8NPN+GD6bjnYm2VuPuCXmpuYvmCXQ==",
+                    "resolved": "https://registry.npmjs.org/safe-buffer/-/safe-buffer-5.2.1.tgz",
+                    "version": "5.2.1"
+                }
+            },
+            "integrity": "sha512-1nmYp/rhMDiE7AYkDw+lLwlAzz0AntGIe51F3RfFfEqyQ3feY2eI/NcwC6umIQVOASPMsWJLJScWKSSvzL9IVA==",
             "requires": {
-                "inherits": "^2.0.1",
-                "safe-buffer": "^5.0.1"
+                "inherits": "^2.0.4",
+                "readable-stream": "^3.6.0",
+                "safe-buffer": "^5.2.0"
             },
-            "resolved": "https://registry.npmjs.org/hash-base/-/hash-base-3.0.4.tgz",
-            "version": "3.0.4"
+            "resolved": "https://registry.npmjs.org/hash-base/-/hash-base-3.1.0.tgz",
+            "version": "3.1.0"
         },
         "hash.js": {
             "integrity": "sha512-taOaskGt4z4SOANNseOviYDvjEJinIkRgmp7LbKP2YTTmVxWBl87s/uzK9r+44BclBSp2X7K1hqeNfz9JbBeXA==",
             "requires": {
                 "inherits": "^2.0.3",
                 "minimalistic-assert": "^1.0.1"
             },
@@ -3263,22 +4504,33 @@
                 "minimalistic-assert": "^1.0.0",
                 "minimalistic-crypto-utils": "^1.0.1"
             },
             "resolved": "https://registry.npmjs.org/hmac-drbg/-/hmac-drbg-1.0.1.tgz",
             "version": "1.0.1"
         },
         "hosted-git-info": {
-            "integrity": "sha512-pzXIvANXEFrc5oFFXRMkbLPQ2rXRoDERwDLyrcUxGhaZhgP54BBSl9Oheh7Vv0T090cszWBxPjkQQ5Sq1PbBRQ==",
-            "resolved": "https://registry.npmjs.org/hosted-git-info/-/hosted-git-info-2.8.4.tgz",
-            "version": "2.8.4"
+            "integrity": "sha512-mxIDAb9Lsm6DoOJ7xH+5+X4y1LU/4Hi50L9C5sIswK3JzULS4bwk1FvjdBgvYR4bzT4tuUQiC15FE2f5HbLvYw==",
+            "resolved": "https://registry.npmjs.org/hosted-git-info/-/hosted-git-info-2.8.9.tgz",
+            "version": "2.8.9"
         },
         "html-entities": {
-            "integrity": "sha1-DfKTUfByEWNRXfueVUPl9u7VFi8=",
-            "resolved": "https://registry.npmjs.org/html-entities/-/html-entities-1.2.1.tgz",
-            "version": "1.2.1"
+            "integrity": "sha512-8nxjcBcd8wovbeKx7h3wTji4e6+rhaVuPNpMqwWgnHh+N9ToqsCs6XztWRBPQ+UtzsoMAdKZtUENoVzU/EMtZA==",
+            "resolved": "https://registry.npmjs.org/html-entities/-/html-entities-1.4.0.tgz",
+            "version": "1.4.0"
+        },
+        "htmlparser2": {
+            "integrity": "sha512-4zDq1a1zhE4gQso/c5LP1OtrhYTncXNSpvJYtWJBtXAETPlMfi3IFNjGuQbYLuVY4ZR0QMqRVvo4Pdy9KLyP8Q==",
+            "requires": {
+                "domelementtype": "^2.0.1",
+                "domhandler": "^3.0.0",
+                "domutils": "^2.0.0",
+                "entities": "^2.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/htmlparser2/-/htmlparser2-4.1.0.tgz",
+            "version": "4.1.0"
         },
         "http-errors": {
             "dependencies": {
                 "inherits": {
                     "integrity": "sha1-Yzwsg+PaQqUC9SRmAiSA9CCCYd4=",
                     "resolved": "https://registry.npmjs.org/inherits/-/inherits-2.0.3.tgz",
                     "version": "2.0.3"
@@ -3291,42 +4543,95 @@
                 "setprototypeof": "1.1.1",
                 "statuses": ">= 1.5.0 < 2",
                 "toidentifier": "1.0.0"
             },
             "resolved": "https://registry.npmjs.org/http-errors/-/http-errors-1.7.2.tgz",
             "version": "1.7.2"
         },
+        "http-proxy": {
+            "integrity": "sha512-84I2iJM/n1d4Hdgc6y2+qY5mDaz2PUVjlg9znE9byl+q0uC3DeByqBGReQu5tpLK0TAqTIXScRUV+dg7+bUPpQ==",
+            "requires": {
+                "eventemitter3": "^4.0.0",
+                "follow-redirects": "^1.0.0",
+                "requires-port": "^1.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/http-proxy/-/http-proxy-1.18.0.tgz",
+            "version": "1.18.0"
+        },
         "https-browserify": {
             "integrity": "sha1-7AbBDgo0wPL68Zn3/X/Hj//QPHM=",
             "resolved": "https://registry.npmjs.org/https-browserify/-/https-browserify-1.0.0.tgz",
             "version": "1.0.0"
         },
         "iconv-lite": {
             "integrity": "sha512-v3MXnZAcvnywkTUEZomIActle7RXXeedOR31wwl7VlyoXO4Qi9arvSenNQWne1TcRwhCL1HwLI21bEqdpj8/rA==",
             "requires": {
                 "safer-buffer": ">= 2.1.2 < 3"
             },
             "resolved": "https://registry.npmjs.org/iconv-lite/-/iconv-lite-0.4.24.tgz",
             "version": "0.4.24"
         },
+        "icss-utils": {
+            "integrity": "sha512-4aFq7wvWyMHKgxsH8QQtGpvbASCf+eM3wPRLI6R+MgAnTCZ6STYsRvttLvRWK0Nfif5piF394St3HeJDaljGPA==",
+            "requires": {
+                "postcss": "^7.0.14"
+            },
+            "resolved": "https://registry.npmjs.org/icss-utils/-/icss-utils-4.1.1.tgz",
+            "version": "4.1.1"
+        },
         "ieee754": {
-            "integrity": "sha512-4vf7I2LYV/HaWerSo3XmlMkp5eZ83i+/CDluXi/IGTs/O1sejBNhTtnxzmRZfvOUqj7lZjqHkeTvpgSFDlWZTg==",
-            "resolved": "https://registry.npmjs.org/ieee754/-/ieee754-1.1.13.tgz",
-            "version": "1.1.13"
+            "integrity": "sha512-dcyqhDvX1C46lXZcVqCpK+FtMRQVdIMN6/Df5js2zouUsqG7I6sFxitIC+7KYK29KdXOLHdu9zL4sFnoVQnqaA==",
+            "resolved": "https://registry.npmjs.org/ieee754/-/ieee754-1.2.1.tgz",
+            "version": "1.2.1"
         },
         "iferr": {
             "integrity": "sha1-xg7taebY/bazEEofy8ocGS3FtQE=",
             "resolved": "https://registry.npmjs.org/iferr/-/iferr-0.1.5.tgz",
             "version": "0.1.5"
         },
+        "ignore-loader": {
+            "integrity": "sha1-2B8kA3bQuk8Nd4lyw60lh0EXpGM=",
+            "resolved": "https://registry.npmjs.org/ignore-loader/-/ignore-loader-0.1.2.tgz",
+            "version": "0.1.2"
+        },
+        "import-cwd": {
+            "integrity": "sha1-qmzzbnInYShcs3HsZRn1PiQ1sKk=",
+            "requires": {
+                "import-from": "^2.1.0"
+            },
+            "resolved": "https://registry.npmjs.org/import-cwd/-/import-cwd-2.1.0.tgz",
+            "version": "2.1.0"
+        },
+        "import-fresh": {
+            "integrity": "sha1-2BNVwVYS04bGH53dOSLUMEgipUY=",
+            "requires": {
+                "caller-path": "^2.0.0",
+                "resolve-from": "^3.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/import-fresh/-/import-fresh-2.0.0.tgz",
+            "version": "2.0.0"
+        },
+        "import-from": {
+            "integrity": "sha1-M1238qev/VOqpHHUuAId7ja387E=",
+            "requires": {
+                "resolve-from": "^3.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/import-from/-/import-from-2.1.0.tgz",
+            "version": "2.1.0"
+        },
         "imurmurhash": {
             "integrity": "sha1-khi5srkoojixPcT7a21XbyMUU+o=",
             "resolved": "https://registry.npmjs.org/imurmurhash/-/imurmurhash-0.1.4.tgz",
             "version": "0.1.4"
         },
+        "indexes-of": {
+            "integrity": "sha1-8w9xbI4r00bHtn0985FVZqfAVgc=",
+            "resolved": "https://registry.npmjs.org/indexes-of/-/indexes-of-1.0.1.tgz",
+            "version": "1.0.1"
+        },
         "infer-owner": {
             "integrity": "sha512-IClj+Xz94+d7irH5qRyfJonOdfTzuDaifE6ZPWfx0N0+/ATZCbuTPq2prFl526urkQd90WyUKIh1DfBQ2hMz9A==",
             "resolved": "https://registry.npmjs.org/infer-owner/-/infer-owner-1.0.4.tgz",
             "version": "1.0.4"
         },
         "inflight": {
             "integrity": "sha1-Sb1jMdfQLQwJvJEKEHW6gWW1bfk=",
@@ -3370,26 +4675,34 @@
         },
         "is-arrayish": {
             "integrity": "sha1-d8mYQFJ6qOyxqLppe4BkWnqSap0=",
             "resolved": "https://registry.npmjs.org/is-arrayish/-/is-arrayish-0.2.1.tgz",
             "version": "0.2.1"
         },
         "is-binary-path": {
-            "integrity": "sha1-dfFmQrSA8YenEcgUFh/TpKdlWJg=",
+            "integrity": "sha512-ZMERYes6pDydyuGidse7OsHxtbI7WVeUEozgR/g7rd0xUimYNlvZRE/K2MgZTjWy725IfelLeVcEM97mmtRGXw==",
             "requires": {
-                "binary-extensions": "^1.0.0"
+                "binary-extensions": "^2.0.0"
             },
-            "resolved": "https://registry.npmjs.org/is-binary-path/-/is-binary-path-1.0.1.tgz",
-            "version": "1.0.1"
+            "resolved": "https://registry.npmjs.org/is-binary-path/-/is-binary-path-2.1.0.tgz",
+            "version": "2.1.0"
         },
         "is-buffer": {
             "integrity": "sha512-NcdALwpXkTm5Zvvbk7owOUSvVvBKDgKP5/ewfXEznmQFfs4ZRmanOeKBTjRVjka3QFoN6XJ+9F3USqfHqTaU5w==",
             "resolved": "https://registry.npmjs.org/is-buffer/-/is-buffer-1.1.6.tgz",
             "version": "1.1.6"
         },
+        "is-core-module": {
+            "integrity": "sha512-TXCMSDsEHMEEZ6eCA8rwRDbLu55MRGmrctljsBX/2v1d9/GzqHOxW5c5oPSgrUt2vBFXebu9rGqckXGPWOlYpg==",
+            "requires": {
+                "has": "^1.0.3"
+            },
+            "resolved": "https://registry.npmjs.org/is-core-module/-/is-core-module-2.5.0.tgz",
+            "version": "2.5.0"
+        },
         "is-data-descriptor": {
             "dependencies": {
                 "kind-of": {
                     "integrity": "sha1-MeohpzS6ubuw8yRm2JOupR5KPGQ=",
                     "requires": {
                         "is-buffer": "^1.1.5"
                     },
@@ -3417,14 +4730,24 @@
                 "is-accessor-descriptor": "^0.1.6",
                 "is-data-descriptor": "^0.1.4",
                 "kind-of": "^5.0.0"
             },
             "resolved": "https://registry.npmjs.org/is-descriptor/-/is-descriptor-0.1.6.tgz",
             "version": "0.1.6"
         },
+        "is-directory": {
+            "integrity": "sha1-YTObbyR1/Hcv2cnYP1yFddwVSuE=",
+            "resolved": "https://registry.npmjs.org/is-directory/-/is-directory-0.3.1.tgz",
+            "version": "0.3.1"
+        },
+        "is-docker": {
+            "integrity": "sha512-pJEdRugimx4fBMra5z2/5iRdZ63OhYV0vr0Dwm5+xtW4D1FvRkB8hamMIhnWfyJeDdyr/aa7BDyNbtG38VxgoQ==",
+            "resolved": "https://registry.npmjs.org/is-docker/-/is-docker-2.0.0.tgz",
+            "version": "2.0.0"
+        },
         "is-extendable": {
             "integrity": "sha1-YrEQ4omkcUGOPsNqYX1HLjAd/Ik=",
             "resolved": "https://registry.npmjs.org/is-extendable/-/is-extendable-0.1.1.tgz",
             "version": "0.1.1"
         },
         "is-extglob": {
             "integrity": "sha1-qIwCU1eR8C7TfHahueqXc8gz+MI=",
@@ -3436,30 +4759,22 @@
             "requires": {
                 "is-extglob": "^2.1.1"
             },
             "resolved": "https://registry.npmjs.org/is-glob/-/is-glob-4.0.1.tgz",
             "version": "4.0.1"
         },
         "is-number": {
-            "dependencies": {
-                "kind-of": {
-                    "integrity": "sha1-MeohpzS6ubuw8yRm2JOupR5KPGQ=",
-                    "requires": {
-                        "is-buffer": "^1.1.5"
-                    },
-                    "resolved": "https://registry.npmjs.org/kind-of/-/kind-of-3.2.2.tgz",
-                    "version": "3.2.2"
-                }
-            },
-            "integrity": "sha1-JP1iAaR4LPUFYcgQJ2r8fRLXEZU=",
-            "requires": {
-                "kind-of": "^3.0.2"
-            },
-            "resolved": "https://registry.npmjs.org/is-number/-/is-number-3.0.0.tgz",
-            "version": "3.0.0"
+            "integrity": "sha512-41Cifkg6e8TylSpdtTpeLVMqvSBEVzTttHvERD741+pnZ8ANv0004MRL43QKPDlK9cGvNp6NZWZUBlbGXYxxng==",
+            "resolved": "https://registry.npmjs.org/is-number/-/is-number-7.0.0.tgz",
+            "version": "7.0.0"
+        },
+        "is-obj": {
+            "integrity": "sha512-drqDG3cbczxxEJRoOXcOjtdp1J/lyp1mNn0xaznRs8+muBhgQcrnbspox5X5fOw0HnMnbfDzvnEMEtqDEJEo8w==",
+            "resolved": "https://registry.npmjs.org/is-obj/-/is-obj-2.0.0.tgz",
+            "version": "2.0.0"
         },
         "is-path-cwd": {
             "integrity": "sha1-0iXsIxMuie3Tj9p2dHLmLmXxEG0=",
             "resolved": "https://registry.npmjs.org/is-path-cwd/-/is-path-cwd-1.0.0.tgz",
             "version": "1.0.0"
         },
         "is-path-in-cwd": {
@@ -3474,36 +4789,46 @@
             "integrity": "sha1-jvW33lBDej/cprToZe96pVy0gDY=",
             "requires": {
                 "path-is-inside": "^1.0.1"
             },
             "resolved": "https://registry.npmjs.org/is-path-inside/-/is-path-inside-1.0.1.tgz",
             "version": "1.0.1"
         },
+        "is-plain-obj": {
+            "integrity": "sha1-caUMhCnfync8kqOQpKA7OfzVHT4=",
+            "resolved": "https://registry.npmjs.org/is-plain-obj/-/is-plain-obj-1.1.0.tgz",
+            "version": "1.1.0"
+        },
         "is-plain-object": {
             "integrity": "sha512-h5PpgXkWitc38BBMYawTYMWJHFZJVnBquFE57xFpjB8pJFiF6gZ+bU+WyI/yqXiFR5mdLsgYNaPe8uao6Uv9Og==",
             "requires": {
                 "isobject": "^3.0.1"
             },
             "resolved": "https://registry.npmjs.org/is-plain-object/-/is-plain-object-2.0.4.tgz",
             "version": "2.0.4"
         },
+        "is-typedarray": {
+            "integrity": "sha1-5HnICFjfDBsR3dppQPlgEfzaSpo=",
+            "resolved": "https://registry.npmjs.org/is-typedarray/-/is-typedarray-1.0.0.tgz",
+            "version": "1.0.0"
+        },
         "is-what": {
             "integrity": "sha512-70wGVRNNT4DHVK9ZEcQmA3dMUW04Rdnlr5TDrcq/qN/7nMGY/2KIi5wwP7RhaNiIoPNeFyuvL8gaP+SRRP72OA==",
             "resolved": "https://registry.npmjs.org/is-what/-/is-what-3.11.0.tgz",
             "version": "3.11.0"
         },
         "is-windows": {
             "integrity": "sha512-eXK1UInq2bPmjyX6e3VHIzMLobc4J94i4AWn+Hpq3OU5KkrRC96OAcR3PRJ/pGu6m8TRnBHP9dkXQVsT/COVIA==",
             "resolved": "https://registry.npmjs.org/is-windows/-/is-windows-1.0.2.tgz",
             "version": "1.0.2"
         },
         "is-wsl": {
-            "integrity": "sha1-HxbkqiKwTRM2tmGIpmrzxgDDpm0=",
-            "resolved": "https://registry.npmjs.org/is-wsl/-/is-wsl-1.1.0.tgz",
-            "version": "1.1.0"
+            "integrity": "sha512-umZHcSrwlDHo2TGMXv0DZ8dIUGunZ2Iv68YZnrmCiBPkZ4aaOhtv7pXJKeki9k3qJ3RJr0cDyitcl5wEH3AYog==",
+            "resolved": "https://registry.npmjs.org/is-wsl/-/is-wsl-2.1.1.tgz",
+            "version": "2.1.1"
         },
         "isarray": {
             "integrity": "sha1-u5NdSFgsuhaMBoNJV6VKPgcSTxE=",
             "resolved": "https://registry.npmjs.org/isarray/-/isarray-1.0.0.tgz",
             "version": "1.0.0"
         },
         "isobject": {
@@ -3516,24 +4841,52 @@
             "requires": {
                 "node-fetch": "^2.2.0",
                 "unfetch": "^4.0.0"
             },
             "resolved": "https://registry.npmjs.org/isomorphic-unfetch/-/isomorphic-unfetch-3.0.0.tgz",
             "version": "3.0.0"
         },
+        "jest-worker": {
+            "dependencies": {
+                "supports-color": {
+                    "integrity": "sha512-qe1jfm1Mg7Nq/NSh6XE24gPXROEVsWHxC1LIx//XNlD9iw7YZQGjZNjYN7xGaEG6iKdA8EtNFW6R0gjnVXp+wQ==",
+                    "requires": {
+                        "has-flag": "^3.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/supports-color/-/supports-color-6.1.0.tgz",
+                    "version": "6.1.0"
+                }
+            },
+            "integrity": "sha512-51PE4haMSXcHohnSMdM42anbvZANYTqMrr52tVKPqqsPJMzoP6FYYDVqahX/HrAoKEKz3uUPzSvKs9A3qR4iVw==",
+            "requires": {
+                "merge-stream": "^2.0.0",
+                "supports-color": "^6.1.0"
+            },
+            "resolved": "https://registry.npmjs.org/jest-worker/-/jest-worker-24.9.0.tgz",
+            "version": "24.9.0"
+        },
         "js-levenshtein": {
             "integrity": "sha512-X2BB11YZtrRqY4EnQcLX5Rh373zbK4alC1FW7D7MBhL2gtcC17cTnr6DmfHZeS0s2rTHjUTMMHfG7gO8SSdw+g==",
             "resolved": "https://registry.npmjs.org/js-levenshtein/-/js-levenshtein-1.1.6.tgz",
             "version": "1.1.6"
         },
         "js-tokens": {
             "integrity": "sha512-RdJUflcE3cUzKiMqQgsCu06FPu9UdIJO0beYbPhHN4k6apgJtifcoCtT9bcxOpYBtpD2kCM6Sbzg4CausW/PKQ==",
             "resolved": "https://registry.npmjs.org/js-tokens/-/js-tokens-4.0.0.tgz",
             "version": "4.0.0"
         },
+        "js-yaml": {
+            "integrity": "sha512-okMH7OXXJ7YrN9Ok3/SXrnu4iX9yOk+25nqX4imS2npuvTYDmo/QEZoqwZkYaIDk3jVvBOTOIEgEhaLOynBS9g==",
+            "requires": {
+                "argparse": "^1.0.7",
+                "esprima": "^4.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/js-yaml/-/js-yaml-3.14.1.tgz",
+            "version": "3.14.1"
+        },
         "jsesc": {
             "integrity": "sha512-OYu7XEzjkCQ3C5Ps3QIZsQfNpqoJyZZA99wd9aWd05NCtC5pWOkShK2mkL6HXQR6/Cy2lbNdPlZBpuQHXE63gA==",
             "resolved": "https://registry.npmjs.org/jsesc/-/jsesc-2.5.2.tgz",
             "version": "2.5.2"
         },
         "json-parse-better-errors": {
             "integrity": "sha512-mrqyZKfX5EhL7hvqcV6WG1yYjnjeuYDzDhhcAAUrq8Po85NBQBJP+ZDUT75qZQ98IkUoBqdkExkukOU7Ts2wrw==",
@@ -3541,26 +4894,67 @@
             "version": "1.0.2"
         },
         "json-schema-traverse": {
             "integrity": "sha512-xbbCH5dCYU5T8LcEhhuh7HJ88HXuW3qsI3Y0zOZFKfZEHcpWiHU/Jxzk629Brsab/mMiHQti9wMP+845RPe3Vg==",
             "resolved": "https://registry.npmjs.org/json-schema-traverse/-/json-schema-traverse-0.4.1.tgz",
             "version": "0.4.1"
         },
+        "json-schema-typed": {
+            "integrity": "sha512-7DE8mpG+/fVw+dTpjbxnx47TaMnDfOI1jwft9g1VybltZCduyRQPJPvc+zzKY9WPHxhPWczyFuYa6I8Mw4iU5A==",
+            "resolved": "https://registry.npmjs.org/json-schema-typed/-/json-schema-typed-7.0.3.tgz",
+            "version": "7.0.3"
+        },
         "json5": {
-            "integrity": "sha512-8Mh9h6xViijj36g7Dxi+Y4S6hNGV96vcJZr/SrlHh1LR/pEn/8j/+qIBbs44YKl69Lrfctp4QD+AdWLTMqEZAQ==",
+            "integrity": "sha512-l+3HXD0GEI3huGq1njuqtzYK8OYJyXMkOLtQ53pjWh89tvWS2h6l+1zMkYWqlb57+SiQodKZyvMEFb2X+KrFhQ==",
             "requires": {
                 "minimist": "^1.2.0"
             },
-            "resolved": "https://registry.npmjs.org/json5/-/json5-2.1.0.tgz",
-            "version": "2.1.0"
+            "resolved": "https://registry.npmjs.org/json5/-/json5-2.1.1.tgz",
+            "version": "2.1.1"
+        },
+        "jsonwebtoken": {
+            "integrity": "sha512-XjwVfRS6jTMsqYs0EsuJ4LGxXV14zQybNd4L2r0UvbVnSF9Af8x7p5MzbJ90Ioz/9TI41/hTCvznF/loiSzn8w==",
+            "requires": {
+                "jws": "^3.2.2",
+                "lodash.includes": "^4.3.0",
+                "lodash.isboolean": "^3.0.3",
+                "lodash.isinteger": "^4.0.4",
+                "lodash.isnumber": "^3.0.3",
+                "lodash.isplainobject": "^4.0.6",
+                "lodash.isstring": "^4.0.1",
+                "lodash.once": "^4.0.0",
+                "ms": "^2.1.1",
+                "semver": "^5.6.0"
+            },
+            "resolved": "https://registry.npmjs.org/jsonwebtoken/-/jsonwebtoken-8.5.1.tgz",
+            "version": "8.5.1"
+        },
+        "jwa": {
+            "integrity": "sha512-qiLX/xhEEFKUAJ6FiBMbes3w9ATzyk5W7Hvzpa/SLYdxNtng+gcurvrI7TbACjIXlsJyr05/S1oUhZrc63evQA==",
+            "requires": {
+                "buffer-equal-constant-time": "1.0.1",
+                "ecdsa-sig-formatter": "1.0.11",
+                "safe-buffer": "^5.0.1"
+            },
+            "resolved": "https://registry.npmjs.org/jwa/-/jwa-1.4.1.tgz",
+            "version": "1.4.1"
+        },
+        "jws": {
+            "integrity": "sha512-YHlZCB6lMTllWDtSPHz/ZXTsi8S00usEV6v1tjq8tOUZzw7DpSDWVXjXDre6ed1w/pd495ODpHZYSdkRTsa0HA==",
+            "requires": {
+                "jwa": "^1.4.1",
+                "safe-buffer": "^5.0.1"
+            },
+            "resolved": "https://registry.npmjs.org/jws/-/jws-3.2.2.tgz",
+            "version": "3.2.2"
         },
         "kind-of": {
-            "integrity": "sha512-s5kLOcnH0XqDO+FvuaLX8DDjZ18CGFk7VygH40QoKPUQhW4e2rvM0rwUq0t8IQDOwYSeLK01U90OjzBTme2QqA==",
-            "resolved": "https://registry.npmjs.org/kind-of/-/kind-of-6.0.2.tgz",
-            "version": "6.0.2"
+            "integrity": "sha512-dcS1ul+9tmeD95T+x28/ehLgd9mENa3LsvDTtzm3vyBEO7RPptvAD+t44WVXaUjTBRcrpFeFlC8WCruUR456hw==",
+            "resolved": "https://registry.npmjs.org/kind-of/-/kind-of-6.0.3.tgz",
+            "version": "6.0.3"
         },
         "launch-editor": {
             "integrity": "sha512-On+V7K2uZK6wK7x691ycSUbLD/FyKKelArkbaAMSSJU8JmqmhwN2+mnJDNINuJWSrh2L0kDk+ZQtbC/gOWUwLw==",
             "requires": {
                 "chalk": "^2.3.0",
                 "shell-quote": "^1.6.1"
             },
@@ -3589,54 +4983,109 @@
             "integrity": "sha512-Jsmr89RcXGIwivFY21FcRrisYZfvLMTWx5kOLc+JTxtpBOG6xML0vzbc6SEQG2FO9/4Fc3wW4LVcB5DmGflaRw==",
             "resolved": "https://registry.npmjs.org/loader-runner/-/loader-runner-2.4.0.tgz",
             "version": "2.4.0"
         },
         "loader-utils": {
             "dependencies": {
                 "json5": {
-                    "integrity": "sha512-aKS4WQjPenRxiQsC93MNfjx+nbF4PAdYzmd/1JIj8HYzqfbu86beTuNgXDzPknWk0n0uARlyewZo4s++ES36Ow==",
+                    "integrity": "sha512-f+8cldu7X/y7RAJurMEJmdoKXGB/X550w2Nr3tTbezL6RwEE/iMcm+tZnXeoZtKuOq6ft8+CqzEkrIgx1fPoQA==",
                     "requires": {
-                        "minimist": "^1.2.0"
+                        "minimist": "^1.2.5"
                     },
-                    "resolved": "https://registry.npmjs.org/json5/-/json5-1.0.1.tgz",
-                    "version": "1.0.1"
+                    "resolved": "https://registry.npmjs.org/json5/-/json5-2.2.0.tgz",
+                    "version": "2.2.0"
                 }
             },
-            "integrity": "sha512-fkpz8ejdnEMG3s37wGL07iSBDg99O9D5yflE9RGNH3hRdx9SOwYfnGYdZOUIZitN8E+E2vkq3MUMYMvPYl5ZZA==",
+            "integrity": "sha512-rP4F0h2RaWSvPEkD7BLDFQnvSf+nK+wr3ESUjNTyAGobqrijmW92zc+SO6d4p4B1wh7+B/Jg1mkQe5NYUEHtHQ==",
             "requires": {
                 "big.js": "^5.2.2",
-                "emojis-list": "^2.0.0",
-                "json5": "^1.0.1"
+                "emojis-list": "^3.0.0",
+                "json5": "^2.1.2"
             },
-            "resolved": "https://registry.npmjs.org/loader-utils/-/loader-utils-1.2.3.tgz",
-            "version": "1.2.3"
+            "resolved": "https://registry.npmjs.org/loader-utils/-/loader-utils-2.0.0.tgz",
+            "version": "2.0.0"
         },
         "locate-path": {
             "integrity": "sha1-K1aLJl7slExtnA3pw9u7ygNUzY4=",
             "requires": {
                 "p-locate": "^2.0.0",
                 "path-exists": "^3.0.0"
             },
             "resolved": "https://registry.npmjs.org/locate-path/-/locate-path-2.0.0.tgz",
             "version": "2.0.0"
         },
         "lodash": {
-            "integrity": "sha512-PlhdFcillOINfeV7Ni6oF1TAEayyZBoZ8bcshTHqOYJYlrqzRK5hagpagky5o4HfCzzd1TRkXPMFq6cKk9rGmA==",
-            "resolved": "https://registry.npmjs.org/lodash/-/lodash-4.17.20.tgz",
-            "version": "4.17.20"
+            "integrity": "sha512-v2kDEe57lecTulaDIuNTPy3Ry4gLGJ6Z1O3vE1krgXZNrsQ+LFTGHVxVjcXPs17LhbZVGedAJv8XZ1tvj5FvSg==",
+            "resolved": "https://registry.npmjs.org/lodash/-/lodash-4.17.21.tgz",
+            "version": "4.17.21"
+        },
+        "lodash.curry": {
+            "integrity": "sha1-JI42By7ekGUB11lmIAqG2riyMXA=",
+            "resolved": "https://registry.npmjs.org/lodash.curry/-/lodash.curry-4.1.1.tgz",
+            "version": "4.1.1"
+        },
+        "lodash.includes": {
+            "integrity": "sha1-YLuYqHy5I8aMoeUTJUgzFISfVT8=",
+            "resolved": "https://registry.npmjs.org/lodash.includes/-/lodash.includes-4.3.0.tgz",
+            "version": "4.3.0"
+        },
+        "lodash.isboolean": {
+            "integrity": "sha1-bC4XHbKiV82WgC/UOwGyDV9YcPY=",
+            "resolved": "https://registry.npmjs.org/lodash.isboolean/-/lodash.isboolean-3.0.3.tgz",
+            "version": "3.0.3"
+        },
+        "lodash.isinteger": {
+            "integrity": "sha1-YZwK89A/iwTDH1iChAt3sRzWg0M=",
+            "resolved": "https://registry.npmjs.org/lodash.isinteger/-/lodash.isinteger-4.0.4.tgz",
+            "version": "4.0.4"
+        },
+        "lodash.isnumber": {
+            "integrity": "sha1-POdoEMWSjQM1IwGsKHMX8RwLH/w=",
+            "resolved": "https://registry.npmjs.org/lodash.isnumber/-/lodash.isnumber-3.0.3.tgz",
+            "version": "3.0.3"
+        },
+        "lodash.isplainobject": {
+            "integrity": "sha1-fFJqUtibRcRcxpC4gWO+BJf1UMs=",
+            "resolved": "https://registry.npmjs.org/lodash.isplainobject/-/lodash.isplainobject-4.0.6.tgz",
+            "version": "4.0.6"
+        },
+        "lodash.isstring": {
+            "integrity": "sha1-1SfftUVuynzJu5XV2ur4i6VKVFE=",
+            "resolved": "https://registry.npmjs.org/lodash.isstring/-/lodash.isstring-4.0.1.tgz",
+            "version": "4.0.1"
+        },
+        "lodash.once": {
+            "integrity": "sha1-DdOXEhPHxW34gJd9UEyI+0cal6w=",
+            "resolved": "https://registry.npmjs.org/lodash.once/-/lodash.once-4.1.1.tgz",
+            "version": "4.1.1"
+        },
+        "log-symbols": {
+            "integrity": "sha512-VeIAFslyIerEJLXHziedo2basKbMKtTw3vfn5IzG0XTjhAVEJyNHnL2p7vc+wBDSdQuUpNw3M2u6xb9QsAY5Eg==",
+            "requires": {
+                "chalk": "^2.0.1"
+            },
+            "resolved": "https://registry.npmjs.org/log-symbols/-/log-symbols-2.2.0.tgz",
+            "version": "2.2.0"
         },
         "loose-envify": {
             "integrity": "sha512-lyuxPGr/Wfhrlem2CL/UcnUc1zcqKAImBDzukY7Y5F/yQiNdko6+fRLevlw1HgMySw7f611UIY408EtxRSoK3Q==",
             "requires": {
                 "js-tokens": "^3.0.0 || ^4.0.0"
             },
             "resolved": "https://registry.npmjs.org/loose-envify/-/loose-envify-1.4.0.tgz",
             "version": "1.4.0"
         },
         "lru-cache": {
+            "dependencies": {
+                "yallist": {
+                    "integrity": "sha512-a4UGQaWPH59mOXUYnAG2ewncQS4i4F43Tv3JoAM+s2VDAmS9NsK8GpDMLrCHPksFT7h3K6TOoUNn2pb7RoXx4g==",
+                    "resolved": "https://registry.npmjs.org/yallist/-/yallist-3.1.1.tgz",
+                    "version": "3.1.1"
+                }
+            },
             "integrity": "sha512-KpNARQA3Iwv+jTA0utUVVbrh+Jlrr1Fv0e56GGzAFOXN7dk/FviaDW8LHmK52DlcH4WP2n6gI8vN1aesBFgo9w==",
             "requires": {
                 "yallist": "^3.0.2"
             },
             "resolved": "https://registry.npmjs.org/lru-cache/-/lru-cache-5.1.1.tgz",
             "version": "5.1.1"
         },
@@ -3694,20 +5143,102 @@
             "integrity": "sha512-l5XlriUDJKQT12bH+rVhAHjwIuXWdAIecGwsYjv2LJo+dA1AeRTmeQS+3QBpO6lEthBMDi2IUMpLC1yyRvGlwQ==",
             "requires": {
                 "is-what": "^3.3.1"
             },
             "resolved": "https://registry.npmjs.org/merge-anything/-/merge-anything-2.4.4.tgz",
             "version": "2.4.4"
         },
+        "merge-stream": {
+            "integrity": "sha512-abv/qOcuPfk3URPfDzmZU1LKmuw8kT+0nIHvKrKgFrwifol/doWcdA4ZqsWQ8ENrFKkd67Mfpo/LovbIUsbt3w==",
+            "resolved": "https://registry.npmjs.org/merge-stream/-/merge-stream-2.0.0.tgz",
+            "version": "2.0.0"
+        },
         "microevent.ts": {
             "integrity": "sha512-jo1OfR4TaEwd5HOrt5+tAZ9mqT4jmpNAusXtyfNzqVm9uiSYFZlKM1wYL4oU7azZW/PxQW53wM0S6OR1JHNa2g==",
             "resolved": "https://registry.npmjs.org/microevent.ts/-/microevent.ts-0.1.1.tgz",
             "version": "0.1.1"
         },
         "micromatch": {
+            "dependencies": {
+                "braces": {
+                    "dependencies": {
+                        "extend-shallow": {
+                            "integrity": "sha1-Ua99YUrZqfYQ6huvu5idaxxWiQ8=",
+                            "requires": {
+                                "is-extendable": "^0.1.0"
+                            },
+                            "resolved": "https://registry.npmjs.org/extend-shallow/-/extend-shallow-2.0.1.tgz",
+                            "version": "2.0.1"
+                        }
+                    },
+                    "integrity": "sha512-aNdbnj9P8PjdXU4ybaWLK2IF3jc/EoDYbC7AazW6to3TRsfXxscC9UXOB5iDiEQrkyIbWp2SLQda4+QAa7nc3w==",
+                    "requires": {
+                        "arr-flatten": "^1.1.0",
+                        "array-unique": "^0.3.2",
+                        "extend-shallow": "^2.0.1",
+                        "fill-range": "^4.0.0",
+                        "isobject": "^3.0.1",
+                        "repeat-element": "^1.1.2",
+                        "snapdragon": "^0.8.1",
+                        "snapdragon-node": "^2.0.1",
+                        "split-string": "^3.0.2",
+                        "to-regex": "^3.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/braces/-/braces-2.3.2.tgz",
+                    "version": "2.3.2"
+                },
+                "fill-range": {
+                    "dependencies": {
+                        "extend-shallow": {
+                            "integrity": "sha1-Ua99YUrZqfYQ6huvu5idaxxWiQ8=",
+                            "requires": {
+                                "is-extendable": "^0.1.0"
+                            },
+                            "resolved": "https://registry.npmjs.org/extend-shallow/-/extend-shallow-2.0.1.tgz",
+                            "version": "2.0.1"
+                        }
+                    },
+                    "integrity": "sha1-1USBHUKPmOsGpj3EAtJAPDKMOPc=",
+                    "requires": {
+                        "extend-shallow": "^2.0.1",
+                        "is-number": "^3.0.0",
+                        "repeat-string": "^1.6.1",
+                        "to-regex-range": "^2.1.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/fill-range/-/fill-range-4.0.0.tgz",
+                    "version": "4.0.0"
+                },
+                "is-number": {
+                    "dependencies": {
+                        "kind-of": {
+                            "integrity": "sha1-MeohpzS6ubuw8yRm2JOupR5KPGQ=",
+                            "requires": {
+                                "is-buffer": "^1.1.5"
+                            },
+                            "resolved": "https://registry.npmjs.org/kind-of/-/kind-of-3.2.2.tgz",
+                            "version": "3.2.2"
+                        }
+                    },
+                    "integrity": "sha1-JP1iAaR4LPUFYcgQJ2r8fRLXEZU=",
+                    "requires": {
+                        "kind-of": "^3.0.2"
+                    },
+                    "resolved": "https://registry.npmjs.org/is-number/-/is-number-3.0.0.tgz",
+                    "version": "3.0.0"
+                },
+                "to-regex-range": {
+                    "integrity": "sha1-fIDBe53+vlmeJzZ+DU3VWQFB2zg=",
+                    "requires": {
+                        "is-number": "^3.0.0",
+                        "repeat-string": "^1.6.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/to-regex-range/-/to-regex-range-2.1.1.tgz",
+                    "version": "2.1.1"
+                }
+            },
             "integrity": "sha512-MWikgl9n9M3w+bpsY3He8L+w9eF9338xRl8IAO5viDizwSzziFEyUzo2xrrloB64ADbTf8uA8vRqqttDTOmccg==",
             "requires": {
                 "arr-diff": "^4.0.0",
                 "array-unique": "^0.3.2",
                 "braces": "^2.3.1",
                 "define-property": "^2.0.2",
                 "extend-shallow": "^3.0.2",
@@ -3720,39 +5251,92 @@
                 "snapdragon": "^0.8.1",
                 "to-regex": "^3.0.2"
             },
             "resolved": "https://registry.npmjs.org/micromatch/-/micromatch-3.1.10.tgz",
             "version": "3.1.10"
         },
         "miller-rabin": {
+            "dependencies": {
+                "bn.js": {
+                    "integrity": "sha512-c98Bf3tPniI+scsdk237ku1Dc3ujXQTSgyiPUDEOe7tRkhrqridvh8klBv0HCEso1OLOYcHuCv/cS6DNxKH+ZA==",
+                    "resolved": "https://registry.npmjs.org/bn.js/-/bn.js-4.12.0.tgz",
+                    "version": "4.12.0"
+                }
+            },
             "integrity": "sha512-115fLhvZVqWwHPbClyntxEVfVDfl9DLLTuJvq3g2O/Oxi8AiNouAHvDSzHS0viUJc+V5vm3eq91Xwqn9dp4jRA==",
             "requires": {
                 "bn.js": "^4.0.0",
                 "brorand": "^1.0.1"
             },
             "resolved": "https://registry.npmjs.org/miller-rabin/-/miller-rabin-4.0.1.tgz",
             "version": "4.0.1"
         },
         "mime": {
             "integrity": "sha512-x0Vn8spI+wuJ1O6S7gnbaQg8Pxh4NNHb7KSINmEWKiPE4RKOplvijn+NkmYmmRgP68mc70j2EbeTFRsrswaQeg==",
             "resolved": "https://registry.npmjs.org/mime/-/mime-1.6.0.tgz",
             "version": "1.6.0"
         },
         "mime-db": {
-            "integrity": "sha512-jYdeOMPy9vnxEqFRRo6ZvTZ8d9oPb+k18PKoYNYUe2stVEBPPwsln/qWzdbmaIvnhZ9v2P+CuecK+fpUfsV2mA==",
-            "resolved": "https://registry.npmjs.org/mime-db/-/mime-db-1.40.0.tgz",
-            "version": "1.40.0"
+            "integrity": "sha512-FM3QwxV+TnZYQ2aRqhlKBMHxk10lTbMt3bBkMAp54ddrNeVSfcQYOOKuGuy3Ddrm38I04If834fOUSq1yzslJQ==",
+            "resolved": "https://registry.npmjs.org/mime-db/-/mime-db-1.48.0.tgz",
+            "version": "1.48.0"
         },
         "mime-types": {
-            "integrity": "sha512-WaFHS3MCl5fapm3oLxU4eYDw77IQM2ACcxQ9RIxfaC3ooc6PFuBMGZZsYpvoXS5D5QTWPieo1jjLdAm3TBP3cQ==",
+            "integrity": "sha512-XGZnNzm3QvgKxa8dpzyhFTHmpP3l5YNusmne07VUOXxou9CqUqYa/HBy124RqtVh/O2pECas/MOcsDgpilPOPg==",
+            "requires": {
+                "mime-db": "1.48.0"
+            },
+            "resolved": "https://registry.npmjs.org/mime-types/-/mime-types-2.1.31.tgz",
+            "version": "2.1.31"
+        },
+        "mimic-fn": {
+            "integrity": "sha512-jf84uxzwiuiIVKiOLpfYk7N46TSy8ubTonmneY9vrpHNAnp0QBt2BxWV9dO3/j+BoVAb+a5G6YDPW3M5HOdMWQ==",
+            "resolved": "https://registry.npmjs.org/mimic-fn/-/mimic-fn-1.2.0.tgz",
+            "version": "1.2.0"
+        },
+        "mini-css-extract-plugin": {
+            "dependencies": {
+                "json5": {
+                    "integrity": "sha512-aKS4WQjPenRxiQsC93MNfjx+nbF4PAdYzmd/1JIj8HYzqfbu86beTuNgXDzPknWk0n0uARlyewZo4s++ES36Ow==",
+                    "requires": {
+                        "minimist": "^1.2.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/json5/-/json5-1.0.1.tgz",
+                    "version": "1.0.1"
+                },
+                "loader-utils": {
+                    "integrity": "sha512-qH0WSMBtn/oHuwjy/NucEgbx5dbxxnxup9s4PVXJUDHZBQY+s0NWA9rJf53RBnQZxfch7euUui7hpoAPvALZdA==",
+                    "requires": {
+                        "big.js": "^5.2.2",
+                        "emojis-list": "^3.0.0",
+                        "json5": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/loader-utils/-/loader-utils-1.4.0.tgz",
+                    "version": "1.4.0"
+                },
+                "schema-utils": {
+                    "integrity": "sha512-i27Mic4KovM/lnGsy8whRCHhc7VicJajAjTrYg11K9zfZXnYIt4k5F+kZkwjnrhKzLic/HLU4j11mjsz2G/75g==",
+                    "requires": {
+                        "ajv": "^6.1.0",
+                        "ajv-errors": "^1.0.0",
+                        "ajv-keywords": "^3.1.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/schema-utils/-/schema-utils-1.0.0.tgz",
+                    "version": "1.0.0"
+                }
+            },
+            "integrity": "sha512-MNpRGbNA52q6U92i0qbVpQNsgk7LExy41MdAlG84FeytfDOtRIf/mCHdEgG8rpTKOaNKiqUnZdlptF469hxqOw==",
             "requires": {
-                "mime-db": "1.40.0"
+                "loader-utils": "^1.1.0",
+                "normalize-url": "1.9.1",
+                "schema-utils": "^1.0.0",
+                "webpack-sources": "^1.1.0"
             },
-            "resolved": "https://registry.npmjs.org/mime-types/-/mime-types-2.1.24.tgz",
-            "version": "2.1.24"
+            "resolved": "https://registry.npmjs.org/mini-css-extract-plugin/-/mini-css-extract-plugin-0.8.0.tgz",
+            "version": "0.8.0"
         },
         "minimalistic-assert": {
             "integrity": "sha512-UtJcAD4yEaGtjPezWuO9wC4nwUnVH/8/Im3yEHQP4b67cXlD/Qr9hdITCU1xDbSEXg2XKNaP8jsReV7vQd00/A==",
             "resolved": "https://registry.npmjs.org/minimalistic-assert/-/minimalistic-assert-1.0.1.tgz",
             "version": "1.0.1"
         },
         "minimalistic-crypto-utils": {
@@ -3765,17 +5349,17 @@
             "requires": {
                 "brace-expansion": "^1.1.7"
             },
             "resolved": "https://registry.npmjs.org/minimatch/-/minimatch-3.0.4.tgz",
             "version": "3.0.4"
         },
         "minimist": {
-            "integrity": "sha1-o1AIsg9BOD7sH7kU9M1d95omQoQ=",
-            "resolved": "https://registry.npmjs.org/minimist/-/minimist-1.2.0.tgz",
-            "version": "1.2.0"
+            "integrity": "sha512-FM9nNUYrRBAELZQT3xeZQ7fmMOBg6nWNmJKTcgsJeaLstP/UODVpGsr5OhXhhXg6f+qtJ8uiZ+PUxkDWcgIXLw==",
+            "resolved": "https://registry.npmjs.org/minimist/-/minimist-1.2.5.tgz",
+            "version": "1.2.5"
         },
         "mississippi": {
             "integrity": "sha512-x471SsVjUtBRtcvd4BzKE9kFC+/2TeWgKCgw0bZcw1b9l2X3QX5vCWgF+KaZaYm87Ss//rHnWryupDrgLvmSkA==",
             "requires": {
                 "concat-stream": "^1.5.0",
                 "duplexify": "^3.4.2",
                 "end-of-stream": "^1.1.0",
@@ -3806,27 +5390,20 @@
                 "for-in": "^1.0.2",
                 "is-extendable": "^1.0.1"
             },
             "resolved": "https://registry.npmjs.org/mixin-deep/-/mixin-deep-1.3.2.tgz",
             "version": "1.3.2"
         },
         "mkdirp": {
-            "dependencies": {
-                "minimist": {
-                    "integrity": "sha1-hX/Kv8M5fSYluCKCYuhqp6ARsF0=",
-                    "resolved": "https://registry.npmjs.org/minimist/-/minimist-0.0.8.tgz",
-                    "version": "0.0.8"
-                }
-            },
-            "integrity": "sha1-MAV0OOrGz3+MR2fzhkjWaX11yQM=",
+            "integrity": "sha512-NKmAlESf6jMGym1++R0Ra7wvhV+wFW63FaSOFPwRahvea0gMUcGUhVeAg/0BC0wiv9ih5NYPB1Wn1UEI1/L+xQ==",
             "requires": {
-                "minimist": "0.0.8"
+                "minimist": "^1.2.5"
             },
-            "resolved": "https://registry.npmjs.org/mkdirp/-/mkdirp-0.5.1.tgz",
-            "version": "0.5.1"
+            "resolved": "https://registry.npmjs.org/mkdirp/-/mkdirp-0.5.5.tgz",
+            "version": "0.5.5"
         },
         "move-concurrently": {
             "integrity": "sha1-viwAX9oy4LKa8fBdfEszIUxwH5I=",
             "requires": {
                 "aproba": "^1.1.1",
                 "copy-concurrently": "^1.0.0",
                 "fs-write-stream-atomic": "^1.0.8",
@@ -3839,18 +5416,18 @@
         },
         "ms": {
             "integrity": "sha512-sGkPx+VjMtmA6MX27oA4FBFELFCZZ4S4XqeGOXCv68tT+jb3vk/RyaKWP0PTKyWtmLSM0b+adUTEvbs1PEaH2w==",
             "resolved": "https://registry.npmjs.org/ms/-/ms-2.1.2.tgz",
             "version": "2.1.2"
         },
         "nan": {
-            "integrity": "sha512-INOFj37C7k3AfaNTtX8RhsTw7qRy7eLET14cROi9+5HAVbbHuIWUHEauBv5qT4Av2tWasiTY1Jw6puUNqRJXQg==",
+            "integrity": "sha512-M2ufzIiINKCuDfBSAUr1vWQ+vuVcA9kqx8JJUsbQi6yf1uGRyb7HfpdfUr5qLXf3B/t8dPvcjhKMmlfnP47EzQ==",
             "optional": true,
-            "resolved": "https://registry.npmjs.org/nan/-/nan-2.14.0.tgz",
-            "version": "2.14.0"
+            "resolved": "https://registry.npmjs.org/nan/-/nan-2.14.2.tgz",
+            "version": "2.14.2"
         },
         "nanomatch": {
             "integrity": "sha512-fpoe2T0RbHwBTBUOftAfBPaDEi06ufaUai0mE6Yn1kacc3SnTErfb/h+X94VXzI64rKFHYImXSvdwGGCmwOqCA==",
             "requires": {
                 "arr-diff": "^4.0.0",
                 "array-unique": "^0.3.2",
                 "define-property": "^2.0.2",
@@ -3862,103 +5439,177 @@
                 "regex-not": "^1.0.0",
                 "snapdragon": "^0.8.1",
                 "to-regex": "^3.0.1"
             },
             "resolved": "https://registry.npmjs.org/nanomatch/-/nanomatch-1.2.13.tgz",
             "version": "1.2.13"
         },
+        "native-url": {
+            "integrity": "sha512-k4bDC87WtgrdD362gZz6zoiXQrl40kYlBmpfmSjwRO1VU0V5ccwJTlxuE72F6m3V0vc1xOf6n3UCP9QyerRqmA==",
+            "requires": {
+                "querystring": "^0.2.0"
+            },
+            "resolved": "https://registry.npmjs.org/native-url/-/native-url-0.2.6.tgz",
+            "version": "0.2.6"
+        },
         "negotiator": {
             "integrity": "sha512-hZXc7K2e+PgeI1eDBe/10Ard4ekbfrrqG8Ep+8Jmf4JID2bNg7NvCPOZN+kfF574pFQI7mum2AUqDidoKqcTOw==",
             "resolved": "https://registry.npmjs.org/negotiator/-/negotiator-0.6.2.tgz",
             "version": "0.6.2"
         },
         "neo-async": {
-            "integrity": "sha512-iyam8fBuCUpWeKPGpaNMetEocMt364qkCsfL9JuhjXX6dRnguRVOfk2GZaDpPjcOKiiXCPINZC1GczQ7iTq3Zw==",
-            "resolved": "https://registry.npmjs.org/neo-async/-/neo-async-2.6.1.tgz",
-            "version": "2.6.1"
+            "integrity": "sha512-Yd3UES5mWCSqR+qNT93S3UoYUkqAZ9lLg8a7g9rimsWmYGK8cVToA4/sF3RrshdyV3sAGMXVUmpMYOw+dLpOuw==",
+            "resolved": "https://registry.npmjs.org/neo-async/-/neo-async-2.6.2.tgz",
+            "version": "2.6.2"
         },
         "next": {
-            "integrity": "sha512-lH4Dz9ravRS0y+0iY97Iij7btVSz53tidd0Nx4YiQSU8c2+ZArZlFPBkfOhg6Ai5nSWemUGmzICwIoJ1OMclLA==",
+            "dependencies": {
+                "@babel/types": {
+                    "integrity": "sha512-cz5Ji23KCi4T+YIE/BolWosrJuSmoZeN1EFnRtBwF+KKLi8GG/Z2c2hOJJeCXPk4mwk4QFvTmwIodJowXgttRA==",
+                    "requires": {
+                        "esutils": "^2.0.2",
+                        "lodash": "^4.17.13",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.7.4.tgz",
+                    "version": "7.7.4"
+                },
+                "escape-string-regexp": {
+                    "integrity": "sha512-UpzcLCXolUWcNu5HtVMHYdXJjArjsF9C0aNnquZYY4uW/Vu0miy5YoWvbV345HauVvcAUnpRuhMMcqTcGOY2+w==",
+                    "resolved": "https://registry.npmjs.org/escape-string-regexp/-/escape-string-regexp-2.0.0.tgz",
+                    "version": "2.0.0"
+                }
+            },
+            "integrity": "sha512-KVNnnFyvtO1DwSEyMgt3wtxpkprnGCldEOyMXbt9Zxf8RcCw3YnRImbg8mVgrcXJRzkQpve4bdJKYY5MVwT/RA==",
             "requires": {
-                "@babel/core": "7.4.5",
-                "@babel/plugin-proposal-class-properties": "7.4.4",
-                "@babel/plugin-proposal-object-rest-spread": "7.4.4",
+                "@ampproject/toolbox-optimizer": "2.0.1",
+                "@babel/core": "7.7.2",
+                "@babel/plugin-proposal-class-properties": "7.7.0",
+                "@babel/plugin-proposal-nullish-coalescing-operator": "7.7.4",
+                "@babel/plugin-proposal-numeric-separator": "7.8.3",
+                "@babel/plugin-proposal-object-rest-spread": "7.6.2",
+                "@babel/plugin-proposal-optional-chaining": "7.7.4",
+                "@babel/plugin-syntax-bigint": "7.8.3",
                 "@babel/plugin-syntax-dynamic-import": "7.2.0",
-                "@babel/plugin-transform-modules-commonjs": "7.4.4",
-                "@babel/plugin-transform-runtime": "7.4.4",
-                "@babel/preset-env": "7.4.5",
-                "@babel/preset-react": "7.0.0",
-                "@babel/preset-typescript": "7.3.3",
-                "@babel/runtime": "7.4.5",
-                "@babel/runtime-corejs2": "7.4.5",
-                "amphtml-validator": "1.0.23",
+                "@babel/plugin-transform-modules-commonjs": "7.7.0",
+                "@babel/plugin-transform-runtime": "7.6.2",
+                "@babel/preset-env": "7.7.1",
+                "@babel/preset-modules": "0.1.1",
+                "@babel/preset-react": "7.7.0",
+                "@babel/preset-typescript": "7.7.2",
+                "@babel/runtime": "7.7.2",
+                "@babel/types": "7.7.4",
+                "@next/polyfill-nomodule": "9.3.2",
+                "amphtml-validator": "1.0.30",
+                "async-retry": "1.2.3",
                 "async-sema": "3.0.0",
                 "autodll-webpack-plugin": "0.4.2",
                 "babel-core": "7.0.0-bridge.0",
                 "babel-loader": "8.0.6",
-                "babel-plugin-transform-define": "1.3.1",
+                "babel-plugin-syntax-jsx": "6.18.0",
+                "babel-plugin-transform-define": "2.0.0",
                 "babel-plugin-transform-react-remove-prop-types": "0.4.24",
+                "browserslist": "4.8.3",
+                "cache-loader": "4.1.0",
                 "chalk": "2.4.2",
+                "ci-info": "2.0.0",
+                "compression": "1.7.4",
+                "conf": "5.0.0",
+                "content-type": "1.0.4",
+                "cookie": "0.4.0",
+                "css-loader": "3.3.0",
+                "cssnano-simple": "1.0.0",
+                "devalue": "2.0.1",
+                "escape-string-regexp": "2.0.0",
+                "etag": "1.8.1",
+                "file-loader": "4.2.0",
+                "finally-polyfill": "0.1.0",
                 "find-up": "4.0.0",
-                "fork-ts-checker-webpack-plugin": "1.3.4",
+                "fork-ts-checker-webpack-plugin": "3.1.1",
                 "fresh": "0.5.2",
+                "gzip-size": "5.1.1",
+                "http-proxy": "1.18.0",
+                "ignore-loader": "0.1.2",
+                "is-docker": "2.0.0",
+                "is-wsl": "2.1.1",
+                "jest-worker": "24.9.0",
+                "json5": "2.1.1",
+                "jsonwebtoken": "8.5.1",
                 "launch-editor": "2.2.1",
-                "loader-utils": "1.2.3",
-                "mkdirp": "0.5.1",
-                "next-server": "9.0.4",
+                "loader-utils": "2.0.0",
+                "lodash.curry": "4.1.1",
+                "lru-cache": "5.1.1",
+                "mini-css-extract-plugin": "0.8.0",
+                "native-url": "0.2.6",
+                "node-fetch": "2.6.0",
+                "ora": "3.4.0",
+                "path-to-regexp": "6.1.0",
+                "pnp-webpack-plugin": "1.5.0",
+                "postcss-flexbugs-fixes": "4.2.0",
+                "postcss-loader": "3.0.0",
+                "postcss-preset-env": "6.7.0",
                 "prop-types": "15.7.2",
                 "prop-types-exact": "1.2.0",
+                "raw-body": "2.4.0",
                 "react-error-overlay": "5.1.6",
                 "react-is": "16.8.6",
-                "serialize-javascript": "1.7.0",
+                "recast": "0.18.5",
+                "resolve-url-loader": "3.1.1",
+                "sass-loader": "8.0.2",
+                "send": "0.17.1",
                 "source-map": "0.6.1",
                 "string-hash": "1.1.3",
                 "strip-ansi": "5.2.0",
-                "styled-jsx": "3.2.1",
-                "terser": "4.0.0",
-                "tty-aware-progress": "1.0.4",
+                "style-loader": "1.0.0",
+                "styled-jsx": "3.2.5",
+                "terser": "4.4.2",
+                "thread-loader": "2.1.3",
                 "unfetch": "4.1.0",
                 "url": "0.11.0",
-                "watchpack": "2.0.0-beta.5",
-                "webpack": "4.39.0",
+                "use-subscription": "1.1.1",
+                "watchpack": "2.0.0-beta.13",
+                "webpack": "4.42.0",
                 "webpack-dev-middleware": "3.7.0",
                 "webpack-hot-middleware": "2.25.0",
-                "webpack-sources": "1.3.0",
-                "worker-farm": "1.7.0"
+                "webpack-sources": "1.4.3"
             },
-            "resolved": "https://registry.npmjs.org/next/-/next-9.0.4.tgz",
-            "version": "9.0.4"
+            "resolved": "https://registry.npmjs.org/next/-/next-9.3.2.tgz",
+            "version": "9.3.2"
         },
-        "next-server": {
-            "integrity": "sha512-dnHOBTQSuGukkOPtJDRtcQX75LDNOcXH71hWOICPUtIh91QIt/MSZguIR/Uv8QFBNHoYTUe47rXHjx/+uy872g==",
-            "requires": {
-                "@ampproject/toolbox-optimizer": "1.0.1",
-                "compression": "1.7.4",
-                "content-type": "1.0.4",
-                "cookie": "0.4.0",
-                "etag": "1.8.1",
-                "find-up": "4.0.0",
-                "fresh": "0.5.2",
-                "path-to-regexp": "2.1.0",
-                "prop-types": "15.7.2",
-                "raw-body": "2.4.0",
-                "react-is": "16.8.6",
-                "send": "0.17.1",
-                "styled-jsx": "3.2.1",
-                "url": "0.11.0"
-            },
-            "resolved": "https://registry.npmjs.org/next-server/-/next-server-9.0.4.tgz",
-            "version": "9.0.4"
+        "next-tick": {
+            "integrity": "sha1-yobR/ogoFpsBICCOPchCS524NCw=",
+            "resolved": "https://registry.npmjs.org/next-tick/-/next-tick-1.0.0.tgz",
+            "version": "1.0.0"
         },
         "node-fetch": {
             "integrity": "sha512-8dG4H5ujfvFiqDmVu9fQ5bOHUC15JMjMY/Zumv26oOvvVJjM67KF8koCWIabKQ1GJIa9r2mMZscBq/TbdOcmNA==",
             "resolved": "https://registry.npmjs.org/node-fetch/-/node-fetch-2.6.0.tgz",
             "version": "2.6.0"
         },
         "node-libs-browser": {
+            "dependencies": {
+                "inherits": {
+                    "integrity": "sha1-Yzwsg+PaQqUC9SRmAiSA9CCCYd4=",
+                    "resolved": "https://registry.npmjs.org/inherits/-/inherits-2.0.3.tgz",
+                    "version": "2.0.3"
+                },
+                "punycode": {
+                    "integrity": "sha1-wNWmOycYgArY4esPpSachN1BhF4=",
+                    "resolved": "https://registry.npmjs.org/punycode/-/punycode-1.4.1.tgz",
+                    "version": "1.4.1"
+                },
+                "util": {
+                    "integrity": "sha512-HShAsny+zS2TZfaXxD9tYj4HQGlBezXZMZuM/S5PKLLoZkShZiGk9o5CzukI1LVHZvjdvZ2Sj1aW/Ndn2NB/HQ==",
+                    "requires": {
+                        "inherits": "2.0.3"
+                    },
+                    "resolved": "https://registry.npmjs.org/util/-/util-0.11.1.tgz",
+                    "version": "0.11.1"
+                }
+            },
             "integrity": "sha512-h/zcD8H9kaDZ9ALUWwlBUDo6TKF8a7qBSCSEGfjTVIYeqsioSKaAX+BN7NgiMGp6iSIXZ3PxgCu8KS3b71YK5Q==",
             "requires": {
                 "assert": "^1.1.1",
                 "browserify-zlib": "^0.2.0",
                 "buffer": "^4.3.0",
                 "console-browserify": "^1.1.0",
                 "constants-browserify": "^1.0.0",
@@ -3981,20 +5632,22 @@
                 "util": "^0.11.0",
                 "vm-browserify": "^1.0.1"
             },
             "resolved": "https://registry.npmjs.org/node-libs-browser/-/node-libs-browser-2.2.1.tgz",
             "version": "2.2.1"
         },
         "node-releases": {
-            "integrity": "sha512-AQw4emh6iSXnCpDiFe0phYcThiccmkNWMZnFZ+lDJjAP8J0m2fVd59duvUUyuTirQOhIAajTFkzG6FHCLBO59g==",
-            "requires": {
-                "semver": "^5.3.0"
-            },
-            "resolved": "https://registry.npmjs.org/node-releases/-/node-releases-1.1.28.tgz",
-            "version": "1.1.28"
+            "integrity": "sha512-uW7fodD6pyW2FZNZnp/Z3hvWKeEW1Y8R1+1CnErE8cXFXzl5blBOoVB41CvMer6P6Q0S5FXDwcHgFd1Wj0U9zg==",
+            "resolved": "https://registry.npmjs.org/node-releases/-/node-releases-1.1.73.tgz",
+            "version": "1.1.73"
+        },
+        "normalize-html-whitespace": {
+            "integrity": "sha512-9ui7CGtOOlehQu0t/OhhlmDyc71mKVlv+4vF+me4iZLPrNtRL2xoquEdfZxasC/bdQi/Hr3iTrpyRKIG+ocabA==",
+            "resolved": "https://registry.npmjs.org/normalize-html-whitespace/-/normalize-html-whitespace-1.0.0.tgz",
+            "version": "1.0.0"
         },
         "normalize-package-data": {
             "integrity": "sha512-/5CMN3T0R4XTj4DcGaexo+roZSdSFW/0AOOTROrjxzCG1wrWXEsGbRKevjlIL+ZDE4sZlJr5ED4YW0yqmkK+eA==",
             "requires": {
                 "hosted-git-info": "^2.1.4",
                 "resolve": "^1.10.0",
                 "semver": "2 || 3 || 4 || 5",
@@ -4004,14 +5657,35 @@
             "version": "2.5.0"
         },
         "normalize-path": {
             "integrity": "sha512-6eZs5Ls3WtCisHWp9S2GUy8dqkpGi4BVSz3GaqiE6ezub0512ESztXUwUB6C6IKbQkY2Pnb/mD4WYojCRwcwLA==",
             "resolved": "https://registry.npmjs.org/normalize-path/-/normalize-path-3.0.0.tgz",
             "version": "3.0.0"
         },
+        "normalize-range": {
+            "integrity": "sha1-LRDAa9/TEuqXd2laTShDlFa3WUI=",
+            "resolved": "https://registry.npmjs.org/normalize-range/-/normalize-range-0.1.2.tgz",
+            "version": "0.1.2"
+        },
+        "normalize-url": {
+            "integrity": "sha1-LMDWazHqIwNkWENuNiDYWVTGbDw=",
+            "requires": {
+                "object-assign": "^4.0.1",
+                "prepend-http": "^1.0.0",
+                "query-string": "^4.1.0",
+                "sort-keys": "^1.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/normalize-url/-/normalize-url-1.9.1.tgz",
+            "version": "1.9.1"
+        },
+        "num2fraction": {
+            "integrity": "sha1-b2gragJ6Tp3fpFZM0lidHU5mnt4=",
+            "resolved": "https://registry.npmjs.org/num2fraction/-/num2fraction-1.2.2.tgz",
+            "version": "1.2.2"
+        },
         "object-assign": {
             "integrity": "sha1-IQmtx5ZYh8/AXLvUQsrIv7s2CGM=",
             "resolved": "https://registry.npmjs.org/object-assign/-/object-assign-4.1.1.tgz",
             "version": "4.1.1"
         },
         "object-copy": {
             "dependencies": {
@@ -4042,32 +5716,37 @@
             "version": "0.1.0"
         },
         "object-keys": {
             "integrity": "sha512-NuAESUOUMrlIXOfHKzD6bpPu3tYt3xvjNdRIQ+FeT0lNb4K8WR70CaDxhuNguS2XG+GjkyMwOzsN5ZktImfhLA==",
             "resolved": "https://registry.npmjs.org/object-keys/-/object-keys-1.1.1.tgz",
             "version": "1.1.1"
         },
+        "object-path": {
+            "integrity": "sha1-NwrnUvvzfePqcKhhwju6iRVpGUk=",
+            "resolved": "https://registry.npmjs.org/object-path/-/object-path-0.11.4.tgz",
+            "version": "0.11.4"
+        },
         "object-visit": {
             "integrity": "sha1-95xEk68MU3e1n+OdOV5BBC3QRbs=",
             "requires": {
                 "isobject": "^3.0.0"
             },
             "resolved": "https://registry.npmjs.org/object-visit/-/object-visit-1.0.1.tgz",
             "version": "1.0.1"
         },
         "object.assign": {
-            "integrity": "sha512-exHJeq6kBKj58mqGyTQ9DFvrZC/eR6OwxzoM9YRoGBqrXYonaFyGiFMuc9VZrXf7DarreEwMpurG3dd+CNyW5w==",
+            "integrity": "sha512-ixT2L5THXsApyiUPYKmW+2EHpXXe5Ii3M+f4e+aJFAHao5amFRW6J0OO6c/LU8Be47utCx2GL89hxGB6XSmKuQ==",
             "requires": {
-                "define-properties": "^1.1.2",
-                "function-bind": "^1.1.1",
-                "has-symbols": "^1.0.0",
-                "object-keys": "^1.0.11"
+                "call-bind": "^1.0.0",
+                "define-properties": "^1.1.3",
+                "has-symbols": "^1.0.1",
+                "object-keys": "^1.1.1"
             },
-            "resolved": "https://registry.npmjs.org/object.assign/-/object.assign-4.1.0.tgz",
-            "version": "4.1.0"
+            "resolved": "https://registry.npmjs.org/object.assign/-/object.assign-4.1.2.tgz",
+            "version": "4.1.2"
         },
         "object.pick": {
             "integrity": "sha1-h6EKxMFpS9Lhy/U1kaZhQftd10c=",
             "requires": {
                 "isobject": "^3.0.1"
             },
             "resolved": "https://registry.npmjs.org/object.pick/-/object.pick-1.3.0.tgz",
@@ -4090,14 +5769,35 @@
             "integrity": "sha1-WDsap3WWHUsROsF9nFC6753Xa9E=",
             "requires": {
                 "wrappy": "1"
             },
             "resolved": "https://registry.npmjs.org/once/-/once-1.4.0.tgz",
             "version": "1.4.0"
         },
+        "onetime": {
+            "integrity": "sha1-BnQoIw/WdEOyeUsiu6UotoZ5YtQ=",
+            "requires": {
+                "mimic-fn": "^1.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/onetime/-/onetime-2.0.1.tgz",
+            "version": "2.0.1"
+        },
+        "ora": {
+            "integrity": "sha512-eNwHudNbO1folBP3JsZ19v9azXWtQZjICdr3Q0TDPIaeBQ3mXLrh54wM+er0+hSp+dWKf+Z8KM58CYzEyIYxYg==",
+            "requires": {
+                "chalk": "^2.4.2",
+                "cli-cursor": "^2.1.0",
+                "cli-spinners": "^2.0.0",
+                "log-symbols": "^2.2.0",
+                "strip-ansi": "^5.2.0",
+                "wcwidth": "^1.0.1"
+            },
+            "resolved": "https://registry.npmjs.org/ora/-/ora-3.4.0.tgz",
+            "version": "3.4.0"
+        },
         "os-browserify": {
             "integrity": "sha1-hUNzx/XCMVkU/Jv8a9gjj92h7Cc=",
             "resolved": "https://registry.npmjs.org/os-browserify/-/os-browserify-0.3.0.tgz",
             "version": "0.3.0"
         },
         "p-limit": {
             "integrity": "sha512-vvcXsLAJ9Dr5rQOPk7toZQZJApBl2K4J6dANSsEuh6QI41JYcsS/qhTGa9ErIUUgK3WNQoJYvylxvjqmiqEA9Q==",
@@ -4122,74 +5822,61 @@
         },
         "p-try": {
             "integrity": "sha1-y8ec26+P1CKOE/Yh8rGiN8GyB7M=",
             "resolved": "https://registry.npmjs.org/p-try/-/p-try-1.0.0.tgz",
             "version": "1.0.0"
         },
         "pako": {
-            "integrity": "sha512-0DTvPVU3ed8+HNXOu5Bs+o//Mbdj9VNQMUOe9oKCwh8l0GNwpTDMKCWbRjgtD291AWnkAgkqA/LOnQS8AmS1tw==",
-            "resolved": "https://registry.npmjs.org/pako/-/pako-1.0.10.tgz",
-            "version": "1.0.10"
+            "integrity": "sha512-4hLB8Py4zZce5s4yd9XzopqwVv/yGNhV1Bl8NTmCq1763HeK2+EwVTv+leGeL13Dnh2wfbqowVPXCIO0z4taYw==",
+            "resolved": "https://registry.npmjs.org/pako/-/pako-1.0.11.tgz",
+            "version": "1.0.11"
         },
         "parallel-transform": {
-            "integrity": "sha1-1BDwZbBdojCB/NEPKIVMKb2jOwY=",
+            "integrity": "sha512-P2vSmIu38uIlvdcU7fDkyrxj33gTUy/ABO5ZUbGowxNCopBq/OoD42bP4UmMrJoPyk4Uqf0mu3mtWBhHCZD8yg==",
             "requires": {
-                "cyclist": "~0.2.2",
+                "cyclist": "^1.0.1",
                 "inherits": "^2.0.3",
                 "readable-stream": "^2.1.5"
             },
-            "resolved": "https://registry.npmjs.org/parallel-transform/-/parallel-transform-1.1.0.tgz",
-            "version": "1.1.0"
+            "resolved": "https://registry.npmjs.org/parallel-transform/-/parallel-transform-1.2.0.tgz",
+            "version": "1.2.0"
         },
         "parse-asn1": {
-            "integrity": "sha512-Qs5duJcuvNExRfFZ99HDD3z4mAi3r9Wl/FOjEOijlxwCZs7E7mW2vjTpgQ4J8LpTF8x5v+1Vn5UQFejmWT11aw==",
+            "integrity": "sha512-RnZRo1EPU6JBnra2vGHj0yhp6ebyjBZpmUCLHWiFhxlzvBCCpAuZ7elsBp1PVAbQN0/04VD/19rfzlBSwLstMw==",
             "requires": {
-                "asn1.js": "^4.0.0",
+                "asn1.js": "^5.2.0",
                 "browserify-aes": "^1.0.0",
-                "create-hash": "^1.1.0",
                 "evp_bytestokey": "^1.0.0",
                 "pbkdf2": "^3.0.3",
                 "safe-buffer": "^5.1.1"
             },
-            "resolved": "https://registry.npmjs.org/parse-asn1/-/parse-asn1-5.1.4.tgz",
-            "version": "5.1.4"
+            "resolved": "https://registry.npmjs.org/parse-asn1/-/parse-asn1-5.1.6.tgz",
+            "version": "5.1.6"
         },
         "parse-json": {
             "integrity": "sha1-9ID0BDTvgHQfhGkJn43qGPVaTck=",
             "requires": {
                 "error-ex": "^1.2.0"
             },
             "resolved": "https://registry.npmjs.org/parse-json/-/parse-json-2.2.0.tgz",
             "version": "2.2.0"
         },
-        "parse5": {
-            "integrity": "sha512-fxNG2sQjHvlVAYmzBZS9YlDp6PTSSDwa98vkD4QgVDDCAo84z5X1t5XyJQ62ImdLXx5NdIIfihey6xpum9/gRQ==",
-            "resolved": "https://registry.npmjs.org/parse5/-/parse5-5.1.0.tgz",
-            "version": "5.1.0"
-        },
-        "parse5-htmlparser2-tree-adapter": {
-            "integrity": "sha512-OrI4DNmghGcwDB3XN8FKKN7g5vBmau91uqj+VYuwuj/r6GhFBMBNymsM+Z9z+Z1p4HHgI0UuQirQRgh3W5d88g==",
-            "requires": {
-                "parse5": "^5.1.0"
-            },
-            "resolved": "https://registry.npmjs.org/parse5-htmlparser2-tree-adapter/-/parse5-htmlparser2-tree-adapter-5.1.0.tgz",
-            "version": "5.1.0"
-        },
         "pascalcase": {
             "integrity": "sha1-s2PlXoAGym/iF4TS2yK9FdeRfxQ=",
             "resolved": "https://registry.npmjs.org/pascalcase/-/pascalcase-0.1.1.tgz",
             "version": "0.1.1"
         },
         "path-browserify": {
             "integrity": "sha512-BapA40NHICOS+USX9SN4tyhq+A2RrN/Ws5F0Z5aMHDp98Fl86lX8Oti8B7uN93L4Ifv4fHOEA+pQw87gmMO/lQ==",
             "resolved": "https://registry.npmjs.org/path-browserify/-/path-browserify-0.0.1.tgz",
             "version": "0.0.1"
         },
         "path-dirname": {
             "integrity": "sha1-zDPSTVJeCZpTiMAzbG4yuRYGCeA=",
+            "optional": true,
             "resolved": "https://registry.npmjs.org/path-dirname/-/path-dirname-1.0.2.tgz",
             "version": "1.0.2"
         },
         "path-exists": {
             "integrity": "sha1-zg6+ql94yxiSXqfYENe1mwEP1RU=",
             "resolved": "https://registry.npmjs.org/path-exists/-/path-exists-3.0.0.tgz",
             "version": "3.0.0"
@@ -4201,22 +5888,22 @@
         },
         "path-is-inside": {
             "integrity": "sha1-NlQX3t5EQw0cEa9hAn+s8HS9/FM=",
             "resolved": "https://registry.npmjs.org/path-is-inside/-/path-is-inside-1.0.2.tgz",
             "version": "1.0.2"
         },
         "path-parse": {
-            "integrity": "sha512-GSmOT2EbHrINBf9SR7CDELwlJ8AENk3Qn7OikK4nFYAu3Ote2+JYNVvkpAEQm3/TLNEJFD/xZJjzyxg3KBWOzw==",
-            "resolved": "https://registry.npmjs.org/path-parse/-/path-parse-1.0.6.tgz",
-            "version": "1.0.6"
+            "integrity": "sha512-LDJzPVEEEPR+y48z93A0Ed0yXb8pAByGWo/k5YYdYgpY2/2EsOsksJrq7lOHxryrVOn1ejG6oAp8ahvOIQD8sw==",
+            "resolved": "https://registry.npmjs.org/path-parse/-/path-parse-1.0.7.tgz",
+            "version": "1.0.7"
         },
         "path-to-regexp": {
-            "integrity": "sha512-dZY7QPCPp5r9cnNuQ955mOv4ZFVDXY/yvqeV7Y1W2PJA3PEFcuow9xKFfJxbBj1pIjOAP+M2B4/7xubmykLrXw==",
-            "resolved": "https://registry.npmjs.org/path-to-regexp/-/path-to-regexp-2.1.0.tgz",
-            "version": "2.1.0"
+            "integrity": "sha512-h9DqehX3zZZDCEm+xbfU0ZmwCGFCAAraPJWMXJ4+v32NjZJilVg3k1TcKsRgIb8IQ/izZSaydDc1OhJCZvs2Dw==",
+            "resolved": "https://registry.npmjs.org/path-to-regexp/-/path-to-regexp-6.1.0.tgz",
+            "version": "6.1.0"
         },
         "path-type": {
             "dependencies": {
                 "pify": {
                     "integrity": "sha1-7RQaasBDqEnqWISY59yosVMw6Qw=",
                     "resolved": "https://registry.npmjs.org/pify/-/pify-2.3.0.tgz",
                     "version": "2.3.0"
@@ -4226,24 +5913,29 @@
             "requires": {
                 "pify": "^2.0.0"
             },
             "resolved": "https://registry.npmjs.org/path-type/-/path-type-2.0.0.tgz",
             "version": "2.0.0"
         },
         "pbkdf2": {
-            "integrity": "sha512-U/il5MsrZp7mGg3mSQfn742na2T+1/vHDCG5/iTI3X9MKUuYUZVLQhyRsg06mCgDBTd57TxzgZt7P+fYfjRLtA==",
+            "integrity": "sha512-iuh7L6jA7JEGu2WxDwtQP1ddOpaJNC4KlDEFfdQajSGgGPNi4OyDc2R7QnbY2bR9QjBVGwgvTdNJZoE7RaxUMA==",
             "requires": {
                 "create-hash": "^1.1.2",
                 "create-hmac": "^1.1.4",
                 "ripemd160": "^2.0.1",
                 "safe-buffer": "^5.0.1",
                 "sha.js": "^2.4.8"
             },
-            "resolved": "https://registry.npmjs.org/pbkdf2/-/pbkdf2-3.0.17.tgz",
-            "version": "3.0.17"
+            "resolved": "https://registry.npmjs.org/pbkdf2/-/pbkdf2-3.1.2.tgz",
+            "version": "3.1.2"
+        },
+        "picomatch": {
+            "integrity": "sha512-lY1Q/PiJGC2zOv/z391WOTD+Z02bCgsFfvxoXXf6h7kv9o+WmsmzYqrAwY63sNgOxE4xEdq0WyUnXfKeBrSvYw==",
+            "resolved": "https://registry.npmjs.org/picomatch/-/picomatch-2.3.0.tgz",
+            "version": "2.3.0"
         },
         "pify": {
             "integrity": "sha1-5aSs0sEB/fPZpNB/DbxNtJ3SgXY=",
             "resolved": "https://registry.npmjs.org/pify/-/pify-3.0.0.tgz",
             "version": "3.0.0"
         },
         "pinkie": {
@@ -4273,24 +5965,573 @@
             "integrity": "sha1-9tXREJ4Z1j7fQo4L1X4Sd3YVM0s=",
             "requires": {
                 "find-up": "^2.1.0"
             },
             "resolved": "https://registry.npmjs.org/pkg-dir/-/pkg-dir-2.0.0.tgz",
             "version": "2.0.0"
         },
+        "pkg-up": {
+            "dependencies": {
+                "find-up": {
+                    "integrity": "sha512-1yD6RmLI1XBfxugvORwlck6f75tYL+iR0jqwsOrOxMZyGYqUuDhJ0l4AXdO1iX/FTs9cBAMEk1gWSEx1kSbylg==",
+                    "requires": {
+                        "locate-path": "^3.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/find-up/-/find-up-3.0.0.tgz",
+                    "version": "3.0.0"
+                },
+                "locate-path": {
+                    "integrity": "sha512-7AO748wWnIhNqAuaty2ZWHkQHRSNfPVIsPIfwEOWO22AmaoVrWavlOcMR5nzTLNYvp36X220/maaRsrec1G65A==",
+                    "requires": {
+                        "p-locate": "^3.0.0",
+                        "path-exists": "^3.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/locate-path/-/locate-path-3.0.0.tgz",
+                    "version": "3.0.0"
+                },
+                "p-limit": {
+                    "integrity": "sha512-//88mFWSJx8lxCzwdAABTJL2MyWB12+eIY7MDL2SqLmAkeKU9qxRvWuSyTjm3FUmpBEMuFfckAIqEaVGUDxb6w==",
+                    "requires": {
+                        "p-try": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/p-limit/-/p-limit-2.3.0.tgz",
+                    "version": "2.3.0"
+                },
+                "p-locate": {
+                    "integrity": "sha512-x+12w/To+4GFfgJhBEpiDcLozRJGegY+Ei7/z0tSLkMmxGZNybVMSfWj9aJn8Z5Fc7dBUNJOOVgPv2H7IwulSQ==",
+                    "requires": {
+                        "p-limit": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/p-locate/-/p-locate-3.0.0.tgz",
+                    "version": "3.0.0"
+                },
+                "p-try": {
+                    "integrity": "sha512-R4nPAVTAU0B9D35/Gk3uJf/7XYbQcyohSKdvAxIRSNghFl4e71hVoGnBNQz9cWaXxO2I10KTC+3jMdvvoKw6dQ==",
+                    "resolved": "https://registry.npmjs.org/p-try/-/p-try-2.2.0.tgz",
+                    "version": "2.2.0"
+                }
+            },
+            "integrity": "sha512-nDywThFk1i4BQK4twPQ6TA4RT8bDY96yeuCVBWL3ePARCiEKDRSrNGbFIgUJpLp+XeIR65v8ra7WuJOFUBtkMA==",
+            "requires": {
+                "find-up": "^3.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/pkg-up/-/pkg-up-3.1.0.tgz",
+            "version": "3.1.0"
+        },
+        "pnp-webpack-plugin": {
+            "integrity": "sha512-jd9olUr9D7do+RN8Wspzhpxhgp1n6Vd0NtQ4SFkmIACZoEL1nkyAdW9Ygrinjec0vgDcWjscFQQ1gDW8rsfKTg==",
+            "requires": {
+                "ts-pnp": "^1.1.2"
+            },
+            "resolved": "https://registry.npmjs.org/pnp-webpack-plugin/-/pnp-webpack-plugin-1.5.0.tgz",
+            "version": "1.5.0"
+        },
         "posix-character-classes": {
             "integrity": "sha1-AerA/jta9xoqbAL+q7jB/vfgDqs=",
             "resolved": "https://registry.npmjs.org/posix-character-classes/-/posix-character-classes-0.1.1.tgz",
             "version": "0.1.1"
         },
+        "postcss": {
+            "dependencies": {
+                "supports-color": {
+                    "integrity": "sha512-qe1jfm1Mg7Nq/NSh6XE24gPXROEVsWHxC1LIx//XNlD9iw7YZQGjZNjYN7xGaEG6iKdA8EtNFW6R0gjnVXp+wQ==",
+                    "requires": {
+                        "has-flag": "^3.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/supports-color/-/supports-color-6.1.0.tgz",
+                    "version": "6.1.0"
+                }
+            },
+            "integrity": "sha512-BebJSIUMwJHRH0HAQoxN4u1CN86glsrwsW0q7T+/m44eXOUAxSNdHRkNZPYz5vVUbg17hFgOQDE7fZk7li3pZw==",
+            "requires": {
+                "chalk": "^2.4.2",
+                "source-map": "^0.6.1",
+                "supports-color": "^6.1.0"
+            },
+            "resolved": "https://registry.npmjs.org/postcss/-/postcss-7.0.36.tgz",
+            "version": "7.0.36"
+        },
+        "postcss-attribute-case-insensitive": {
+            "integrity": "sha512-clkFxk/9pcdb4Vkn0hAHq3YnxBQ2p0CGD1dy24jN+reBck+EWxMbxSUqN4Yj7t0w8csl87K6p0gxBe1utkJsYA==",
+            "requires": {
+                "postcss": "^7.0.2",
+                "postcss-selector-parser": "^6.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-attribute-case-insensitive/-/postcss-attribute-case-insensitive-4.0.2.tgz",
+            "version": "4.0.2"
+        },
+        "postcss-color-functional-notation": {
+            "integrity": "sha512-ZBARCypjEDofW4P6IdPVTLhDNXPRn8T2s1zHbZidW6rPaaZvcnCS2soYFIQJrMZSxiePJ2XIYTlcb2ztr/eT2g==",
+            "requires": {
+                "postcss": "^7.0.2",
+                "postcss-values-parser": "^2.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-color-functional-notation/-/postcss-color-functional-notation-2.0.1.tgz",
+            "version": "2.0.1"
+        },
+        "postcss-color-gray": {
+            "integrity": "sha512-q6BuRnAGKM/ZRpfDascZlIZPjvwsRye7UDNalqVz3s7GDxMtqPY6+Q871liNxsonUw8oC61OG+PSaysYpl1bnw==",
+            "requires": {
+                "@csstools/convert-colors": "^1.4.0",
+                "postcss": "^7.0.5",
+                "postcss-values-parser": "^2.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-color-gray/-/postcss-color-gray-5.0.0.tgz",
+            "version": "5.0.0"
+        },
+        "postcss-color-hex-alpha": {
+            "integrity": "sha512-PF4GDel8q3kkreVXKLAGNpHKilXsZ6xuu+mOQMHWHLPNyjiUBOr75sp5ZKJfmv1MCus5/DWUGcK9hm6qHEnXYw==",
+            "requires": {
+                "postcss": "^7.0.14",
+                "postcss-values-parser": "^2.0.1"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-color-hex-alpha/-/postcss-color-hex-alpha-5.0.3.tgz",
+            "version": "5.0.3"
+        },
+        "postcss-color-mod-function": {
+            "integrity": "sha512-YP4VG+xufxaVtzV6ZmhEtc+/aTXH3d0JLpnYfxqTvwZPbJhWqp8bSY3nfNzNRFLgB4XSaBA82OE4VjOOKpCdVQ==",
+            "requires": {
+                "@csstools/convert-colors": "^1.4.0",
+                "postcss": "^7.0.2",
+                "postcss-values-parser": "^2.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-color-mod-function/-/postcss-color-mod-function-3.0.3.tgz",
+            "version": "3.0.3"
+        },
+        "postcss-color-rebeccapurple": {
+            "integrity": "sha512-aAe3OhkS6qJXBbqzvZth2Au4V3KieR5sRQ4ptb2b2O8wgvB3SJBsdG+jsn2BZbbwekDG8nTfcCNKcSfe/lEy8g==",
+            "requires": {
+                "postcss": "^7.0.2",
+                "postcss-values-parser": "^2.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-color-rebeccapurple/-/postcss-color-rebeccapurple-4.0.1.tgz",
+            "version": "4.0.1"
+        },
+        "postcss-custom-media": {
+            "integrity": "sha512-c9s5iX0Ge15o00HKbuRuTqNndsJUbaXdiNsksnVH8H4gdc+zbLzr/UasOwNG6CTDpLFekVY4672eWdiiWu2GUg==",
+            "requires": {
+                "postcss": "^7.0.14"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-custom-media/-/postcss-custom-media-7.0.8.tgz",
+            "version": "7.0.8"
+        },
+        "postcss-custom-properties": {
+            "integrity": "sha512-nm+o0eLdYqdnJ5abAJeXp4CEU1c1k+eB2yMCvhgzsds/e0umabFrN6HoTy/8Q4K5ilxERdl/JD1LO5ANoYBeMA==",
+            "requires": {
+                "postcss": "^7.0.17",
+                "postcss-values-parser": "^2.0.1"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-custom-properties/-/postcss-custom-properties-8.0.11.tgz",
+            "version": "8.0.11"
+        },
+        "postcss-custom-selectors": {
+            "dependencies": {
+                "cssesc": {
+                    "integrity": "sha512-MsCAG1z9lPdoO/IUMLSBWBSVxVtJ1395VGIQ+Fc2gNdkQ1hNDnQdw3YhA71WJCBW1vdwA0cAnk/DnW6bqoEUYg==",
+                    "resolved": "https://registry.npmjs.org/cssesc/-/cssesc-2.0.0.tgz",
+                    "version": "2.0.0"
+                },
+                "postcss-selector-parser": {
+                    "integrity": "sha512-w+zLE5Jhg6Liz8+rQOWEAwtwkyqpfnmsinXjXg6cY7YIONZZtgvE0v2O0uhQBs0peNomOJwWRKt6JBfTdTd3OQ==",
+                    "requires": {
+                        "cssesc": "^2.0.0",
+                        "indexes-of": "^1.0.1",
+                        "uniq": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/postcss-selector-parser/-/postcss-selector-parser-5.0.0.tgz",
+                    "version": "5.0.0"
+                }
+            },
+            "integrity": "sha512-DSGDhqinCqXqlS4R7KGxL1OSycd1lydugJ1ky4iRXPHdBRiozyMHrdu0H3o7qNOCiZwySZTUI5MV0T8QhCLu+w==",
+            "requires": {
+                "postcss": "^7.0.2",
+                "postcss-selector-parser": "^5.0.0-rc.3"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-custom-selectors/-/postcss-custom-selectors-5.1.2.tgz",
+            "version": "5.1.2"
+        },
+        "postcss-dir-pseudo-class": {
+            "dependencies": {
+                "cssesc": {
+                    "integrity": "sha512-MsCAG1z9lPdoO/IUMLSBWBSVxVtJ1395VGIQ+Fc2gNdkQ1hNDnQdw3YhA71WJCBW1vdwA0cAnk/DnW6bqoEUYg==",
+                    "resolved": "https://registry.npmjs.org/cssesc/-/cssesc-2.0.0.tgz",
+                    "version": "2.0.0"
+                },
+                "postcss-selector-parser": {
+                    "integrity": "sha512-w+zLE5Jhg6Liz8+rQOWEAwtwkyqpfnmsinXjXg6cY7YIONZZtgvE0v2O0uhQBs0peNomOJwWRKt6JBfTdTd3OQ==",
+                    "requires": {
+                        "cssesc": "^2.0.0",
+                        "indexes-of": "^1.0.1",
+                        "uniq": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/postcss-selector-parser/-/postcss-selector-parser-5.0.0.tgz",
+                    "version": "5.0.0"
+                }
+            },
+            "integrity": "sha512-3pm4oq8HYWMZePJY+5ANriPs3P07q+LW6FAdTlkFH2XqDdP4HeeJYMOzn0HYLhRSjBO3fhiqSwwU9xEULSrPgw==",
+            "requires": {
+                "postcss": "^7.0.2",
+                "postcss-selector-parser": "^5.0.0-rc.3"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-dir-pseudo-class/-/postcss-dir-pseudo-class-5.0.0.tgz",
+            "version": "5.0.0"
+        },
+        "postcss-double-position-gradients": {
+            "integrity": "sha512-G+nV8EnQq25fOI8CH/B6krEohGWnF5+3A6H/+JEpOncu5dCnkS1QQ6+ct3Jkaepw1NGVqqOZH6lqrm244mCftA==",
+            "requires": {
+                "postcss": "^7.0.5",
+                "postcss-values-parser": "^2.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-double-position-gradients/-/postcss-double-position-gradients-1.0.0.tgz",
+            "version": "1.0.0"
+        },
+        "postcss-env-function": {
+            "integrity": "sha512-rwac4BuZlITeUbiBq60h/xbLzXY43qOsIErngWa4l7Mt+RaSkT7QBjXVGTcBHupykkblHMDrBFh30zchYPaOUw==",
+            "requires": {
+                "postcss": "^7.0.2",
+                "postcss-values-parser": "^2.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-env-function/-/postcss-env-function-2.0.2.tgz",
+            "version": "2.0.2"
+        },
+        "postcss-flexbugs-fixes": {
+            "integrity": "sha512-QRE0n3hpkxxS/OGvzOa+PDuy4mh/Jg4o9ui22/ko5iGYOG3M5dfJabjnAZjTdh2G9F85c7Hv8hWcEDEKW/xceQ==",
+            "requires": {
+                "postcss": "^7.0.26"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-flexbugs-fixes/-/postcss-flexbugs-fixes-4.2.0.tgz",
+            "version": "4.2.0"
+        },
+        "postcss-focus-visible": {
+            "integrity": "sha512-Z5CkWBw0+idJHSV6+Bgf2peDOFf/x4o+vX/pwcNYrWpXFrSfTkQ3JQ1ojrq9yS+upnAlNRHeg8uEwFTgorjI8g==",
+            "requires": {
+                "postcss": "^7.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-focus-visible/-/postcss-focus-visible-4.0.0.tgz",
+            "version": "4.0.0"
+        },
+        "postcss-focus-within": {
+            "integrity": "sha512-W0APui8jQeBKbCGZudW37EeMCjDeVxKgiYfIIEo8Bdh5SpB9sxds/Iq8SEuzS0Q4YFOlG7EPFulbbxujpkrV2w==",
+            "requires": {
+                "postcss": "^7.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-focus-within/-/postcss-focus-within-3.0.0.tgz",
+            "version": "3.0.0"
+        },
+        "postcss-font-variant": {
+            "integrity": "sha512-I3ADQSTNtLTTd8uxZhtSOrTCQ9G4qUVKPjHiDk0bV75QSxXjVWiJVJ2VLdspGUi9fbW9BcjKJoRvxAH1pckqmA==",
+            "requires": {
+                "postcss": "^7.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-font-variant/-/postcss-font-variant-4.0.1.tgz",
+            "version": "4.0.1"
+        },
+        "postcss-gap-properties": {
+            "integrity": "sha512-QZSqDaMgXCHuHTEzMsS2KfVDOq7ZFiknSpkrPJY6jmxbugUPTuSzs/vuE5I3zv0WAS+3vhrlqhijiprnuQfzmg==",
+            "requires": {
+                "postcss": "^7.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-gap-properties/-/postcss-gap-properties-2.0.0.tgz",
+            "version": "2.0.0"
+        },
+        "postcss-image-set-function": {
+            "integrity": "sha512-oPTcFFip5LZy8Y/whto91L9xdRHCWEMs3e1MdJxhgt4jy2WYXfhkng59fH5qLXSCPN8k4n94p1Czrfe5IOkKUw==",
+            "requires": {
+                "postcss": "^7.0.2",
+                "postcss-values-parser": "^2.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-image-set-function/-/postcss-image-set-function-3.0.1.tgz",
+            "version": "3.0.1"
+        },
+        "postcss-initial": {
+            "integrity": "sha512-3RLn6DIpMsK1l5UUy9jxQvoDeUN4gP939tDcKUHD/kM8SGSKbFAnvkpFpj3Bhtz3HGk1jWY5ZNWX6mPta5M9fg==",
+            "requires": {
+                "postcss": "^7.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-initial/-/postcss-initial-3.0.4.tgz",
+            "version": "3.0.4"
+        },
+        "postcss-lab-function": {
+            "integrity": "sha512-whLy1IeZKY+3fYdqQFuDBf8Auw+qFuVnChWjmxm/UhHWqNHZx+B99EwxTvGYmUBqe3Fjxs4L1BoZTJmPu6usVg==",
+            "requires": {
+                "@csstools/convert-colors": "^1.4.0",
+                "postcss": "^7.0.2",
+                "postcss-values-parser": "^2.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-lab-function/-/postcss-lab-function-2.0.1.tgz",
+            "version": "2.0.1"
+        },
+        "postcss-load-config": {
+            "integrity": "sha512-/rDeGV6vMUo3mwJZmeHfEDvwnTKKqQ0S7OHUi/kJvvtx3aWtyWG2/0ZWnzCt2keEclwN6Tf0DST2v9kITdOKYw==",
+            "requires": {
+                "cosmiconfig": "^5.0.0",
+                "import-cwd": "^2.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-load-config/-/postcss-load-config-2.1.2.tgz",
+            "version": "2.1.2"
+        },
+        "postcss-loader": {
+            "dependencies": {
+                "json5": {
+                    "integrity": "sha512-aKS4WQjPenRxiQsC93MNfjx+nbF4PAdYzmd/1JIj8HYzqfbu86beTuNgXDzPknWk0n0uARlyewZo4s++ES36Ow==",
+                    "requires": {
+                        "minimist": "^1.2.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/json5/-/json5-1.0.1.tgz",
+                    "version": "1.0.1"
+                },
+                "loader-utils": {
+                    "integrity": "sha512-qH0WSMBtn/oHuwjy/NucEgbx5dbxxnxup9s4PVXJUDHZBQY+s0NWA9rJf53RBnQZxfch7euUui7hpoAPvALZdA==",
+                    "requires": {
+                        "big.js": "^5.2.2",
+                        "emojis-list": "^3.0.0",
+                        "json5": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/loader-utils/-/loader-utils-1.4.0.tgz",
+                    "version": "1.4.0"
+                },
+                "schema-utils": {
+                    "integrity": "sha512-i27Mic4KovM/lnGsy8whRCHhc7VicJajAjTrYg11K9zfZXnYIt4k5F+kZkwjnrhKzLic/HLU4j11mjsz2G/75g==",
+                    "requires": {
+                        "ajv": "^6.1.0",
+                        "ajv-errors": "^1.0.0",
+                        "ajv-keywords": "^3.1.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/schema-utils/-/schema-utils-1.0.0.tgz",
+                    "version": "1.0.0"
+                }
+            },
+            "integrity": "sha512-cLWoDEY5OwHcAjDnkyRQzAXfs2jrKjXpO/HQFcc5b5u/r7aa471wdmChmwfnv7x2u840iat/wi0lQ5nbRgSkUA==",
+            "requires": {
+                "loader-utils": "^1.1.0",
+                "postcss": "^7.0.0",
+                "postcss-load-config": "^2.0.0",
+                "schema-utils": "^1.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-loader/-/postcss-loader-3.0.0.tgz",
+            "version": "3.0.0"
+        },
+        "postcss-logical": {
+            "integrity": "sha512-1SUKdJc2vuMOmeItqGuNaC+N8MzBWFWEkAnRnLpFYj1tGGa7NqyVBujfRtgNa2gXR+6RkGUiB2O5Vmh7E2RmiA==",
+            "requires": {
+                "postcss": "^7.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-logical/-/postcss-logical-3.0.0.tgz",
+            "version": "3.0.0"
+        },
+        "postcss-media-minmax": {
+            "integrity": "sha512-fo9moya6qyxsjbFAYl97qKO9gyre3qvbMnkOZeZwlsW6XYFsvs2DMGDlchVLfAd8LHPZDxivu/+qW2SMQeTHBw==",
+            "requires": {
+                "postcss": "^7.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-media-minmax/-/postcss-media-minmax-4.0.0.tgz",
+            "version": "4.0.0"
+        },
+        "postcss-modules-extract-imports": {
+            "integrity": "sha512-LaYLDNS4SG8Q5WAWqIJgdHPJrDDr/Lv775rMBFUbgjTz6j34lUznACHcdRWroPvXANP2Vj7yNK57vp9eFqzLWQ==",
+            "requires": {
+                "postcss": "^7.0.5"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-modules-extract-imports/-/postcss-modules-extract-imports-2.0.0.tgz",
+            "version": "2.0.0"
+        },
+        "postcss-modules-local-by-default": {
+            "dependencies": {
+                "postcss-value-parser": {
+                    "integrity": "sha512-97DXOFbQJhk71ne5/Mt6cOu6yxsSfM0QGQyl0L25Gca4yGWEGJaig7l7gbCX623VqTBNGLRLaVUCnNkcedlRSQ==",
+                    "resolved": "https://registry.npmjs.org/postcss-value-parser/-/postcss-value-parser-4.1.0.tgz",
+                    "version": "4.1.0"
+                }
+            },
+            "integrity": "sha512-e3xDq+LotiGesympRlKNgaJ0PCzoUIdpH0dj47iWAui/kyTgh3CiAr1qP54uodmJhl6p9rN6BoNcdEDVJx9RDw==",
+            "requires": {
+                "icss-utils": "^4.1.1",
+                "postcss": "^7.0.32",
+                "postcss-selector-parser": "^6.0.2",
+                "postcss-value-parser": "^4.1.0"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-modules-local-by-default/-/postcss-modules-local-by-default-3.0.3.tgz",
+            "version": "3.0.3"
+        },
+        "postcss-modules-scope": {
+            "integrity": "sha512-YyEgsTMRpNd+HmyC7H/mh3y+MeFWevy7V1evVhJWewmMbjDHIbZbOXICC2y+m1xI1UVfIT1HMW/O04Hxyu9oXQ==",
+            "requires": {
+                "postcss": "^7.0.6",
+                "postcss-selector-parser": "^6.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-modules-scope/-/postcss-modules-scope-2.2.0.tgz",
+            "version": "2.2.0"
+        },
+        "postcss-modules-values": {
+            "integrity": "sha512-1//E5jCBrZ9DmRX+zCtmQtRSV6PV42Ix7Bzj9GbwJceduuf7IqP8MgeTXuRDHOWj2m0VzZD5+roFWDuU8RQjcg==",
+            "requires": {
+                "icss-utils": "^4.0.0",
+                "postcss": "^7.0.6"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-modules-values/-/postcss-modules-values-3.0.0.tgz",
+            "version": "3.0.0"
+        },
+        "postcss-nesting": {
+            "integrity": "sha512-FrorPb0H3nuVq0Sff7W2rnc3SmIcruVC6YwpcS+k687VxyxO33iE1amna7wHuRVzM8vfiYofXSBHNAZ3QhLvYg==",
+            "requires": {
+                "postcss": "^7.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-nesting/-/postcss-nesting-7.0.1.tgz",
+            "version": "7.0.1"
+        },
+        "postcss-overflow-shorthand": {
+            "integrity": "sha512-aK0fHc9CBNx8jbzMYhshZcEv8LtYnBIRYQD5i7w/K/wS9c2+0NSR6B3OVMu5y0hBHYLcMGjfU+dmWYNKH0I85g==",
+            "requires": {
+                "postcss": "^7.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-overflow-shorthand/-/postcss-overflow-shorthand-2.0.0.tgz",
+            "version": "2.0.0"
+        },
+        "postcss-page-break": {
+            "integrity": "sha512-tkpTSrLpfLfD9HvgOlJuigLuk39wVTbbd8RKcy8/ugV2bNBUW3xU+AIqyxhDrQr1VUj1RmyJrBn1YWrqUm9zAQ==",
+            "requires": {
+                "postcss": "^7.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-page-break/-/postcss-page-break-2.0.0.tgz",
+            "version": "2.0.0"
+        },
+        "postcss-place": {
+            "integrity": "sha512-Zb6byCSLkgRKLODj/5mQugyuj9bvAAw9LqJJjgwz5cYryGeXfFZfSXoP1UfveccFmeq0b/2xxwcTEVScnqGxBg==",
+            "requires": {
+                "postcss": "^7.0.2",
+                "postcss-values-parser": "^2.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-place/-/postcss-place-4.0.1.tgz",
+            "version": "4.0.1"
+        },
+        "postcss-preset-env": {
+            "integrity": "sha512-eU4/K5xzSFwUFJ8hTdTQzo2RBLbDVt83QZrAvI07TULOkmyQlnYlpwep+2yIK+K+0KlZO4BvFcleOCCcUtwchg==",
+            "requires": {
+                "autoprefixer": "^9.6.1",
+                "browserslist": "^4.6.4",
+                "caniuse-lite": "^1.0.30000981",
+                "css-blank-pseudo": "^0.1.4",
+                "css-has-pseudo": "^0.10.0",
+                "css-prefers-color-scheme": "^3.1.1",
+                "cssdb": "^4.4.0",
+                "postcss": "^7.0.17",
+                "postcss-attribute-case-insensitive": "^4.0.1",
+                "postcss-color-functional-notation": "^2.0.1",
+                "postcss-color-gray": "^5.0.0",
+                "postcss-color-hex-alpha": "^5.0.3",
+                "postcss-color-mod-function": "^3.0.3",
+                "postcss-color-rebeccapurple": "^4.0.1",
+                "postcss-custom-media": "^7.0.8",
+                "postcss-custom-properties": "^8.0.11",
+                "postcss-custom-selectors": "^5.1.2",
+                "postcss-dir-pseudo-class": "^5.0.0",
+                "postcss-double-position-gradients": "^1.0.0",
+                "postcss-env-function": "^2.0.2",
+                "postcss-focus-visible": "^4.0.0",
+                "postcss-focus-within": "^3.0.0",
+                "postcss-font-variant": "^4.0.0",
+                "postcss-gap-properties": "^2.0.0",
+                "postcss-image-set-function": "^3.0.1",
+                "postcss-initial": "^3.0.0",
+                "postcss-lab-function": "^2.0.1",
+                "postcss-logical": "^3.0.0",
+                "postcss-media-minmax": "^4.0.0",
+                "postcss-nesting": "^7.0.0",
+                "postcss-overflow-shorthand": "^2.0.0",
+                "postcss-page-break": "^2.0.0",
+                "postcss-place": "^4.0.1",
+                "postcss-pseudo-class-any-link": "^6.0.0",
+                "postcss-replace-overflow-wrap": "^3.0.0",
+                "postcss-selector-matches": "^4.0.0",
+                "postcss-selector-not": "^4.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-preset-env/-/postcss-preset-env-6.7.0.tgz",
+            "version": "6.7.0"
+        },
+        "postcss-pseudo-class-any-link": {
+            "dependencies": {
+                "cssesc": {
+                    "integrity": "sha512-MsCAG1z9lPdoO/IUMLSBWBSVxVtJ1395VGIQ+Fc2gNdkQ1hNDnQdw3YhA71WJCBW1vdwA0cAnk/DnW6bqoEUYg==",
+                    "resolved": "https://registry.npmjs.org/cssesc/-/cssesc-2.0.0.tgz",
+                    "version": "2.0.0"
+                },
+                "postcss-selector-parser": {
+                    "integrity": "sha512-w+zLE5Jhg6Liz8+rQOWEAwtwkyqpfnmsinXjXg6cY7YIONZZtgvE0v2O0uhQBs0peNomOJwWRKt6JBfTdTd3OQ==",
+                    "requires": {
+                        "cssesc": "^2.0.0",
+                        "indexes-of": "^1.0.1",
+                        "uniq": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/postcss-selector-parser/-/postcss-selector-parser-5.0.0.tgz",
+                    "version": "5.0.0"
+                }
+            },
+            "integrity": "sha512-lgXW9sYJdLqtmw23otOzrtbDXofUdfYzNm4PIpNE322/swES3VU9XlXHeJS46zT2onFO7V1QFdD4Q9LiZj8mew==",
+            "requires": {
+                "postcss": "^7.0.2",
+                "postcss-selector-parser": "^5.0.0-rc.3"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-pseudo-class-any-link/-/postcss-pseudo-class-any-link-6.0.0.tgz",
+            "version": "6.0.0"
+        },
+        "postcss-replace-overflow-wrap": {
+            "integrity": "sha512-2T5hcEHArDT6X9+9dVSPQdo7QHzG4XKclFT8rU5TzJPDN7RIRTbO9c4drUISOVemLj03aezStHCR2AIcr8XLpw==",
+            "requires": {
+                "postcss": "^7.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-replace-overflow-wrap/-/postcss-replace-overflow-wrap-3.0.0.tgz",
+            "version": "3.0.0"
+        },
+        "postcss-selector-matches": {
+            "integrity": "sha512-LgsHwQR/EsRYSqlwdGzeaPKVT0Ml7LAT6E75T8W8xLJY62CE4S/l03BWIt3jT8Taq22kXP08s2SfTSzaraoPww==",
+            "requires": {
+                "balanced-match": "^1.0.0",
+                "postcss": "^7.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-selector-matches/-/postcss-selector-matches-4.0.0.tgz",
+            "version": "4.0.0"
+        },
+        "postcss-selector-not": {
+            "integrity": "sha512-YolvBgInEK5/79C+bdFMyzqTg6pkYqDbzZIST/PDMqa/o3qtXenD05apBG2jLgT0/BQ77d4U2UK12jWpilqMAQ==",
+            "requires": {
+                "balanced-match": "^1.0.0",
+                "postcss": "^7.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-selector-not/-/postcss-selector-not-4.0.1.tgz",
+            "version": "4.0.1"
+        },
+        "postcss-selector-parser": {
+            "integrity": "sha512-9LXrvaaX3+mcv5xkg5kFwqSzSH1JIObIx51PrndZwlmznwXRfxMddDvo9gve3gVR8ZTKgoFDdWkbRFmEhT4PMg==",
+            "requires": {
+                "cssesc": "^3.0.0",
+                "util-deprecate": "^1.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-selector-parser/-/postcss-selector-parser-6.0.6.tgz",
+            "version": "6.0.6"
+        },
         "postcss-value-parser": {
             "integrity": "sha512-pISE66AbVkp4fDQ7VHBwRNXzAAKJjw4Vw7nWI/+Q3vuly7SNfgYXvm6i5IgFylHGK5sP/xHAbB7N49OS4gWNyQ==",
             "resolved": "https://registry.npmjs.org/postcss-value-parser/-/postcss-value-parser-3.3.1.tgz",
             "version": "3.3.1"
         },
+        "postcss-values-parser": {
+            "integrity": "sha512-2tLuBsA6P4rYTNKCXYG/71C7j1pU6pK503suYOmn4xYrQIzW+opD+7FAFNuGSdZC/3Qfy334QbeMu7MEb8gOxg==",
+            "requires": {
+                "flatten": "^1.0.2",
+                "indexes-of": "^1.0.1",
+                "uniq": "^1.0.1"
+            },
+            "resolved": "https://registry.npmjs.org/postcss-values-parser/-/postcss-values-parser-2.0.1.tgz",
+            "version": "2.0.1"
+        },
+        "prepend-http": {
+            "integrity": "sha1-1PRWKwzjaW5BrFLQ4ALlemNdxtw=",
+            "resolved": "https://registry.npmjs.org/prepend-http/-/prepend-http-1.0.4.tgz",
+            "version": "1.0.4"
+        },
         "private": {
             "integrity": "sha512-VvivMrbvd2nKkiG38qjULzlc+4Vx4wm/whI9pQD35YrARNnhxeiRktSOhSukRLFNlzg6Br/cJPet5J/u19r/mg==",
             "resolved": "https://registry.npmjs.org/private/-/private-0.1.8.tgz",
             "version": "0.1.8"
         },
         "process": {
             "integrity": "sha1-czIwDoQBYb2j5podHZGn1LwW8YI=",
@@ -4298,26 +6539,21 @@
             "version": "0.11.10"
         },
         "process-nextick-args": {
             "integrity": "sha512-3ouUOpQhtgrbOa17J7+uxOTpITYWaGP7/AhoR3+A+/1e9skrzelGi/dXzEYyvbxubEF6Wn2ypscTKiKJFFn1ag==",
             "resolved": "https://registry.npmjs.org/process-nextick-args/-/process-nextick-args-2.0.1.tgz",
             "version": "2.0.1"
         },
-        "progress": {
-            "integrity": "sha512-7PiHtLll5LdnKIMw100I+8xJXR5gW2QwWYkT6iJva0bXitZKa/XMrSbdmg3r2Xnaidz9Qumd0VPaMrZlF9V9sA==",
-            "resolved": "https://registry.npmjs.org/progress/-/progress-2.0.3.tgz",
-            "version": "2.0.3"
-        },
         "promise": {
-            "integrity": "sha1-SJZUxpJha4qlWwck+oCbt9tJxb8=",
+            "integrity": "sha1-5F1osAoXZHttpxG/he1u1HII9FA=",
             "requires": {
                 "asap": "~2.0.3"
             },
-            "resolved": "https://registry.npmjs.org/promise/-/promise-7.1.1.tgz",
-            "version": "7.1.1"
+            "resolved": "https://registry.npmjs.org/promise/-/promise-8.0.1.tgz",
+            "version": "8.0.1"
         },
         "promise-inflight": {
             "integrity": "sha1-mEcocL8igTL8vdhoEputEsPAKeM=",
             "resolved": "https://registry.npmjs.org/promise-inflight/-/promise-inflight-1.0.1.tgz",
             "version": "1.0.1"
         },
         "prop-types": {
@@ -4342,14 +6578,21 @@
         },
         "prr": {
             "integrity": "sha1-0/wRS6BplaRexok/SEzrHXj19HY=",
             "resolved": "https://registry.npmjs.org/prr/-/prr-1.0.1.tgz",
             "version": "1.0.1"
         },
         "public-encrypt": {
+            "dependencies": {
+                "bn.js": {
+                    "integrity": "sha512-c98Bf3tPniI+scsdk237ku1Dc3ujXQTSgyiPUDEOe7tRkhrqridvh8klBv0HCEso1OLOYcHuCv/cS6DNxKH+ZA==",
+                    "resolved": "https://registry.npmjs.org/bn.js/-/bn.js-4.12.0.tgz",
+                    "version": "4.12.0"
+                }
+            },
             "integrity": "sha512-zVpa8oKZSz5bTMTFClc1fQOnyyEzpl5ozpi1B5YcvBrdohMjH2rfsBtyXcuNuwjsDIXmBYlF2N5FlJYhR29t8Q==",
             "requires": {
                 "bn.js": "^4.1.0",
                 "browserify-rsa": "^4.0.0",
                 "create-hash": "^1.1.0",
                 "parse-asn1": "^5.0.0",
                 "randombytes": "^2.0.1",
@@ -4385,22 +6628,31 @@
                 "inherits": "^2.0.3",
                 "pump": "^2.0.0"
             },
             "resolved": "https://registry.npmjs.org/pumpify/-/pumpify-1.5.1.tgz",
             "version": "1.5.1"
         },
         "punycode": {
-            "integrity": "sha1-llOgNvt8HuQjQvIyXM7v6jkmxI0=",
-            "resolved": "https://registry.npmjs.org/punycode/-/punycode-1.3.2.tgz",
-            "version": "1.3.2"
+            "integrity": "sha512-XRsRjdf+j5ml+y/6GKHPZbrF/8p2Yga0JPtdqTIY2Xe5ohJPD9saDJJLPvp9+NSBprVvevdXZybnj2cv8OEd0A==",
+            "resolved": "https://registry.npmjs.org/punycode/-/punycode-2.1.1.tgz",
+            "version": "2.1.1"
+        },
+        "query-string": {
+            "integrity": "sha1-u7aTucqRXCMlFbIosaArYJBD2+s=",
+            "requires": {
+                "object-assign": "^4.1.0",
+                "strict-uri-encode": "^1.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/query-string/-/query-string-4.3.4.tgz",
+            "version": "4.3.4"
         },
         "querystring": {
-            "integrity": "sha1-sgmEkgO7Jd+CDadW50cAWHhSFiA=",
-            "resolved": "https://registry.npmjs.org/querystring/-/querystring-0.2.0.tgz",
-            "version": "0.2.0"
+            "integrity": "sha512-wkvS7mL/JMugcup3/rMitHmd9ecIGd2lhFhK9N3UUQ450h66d1r3Y9nvXzQAW1Lq+wyx61k/1pfKS5KuKiyEbg==",
+            "resolved": "https://registry.npmjs.org/querystring/-/querystring-0.2.1.tgz",
+            "version": "0.2.1"
         },
         "querystring-es3": {
             "integrity": "sha1-nsYfeQSYdXB9aUFFlv2Qek1xHnM=",
             "resolved": "https://registry.npmjs.org/querystring-es3/-/querystring-es3-0.2.1.tgz",
             "version": "0.2.1"
         },
         "randombytes": {
@@ -4481,148 +6733,282 @@
                 "normalize-package-data": "^2.3.2",
                 "path-type": "^2.0.0"
             },
             "resolved": "https://registry.npmjs.org/read-pkg/-/read-pkg-2.0.0.tgz",
             "version": "2.0.0"
         },
         "readable-stream": {
-            "integrity": "sha512-tQtKA9WIAhBF3+VLAseyMqZeBjW0AHJoxOtYqSUZNJxauErmLbVm2FW1y+J/YA9dUrAC39ITejlZWhVIwawkKw==",
+            "integrity": "sha512-Ebho8K4jIbHAxnuxi7o42OrZgF/ZTNcsZj6nRKyUmkhLFq8CHItp/fy6hQZuZmP/n3yZ9VBUbp4zz/mX8hmYPw==",
             "requires": {
                 "core-util-is": "~1.0.0",
                 "inherits": "~2.0.3",
                 "isarray": "~1.0.0",
                 "process-nextick-args": "~2.0.0",
                 "safe-buffer": "~5.1.1",
                 "string_decoder": "~1.1.1",
                 "util-deprecate": "~1.0.1"
             },
-            "resolved": "https://registry.npmjs.org/readable-stream/-/readable-stream-2.3.6.tgz",
-            "version": "2.3.6"
+            "resolved": "https://registry.npmjs.org/readable-stream/-/readable-stream-2.3.7.tgz",
+            "version": "2.3.7"
         },
         "readdirp": {
-            "integrity": "sha512-1JU/8q+VgFZyxwrJ+SVIOsh+KywWGpds3NTqikiKpDMZWScmAYyKIgqkO+ARvNWJfXeXR1zxz7aHF4u4CyH6vQ==",
+            "integrity": "sha512-hOS089on8RduqdbhvQ5Z37A0ESjsqz6qnRcffsMU3495FuTdqSm+7bhJ29JvIOsBDEEnan5DPu9t3To9VRlMzA==",
             "requires": {
-                "graceful-fs": "^4.1.11",
-                "micromatch": "^3.1.10",
-                "readable-stream": "^2.0.2"
+                "picomatch": "^2.2.1"
             },
-            "resolved": "https://registry.npmjs.org/readdirp/-/readdirp-2.2.1.tgz",
-            "version": "2.2.1"
+            "resolved": "https://registry.npmjs.org/readdirp/-/readdirp-3.6.0.tgz",
+            "version": "3.6.0"
+        },
+        "recast": {
+            "integrity": "sha512-sD1WJrpLQAkXGyQZyGzTM75WJvyAd98II5CHdK3IYbt/cZlU0UzCRVU11nUFNXX9fBVEt4E9ajkMjBlUlG+Oog==",
+            "requires": {
+                "ast-types": "0.13.2",
+                "esprima": "~4.0.0",
+                "private": "^0.1.8",
+                "source-map": "~0.6.1"
+            },
+            "resolved": "https://registry.npmjs.org/recast/-/recast-0.18.5.tgz",
+            "version": "0.18.5"
         },
         "reflect.ownkeys": {
             "integrity": "sha1-dJrO7H8/34tj+SegSAnpDFwLNGA=",
             "resolved": "https://registry.npmjs.org/reflect.ownkeys/-/reflect.ownkeys-0.2.0.tgz",
             "version": "0.2.0"
         },
         "regenerate": {
-            "integrity": "sha512-1G6jJVDWrt0rK99kBjvEtziZNCICAuvIPkSiUFIQxVP06RCVpq3dmDo2oi6ABpYaDYaTRr67BEhL8r1wgEZZKg==",
-            "resolved": "https://registry.npmjs.org/regenerate/-/regenerate-1.4.0.tgz",
-            "version": "1.4.0"
+            "integrity": "sha512-zrceR/XhGYU/d/opr2EKO7aRHUeiBI8qjtfHqADTwZd6Szfy16la6kqD0MIUs5z5hx6AaKa+PixpPrR289+I0A==",
+            "resolved": "https://registry.npmjs.org/regenerate/-/regenerate-1.4.2.tgz",
+            "version": "1.4.2"
         },
         "regenerate-unicode-properties": {
-            "integrity": "sha512-LGZzkgtLY79GeXLm8Dp0BVLdQlWICzBnJz/ipWUgo59qBaZ+BHtq51P2q1uVZlppMuUAT37SDk39qUbjTWB7bA==",
+            "integrity": "sha512-F9DjY1vKLo/tPePDycuH3dn9H1OTPIkVD9Kz4LODu+F2C75mgjAJ7x/gwy6ZcSNRAAkhNlJSOHRe8k3p+K9WhA==",
             "requires": {
                 "regenerate": "^1.4.0"
             },
-            "resolved": "https://registry.npmjs.org/regenerate-unicode-properties/-/regenerate-unicode-properties-8.1.0.tgz",
-            "version": "8.1.0"
+            "resolved": "https://registry.npmjs.org/regenerate-unicode-properties/-/regenerate-unicode-properties-8.2.0.tgz",
+            "version": "8.2.0"
         },
         "regenerator-runtime": {
-            "integrity": "sha512-naKIZz2GQ8JWh///G7L3X6LaQUAMp2lvb1rvwwsURe/VXwD6VMfr+/1NuNw3ag8v2kY1aQ/go5SNn79O9JU7yw==",
-            "resolved": "https://registry.npmjs.org/regenerator-runtime/-/regenerator-runtime-0.13.3.tgz",
-            "version": "0.13.3"
+            "integrity": "sha512-p3VT+cOEgxFsRRA9X4lkI1E+k2/CtnKtU4gcxyaCUreilL/vqI6CdZ3wxVUx3UOUg+gnUOQQcRI7BmSI656MYA==",
+            "resolved": "https://registry.npmjs.org/regenerator-runtime/-/regenerator-runtime-0.13.9.tgz",
+            "version": "0.13.9"
         },
         "regenerator-transform": {
-            "integrity": "sha512-flVuee02C3FKRISbxhXl9mGzdbWUVHubl1SMaknjxkFB1/iqpJhArQUvRxOOPEc/9tAiX0BaQ28FJH10E4isSQ==",
+            "dependencies": {
+                "@babel/runtime": {
+                    "integrity": "sha512-twj3L8Og5SaCRCErB4x4ajbvBIVV77CGeFglHpeg5WC5FF8TZzBWXtTJ4MqaD9QszLYTtr+IsaAL2rEUevb+eg==",
+                    "requires": {
+                        "regenerator-runtime": "^0.13.4"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/runtime/-/runtime-7.14.8.tgz",
+                    "version": "7.14.8"
+                }
+            },
+            "integrity": "sha512-eOf6vka5IO151Jfsw2NO9WpGX58W6wWmefK3I1zEGr0lOD0u8rwPaNqQL1aRxUaxLeKO3ArNh3VYg1KbaD+FFw==",
             "requires": {
-                "private": "^0.1.6"
+                "@babel/runtime": "^7.8.4"
             },
-            "resolved": "https://registry.npmjs.org/regenerator-transform/-/regenerator-transform-0.14.1.tgz",
-            "version": "0.14.1"
+            "resolved": "https://registry.npmjs.org/regenerator-transform/-/regenerator-transform-0.14.5.tgz",
+            "version": "0.14.5"
         },
         "regex-not": {
             "integrity": "sha512-J6SDjUgDxQj5NusnOtdFxDwN/+HWykR8GELwctJ7mdqhcyy1xEc4SRFHUXvxTp661YaVKAjfRLZ9cCqS6tn32A==",
             "requires": {
                 "extend-shallow": "^3.0.2",
                 "safe-regex": "^1.1.0"
             },
             "resolved": "https://registry.npmjs.org/regex-not/-/regex-not-1.0.2.tgz",
             "version": "1.0.2"
         },
-        "regexp-tree": {
-            "integrity": "sha512-7/l/DgapVVDzZobwMCCgMlqiqyLFJ0cduo/j+3BcDJIB+yJdsYCfKuI3l/04NV+H/rfNRdPIDbXNZHM9XvQatg==",
-            "resolved": "https://registry.npmjs.org/regexp-tree/-/regexp-tree-0.1.11.tgz",
-            "version": "0.1.11"
+        "regex-parser": {
+            "integrity": "sha512-8t6074A68gHfU8Neftl0Le6KTDwfGAj7IyjPIMSfikI2wJUTHDMaIq42bUsfVnj8mhx0R+45rdUXHGpN164avA==",
+            "resolved": "https://registry.npmjs.org/regex-parser/-/regex-parser-2.2.10.tgz",
+            "version": "2.2.10"
         },
         "regexpu-core": {
-            "integrity": "sha512-FpI67+ky9J+cDizQUJlIlNZFKual/lUkFr1AG6zOCpwZ9cLrg8UUVakyUQJD7fCDIe9Z2nwTQJNPyonatNmDFQ==",
+            "integrity": "sha512-ywH2VUraA44DZQuRKzARmw6S66mr48pQVva4LBeRhcOltJ6hExvWly5ZjFLYo67xbIxb6W1q4bAGtgfEl20zfQ==",
             "requires": {
                 "regenerate": "^1.4.0",
-                "regenerate-unicode-properties": "^8.1.0",
-                "regjsgen": "^0.5.0",
-                "regjsparser": "^0.6.0",
+                "regenerate-unicode-properties": "^8.2.0",
+                "regjsgen": "^0.5.1",
+                "regjsparser": "^0.6.4",
                 "unicode-match-property-ecmascript": "^1.0.4",
-                "unicode-match-property-value-ecmascript": "^1.1.0"
+                "unicode-match-property-value-ecmascript": "^1.2.0"
             },
-            "resolved": "https://registry.npmjs.org/regexpu-core/-/regexpu-core-4.5.5.tgz",
-            "version": "4.5.5"
+            "resolved": "https://registry.npmjs.org/regexpu-core/-/regexpu-core-4.7.1.tgz",
+            "version": "4.7.1"
         },
         "regjsgen": {
-            "integrity": "sha512-RnIrLhrXCX5ow/E5/Mh2O4e/oa1/jW0eaBKTSy3LaCj+M3Bqvm97GWDp2yUtzIs4LEn65zR2yiYGFqb2ApnzDA==",
-            "resolved": "https://registry.npmjs.org/regjsgen/-/regjsgen-0.5.0.tgz",
-            "version": "0.5.0"
+            "integrity": "sha512-OFFT3MfrH90xIW8OOSyUrk6QHD5E9JOTeGodiJeBS3J6IwlgzJMNE/1bZklWz5oTg+9dCMyEetclvCVXOPoN3A==",
+            "resolved": "https://registry.npmjs.org/regjsgen/-/regjsgen-0.5.2.tgz",
+            "version": "0.5.2"
         },
         "regjsparser": {
             "dependencies": {
                 "jsesc": {
                     "integrity": "sha1-597mbjXW/Bb3EP6R1c9p9w8IkR0=",
                     "resolved": "https://registry.npmjs.org/jsesc/-/jsesc-0.5.0.tgz",
                     "version": "0.5.0"
                 }
             },
-            "integrity": "sha512-RQ7YyokLiQBomUJuUG8iGVvkgOLxwyZM8k6d3q5SAXpg4r5TZJZigKFvC6PpD+qQ98bCDC5YelPeA3EucDoNeQ==",
+            "integrity": "sha512-ZqbNRz1SNjLAiYuwY0zoXW8Ne675IX5q+YHioAGbCw4X96Mjl2+dcX9B2ciaeyYjViDAfvIjFpQjJgLttTEERQ==",
             "requires": {
                 "jsesc": "~0.5.0"
             },
-            "resolved": "https://registry.npmjs.org/regjsparser/-/regjsparser-0.6.0.tgz",
-            "version": "0.6.0"
+            "resolved": "https://registry.npmjs.org/regjsparser/-/regjsparser-0.6.9.tgz",
+            "version": "0.6.9"
         },
         "remove-trailing-separator": {
             "integrity": "sha1-wkvOKig62tW8P1jg1IJJuSN52O8=",
+            "optional": true,
             "resolved": "https://registry.npmjs.org/remove-trailing-separator/-/remove-trailing-separator-1.1.0.tgz",
             "version": "1.1.0"
         },
         "repeat-element": {
-            "integrity": "sha512-ahGq0ZnV5m5XtZLMb+vP76kcAM5nkLqk0lpqAuojSKGgQtn4eRi4ZZGm2olo2zKFH+sMsWaqOCW1dqAnOru72g==",
-            "resolved": "https://registry.npmjs.org/repeat-element/-/repeat-element-1.1.3.tgz",
-            "version": "1.1.3"
+            "integrity": "sha512-LFiNfRcSu7KK3evMyYOuCzv3L10TW7yC1G2/+StMjK8Y6Vqd2MG7r/Qjw4ghtuCOjFvlnms/iMmLqpvW/ES/WQ==",
+            "resolved": "https://registry.npmjs.org/repeat-element/-/repeat-element-1.1.4.tgz",
+            "version": "1.1.4"
         },
         "repeat-string": {
             "integrity": "sha1-jcrkcOHIirwtYA//Sndihtp15jc=",
             "resolved": "https://registry.npmjs.org/repeat-string/-/repeat-string-1.6.1.tgz",
             "version": "1.6.1"
         },
+        "requires-port": {
+            "integrity": "sha1-kl0mAdOaxIXgkc8NpcbmlNw9yv8=",
+            "resolved": "https://registry.npmjs.org/requires-port/-/requires-port-1.0.0.tgz",
+            "version": "1.0.0"
+        },
         "resolve": {
-            "integrity": "sha512-B/dOmuoAik5bKcD6s6nXDCjzUKnaDvdkRyAk6rsmsKLipWj4797iothd7jmmUhWTfinVMU+wc56rYKsit2Qy4w==",
+            "integrity": "sha512-wENBPt4ySzg4ybFQW2TT1zMQucPK95HSh/nq2CFTZVOGut2+pQvSsgtda4d26YrYcr067wjbmzOG8byDPBX63A==",
             "requires": {
+                "is-core-module": "^2.2.0",
                 "path-parse": "^1.0.6"
             },
-            "resolved": "https://registry.npmjs.org/resolve/-/resolve-1.12.0.tgz",
-            "version": "1.12.0"
+            "resolved": "https://registry.npmjs.org/resolve/-/resolve-1.20.0.tgz",
+            "version": "1.20.0"
+        },
+        "resolve-from": {
+            "integrity": "sha1-six699nWiBvItuZTM17rywoYh0g=",
+            "resolved": "https://registry.npmjs.org/resolve-from/-/resolve-from-3.0.0.tgz",
+            "version": "3.0.0"
         },
         "resolve-url": {
             "integrity": "sha1-LGN/53yJOv0qZj/iGqkIAGjiBSo=",
             "resolved": "https://registry.npmjs.org/resolve-url/-/resolve-url-0.2.1.tgz",
             "version": "0.2.1"
         },
+        "resolve-url-loader": {
+            "dependencies": {
+                "convert-source-map": {
+                    "integrity": "sha512-4FJkXzKXEDB1snCFZlLP4gpC3JILicCpGbzG9f9G7tGqGCzETQ2hWPrcinA9oU4wtf2biUaEH5065UnMeR33oA==",
+                    "requires": {
+                        "safe-buffer": "~5.1.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/convert-source-map/-/convert-source-map-1.7.0.tgz",
+                    "version": "1.7.0"
+                },
+                "emojis-list": {
+                    "integrity": "sha1-TapNnbAPmBmIDHn6RXrlsJof04k=",
+                    "resolved": "https://registry.npmjs.org/emojis-list/-/emojis-list-2.1.0.tgz",
+                    "version": "2.1.0"
+                },
+                "json5": {
+                    "integrity": "sha512-aKS4WQjPenRxiQsC93MNfjx+nbF4PAdYzmd/1JIj8HYzqfbu86beTuNgXDzPknWk0n0uARlyewZo4s++ES36Ow==",
+                    "requires": {
+                        "minimist": "^1.2.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/json5/-/json5-1.0.1.tgz",
+                    "version": "1.0.1"
+                },
+                "loader-utils": {
+                    "integrity": "sha512-fkpz8ejdnEMG3s37wGL07iSBDg99O9D5yflE9RGNH3hRdx9SOwYfnGYdZOUIZitN8E+E2vkq3MUMYMvPYl5ZZA==",
+                    "requires": {
+                        "big.js": "^5.2.2",
+                        "emojis-list": "^2.0.0",
+                        "json5": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/loader-utils/-/loader-utils-1.2.3.tgz",
+                    "version": "1.2.3"
+                },
+                "postcss": {
+                    "integrity": "sha512-uIFtJElxJo29QC753JzhidoAhvp/e/Exezkdhfmt8AymWT6/5B7W1WmponYWkHk2eg6sONyTch0A3nkMPun3SQ==",
+                    "requires": {
+                        "chalk": "^2.4.2",
+                        "source-map": "^0.6.1",
+                        "supports-color": "^6.1.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/postcss/-/postcss-7.0.21.tgz",
+                    "version": "7.0.21"
+                },
+                "supports-color": {
+                    "integrity": "sha512-qe1jfm1Mg7Nq/NSh6XE24gPXROEVsWHxC1LIx//XNlD9iw7YZQGjZNjYN7xGaEG6iKdA8EtNFW6R0gjnVXp+wQ==",
+                    "requires": {
+                        "has-flag": "^3.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/supports-color/-/supports-color-6.1.0.tgz",
+                    "version": "6.1.0"
+                }
+            },
+            "integrity": "sha512-K1N5xUjj7v0l2j/3Sgs5b8CjrrgtC70SmdCuZiJ8tSyb5J+uk3FoeZ4b7yTnH6j7ngI+Bc5bldHJIa8hYdu2gQ==",
+            "requires": {
+                "adjust-sourcemap-loader": "2.0.0",
+                "camelcase": "5.3.1",
+                "compose-function": "3.0.3",
+                "convert-source-map": "1.7.0",
+                "es6-iterator": "2.0.3",
+                "loader-utils": "1.2.3",
+                "postcss": "7.0.21",
+                "rework": "1.0.1",
+                "rework-visit": "1.0.0",
+                "source-map": "0.6.1"
+            },
+            "resolved": "https://registry.npmjs.org/resolve-url-loader/-/resolve-url-loader-3.1.1.tgz",
+            "version": "3.1.1"
+        },
+        "restore-cursor": {
+            "integrity": "sha1-n37ih/gv0ybU/RYpI9YhKe7g368=",
+            "requires": {
+                "onetime": "^2.0.0",
+                "signal-exit": "^3.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/restore-cursor/-/restore-cursor-2.0.0.tgz",
+            "version": "2.0.0"
+        },
         "ret": {
             "integrity": "sha512-TTlYpa+OL+vMMNG24xSlQGEJ3B/RzEfUlLct7b5G/ytav+wPrplCpVMFuwzXbkecJrb6IYo1iFb0S9v37754mg==",
             "resolved": "https://registry.npmjs.org/ret/-/ret-0.1.15.tgz",
             "version": "0.1.15"
         },
+        "retry": {
+            "integrity": "sha1-G0KmJmoh8HQh0bC1S33BZ7AcATs=",
+            "resolved": "https://registry.npmjs.org/retry/-/retry-0.12.0.tgz",
+            "version": "0.12.0"
+        },
+        "rework": {
+            "dependencies": {
+                "convert-source-map": {
+                    "integrity": "sha1-8dgClQr33SYxof6+BZZVDIarMZA=",
+                    "resolved": "https://registry.npmjs.org/convert-source-map/-/convert-source-map-0.3.5.tgz",
+                    "version": "0.3.5"
+                }
+            },
+            "integrity": "sha1-MIBqhBNCtUUQqkEQhQzUhTQUSqc=",
+            "requires": {
+                "convert-source-map": "^0.3.3",
+                "css": "^2.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/rework/-/rework-1.0.1.tgz",
+            "version": "1.0.1"
+        },
+        "rework-visit": {
+            "integrity": "sha1-mUWygD8hni96ygCtuLyfZA+ELJo=",
+            "resolved": "https://registry.npmjs.org/rework-visit/-/rework-visit-1.0.0.tgz",
+            "version": "1.0.0"
+        },
         "rimraf": {
             "integrity": "sha512-uWjbaKIK3T1OSVptzX7Nl6PvQ3qAGtKEtVRjRuazjfL3Bx5eI409VZSqgND+4UNnmzLVdPj9FqFJNPqBZFve4w==",
             "requires": {
                 "glob": "^7.1.3"
             },
             "resolved": "https://registry.npmjs.org/rimraf/-/rimraf-2.7.1.tgz",
             "version": "2.7.1"
@@ -4658,32 +7044,69 @@
             "version": "1.1.0"
         },
         "safer-buffer": {
             "integrity": "sha512-YZo3K82SD7Riyi0E1EQPojLz7kpepnSQI9IyPbHHg1XXXevb5dJI7tpyN2ADxGcQbHG7vcyRHk0cbwqcQriUtg==",
             "resolved": "https://registry.npmjs.org/safer-buffer/-/safer-buffer-2.1.2.tgz",
             "version": "2.1.2"
         },
+        "sass-loader": {
+            "dependencies": {
+                "json5": {
+                    "integrity": "sha512-aKS4WQjPenRxiQsC93MNfjx+nbF4PAdYzmd/1JIj8HYzqfbu86beTuNgXDzPknWk0n0uARlyewZo4s++ES36Ow==",
+                    "requires": {
+                        "minimist": "^1.2.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/json5/-/json5-1.0.1.tgz",
+                    "version": "1.0.1"
+                },
+                "loader-utils": {
+                    "integrity": "sha512-qH0WSMBtn/oHuwjy/NucEgbx5dbxxnxup9s4PVXJUDHZBQY+s0NWA9rJf53RBnQZxfch7euUui7hpoAPvALZdA==",
+                    "requires": {
+                        "big.js": "^5.2.2",
+                        "emojis-list": "^3.0.0",
+                        "json5": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/loader-utils/-/loader-utils-1.4.0.tgz",
+                    "version": "1.4.0"
+                },
+                "semver": {
+                    "integrity": "sha512-b39TBaTSfV6yBrapU89p5fKekE2m/NwnDocOVruQFS1/veMgdzuPcnOM34M6CwxW8jH/lxEa5rBoDeUwu5HHTw==",
+                    "resolved": "https://registry.npmjs.org/semver/-/semver-6.3.0.tgz",
+                    "version": "6.3.0"
+                }
+            },
+            "integrity": "sha512-7o4dbSK8/Ol2KflEmSco4jTjQoV988bM82P9CZdmo9hR3RLnvNc0ufMNdMrB0caq38JQ/FgF4/7RcbcfKzxoFQ==",
+            "requires": {
+                "clone-deep": "^4.0.1",
+                "loader-utils": "^1.2.3",
+                "neo-async": "^2.6.1",
+                "schema-utils": "^2.6.1",
+                "semver": "^6.3.0"
+            },
+            "resolved": "https://registry.npmjs.org/sass-loader/-/sass-loader-8.0.2.tgz",
+            "version": "8.0.2"
+        },
         "scheduler": {
             "integrity": "sha512-xAefmSfN6jqAa7Kuq7LIJY0bwAPG3xlCj0HMEBQk1lxYiDKZscY2xJ5U/61ZTrYbmNQbXa+gc7czPkVo11tnCg==",
             "requires": {
                 "loose-envify": "^1.1.0",
                 "object-assign": "^4.1.1"
             },
             "resolved": "https://registry.npmjs.org/scheduler/-/scheduler-0.15.0.tgz",
             "version": "0.15.0"
         },
         "schema-utils": {
-            "integrity": "sha512-i27Mic4KovM/lnGsy8whRCHhc7VicJajAjTrYg11K9zfZXnYIt4k5F+kZkwjnrhKzLic/HLU4j11mjsz2G/75g==",
+            "integrity": "sha512-SHiNtMOUGWBQJwzISiVYKu82GiV4QYGePp3odlY1tuKO7gPtphAT5R/py0fA6xtbgLL/RvtJZnU9b8s0F1q0Xg==",
             "requires": {
-                "ajv": "^6.1.0",
-                "ajv-errors": "^1.0.0",
-                "ajv-keywords": "^3.1.0"
+                "@types/json-schema": "^7.0.5",
+                "ajv": "^6.12.4",
+                "ajv-keywords": "^3.5.2"
             },
-            "resolved": "https://registry.npmjs.org/schema-utils/-/schema-utils-1.0.0.tgz",
-            "version": "1.0.0"
+            "resolved": "https://registry.npmjs.org/schema-utils/-/schema-utils-2.7.1.tgz",
+            "version": "2.7.1"
         },
         "semver": {
             "integrity": "sha512-sauaDf/PZdVgrLTNYHRtpXa1iRiKcaebiKQ1BJdpQlWH2lCvexQdX55snPFyK7QzpudqbCI0qXFfOasHdyNDGQ==",
             "resolved": "https://registry.npmjs.org/semver/-/semver-5.7.1.tgz",
             "version": "5.7.1"
         },
         "send": {
@@ -4725,17 +7148,20 @@
                 "range-parser": "~1.2.1",
                 "statuses": "~1.5.0"
             },
             "resolved": "https://registry.npmjs.org/send/-/send-0.17.1.tgz",
             "version": "0.17.1"
         },
         "serialize-javascript": {
-            "integrity": "sha512-ke8UG8ulpFOxO8f8gRYabHQe/ZntKlcig2Mp+8+URDP1D8vJZ0KUt7LYo07q25Z/+JVSgpr/cui9PIp5H6/+nA==",
-            "resolved": "https://registry.npmjs.org/serialize-javascript/-/serialize-javascript-1.7.0.tgz",
-            "version": "1.7.0"
+            "integrity": "sha512-GaNA54380uFefWghODBWEGisLZFj00nS5ACs6yHa9nLqlLpVLO8ChDGeKRjZnV4Nh4n0Qi7nhYZD/9fCPzEqkw==",
+            "requires": {
+                "randombytes": "^2.1.0"
+            },
+            "resolved": "https://registry.npmjs.org/serialize-javascript/-/serialize-javascript-4.0.0.tgz",
+            "version": "4.0.0"
         },
         "set-value": {
             "dependencies": {
                 "extend-shallow": {
                     "integrity": "sha1-Ua99YUrZqfYQ6huvu5idaxxWiQ8=",
                     "requires": {
                         "is-extendable": "^0.1.0"
@@ -4769,18 +7195,31 @@
             "requires": {
                 "inherits": "^2.0.1",
                 "safe-buffer": "^5.0.1"
             },
             "resolved": "https://registry.npmjs.org/sha.js/-/sha.js-2.4.11.tgz",
             "version": "2.4.11"
         },
+        "shallow-clone": {
+            "integrity": "sha512-/6KqX+GVUdqPuPPd2LxDDxzX6CAbjJehAAOKlNpqqUpAqPM6HeL8f+o3a+JsyGjn2lv0WY8UsTgUJjU9Ok55NA==",
+            "requires": {
+                "kind-of": "^6.0.2"
+            },
+            "resolved": "https://registry.npmjs.org/shallow-clone/-/shallow-clone-3.0.1.tgz",
+            "version": "3.0.1"
+        },
         "shell-quote": {
-            "integrity": "sha512-2kUqeAGnMAu6YrTPX4E3LfxacH9gKljzVjlkUeSqY0soGwK4KLl7TURXCem712tkhBCeeaFP9QK4dKn88s3Icg==",
-            "resolved": "https://registry.npmjs.org/shell-quote/-/shell-quote-1.7.1.tgz",
-            "version": "1.7.1"
+            "integrity": "sha512-mRz/m/JVscCrkMyPqHc/bczi3OQHkLTqXHEFu0zDhK/qfv3UcOA4SVmRCLmos4bhjr9ekVQubj/R7waKapmiQg==",
+            "resolved": "https://registry.npmjs.org/shell-quote/-/shell-quote-1.7.2.tgz",
+            "version": "1.7.2"
+        },
+        "signal-exit": {
+            "integrity": "sha512-VUJ49FC8U1OxwZLxIbTTrDvLnf/6TDgxZcK8wxR8zs13xpx7xbG60ndBlhNrFi2EMuFRoeDoJO7wthSLq42EjA==",
+            "resolved": "https://registry.npmjs.org/signal-exit/-/signal-exit-3.0.3.tgz",
+            "version": "3.0.3"
         },
         "snapdragon": {
             "dependencies": {
                 "debug": {
                     "integrity": "sha512-bC7ElrdJaJnPbAP+1EotYvqZsb3ecl5wi6Bfi6BJTUcNowp6cvspg0jXznRTKDjm/E7AdgFBVeAPVMNcKGsHMA==",
                     "requires": {
                         "ms": "2.0.0"
@@ -4889,93 +7328,106 @@
             "integrity": "sha512-mbKkMdQKsjX4BAL4bRYTj21edOf8cN7XHdYUJEe+Zn99hVEYcMvKPct1IqNe7+AZPirn8BCDOQBHQZknqmKlZQ==",
             "requires": {
                 "kind-of": "^3.2.0"
             },
             "resolved": "https://registry.npmjs.org/snapdragon-util/-/snapdragon-util-3.0.1.tgz",
             "version": "3.0.1"
         },
+        "sort-keys": {
+            "integrity": "sha1-RBttTTRnmPG05J6JIK37oOVD+a0=",
+            "requires": {
+                "is-plain-obj": "^1.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/sort-keys/-/sort-keys-1.1.2.tgz",
+            "version": "1.1.2"
+        },
         "source-list-map": {
             "integrity": "sha512-qnQ7gVMxGNxsiL4lEuJwe/To8UnK7fAnmbGEEH8RpLouuKbeEm0lhbQVFIrNSuB+G7tVrAlVsZgETT5nljf+Iw==",
             "resolved": "https://registry.npmjs.org/source-list-map/-/source-list-map-2.0.1.tgz",
             "version": "2.0.1"
         },
         "source-map": {
             "integrity": "sha512-UjgapumWlbMhkBgzT7Ykc5YXUT46F0iKu8SGXq0bcwP5dz/h0Plj6enJqjz1Zbq2l5WaqYnrVbwWOWMyF3F47g==",
             "resolved": "https://registry.npmjs.org/source-map/-/source-map-0.6.1.tgz",
             "version": "0.6.1"
         },
         "source-map-resolve": {
-            "integrity": "sha512-MjqsvNwyz1s0k81Goz/9vRBe9SZdB09Bdw+/zYyO+3CuPk6fouTaxscHkgtE8jKvf01kVfl8riHzERQ/kefaSA==",
+            "integrity": "sha512-Htz+RnsXWk5+P2slx5Jh3Q66vhQj1Cllm0zvnaY98+NFx+Dv2CF/f5O/t8x+KaNdrdIAsruNzoh/KpialbqAnw==",
             "requires": {
-                "atob": "^2.1.1",
+                "atob": "^2.1.2",
                 "decode-uri-component": "^0.2.0",
                 "resolve-url": "^0.2.1",
                 "source-map-url": "^0.4.0",
                 "urix": "^0.1.0"
             },
-            "resolved": "https://registry.npmjs.org/source-map-resolve/-/source-map-resolve-0.5.2.tgz",
-            "version": "0.5.2"
+            "resolved": "https://registry.npmjs.org/source-map-resolve/-/source-map-resolve-0.5.3.tgz",
+            "version": "0.5.3"
         },
         "source-map-support": {
-            "integrity": "sha512-SHSKFHadjVA5oR4PPqhtAVdcBWwRYVd6g6cAXnIbRiIwc2EhPrTuKUBdSLvlEKyIP3GCf89fltvcZiP9MMFA1w==",
+            "integrity": "sha512-Wonm7zOCIJzBGQdB+thsPar0kYuCIzYvxZwlBa87yi/Mdjv7Tip2cyVbLj5o0cFPN4EVkuTwb3GDDyUx2DGnGw==",
             "requires": {
                 "buffer-from": "^1.0.0",
                 "source-map": "^0.6.0"
             },
-            "resolved": "https://registry.npmjs.org/source-map-support/-/source-map-support-0.5.13.tgz",
-            "version": "0.5.13"
+            "resolved": "https://registry.npmjs.org/source-map-support/-/source-map-support-0.5.19.tgz",
+            "version": "0.5.19"
         },
         "source-map-url": {
-            "integrity": "sha1-PpNdfd1zYxuXZZlW1VEo6HtQhKM=",
-            "resolved": "https://registry.npmjs.org/source-map-url/-/source-map-url-0.4.0.tgz",
-            "version": "0.4.0"
+            "integrity": "sha512-cPiFOTLUKvJFIg4SKVScy4ilPPW6rFgMgfuZJPNoDuMs3nC1HbMUycBoJw77xFIp6z1UJQJOfx6C9GMH80DiTw==",
+            "resolved": "https://registry.npmjs.org/source-map-url/-/source-map-url-0.4.1.tgz",
+            "version": "0.4.1"
         },
         "spdx-correct": {
-            "integrity": "sha512-lr2EZCctC2BNR7j7WzJ2FpDznxky1sjfxvvYEyzxNyb6lZXHODmEoJeFu4JupYlkfha1KZpJyoqiJ7pgA1qq8Q==",
+            "integrity": "sha512-cOYcUWwhCuHCXi49RhFRCyJEK3iPj1Ziz9DpViV3tbZOwXD49QzIN3MpOLJNxh2qwq2lJJZaKMVw9qNi4jTC0w==",
             "requires": {
                 "spdx-expression-parse": "^3.0.0",
                 "spdx-license-ids": "^3.0.0"
             },
-            "resolved": "https://registry.npmjs.org/spdx-correct/-/spdx-correct-3.1.0.tgz",
-            "version": "3.1.0"
+            "resolved": "https://registry.npmjs.org/spdx-correct/-/spdx-correct-3.1.1.tgz",
+            "version": "3.1.1"
         },
         "spdx-exceptions": {
-            "integrity": "sha512-2XQACfElKi9SlVb1CYadKDXvoajPgBVPn/gOQLrTvHdElaVhr7ZEbqJaRnJLVNeaI4cMEAgVCeBMKF6MWRDCRA==",
-            "resolved": "https://registry.npmjs.org/spdx-exceptions/-/spdx-exceptions-2.2.0.tgz",
-            "version": "2.2.0"
+            "integrity": "sha512-/tTrYOC7PPI1nUAgx34hUpqXuyJG+DTHJTnIULG4rDygi4xu/tfgmq1e1cIRwRzwZgo4NLySi+ricLkZkw4i5A==",
+            "resolved": "https://registry.npmjs.org/spdx-exceptions/-/spdx-exceptions-2.3.0.tgz",
+            "version": "2.3.0"
         },
         "spdx-expression-parse": {
-            "integrity": "sha512-Yg6D3XpRD4kkOmTpdgbUiEJFKghJH03fiC1OPll5h/0sO6neh2jqRDVHOQ4o/LMea0tgCkbMgea5ip/e+MkWyg==",
+            "integrity": "sha512-cbqHunsQWnJNE6KhVSMsMeH5H/L9EpymbzqTQ3uLwNCLZ1Q481oWaofqH7nO6V07xlXwY6PhQdQ2IedWx/ZK4Q==",
             "requires": {
                 "spdx-exceptions": "^2.1.0",
                 "spdx-license-ids": "^3.0.0"
             },
-            "resolved": "https://registry.npmjs.org/spdx-expression-parse/-/spdx-expression-parse-3.0.0.tgz",
-            "version": "3.0.0"
+            "resolved": "https://registry.npmjs.org/spdx-expression-parse/-/spdx-expression-parse-3.0.1.tgz",
+            "version": "3.0.1"
         },
         "spdx-license-ids": {
-            "integrity": "sha512-J+FWzZoynJEXGphVIS+XEh3kFSjZX/1i9gFBaWQcB+/tmpe2qUsSBABpcxqxnAxFdiUFEgAX1bjYGQvIZmoz9Q==",
-            "resolved": "https://registry.npmjs.org/spdx-license-ids/-/spdx-license-ids-3.0.5.tgz",
-            "version": "3.0.5"
+            "integrity": "sha512-Ki212dKK4ogX+xDo4CtOZBVIwhsKBEfsEEcwmJfLQzirgc2jIWdzg40Unxz/HzEUqM1WFzVlQSMF9kZZ2HboLQ==",
+            "resolved": "https://registry.npmjs.org/spdx-license-ids/-/spdx-license-ids-3.0.9.tgz",
+            "version": "3.0.9"
         },
         "split-string": {
             "integrity": "sha512-NzNVhJDYpwceVVii8/Hu6DKfD2G+NrQHlS/V/qgv763EYudVwEcMQNxd2lh+0VrUByXN/oJkl5grOhYWvQUYiw==",
             "requires": {
                 "extend-shallow": "^3.0.0"
             },
             "resolved": "https://registry.npmjs.org/split-string/-/split-string-3.1.0.tgz",
             "version": "3.1.0"
         },
+        "sprintf-js": {
+            "integrity": "sha1-BOaSb2YolTVPPdAVIDYzuFcpfiw=",
+            "resolved": "https://registry.npmjs.org/sprintf-js/-/sprintf-js-1.0.3.tgz",
+            "version": "1.0.3"
+        },
         "ssri": {
-            "integrity": "sha512-3Wge10hNcT1Kur4PDFwEieXSCMCJs/7WvSACcrMYrNp+b8kDL1/0wJch5Ni2WrtwEa2IO8OsVfeKIciKCDx/QA==",
+            "integrity": "sha512-cepbSq/neFK7xB6A50KHN0xHDotYzq58wWCa5LeWqnPrHG8GzfEjO/4O8kpmcGW+oaxkvhEJCWgbgNk4/ZV93Q==",
             "requires": {
                 "figgy-pudding": "^3.5.1"
             },
-            "resolved": "https://registry.npmjs.org/ssri/-/ssri-6.0.1.tgz",
-            "version": "6.0.1"
+            "resolved": "https://registry.npmjs.org/ssri/-/ssri-6.0.2.tgz",
+            "version": "6.0.2"
         },
         "static-extend": {
             "dependencies": {
                 "define-property": {
                     "integrity": "sha1-w1se+RjsPJkPmlvFe+BKrOxcgRY=",
                     "requires": {
                         "is-descriptor": "^0.1.0"
@@ -5024,17 +7476,22 @@
                 "to-arraybuffer": "^1.0.0",
                 "xtend": "^4.0.0"
             },
             "resolved": "https://registry.npmjs.org/stream-http/-/stream-http-2.8.3.tgz",
             "version": "2.8.3"
         },
         "stream-shift": {
-            "integrity": "sha1-1cdSgl5TZ+eG944Y5EXqIjoVWVI=",
-            "resolved": "https://registry.npmjs.org/stream-shift/-/stream-shift-1.0.0.tgz",
-            "version": "1.0.0"
+            "integrity": "sha512-AiisoFqQ0vbGcZgQPY1cdP2I76glaVA/RauYR4G4thNFgkTqr90yXTo4LYX60Jl+sIlPNHHdGSwo01AvbKUSVQ==",
+            "resolved": "https://registry.npmjs.org/stream-shift/-/stream-shift-1.0.1.tgz",
+            "version": "1.0.1"
+        },
+        "strict-uri-encode": {
+            "integrity": "sha1-J5siXfHVgrH1TmWt3UNS4Y+qBxM=",
+            "resolved": "https://registry.npmjs.org/strict-uri-encode/-/strict-uri-encode-1.1.0.tgz",
+            "version": "1.1.0"
         },
         "string-hash": {
             "integrity": "sha1-6Kr8CsGFW0Zmkp7X3RJ1311sgRs=",
             "resolved": "https://registry.npmjs.org/string-hash/-/string-hash-1.1.3.tgz",
             "version": "1.1.3"
         },
         "string_decoder": {
@@ -5061,14 +7518,43 @@
             "version": "5.2.0"
         },
         "strip-bom": {
             "integrity": "sha1-IzTBjpx1n3vdVv3vfprj1YjmjtM=",
             "resolved": "https://registry.npmjs.org/strip-bom/-/strip-bom-3.0.0.tgz",
             "version": "3.0.0"
         },
+        "style-loader": {
+            "dependencies": {
+                "json5": {
+                    "integrity": "sha512-aKS4WQjPenRxiQsC93MNfjx+nbF4PAdYzmd/1JIj8HYzqfbu86beTuNgXDzPknWk0n0uARlyewZo4s++ES36Ow==",
+                    "requires": {
+                        "minimist": "^1.2.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/json5/-/json5-1.0.1.tgz",
+                    "version": "1.0.1"
+                },
+                "loader-utils": {
+                    "integrity": "sha512-qH0WSMBtn/oHuwjy/NucEgbx5dbxxnxup9s4PVXJUDHZBQY+s0NWA9rJf53RBnQZxfch7euUui7hpoAPvALZdA==",
+                    "requires": {
+                        "big.js": "^5.2.2",
+                        "emojis-list": "^3.0.0",
+                        "json5": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/loader-utils/-/loader-utils-1.4.0.tgz",
+                    "version": "1.4.0"
+                }
+            },
+            "integrity": "sha512-B0dOCFwv7/eY31a5PCieNwMgMhVGFe9w+rh7s/Bx8kfFkrth9zfTZquoYvdw8URgiqxObQKcpW51Ugz1HjfdZw==",
+            "requires": {
+                "loader-utils": "^1.2.3",
+                "schema-utils": "^2.0.1"
+            },
+            "resolved": "https://registry.npmjs.org/style-loader/-/style-loader-1.0.0.tgz",
+            "version": "1.0.0"
+        },
         "styled-components": {
             "integrity": "sha512-RNqj14kYzw++6Sr38n7197xG33ipEOktGElty4I70IKzQF1jzaD1U4xQ+Ny/i03UUhHlC5NWEO+d8olRCDji6g==",
             "requires": {
                 "@babel/helper-module-imports": "^7.0.0",
                 "@babel/traverse": "^7.0.0",
                 "@emotion/is-prop-valid": "^0.8.1",
                 "@emotion/unitless": "^0.7.0",
@@ -5083,33 +7569,74 @@
                 "supports-color": "^5.5.0"
             },
             "resolved": "https://registry.npmjs.org/styled-components/-/styled-components-4.4.1.tgz",
             "version": "4.4.1"
         },
         "styled-jsx": {
             "dependencies": {
+                "@babel/types": {
+                    "integrity": "sha512-jBD+G8+LWpMBBWvVcdr4QysjUE4mU/syrhN17o1u3gx0/WzJB1kwiVZAXRtWbsIPOwW8pF/YJV5+nmetPzepXg==",
+                    "requires": {
+                        "esutils": "^2.0.2",
+                        "lodash": "^4.17.13",
+                        "to-fast-properties": "^2.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.8.3.tgz",
+                    "version": "7.8.3"
+                },
+                "convert-source-map": {
+                    "integrity": "sha512-4FJkXzKXEDB1snCFZlLP4gpC3JILicCpGbzG9f9G7tGqGCzETQ2hWPrcinA9oU4wtf2biUaEH5065UnMeR33oA==",
+                    "requires": {
+                        "safe-buffer": "~5.1.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/convert-source-map/-/convert-source-map-1.7.0.tgz",
+                    "version": "1.7.0"
+                },
+                "emojis-list": {
+                    "integrity": "sha1-TapNnbAPmBmIDHn6RXrlsJof04k=",
+                    "resolved": "https://registry.npmjs.org/emojis-list/-/emojis-list-2.1.0.tgz",
+                    "version": "2.1.0"
+                },
+                "json5": {
+                    "integrity": "sha512-aKS4WQjPenRxiQsC93MNfjx+nbF4PAdYzmd/1JIj8HYzqfbu86beTuNgXDzPknWk0n0uARlyewZo4s++ES36Ow==",
+                    "requires": {
+                        "minimist": "^1.2.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/json5/-/json5-1.0.1.tgz",
+                    "version": "1.0.1"
+                },
+                "loader-utils": {
+                    "integrity": "sha512-fkpz8ejdnEMG3s37wGL07iSBDg99O9D5yflE9RGNH3hRdx9SOwYfnGYdZOUIZitN8E+E2vkq3MUMYMvPYl5ZZA==",
+                    "requires": {
+                        "big.js": "^5.2.2",
+                        "emojis-list": "^2.0.0",
+                        "json5": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/loader-utils/-/loader-utils-1.2.3.tgz",
+                    "version": "1.2.3"
+                },
                 "source-map": {
                     "integrity": "sha512-CkCj6giN3S+n9qrYiBTX5gystlENnRW5jZeNLHpe6aue+SrHcG5VYwujhW9s4dY31mEGsxBDrHR6oI69fTXsaQ==",
                     "resolved": "https://registry.npmjs.org/source-map/-/source-map-0.7.3.tgz",
                     "version": "0.7.3"
                 }
             },
-            "integrity": "sha512-gM/WOrWYRpWReivzQqetEGohUc/TJSvUoZ5T/UJxJZIsVIPlRQLnp7R8Oue4q49sI08EBRQjQl2oBL3sfdrw2g==",
+            "integrity": "sha512-prEahkYwQHomUljJzXzrFnBmQrSMtWOBbXn8QeEkpfFkqMZQGshxzzp4H8ebBIsbVlHF/3+GSXMnmK/fp7qVYQ==",
             "requires": {
+                "@babel/types": "7.8.3",
                 "babel-plugin-syntax-jsx": "6.18.0",
-                "babel-types": "6.26.0",
-                "convert-source-map": "1.6.0",
+                "convert-source-map": "1.7.0",
                 "loader-utils": "1.2.3",
                 "source-map": "0.7.3",
                 "string-hash": "1.1.3",
                 "stylis": "3.5.4",
                 "stylis-rule-sheet": "0.0.10"
             },
-            "resolved": "https://registry.npmjs.org/styled-jsx/-/styled-jsx-3.2.1.tgz",
-            "version": "3.2.1"
+            "resolved": "https://registry.npmjs.org/styled-jsx/-/styled-jsx-3.2.5.tgz",
+            "version": "3.2.5"
         },
         "stylis": {
             "integrity": "sha512-8/3pSmthWM7lsPBKv7NXkzn2Uc9W7NotcwGNpJaa3k7WMM1XDCA4MgT5k/8BIexd5ydZdboXtU90XH9Ec4Bv/Q==",
             "resolved": "https://registry.npmjs.org/stylis/-/stylis-3.5.4.tgz",
             "version": "3.5.4"
         },
         "stylis-rule-sheet": {
@@ -5127,37 +7654,25 @@
         },
         "tapable": {
             "integrity": "sha512-4WK/bYZmj8xLr+HUCODHGF1ZFzsYffasLUgEiMBY4fgtltdO6B4WJtlSbPaDTLpYTcGVwM2qLnFTICEcNxs3kA==",
             "resolved": "https://registry.npmjs.org/tapable/-/tapable-1.1.3.tgz",
             "version": "1.1.3"
         },
         "terser": {
-            "dependencies": {
-                "commander": {
-                    "integrity": "sha512-7j2y+40w61zy6YC2iRNpUe/NwhNyoXrYpHMrSunaMG64nRnaf96zO/KMQR4OyN/UnE5KLyEBnKHd4aG3rskjpQ==",
-                    "resolved": "https://registry.npmjs.org/commander/-/commander-2.20.0.tgz",
-                    "version": "2.20.0"
-                }
-            },
-            "integrity": "sha512-dOapGTU0hETFl1tCo4t56FN+2jffoKyER9qBGoUFyZ6y7WLoKT0bF+lAYi6B6YsILcGF3q1C2FBh8QcKSCgkgA==",
+            "integrity": "sha512-Uufrsvhj9O1ikwgITGsZ5EZS6qPokUOkCegS7fYOdGTv+OA90vndUbU6PEjr5ePqHfNUbGyMO7xyIZv2MhsALQ==",
             "requires": {
-                "commander": "^2.19.0",
+                "commander": "^2.20.0",
                 "source-map": "~0.6.1",
-                "source-map-support": "~0.5.10"
+                "source-map-support": "~0.5.12"
             },
-            "resolved": "https://registry.npmjs.org/terser/-/terser-4.0.0.tgz",
-            "version": "4.0.0"
+            "resolved": "https://registry.npmjs.org/terser/-/terser-4.4.2.tgz",
+            "version": "4.4.2"
         },
         "terser-webpack-plugin": {
             "dependencies": {
-                "commander": {
-                    "integrity": "sha512-7j2y+40w61zy6YC2iRNpUe/NwhNyoXrYpHMrSunaMG64nRnaf96zO/KMQR4OyN/UnE5KLyEBnKHd4aG3rskjpQ==",
-                    "resolved": "https://registry.npmjs.org/commander/-/commander-2.20.0.tgz",
-                    "version": "2.20.0"
-                },
                 "find-cache-dir": {
                     "integrity": "sha512-Tq6PixE0w/VMFfCgbONnkiQIVol/JJL7nRMi20fqzA4NRs9AfeqMGeRdPi3wIhYkxjeBaWh2rxwapn5Tu3IqOQ==",
                     "requires": {
                         "commondir": "^1.0.1",
                         "make-dir": "^2.0.0",
                         "pkg-dir": "^3.0.0"
                     },
@@ -5168,14 +7683,19 @@
                     "integrity": "sha512-1yD6RmLI1XBfxugvORwlck6f75tYL+iR0jqwsOrOxMZyGYqUuDhJ0l4AXdO1iX/FTs9cBAMEk1gWSEx1kSbylg==",
                     "requires": {
                         "locate-path": "^3.0.0"
                     },
                     "resolved": "https://registry.npmjs.org/find-up/-/find-up-3.0.0.tgz",
                     "version": "3.0.0"
                 },
+                "is-wsl": {
+                    "integrity": "sha1-HxbkqiKwTRM2tmGIpmrzxgDDpm0=",
+                    "resolved": "https://registry.npmjs.org/is-wsl/-/is-wsl-1.1.0.tgz",
+                    "version": "1.1.0"
+                },
                 "locate-path": {
                     "integrity": "sha512-7AO748wWnIhNqAuaty2ZWHkQHRSNfPVIsPIfwEOWO22AmaoVrWavlOcMR5nzTLNYvp36X220/maaRsrec1G65A==",
                     "requires": {
                         "p-locate": "^3.0.0",
                         "path-exists": "^3.0.0"
                     },
                     "resolved": "https://registry.npmjs.org/locate-path/-/locate-path-3.0.0.tgz",
@@ -5187,20 +7707,20 @@
                         "pify": "^4.0.1",
                         "semver": "^5.6.0"
                     },
                     "resolved": "https://registry.npmjs.org/make-dir/-/make-dir-2.1.0.tgz",
                     "version": "2.1.0"
                 },
                 "p-limit": {
-                    "integrity": "sha512-85Tk+90UCVWvbDavCLKPOLC9vvY8OwEX/RtKF+/1OADJMVlFfEHOiMTPVyxg7mk/dKa+ipdHm0OUkTvCpMTuwg==",
+                    "integrity": "sha512-//88mFWSJx8lxCzwdAABTJL2MyWB12+eIY7MDL2SqLmAkeKU9qxRvWuSyTjm3FUmpBEMuFfckAIqEaVGUDxb6w==",
                     "requires": {
                         "p-try": "^2.0.0"
                     },
-                    "resolved": "https://registry.npmjs.org/p-limit/-/p-limit-2.2.1.tgz",
-                    "version": "2.2.1"
+                    "resolved": "https://registry.npmjs.org/p-limit/-/p-limit-2.3.0.tgz",
+                    "version": "2.3.0"
                 },
                 "p-locate": {
                     "integrity": "sha512-x+12w/To+4GFfgJhBEpiDcLozRJGegY+Ei7/z0tSLkMmxGZNybVMSfWj9aJn8Z5Fc7dBUNJOOVgPv2H7IwulSQ==",
                     "requires": {
                         "p-limit": "^2.0.0"
                     },
                     "resolved": "https://registry.npmjs.org/p-locate/-/p-locate-3.0.0.tgz",
@@ -5220,65 +7740,86 @@
                     "integrity": "sha512-/E57AYkoeQ25qkxMj5PBOVgF8Kiu/h7cYS30Z5+R7WaiCCBfLq58ZI/dSeaEKb9WVJV5n/03QwrN3IeWIFllvw==",
                     "requires": {
                         "find-up": "^3.0.0"
                     },
                     "resolved": "https://registry.npmjs.org/pkg-dir/-/pkg-dir-3.0.0.tgz",
                     "version": "3.0.0"
                 },
-                "terser": {
-                    "integrity": "sha512-6lPt7lZdZ/13icQJp8XasFOwZjFJkxFFIb/N1fhYEQNoNI3Ilo3KABZ9OocZvZoB39r6SiIk/0+v/bt8nZoSeA==",
+                "schema-utils": {
+                    "integrity": "sha512-i27Mic4KovM/lnGsy8whRCHhc7VicJajAjTrYg11K9zfZXnYIt4k5F+kZkwjnrhKzLic/HLU4j11mjsz2G/75g==",
                     "requires": {
-                        "commander": "^2.20.0",
-                        "source-map": "~0.6.1",
-                        "source-map-support": "~0.5.12"
+                        "ajv": "^6.1.0",
+                        "ajv-errors": "^1.0.0",
+                        "ajv-keywords": "^3.1.0"
                     },
-                    "resolved": "https://registry.npmjs.org/terser/-/terser-4.2.0.tgz",
-                    "version": "4.2.0"
-                },
-                "webpack-sources": {
-                    "integrity": "sha512-lgTS3Xhv1lCOKo7SA5TjKXMjpSM4sBjNV5+q2bqesbSPs5FjGmU6jjtBSkX9b4qW87vDIsCIlUPOEhbZrMdjeQ==",
-                    "requires": {
-                        "source-list-map": "^2.0.0",
-                        "source-map": "~0.6.1"
-                    },
-                    "resolved": "https://registry.npmjs.org/webpack-sources/-/webpack-sources-1.4.3.tgz",
-                    "version": "1.4.3"
+                    "resolved": "https://registry.npmjs.org/schema-utils/-/schema-utils-1.0.0.tgz",
+                    "version": "1.0.0"
                 }
             },
-            "integrity": "sha512-ZXmmfiwtCLfz8WKZyYUuuHf3dMYEjg8NrjHMb0JqHVHVOSkzp3cW2/XG1fP3tRhqEqSzMwzzRQGtAPbs4Cncxg==",
+            "integrity": "sha512-04Rfe496lN8EYruwi6oPQkG0vo8C+HT49X687FZnpPF0qMAIHONI6HEXYPKDOE8e5HjXTyKfqRd/agHtH0kOtw==",
             "requires": {
                 "cacache": "^12.0.2",
                 "find-cache-dir": "^2.1.0",
                 "is-wsl": "^1.1.0",
                 "schema-utils": "^1.0.0",
-                "serialize-javascript": "^1.7.0",
+                "serialize-javascript": "^4.0.0",
                 "source-map": "^0.6.1",
                 "terser": "^4.1.2",
                 "webpack-sources": "^1.4.0",
                 "worker-farm": "^1.7.0"
             },
-            "resolved": "https://registry.npmjs.org/terser-webpack-plugin/-/terser-webpack-plugin-1.4.1.tgz",
-            "version": "1.4.1"
+            "resolved": "https://registry.npmjs.org/terser-webpack-plugin/-/terser-webpack-plugin-1.4.5.tgz",
+            "version": "1.4.5"
+        },
+        "thread-loader": {
+            "dependencies": {
+                "json5": {
+                    "integrity": "sha512-aKS4WQjPenRxiQsC93MNfjx+nbF4PAdYzmd/1JIj8HYzqfbu86beTuNgXDzPknWk0n0uARlyewZo4s++ES36Ow==",
+                    "requires": {
+                        "minimist": "^1.2.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/json5/-/json5-1.0.1.tgz",
+                    "version": "1.0.1"
+                },
+                "loader-utils": {
+                    "integrity": "sha512-qH0WSMBtn/oHuwjy/NucEgbx5dbxxnxup9s4PVXJUDHZBQY+s0NWA9rJf53RBnQZxfch7euUui7hpoAPvALZdA==",
+                    "requires": {
+                        "big.js": "^5.2.2",
+                        "emojis-list": "^3.0.0",
+                        "json5": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/loader-utils/-/loader-utils-1.4.0.tgz",
+                    "version": "1.4.0"
+                }
+            },
+            "integrity": "sha512-wNrVKH2Lcf8ZrWxDF/khdlLlsTMczdcwPA9VEK4c2exlEPynYWxi9op3nPTo5lAnDIkE0rQEB3VBP+4Zncc9Hg==",
+            "requires": {
+                "loader-runner": "^2.3.1",
+                "loader-utils": "^1.1.0",
+                "neo-async": "^2.6.0"
+            },
+            "resolved": "https://registry.npmjs.org/thread-loader/-/thread-loader-2.1.3.tgz",
+            "version": "2.1.3"
         },
         "through2": {
             "integrity": "sha512-/mrRod8xqpA+IHSLyGCQ2s8SPHiCDEeQJSep1jqLYeEUClOFG2Qsh+4FU6G9VeqpZnGW/Su8LQGc4YKni5rYSQ==",
             "requires": {
                 "readable-stream": "~2.3.6",
                 "xtend": "~4.0.1"
             },
             "resolved": "https://registry.npmjs.org/through2/-/through2-2.0.5.tgz",
             "version": "2.0.5"
         },
         "timers-browserify": {
-            "integrity": "sha512-60aV6sgJ5YEbzUdn9c8kYGIqOubPoUdqQCul3SBAsRCZ40s6Y5cMcrW4dt3/k/EsbLVJNl9n6Vz3fTc+k2GeKQ==",
+            "integrity": "sha512-9phl76Cqm6FhSX9Xe1ZUAMLtm1BLkKj2Qd5ApyWkXzsMRaA7dgr81kf4wJmQf/hAvg8EEyJxDo3du/0KlhPiKQ==",
             "requires": {
                 "setimmediate": "^1.0.4"
             },
-            "resolved": "https://registry.npmjs.org/timers-browserify/-/timers-browserify-2.0.11.tgz",
-            "version": "2.0.11"
+            "resolved": "https://registry.npmjs.org/timers-browserify/-/timers-browserify-2.0.12.tgz",
+            "version": "2.0.12"
         },
         "to-arraybuffer": {
             "integrity": "sha1-fSKbH8xjfkZsoIEYCDanqr/4P0M=",
             "resolved": "https://registry.npmjs.org/to-arraybuffer/-/to-arraybuffer-1.0.1.tgz",
             "version": "1.0.1"
         },
         "to-fast-properties": {
@@ -5312,21 +7853,20 @@
                 "regex-not": "^1.0.2",
                 "safe-regex": "^1.1.0"
             },
             "resolved": "https://registry.npmjs.org/to-regex/-/to-regex-3.0.2.tgz",
             "version": "3.0.2"
         },
         "to-regex-range": {
-            "integrity": "sha1-fIDBe53+vlmeJzZ+DU3VWQFB2zg=",
+            "integrity": "sha512-65P7iz6X5yEr1cwcgvQxbbIw7Uk3gOy5dIdtZ4rDveLqhrdJP+Li/Hx6tyK0NEb+2GCyneCMJiGqrADCSNk8sQ==",
             "requires": {
-                "is-number": "^3.0.0",
-                "repeat-string": "^1.6.1"
+                "is-number": "^7.0.0"
             },
-            "resolved": "https://registry.npmjs.org/to-regex-range/-/to-regex-range-2.1.1.tgz",
-            "version": "2.1.1"
+            "resolved": "https://registry.npmjs.org/to-regex-range/-/to-regex-range-5.0.1.tgz",
+            "version": "5.0.1"
         },
         "toidentifier": {
             "integrity": "sha512-yaOH/Pk/VEhBWWTlhI+qXxDFXlejDGcQipMlyxda9nthulaxLZUNcUqFxokp0vcYnvteJln5FNQDRrxj3YcbVw==",
             "resolved": "https://registry.npmjs.org/toidentifier/-/toidentifier-1.0.0.tgz",
             "version": "1.0.0"
         },
         "traverse": {
@@ -5335,37 +7875,42 @@
             "version": "0.6.6"
         },
         "trim-right": {
             "integrity": "sha1-yy4SAwZ+DI3h9hQJS5/kVwTqYAM=",
             "resolved": "https://registry.npmjs.org/trim-right/-/trim-right-1.0.1.tgz",
             "version": "1.0.1"
         },
-        "tslib": {
-            "integrity": "sha512-qOebF53frne81cf0S9B41ByenJ3/IuH8yJKngAX35CmiZySA0khhkovshKK+jGCaMnVomla7gVlIcc3EvKPbTQ==",
-            "resolved": "https://registry.npmjs.org/tslib/-/tslib-1.10.0.tgz",
-            "version": "1.10.0"
-        },
-        "tty-aware-progress": {
-            "integrity": "sha512-ynqjeu8FOAjnv78ku9iHSS9zJB9d4SNPeUAskOsTJfwdpGjJchSVmzngTUQZpg5hXqvE3vWF5FjN5SAHiutA0w==",
-            "requires": {
-                "progress": "2.0.3"
-            },
-            "resolved": "https://registry.npmjs.org/tty-aware-progress/-/tty-aware-progress-1.0.4.tgz",
-            "version": "1.0.4"
+        "ts-pnp": {
+            "integrity": "sha512-csd+vJOb/gkzvcCHgTGSChYpy5f1/XKNsmvBGO4JXS+z1v2HobugDz4s1IeFXM3wZB44uczs+eazB5Q/ccdhQw==",
+            "resolved": "https://registry.npmjs.org/ts-pnp/-/ts-pnp-1.2.0.tgz",
+            "version": "1.2.0"
         },
         "tty-browserify": {
             "integrity": "sha1-oVe6QC2iTpv5V/mqadUk7tQpAaY=",
             "resolved": "https://registry.npmjs.org/tty-browserify/-/tty-browserify-0.0.0.tgz",
             "version": "0.0.0"
         },
+        "type": {
+            "integrity": "sha512-+5nt5AAniqsCnu2cEQQdpzCAh33kVx8n0VoFidKpB1dVVLAN/F+bgVOqOJqOnEnrhp222clB5p3vUlD+1QAnfg==",
+            "resolved": "https://registry.npmjs.org/type/-/type-1.2.0.tgz",
+            "version": "1.2.0"
+        },
         "typedarray": {
             "integrity": "sha1-hnrHTjhkGHsdPUfZlqeOxciDB3c=",
             "resolved": "https://registry.npmjs.org/typedarray/-/typedarray-0.0.6.tgz",
             "version": "0.0.6"
         },
+        "typedarray-to-buffer": {
+            "integrity": "sha512-zdu8XMNEDepKKR+XYOXAVPtWui0ly0NtohUscw+UmaHiAWT8hrV1rr//H6V+0DvJ3OQ19S979M0laLfX8rm82Q==",
+            "requires": {
+                "is-typedarray": "^1.0.0"
+            },
+            "resolved": "https://registry.npmjs.org/typedarray-to-buffer/-/typedarray-to-buffer-3.1.5.tgz",
+            "version": "3.1.5"
+        },
         "typescript": {
             "dev": true,
             "integrity": "sha512-BLbiRkiBzAwsjut4x/dsibSTB6yWpwT5qWmC2OfuCg3GgVQCSgMs4vEctYPhsaGtd0AeuuHMkjZ2h2WG8MSzRw==",
             "resolved": "https://registry.npmjs.org/typescript/-/typescript-3.9.7.tgz",
             "version": "3.9.7"
         },
         "unfetch": {
@@ -5384,34 +7929,39 @@
                 "unicode-canonical-property-names-ecmascript": "^1.0.4",
                 "unicode-property-aliases-ecmascript": "^1.0.4"
             },
             "resolved": "https://registry.npmjs.org/unicode-match-property-ecmascript/-/unicode-match-property-ecmascript-1.0.4.tgz",
             "version": "1.0.4"
         },
         "unicode-match-property-value-ecmascript": {
-            "integrity": "sha512-hDTHvaBk3RmFzvSl0UVrUmC3PuW9wKVnpoUDYH0JDkSIovzw+J5viQmeYHxVSBptubnr7PbH2e0fnpDRQnQl5g==",
-            "resolved": "https://registry.npmjs.org/unicode-match-property-value-ecmascript/-/unicode-match-property-value-ecmascript-1.1.0.tgz",
-            "version": "1.1.0"
+            "integrity": "sha512-wjuQHGQVofmSJv1uVISKLE5zO2rNGzM/KCYZch/QQvez7C1hUhBIuZ701fYXExuufJFMPhv2SyL8CyoIfMLbIQ==",
+            "resolved": "https://registry.npmjs.org/unicode-match-property-value-ecmascript/-/unicode-match-property-value-ecmascript-1.2.0.tgz",
+            "version": "1.2.0"
         },
         "unicode-property-aliases-ecmascript": {
-            "integrity": "sha512-L5RAqCfXqAwR3RriF8pM0lU0w4Ryf/GgzONwi6KnL1taJQa7x1TCxdJnILX59WIGOwR57IVxn7Nej0fz1Ny6fw==",
-            "resolved": "https://registry.npmjs.org/unicode-property-aliases-ecmascript/-/unicode-property-aliases-ecmascript-1.0.5.tgz",
-            "version": "1.0.5"
+            "integrity": "sha512-PqSoPh/pWetQ2phoj5RLiaqIk4kCNwoV3CI+LfGmWLKI3rE3kl1h59XpX2BjgDrmbxD9ARtQobPGU1SguCYuQg==",
+            "resolved": "https://registry.npmjs.org/unicode-property-aliases-ecmascript/-/unicode-property-aliases-ecmascript-1.1.0.tgz",
+            "version": "1.1.0"
         },
         "union-value": {
             "integrity": "sha512-tJfXmxMeWYnczCVs7XAEvIV7ieppALdyepWMkHkwciRpZraG/xwT+s2JN8+pr1+8jCRf80FFzvr+MpQeeoF4Xg==",
             "requires": {
                 "arr-union": "^3.1.0",
                 "get-value": "^2.0.6",
                 "is-extendable": "^0.1.1",
                 "set-value": "^2.0.1"
             },
             "resolved": "https://registry.npmjs.org/union-value/-/union-value-1.0.1.tgz",
             "version": "1.0.1"
         },
+        "uniq": {
+            "integrity": "sha1-sxxa6CVIRKOoKBVBzisEuGWnNP8=",
+            "resolved": "https://registry.npmjs.org/uniq/-/uniq-1.0.1.tgz",
+            "version": "1.0.1"
+        },
         "unique-filename": {
             "integrity": "sha512-Vmp0jIp2ln35UTXuryvjzkjGdRyf9b2lTXuSYUiPmzRcl3FDtYqAwOnTJkAngD9SWhnoJzDbTKwaOrZ+STtxNQ==",
             "requires": {
                 "unique-slug": "^2.0.0"
             },
             "resolved": "https://registry.npmjs.org/unique-filename/-/unique-filename-1.1.1.tgz",
             "version": "1.1.1"
@@ -5462,76 +8012,87 @@
                 "has-value": "^0.3.1",
                 "isobject": "^3.0.0"
             },
             "resolved": "https://registry.npmjs.org/unset-value/-/unset-value-1.0.0.tgz",
             "version": "1.0.0"
         },
         "upath": {
-            "integrity": "sha512-kXpym8nmDmlCBr7nKdIx8P2jNBa+pBpIUFRnKJ4dr8htyYGJFokkr2ZvERRtUN+9SY+JqXouNgUPtv6JQva/2Q==",
-            "resolved": "https://registry.npmjs.org/upath/-/upath-1.1.2.tgz",
-            "version": "1.1.2"
+            "integrity": "sha512-aZwGpamFO61g3OlfT7OQCHqhGnW43ieH9WZeP7QxN/G/jS4jfqUkZxoryvJgVPEcrl5NL/ggHsSmLMHuH64Lhg==",
+            "optional": true,
+            "resolved": "https://registry.npmjs.org/upath/-/upath-1.2.0.tgz",
+            "version": "1.2.0"
         },
         "uri-js": {
-            "dependencies": {
-                "punycode": {
-                    "integrity": "sha512-XRsRjdf+j5ml+y/6GKHPZbrF/8p2Yga0JPtdqTIY2Xe5ohJPD9saDJJLPvp9+NSBprVvevdXZybnj2cv8OEd0A==",
-                    "resolved": "https://registry.npmjs.org/punycode/-/punycode-2.1.1.tgz",
-                    "version": "2.1.1"
-                }
-            },
-            "integrity": "sha512-KY9Frmirql91X2Qgjry0Wd4Y+YTdrdZheS8TFwvkbLWf/G5KNJDCh6pKL5OZctEW4+0Baa5idK2ZQuELRwPznQ==",
+            "integrity": "sha512-7rKUyy33Q1yc98pQ1DAmLtwX109F7TIfWlW1Ydo8Wl1ii1SeHieeh0HHfPeL2fMXK6z0s8ecKs9frCuLJvndBg==",
             "requires": {
                 "punycode": "^2.1.0"
             },
-            "resolved": "https://registry.npmjs.org/uri-js/-/uri-js-4.2.2.tgz",
-            "version": "4.2.2"
+            "resolved": "https://registry.npmjs.org/uri-js/-/uri-js-4.4.1.tgz",
+            "version": "4.4.1"
         },
         "urix": {
             "integrity": "sha1-2pN/emLiH+wf0Y1Js1wpNQZ6bHI=",
             "resolved": "https://registry.npmjs.org/urix/-/urix-0.1.0.tgz",
             "version": "0.1.0"
         },
         "url": {
+            "dependencies": {
+                "punycode": {
+                    "integrity": "sha1-llOgNvt8HuQjQvIyXM7v6jkmxI0=",
+                    "resolved": "https://registry.npmjs.org/punycode/-/punycode-1.3.2.tgz",
+                    "version": "1.3.2"
+                },
+                "querystring": {
+                    "integrity": "sha1-sgmEkgO7Jd+CDadW50cAWHhSFiA=",
+                    "resolved": "https://registry.npmjs.org/querystring/-/querystring-0.2.0.tgz",
+                    "version": "0.2.0"
+                }
+            },
             "integrity": "sha1-ODjpfPxgUh63PFJajlW/3Z4uKPE=",
             "requires": {
                 "punycode": "1.3.2",
                 "querystring": "0.2.0"
             },
             "resolved": "https://registry.npmjs.org/url/-/url-0.11.0.tgz",
             "version": "0.11.0"
         },
         "use": {
             "integrity": "sha512-cwESVXlO3url9YWlFW/TA9cshCEhtu7IKJ/p5soJ/gGpj7vbvFrAY/eIioQ6Dw23KjZhYgiIo8HOs1nQ2vr/oQ==",
             "resolved": "https://registry.npmjs.org/use/-/use-3.1.1.tgz",
             "version": "3.1.1"
         },
+        "use-subscription": {
+            "integrity": "sha512-gk4fPTYvNhs6Ia7u8/+K7bM7sZ7O7AMfWtS+zPO8luH+zWuiGgGcrW0hL4MRWZSzXo+4ofNorf87wZwBKz2YdQ==",
+            "resolved": "https://registry.npmjs.org/use-subscription/-/use-subscription-1.1.1.tgz",
+            "version": "1.1.1"
+        },
         "util": {
             "dependencies": {
                 "inherits": {
-                    "integrity": "sha1-Yzwsg+PaQqUC9SRmAiSA9CCCYd4=",
-                    "resolved": "https://registry.npmjs.org/inherits/-/inherits-2.0.3.tgz",
-                    "version": "2.0.3"
+                    "integrity": "sha1-sX0I0ya0Qj5Wjv9xn5GwscvfafE=",
+                    "resolved": "https://registry.npmjs.org/inherits/-/inherits-2.0.1.tgz",
+                    "version": "2.0.1"
                 }
             },
-            "integrity": "sha512-HShAsny+zS2TZfaXxD9tYj4HQGlBezXZMZuM/S5PKLLoZkShZiGk9o5CzukI1LVHZvjdvZ2Sj1aW/Ndn2NB/HQ==",
+            "integrity": "sha1-evsa/lCAUkZInj23/g7TeTNqwPk=",
             "requires": {
-                "inherits": "2.0.3"
+                "inherits": "2.0.1"
             },
-            "resolved": "https://registry.npmjs.org/util/-/util-0.11.1.tgz",
-            "version": "0.11.1"
+            "resolved": "https://registry.npmjs.org/util/-/util-0.10.3.tgz",
+            "version": "0.10.3"
         },
         "util-deprecate": {
             "integrity": "sha1-RQ1Nyfpw3nMnYvvS1KKJgUGaDM8=",
             "resolved": "https://registry.npmjs.org/util-deprecate/-/util-deprecate-1.0.2.tgz",
             "version": "1.0.2"
         },
         "uuid": {
-            "integrity": "sha512-pW0No1RGHgzlpHJO1nsVrHKpOEIxkGg1xB+v0ZmdNH5OAeAwzAVrCnI2/6Mtx+Uys6iaylxa+D3g4j63IKKjSQ==",
-            "resolved": "https://registry.npmjs.org/uuid/-/uuid-3.3.3.tgz",
-            "version": "3.3.3"
+            "integrity": "sha512-HjSDRw6gZE5JMggctHBcjVak08+KEVhSIiDzFnT9S9aegmp85S/bReBVTb4QTFaRNptJ9kuYaNhnbNEOkbKb/A==",
+            "resolved": "https://registry.npmjs.org/uuid/-/uuid-3.4.0.tgz",
+            "version": "3.4.0"
         },
         "validate-npm-package-license": {
             "integrity": "sha512-DpKm2Ui/xN7/HQKCtpZxoRWBhZ9Z0kqtygG8XCgNQ8ZlDnxuQmWhj566j8fN4Cu3/JmbhsDo7fcAJq4s9h27Ew==",
             "requires": {
                 "spdx-correct": "^3.0.0",
                 "spdx-expression-parse": "^3.0.0"
             },
@@ -5540,51 +8101,254 @@
         },
         "vary": {
             "integrity": "sha1-IpnwLG3tMNSllhsLn3RSShj2NPw=",
             "resolved": "https://registry.npmjs.org/vary/-/vary-1.1.2.tgz",
             "version": "1.1.2"
         },
         "vm-browserify": {
-            "integrity": "sha512-iq+S7vZJE60yejDYM0ek6zg308+UZsdtPExWP9VZoCFCz1zkJoXFnAX7aZfd/ZwrkidzdUZL0C/ryW+JwAiIGw==",
-            "resolved": "https://registry.npmjs.org/vm-browserify/-/vm-browserify-1.1.0.tgz",
-            "version": "1.1.0"
+            "integrity": "sha512-2ham8XPWTONajOR0ohOKOHXkm3+gaBmGut3SRuu75xLd/RRaY6vqgh8NBYYk7+RW3u5AtzPQZG8F10LHkl0lAQ==",
+            "resolved": "https://registry.npmjs.org/vm-browserify/-/vm-browserify-1.1.2.tgz",
+            "version": "1.1.2"
         },
         "watchpack": {
-            "integrity": "sha512-HGqh9e9QZFhow8JYX+1+E+kIYK0uTTsk6rCOkI0ff0f9kMO0wX783yW8saQC9WDx7qHpVGPXsRnld9nY7iwzQA==",
+            "integrity": "sha512-ZEFq2mx/k5qgQwgi6NOm+2ImICb8ngAkA/rZ6oyXZ7SgPn3pncf+nfhYTCrs3lmHwOxnPtGLTOuFLfpSMh1VMA==",
             "requires": {
                 "glob-to-regexp": "^0.4.1",
-                "graceful-fs": "^4.1.2",
-                "neo-async": "^2.5.0"
+                "graceful-fs": "^4.1.2"
+            },
+            "resolved": "https://registry.npmjs.org/watchpack/-/watchpack-2.0.0-beta.13.tgz",
+            "version": "2.0.0-beta.13"
+        },
+        "watchpack-chokidar2": {
+            "dependencies": {
+                "anymatch": {
+                    "dependencies": {
+                        "normalize-path": {
+                            "integrity": "sha1-GrKLVW4Zg2Oowab35vogE3/mrtk=",
+                            "optional": true,
+                            "requires": {
+                                "remove-trailing-separator": "^1.0.1"
+                            },
+                            "resolved": "https://registry.npmjs.org/normalize-path/-/normalize-path-2.1.1.tgz",
+                            "version": "2.1.1"
+                        }
+                    },
+                    "integrity": "sha512-5teOsQWABXHHBFP9y3skS5P3d/WfWXpv3FUpy+LorMrNYaT9pI4oLMQX7jzQ2KklNpGpWHzdCXTDT2Y3XGlZBw==",
+                    "optional": true,
+                    "requires": {
+                        "micromatch": "^3.1.4",
+                        "normalize-path": "^2.1.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/anymatch/-/anymatch-2.0.0.tgz",
+                    "version": "2.0.0"
+                },
+                "binary-extensions": {
+                    "integrity": "sha512-Un7MIEDdUC5gNpcGDV97op1Ywk748MpHcFTHoYs6qnj1Z3j7I53VG3nwZhKzoBZmbdRNnb6WRdFlwl7tSDuZGw==",
+                    "optional": true,
+                    "resolved": "https://registry.npmjs.org/binary-extensions/-/binary-extensions-1.13.1.tgz",
+                    "version": "1.13.1"
+                },
+                "braces": {
+                    "integrity": "sha512-aNdbnj9P8PjdXU4ybaWLK2IF3jc/EoDYbC7AazW6to3TRsfXxscC9UXOB5iDiEQrkyIbWp2SLQda4+QAa7nc3w==",
+                    "optional": true,
+                    "requires": {
+                        "arr-flatten": "^1.1.0",
+                        "array-unique": "^0.3.2",
+                        "extend-shallow": "^2.0.1",
+                        "fill-range": "^4.0.0",
+                        "isobject": "^3.0.1",
+                        "repeat-element": "^1.1.2",
+                        "snapdragon": "^0.8.1",
+                        "snapdragon-node": "^2.0.1",
+                        "split-string": "^3.0.2",
+                        "to-regex": "^3.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/braces/-/braces-2.3.2.tgz",
+                    "version": "2.3.2"
+                },
+                "chokidar": {
+                    "integrity": "sha512-ZmZUazfOzf0Nve7duiCKD23PFSCs4JPoYyccjUFF3aQkQadqBhfzhjkwBH2mNOG9cTBwhamM37EIsIkZw3nRgg==",
+                    "optional": true,
+                    "requires": {
+                        "anymatch": "^2.0.0",
+                        "async-each": "^1.0.1",
+                        "braces": "^2.3.2",
+                        "fsevents": "^1.2.7",
+                        "glob-parent": "^3.1.0",
+                        "inherits": "^2.0.3",
+                        "is-binary-path": "^1.0.0",
+                        "is-glob": "^4.0.0",
+                        "normalize-path": "^3.0.0",
+                        "path-is-absolute": "^1.0.0",
+                        "readdirp": "^2.2.1",
+                        "upath": "^1.1.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/chokidar/-/chokidar-2.1.8.tgz",
+                    "version": "2.1.8"
+                },
+                "extend-shallow": {
+                    "integrity": "sha1-Ua99YUrZqfYQ6huvu5idaxxWiQ8=",
+                    "optional": true,
+                    "requires": {
+                        "is-extendable": "^0.1.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/extend-shallow/-/extend-shallow-2.0.1.tgz",
+                    "version": "2.0.1"
+                },
+                "fill-range": {
+                    "integrity": "sha1-1USBHUKPmOsGpj3EAtJAPDKMOPc=",
+                    "optional": true,
+                    "requires": {
+                        "extend-shallow": "^2.0.1",
+                        "is-number": "^3.0.0",
+                        "repeat-string": "^1.6.1",
+                        "to-regex-range": "^2.1.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/fill-range/-/fill-range-4.0.0.tgz",
+                    "version": "4.0.0"
+                },
+                "fsevents": {
+                    "integrity": "sha512-oWb1Z6mkHIskLzEJ/XWX0srkpkTQ7vaopMQkyaEIoq0fmtFVxOthb8cCxeT+p3ynTdkk/RZwbgG4brR5BeWECw==",
+                    "optional": true,
+                    "requires": {
+                        "bindings": "^1.5.0",
+                        "nan": "^2.12.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/fsevents/-/fsevents-1.2.13.tgz",
+                    "version": "1.2.13"
+                },
+                "glob-parent": {
+                    "dependencies": {
+                        "is-glob": {
+                            "integrity": "sha1-e6WuJCF4BKxwcHuWkiVnSGzD6Eo=",
+                            "optional": true,
+                            "requires": {
+                                "is-extglob": "^2.1.0"
+                            },
+                            "resolved": "https://registry.npmjs.org/is-glob/-/is-glob-3.1.0.tgz",
+                            "version": "3.1.0"
+                        }
+                    },
+                    "integrity": "sha1-nmr2KZ2NO9K9QEMIMr0RPfkGxa4=",
+                    "optional": true,
+                    "requires": {
+                        "is-glob": "^3.1.0",
+                        "path-dirname": "^1.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/glob-parent/-/glob-parent-3.1.0.tgz",
+                    "version": "3.1.0"
+                },
+                "is-binary-path": {
+                    "integrity": "sha1-dfFmQrSA8YenEcgUFh/TpKdlWJg=",
+                    "optional": true,
+                    "requires": {
+                        "binary-extensions": "^1.0.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/is-binary-path/-/is-binary-path-1.0.1.tgz",
+                    "version": "1.0.1"
+                },
+                "is-number": {
+                    "integrity": "sha1-JP1iAaR4LPUFYcgQJ2r8fRLXEZU=",
+                    "optional": true,
+                    "requires": {
+                        "kind-of": "^3.0.2"
+                    },
+                    "resolved": "https://registry.npmjs.org/is-number/-/is-number-3.0.0.tgz",
+                    "version": "3.0.0"
+                },
+                "kind-of": {
+                    "integrity": "sha1-MeohpzS6ubuw8yRm2JOupR5KPGQ=",
+                    "optional": true,
+                    "requires": {
+                        "is-buffer": "^1.1.5"
+                    },
+                    "resolved": "https://registry.npmjs.org/kind-of/-/kind-of-3.2.2.tgz",
+                    "version": "3.2.2"
+                },
+                "readdirp": {
+                    "integrity": "sha512-1JU/8q+VgFZyxwrJ+SVIOsh+KywWGpds3NTqikiKpDMZWScmAYyKIgqkO+ARvNWJfXeXR1zxz7aHF4u4CyH6vQ==",
+                    "optional": true,
+                    "requires": {
+                        "graceful-fs": "^4.1.11",
+                        "micromatch": "^3.1.10",
+                        "readable-stream": "^2.0.2"
+                    },
+                    "resolved": "https://registry.npmjs.org/readdirp/-/readdirp-2.2.1.tgz",
+                    "version": "2.2.1"
+                },
+                "to-regex-range": {
+                    "integrity": "sha1-fIDBe53+vlmeJzZ+DU3VWQFB2zg=",
+                    "optional": true,
+                    "requires": {
+                        "is-number": "^3.0.0",
+                        "repeat-string": "^1.6.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/to-regex-range/-/to-regex-range-2.1.1.tgz",
+                    "version": "2.1.1"
+                }
+            },
+            "integrity": "sha512-nCFfBIPKr5Sh61s4LPpy1Wtfi0HE8isJ3d2Yb5/Ppw2P2B/3eVSEBjKfN0fmHJSK14+31KwMKmcrzs2GM4P0Ww==",
+            "optional": true,
+            "requires": {
+                "chokidar": "^2.1.8"
             },
-            "resolved": "https://registry.npmjs.org/watchpack/-/watchpack-2.0.0-beta.5.tgz",
-            "version": "2.0.0-beta.5"
+            "resolved": "https://registry.npmjs.org/watchpack-chokidar2/-/watchpack-chokidar2-2.0.1.tgz",
+            "version": "2.0.1"
+        },
+        "wcwidth": {
+            "integrity": "sha1-8LDc+RW8X/FSivrbLA4XtTLaL+g=",
+            "requires": {
+                "defaults": "^1.0.3"
+            },
+            "resolved": "https://registry.npmjs.org/wcwidth/-/wcwidth-1.0.1.tgz",
+            "version": "1.0.1"
         },
         "webpack": {
             "dependencies": {
-                "watchpack": {
-                    "integrity": "sha512-i6dHe3EyLjMmDlU1/bGQpEw25XSjkJULPuAVKCbNRefQVq48yXKUpwg538F7AZTf9kyr57zj++pQFltUa5H7yA==",
+                "json5": {
+                    "integrity": "sha512-aKS4WQjPenRxiQsC93MNfjx+nbF4PAdYzmd/1JIj8HYzqfbu86beTuNgXDzPknWk0n0uARlyewZo4s++ES36Ow==",
                     "requires": {
-                        "chokidar": "^2.0.2",
-                        "graceful-fs": "^4.1.2",
-                        "neo-async": "^2.5.0"
+                        "minimist": "^1.2.0"
+                    },
+                    "resolved": "https://registry.npmjs.org/json5/-/json5-1.0.1.tgz",
+                    "version": "1.0.1"
+                },
+                "loader-utils": {
+                    "integrity": "sha512-qH0WSMBtn/oHuwjy/NucEgbx5dbxxnxup9s4PVXJUDHZBQY+s0NWA9rJf53RBnQZxfch7euUui7hpoAPvALZdA==",
+                    "requires": {
+                        "big.js": "^5.2.2",
+                        "emojis-list": "^3.0.0",
+                        "json5": "^1.0.1"
+                    },
+                    "resolved": "https://registry.npmjs.org/loader-utils/-/loader-utils-1.4.0.tgz",
+                    "version": "1.4.0"
+                },
+                "schema-utils": {
+                    "integrity": "sha512-i27Mic4KovM/lnGsy8whRCHhc7VicJajAjTrYg11K9zfZXnYIt4k5F+kZkwjnrhKzLic/HLU4j11mjsz2G/75g==",
+                    "requires": {
+                        "ajv": "^6.1.0",
+                        "ajv-errors": "^1.0.0",
+                        "ajv-keywords": "^3.1.0"
                     },
-                    "resolved": "https://registry.npmjs.org/watchpack/-/watchpack-1.6.0.tgz",
-                    "version": "1.6.0"
+                    "resolved": "https://registry.npmjs.org/schema-utils/-/schema-utils-1.0.0.tgz",
+                    "version": "1.0.0"
                 },
-                "webpack-sources": {
-                    "integrity": "sha512-lgTS3Xhv1lCOKo7SA5TjKXMjpSM4sBjNV5+q2bqesbSPs5FjGmU6jjtBSkX9b4qW87vDIsCIlUPOEhbZrMdjeQ==",
+                "watchpack": {
+                    "integrity": "sha512-9P3MWk6SrKjHsGkLT2KHXdQ/9SNkyoJbabxnKOoJepsvJjJG8uYTR3yTPxPQvNDI3w4Nz1xnE0TLHK4RIVe/MQ==",
                     "requires": {
-                        "source-list-map": "^2.0.0",
-                        "source-map": "~0.6.1"
+                        "chokidar": "^3.4.1",
+                        "graceful-fs": "^4.1.2",
+                        "neo-async": "^2.5.0",
+                        "watchpack-chokidar2": "^2.0.1"
                     },
-                    "resolved": "https://registry.npmjs.org/webpack-sources/-/webpack-sources-1.4.3.tgz",
-                    "version": "1.4.3"
+                    "resolved": "https://registry.npmjs.org/watchpack/-/watchpack-1.7.5.tgz",
+                    "version": "1.7.5"
                 }
             },
-            "integrity": "sha512-nrxFNSEKm4T1C/EsgOgN50skt//Pl4X7kgJC1MrlE47M292LSCVmMOC47iTGL0CGxbdwhKGgeThrJcw0bstEfA==",
+            "integrity": "sha512-EzJRHvwQyBiYrYqhyjW9AqM90dE4+s1/XtCfn7uWg6cS72zH+2VPFAlsnW0+W0cDi0XRjNKUMoJtpSi50+Ph6w==",
             "requires": {
                 "@webassemblyjs/ast": "1.8.5",
                 "@webassemblyjs/helper-module-context": "1.8.5",
                 "@webassemblyjs/wasm-edit": "1.8.5",
                 "@webassemblyjs/wasm-parser": "1.8.5",
                 "acorn": "^6.2.1",
                 "ajv": "^6.10.2",
@@ -5598,27 +8362,27 @@
                 "memory-fs": "^0.4.1",
                 "micromatch": "^3.1.10",
                 "mkdirp": "^0.5.1",
                 "neo-async": "^2.6.1",
                 "node-libs-browser": "^2.2.1",
                 "schema-utils": "^1.0.0",
                 "tapable": "^1.1.3",
-                "terser-webpack-plugin": "^1.4.1",
+                "terser-webpack-plugin": "^1.4.3",
                 "watchpack": "^1.6.0",
                 "webpack-sources": "^1.4.1"
             },
-            "resolved": "https://registry.npmjs.org/webpack/-/webpack-4.39.0.tgz",
-            "version": "4.39.0"
+            "resolved": "https://registry.npmjs.org/webpack/-/webpack-4.42.0.tgz",
+            "version": "4.42.0"
         },
         "webpack-dev-middleware": {
             "dependencies": {
                 "mime": {
-                    "integrity": "sha512-LRxmNwziLPT828z+4YkNzloCFC2YM4wrB99k+AV5ZbEyfGNWfG8SO1FUXLmLDBSo89NrJZ4DIWeLjy1CHGhMGA==",
-                    "resolved": "https://registry.npmjs.org/mime/-/mime-2.4.4.tgz",
-                    "version": "2.4.4"
+                    "integrity": "sha512-tqkh47FzKeCPD2PUiPB6pkbMzsCasjxAfC62/Wap5qrUWcb+sFasXUC5I3gYM5iBM8v/Qpn4UK0x+j0iHyFPDg==",
+                    "resolved": "https://registry.npmjs.org/mime/-/mime-2.5.2.tgz",
+                    "version": "2.5.2"
                 }
             },
             "integrity": "sha512-qvDesR1QZRIAZHOE3iQ4CXLZZSQ1lAUsSpnQmlB1PBfoN/xdRjmge3Dok0W4IdaVLJOGJy3sGI4sZHwjRU0PCA==",
             "requires": {
                 "memory-fs": "^0.4.1",
                 "mime": "^2.4.2",
                 "range-parser": "^1.2.1",
@@ -5654,29 +8418,29 @@
                 "ansi-colors": "^3.0.0",
                 "uuid": "^3.3.2"
             },
             "resolved": "https://registry.npmjs.org/webpack-log/-/webpack-log-2.0.0.tgz",
             "version": "2.0.0"
         },
         "webpack-merge": {
-            "integrity": "sha512-4p8WQyS98bUJcCvFMbdGZyZmsKuWjWVnVHnAS3FFg0HDaRVrPbkivx2RYCre8UiemD67RsiFFLfn4JhLAin8Vw==",
+            "integrity": "sha512-TUE1UGoTX2Cd42j3krGYqObZbOD+xF7u28WB7tfUordytSjbWTIjK/8V0amkBfTYN4/pB/GIDlJZZ657BGG19g==",
             "requires": {
-                "lodash": "^4.17.5"
+                "lodash": "^4.17.15"
             },
-            "resolved": "https://registry.npmjs.org/webpack-merge/-/webpack-merge-4.2.1.tgz",
-            "version": "4.2.1"
+            "resolved": "https://registry.npmjs.org/webpack-merge/-/webpack-merge-4.2.2.tgz",
+            "version": "4.2.2"
         },
         "webpack-sources": {
-            "integrity": "sha512-OiVgSrbGu7NEnEvQJJgdSFPl2qWKkWq5lHMhgiToIiN9w34EBnjYzSYs+VbL5KoYiLNtFFa7BZIKxRED3I32pA==",
+            "integrity": "sha512-lgTS3Xhv1lCOKo7SA5TjKXMjpSM4sBjNV5+q2bqesbSPs5FjGmU6jjtBSkX9b4qW87vDIsCIlUPOEhbZrMdjeQ==",
             "requires": {
                 "source-list-map": "^2.0.0",
                 "source-map": "~0.6.1"
             },
-            "resolved": "https://registry.npmjs.org/webpack-sources/-/webpack-sources-1.3.0.tgz",
-            "version": "1.3.0"
+            "resolved": "https://registry.npmjs.org/webpack-sources/-/webpack-sources-1.4.3.tgz",
+            "version": "1.4.3"
         },
         "worker-farm": {
             "integrity": "sha512-rvw3QTZc8lAxyVrqcSGVm5yP/IJ2UcB3U0graE3LCFoZ0Yn2x4EoVSqJKdB/T5M+FLcRPjz4TDacRf3OCfNUzw==",
             "requires": {
                 "errno": "~0.1.7"
             },
             "resolved": "https://registry.npmjs.org/worker-farm/-/worker-farm-1.7.0.tgz",
@@ -5691,27 +8455,38 @@
             "version": "0.1.1"
         },
         "wrappy": {
             "integrity": "sha1-tSQ9jz7BqjXxNkYFvA0QNuMKtp8=",
             "resolved": "https://registry.npmjs.org/wrappy/-/wrappy-1.0.2.tgz",
             "version": "1.0.2"
         },
+        "write-file-atomic": {
+            "integrity": "sha512-AvHcyZ5JnSfq3ioSyjrBkH9yW4m7Ayk8/9My/DD9onKeu/94fwrMocemO2QAJFAlnnDN+ZDS+ZjAR5ua1/PV/Q==",
+            "requires": {
+                "imurmurhash": "^0.1.4",
+                "is-typedarray": "^1.0.0",
+                "signal-exit": "^3.0.2",
+                "typedarray-to-buffer": "^3.1.5"
+            },
+            "resolved": "https://registry.npmjs.org/write-file-atomic/-/write-file-atomic-3.0.3.tgz",
+            "version": "3.0.3"
+        },
         "xtend": {
             "integrity": "sha512-LKYU1iAXJXUgAXn9URjiu+MWhyUXHsvfp7mcuYm9dSUKK0/CjtrUwFAxD82/mCWbtLsGjFIad0wIsod4zrTAEQ==",
             "resolved": "https://registry.npmjs.org/xtend/-/xtend-4.0.2.tgz",
             "version": "4.0.2"
         },
         "y18n": {
-            "integrity": "sha512-r9S/ZyXu/Xu9q1tYlpsLIsa3EeLXXk0VwlxqTcFRfg9EhMW+17kbt9G0NrgCmhGb5vT2hyhJZLfDGx+7+5Uj/w==",
-            "resolved": "https://registry.npmjs.org/y18n/-/y18n-4.0.0.tgz",
-            "version": "4.0.0"
+            "integrity": "sha512-JKhqTOwSrqNA1NY5lSztJ1GrBiUodLMmIZuLiDaMRJ+itFd+ABVE8XBjOvIWL+rSqNDC74LCSFmlb/U4UZ4hJQ==",
+            "resolved": "https://registry.npmjs.org/y18n/-/y18n-4.0.3.tgz",
+            "version": "4.0.3"
         },
         "yallist": {
-            "integrity": "sha512-S+Zk8DEWE6oKpV+vI3qWkaK+jSbIK86pCwe2IF/xwIpQ8jEuxpw9NyaGjmp9+BoJv5FV2piqCDcoCtStppiq2A==",
-            "resolved": "https://registry.npmjs.org/yallist/-/yallist-3.0.3.tgz",
-            "version": "3.0.3"
+            "integrity": "sha512-3wdGidZyq5PB084XLES5TpOSRA3wjXAlIWMhum2kRcv/41Sn2emQ0dycQW4uZXLejwKvg6EsvbdlVL+FYEct7A==",
+            "resolved": "https://registry.npmjs.org/yallist/-/yallist-4.0.0.tgz",
+            "version": "4.0.0"
         }
     },
     "lockfileVersion": 1,
     "name": "create-next-example-app",
     "requires": true
 }
```

### Comparing `weco-datascience-0.1.8/ui/pages/_app.tsx` & `weco-datascience-0.1.9/api_interfaces/palette/pages/_app.tsx`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/ui/pages/palette.tsx` & `weco-datascience-0.1.9/api_interfaces/palette/pages/palette.tsx`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/ui/static/favicon.ico` & `weco-datascience-0.1.9/api_interfaces/palette/static/favicon.ico`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/ui/tsconfig.json` & `weco-datascience-0.1.9/api_interfaces/palette/tsconfig.json`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/ui/yarn.lock` & `weco-datascience-0.1.9/api_interfaces/palette/yarn.lock`

 * *Files 12% similar despite different names*

```diff
@@ -1,64 +1,103 @@
 # THIS IS AN AUTOGENERATED FILE. DO NOT EDIT THIS FILE DIRECTLY.
 # yarn lockfile v1
 
 
-"@ampproject/toolbox-core@^1.0.1":
-  version "1.0.1"
-  resolved "https://registry.yarnpkg.com/@ampproject/toolbox-core/-/toolbox-core-1.0.1.tgz#e32b7d9e84a3bd0a3e1bd40ebdcdc7dd37bf3e55"
-  integrity sha512-8aONoeOAVujavLUezSCtpUjg9khkVndpArbn25cLab6/UG+ZgrFPvU3A7z1TjBvB31bte4pXxH6U004BC0VdfA==
+"@ampproject/toolbox-core@^2.0.0", "@ampproject/toolbox-core@^2.8.0":
+  version "2.8.0"
+  resolved "https://registry.yarnpkg.com/@ampproject/toolbox-core/-/toolbox-core-2.8.0.tgz#4c291c470fa30c0c2f3c7952dbcd6df2a4ec0df9"
+  integrity sha512-YrMRrE9zfAChPlFLT+B4yoGEH6CR/Yerjm6SCxuFSPARK/LaytUV+ZhZ03tlMv5wUHDH2Lq8e/lGymME0CXBhA==
   dependencies:
-    node-fetch "2.6.0"
+    cross-fetch "3.1.2"
+    lru-cache "6.0.0"
 
-"@ampproject/toolbox-optimizer@1.0.1":
-  version "1.0.1"
-  resolved "https://registry.yarnpkg.com/@ampproject/toolbox-optimizer/-/toolbox-optimizer-1.0.1.tgz#5eeda7bc84c23237479c35442d4696c4bdbeb1d3"
-  integrity sha512-zz1cJsQWBvfg2h1ce2/bbgNdSkTjIY7PaF7QhWMzYVcfvdxGSAykA+Ajt+F13H6adNAtIn09s96z/+6pn7XiXQ==
+"@ampproject/toolbox-optimizer@2.0.1":
+  version "2.0.1"
+  resolved "https://registry.yarnpkg.com/@ampproject/toolbox-optimizer/-/toolbox-optimizer-2.0.1.tgz#943681faf24443044aa66f0b55eefb13cdcc068c"
+  integrity sha512-zroXqrV7mY77+/6hV7kaaWxp4LA85V0B/2vg7WdF+FrwiO9Wior/lIW8UbpRek6INjw0VOp1ED73MmGJkwaDhA==
   dependencies:
-    "@ampproject/toolbox-core" "^1.0.1"
-    "@ampproject/toolbox-runtime-version" "^1.0.1"
+    "@ampproject/toolbox-core" "^2.0.0"
+    "@ampproject/toolbox-runtime-version" "^2.0.0"
+    "@ampproject/toolbox-script-csp" "^2.0.0"
+    "@ampproject/toolbox-validator-rules" "^2.0.0"
     css "2.2.4"
-    parse5 "5.1.0"
-    parse5-htmlparser2-tree-adapter "5.1.0"
-
-"@ampproject/toolbox-runtime-version@^1.0.1":
-  version "1.0.1"
-  resolved "https://registry.yarnpkg.com/@ampproject/toolbox-runtime-version/-/toolbox-runtime-version-1.0.1.tgz#2c13a17c08d1376ef55f44ef6679c25ff03828e0"
-  integrity sha512-OFky5rUfP9Hw/NlvEH+/8LqeSZ5DiXY2/RUvWSnY0r0/Uk4ooPyRCWEcVgRF7Y+wY+K1oro5UBZfE9MRYz+hpA==
+    domhandler "3.0.0"
+    domutils "2.0.0"
+    htmlparser2 "4.1.0"
+    normalize-html-whitespace "1.0.0"
+    terser "4.6.7"
+
+"@ampproject/toolbox-runtime-version@^2.0.0":
+  version "2.8.0"
+  resolved "https://registry.yarnpkg.com/@ampproject/toolbox-runtime-version/-/toolbox-runtime-version-2.8.0.tgz#17fd41778c73a8fdf0d2821f23f6ff1066bb4956"
+  integrity sha512-vkotDc6S3Q3Xm6LIPzWo2T1+yxvj+bIDrD4SObk6J4SVqilIlPEunLayS602Su+ZXqNC82VjEeD1ARAtc613dQ==
+  dependencies:
+    "@ampproject/toolbox-core" "^2.8.0"
+
+"@ampproject/toolbox-script-csp@^2.0.0":
+  version "2.8.0"
+  resolved "https://registry.yarnpkg.com/@ampproject/toolbox-script-csp/-/toolbox-script-csp-2.8.0.tgz#3854d6a0eef962cf2e2178d5ee7ad185648e5a8b"
+  integrity sha512-5/ytdTzhmdIyOkcEBskh5ZlLJ8V4bbe+1pY9LZQ8DfWrSOVD1pJ+LtAO/7lmTM+HXxMAKPYDRpvsJc0vvbY0tw==
+
+"@ampproject/toolbox-validator-rules@^2.0.0":
+  version "2.8.0"
+  resolved "https://registry.yarnpkg.com/@ampproject/toolbox-validator-rules/-/toolbox-validator-rules-2.8.0.tgz#b1e04ac1f32a662318157012ee708ef06f46d26b"
+  integrity sha512-kbInwnzpEPVZkKigpKFkF/DQ2LsuZ5b8vrEFHjJ4P+meKVQg2QF/UWAQpIMMdjGe1AQBT+DWm91n9UyjgqfnWQ==
   dependencies:
-    "@ampproject/toolbox-core" "^1.0.1"
+    cross-fetch "3.1.2"
 
 "@babel/code-frame@^7.0.0", "@babel/code-frame@^7.5.5":
   version "7.5.5"
   resolved "https://registry.yarnpkg.com/@babel/code-frame/-/code-frame-7.5.5.tgz#bc0782f6d69f7b7d49531219699b988f669a8f9d"
   integrity sha512-27d4lZoomVyo51VegxI20xZPuSHusqbQag/ztrBC7wegWoQ1nLREPVSKSW8byhTlzTKyNE4ifaTA6lCp7JjpFw==
   dependencies:
     "@babel/highlight" "^7.0.0"
 
-"@babel/core@7.4.5":
-  version "7.4.5"
-  resolved "https://registry.yarnpkg.com/@babel/core/-/core-7.4.5.tgz#081f97e8ffca65a9b4b0fdc7e274e703f000c06a"
-  integrity sha512-OvjIh6aqXtlsA8ujtGKfC7LYWksYSX8yQcM8Ay3LuvVeQ63lcOKgoZWVqcpFwkd29aYU9rVx7jxhfhiEDV9MZA==
+"@babel/code-frame@^7.14.5":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/code-frame/-/code-frame-7.14.5.tgz#23b08d740e83f49c5e59945fbf1b43e80bbf4edb"
+  integrity sha512-9pzDqyc6OLDaqe+zbACgFkb6fKMNG6CObKpnYXChRsvYGyEdc7CA2BaqeOM+vOtCS5ndmJicPJhKAwYRI6UfFw==
+  dependencies:
+    "@babel/highlight" "^7.14.5"
+
+"@babel/compat-data@^7.14.5", "@babel/compat-data@^7.14.7":
+  version "7.14.7"
+  resolved "https://registry.yarnpkg.com/@babel/compat-data/-/compat-data-7.14.7.tgz#7b047d7a3a89a67d2258dc61f604f098f1bc7e08"
+  integrity sha512-nS6dZaISCXJ3+518CWiBfEr//gHyMO02uDxBkXTKZDN5POruCnOZ1N4YBRZDCabwF8nZMWBpRxIicmXtBs+fvw==
+
+"@babel/core@7.7.2":
+  version "7.7.2"
+  resolved "https://registry.yarnpkg.com/@babel/core/-/core-7.7.2.tgz#ea5b99693bcfc058116f42fa1dd54da412b29d91"
+  integrity sha512-eeD7VEZKfhK1KUXGiyPFettgF3m513f8FoBSWiQ1xTvl1RAopLs42Wp9+Ze911I6H0N9lNqJMDgoZT7gHsipeQ==
   dependencies:
-    "@babel/code-frame" "^7.0.0"
-    "@babel/generator" "^7.4.4"
-    "@babel/helpers" "^7.4.4"
-    "@babel/parser" "^7.4.5"
-    "@babel/template" "^7.4.4"
-    "@babel/traverse" "^7.4.5"
-    "@babel/types" "^7.4.4"
-    convert-source-map "^1.1.0"
+    "@babel/code-frame" "^7.5.5"
+    "@babel/generator" "^7.7.2"
+    "@babel/helpers" "^7.7.0"
+    "@babel/parser" "^7.7.2"
+    "@babel/template" "^7.7.0"
+    "@babel/traverse" "^7.7.2"
+    "@babel/types" "^7.7.2"
+    convert-source-map "^1.7.0"
     debug "^4.1.0"
     json5 "^2.1.0"
-    lodash "^4.17.11"
+    lodash "^4.17.13"
     resolve "^1.3.2"
     semver "^5.4.1"
     source-map "^0.5.0"
 
-"@babel/generator@^7.4.4", "@babel/generator@^7.6.2":
+"@babel/generator@^7.14.8", "@babel/generator@^7.7.2":
+  version "7.14.8"
+  resolved "https://registry.yarnpkg.com/@babel/generator/-/generator-7.14.8.tgz#bf86fd6af96cf3b74395a8ca409515f89423e070"
+  integrity sha512-cYDUpvIzhBVnMzRoY1fkSEhK/HmwEVwlyULYgn/tMQYd6Obag3ylCjONle3gdErfXBW61SVTlR9QR7uWlgeIkg==
+  dependencies:
+    "@babel/types" "^7.14.8"
+    jsesc "^2.5.1"
+    source-map "^0.5.0"
+
+"@babel/generator@^7.6.2":
   version "7.6.2"
   resolved "https://registry.yarnpkg.com/@babel/generator/-/generator-7.6.2.tgz#dac8a3c2df118334c2a29ff3446da1636a8f8c03"
   integrity sha512-j8iHaIW4gGPnViaIHI7e9t/Hl8qLjERI6DcV9kEpAIDJsAOrcnXqRS7t+QbhL76pwbtqP+QCQLL0z1CyVmtjjQ==
   dependencies:
     "@babel/types" "^7.6.0"
     jsesc "^2.5.1"
     lodash "^4.17.13"
@@ -67,59 +106,67 @@
 "@babel/helper-annotate-as-pure@^7.0.0":
   version "7.0.0"
   resolved "https://registry.yarnpkg.com/@babel/helper-annotate-as-pure/-/helper-annotate-as-pure-7.0.0.tgz#323d39dd0b50e10c7c06ca7d7638e6864d8c5c32"
   integrity sha512-3UYcJUj9kvSLbLbUIfQTqzcy5VX7GRZ/CCDrnOaZorFFM01aXp1+GJwuFGV4NDDoAS+mOUyHcO6UD/RfqOks3Q==
   dependencies:
     "@babel/types" "^7.0.0"
 
+"@babel/helper-annotate-as-pure@^7.14.5":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/helper-annotate-as-pure/-/helper-annotate-as-pure-7.14.5.tgz#7bf478ec3b71726d56a8ca5775b046fc29879e61"
+  integrity sha512-EivH9EgBIb+G8ij1B2jAwSH36WnGvkQSEC6CkX/6v6ZFlw5fVOHvsgGF4uiEHO2GzMvunZb6tDLQEQSdrdocrA==
+  dependencies:
+    "@babel/types" "^7.14.5"
+
 "@babel/helper-builder-binary-assignment-operator-visitor@^7.1.0":
   version "7.1.0"
   resolved "https://registry.yarnpkg.com/@babel/helper-builder-binary-assignment-operator-visitor/-/helper-builder-binary-assignment-operator-visitor-7.1.0.tgz#6b69628dfe4087798e0c4ed98e3d4a6b2fbd2f5f"
   integrity sha512-qNSR4jrmJ8M1VMM9tibvyRAHXQs2PmaksQF7c1CGJNipfe3D8p+wgNwgso/P2A2r2mdgBWAXljNWR0QRZAMW8w==
   dependencies:
     "@babel/helper-explode-assignable-expression" "^7.1.0"
     "@babel/types" "^7.0.0"
 
-"@babel/helper-builder-react-jsx@^7.3.0":
-  version "7.3.0"
-  resolved "https://registry.yarnpkg.com/@babel/helper-builder-react-jsx/-/helper-builder-react-jsx-7.3.0.tgz#a1ac95a5d2b3e88ae5e54846bf462eeb81b318a4"
-  integrity sha512-MjA9KgwCuPEkQd9ncSXvSyJ5y+j2sICHyrI0M3L+6fnS4wMSNDc1ARXsbTfbb2cXHn17VisSnU/sHFTCxVxSMw==
-  dependencies:
-    "@babel/types" "^7.3.0"
-    esutils "^2.0.0"
-
 "@babel/helper-call-delegate@^7.4.4":
   version "7.4.4"
   resolved "https://registry.yarnpkg.com/@babel/helper-call-delegate/-/helper-call-delegate-7.4.4.tgz#87c1f8ca19ad552a736a7a27b1c1fcf8b1ff1f43"
   integrity sha512-l79boDFJ8S1c5hvQvG+rc+wHw6IuH7YldmRKsYtpbawsxURu/paVy57FZMomGK22/JckepaikOkY0MoAmdyOlQ==
   dependencies:
     "@babel/helper-hoist-variables" "^7.4.4"
     "@babel/traverse" "^7.4.4"
     "@babel/types" "^7.4.4"
 
-"@babel/helper-create-class-features-plugin@^7.4.4", "@babel/helper-create-class-features-plugin@^7.6.0":
-  version "7.6.0"
-  resolved "https://registry.yarnpkg.com/@babel/helper-create-class-features-plugin/-/helper-create-class-features-plugin-7.6.0.tgz#769711acca889be371e9bc2eb68641d55218021f"
-  integrity sha512-O1QWBko4fzGju6VoVvrZg0RROCVifcLxiApnGP3OWfWzvxRZFCoBD81K5ur5e3bVY2Vf/5rIJm8cqPKn8HUJng==
-  dependencies:
-    "@babel/helper-function-name" "^7.1.0"
-    "@babel/helper-member-expression-to-functions" "^7.5.5"
-    "@babel/helper-optimise-call-expression" "^7.0.0"
-    "@babel/helper-plugin-utils" "^7.0.0"
-    "@babel/helper-replace-supers" "^7.5.5"
-    "@babel/helper-split-export-declaration" "^7.4.4"
+"@babel/helper-compilation-targets@^7.14.5":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/helper-compilation-targets/-/helper-compilation-targets-7.14.5.tgz#7a99c5d0967911e972fe2c3411f7d5b498498ecf"
+  integrity sha512-v+QtZqXEiOnpO6EYvlImB6zCD2Lel06RzOPzmkz/D/XgQiUu3C/Jb1LOqSt/AIA34TYi/Q+KlT8vTQrgdxkbLw==
+  dependencies:
+    "@babel/compat-data" "^7.14.5"
+    "@babel/helper-validator-option" "^7.14.5"
+    browserslist "^4.16.6"
+    semver "^6.3.0"
 
-"@babel/helper-define-map@^7.5.5":
-  version "7.5.5"
-  resolved "https://registry.yarnpkg.com/@babel/helper-define-map/-/helper-define-map-7.5.5.tgz#3dec32c2046f37e09b28c93eb0b103fd2a25d369"
-  integrity sha512-fTfxx7i0B5NJqvUOBBGREnrqbTxRh7zinBANpZXAVDlsZxYdclDp467G1sQ8VZYMnAURY3RpBUAgOYT9GfzHBg==
+"@babel/helper-create-class-features-plugin@^7.14.6", "@babel/helper-create-class-features-plugin@^7.7.0":
+  version "7.14.8"
+  resolved "https://registry.yarnpkg.com/@babel/helper-create-class-features-plugin/-/helper-create-class-features-plugin-7.14.8.tgz#a6f8c3de208b1e5629424a9a63567f56501955fc"
+  integrity sha512-bpYvH8zJBWzeqi1o+co8qOrw+EXzQ/0c74gVmY205AWXy9nifHrOg77y+1zwxX5lXE7Icq4sPlSQ4O2kWBrteQ==
+  dependencies:
+    "@babel/helper-annotate-as-pure" "^7.14.5"
+    "@babel/helper-function-name" "^7.14.5"
+    "@babel/helper-member-expression-to-functions" "^7.14.7"
+    "@babel/helper-optimise-call-expression" "^7.14.5"
+    "@babel/helper-replace-supers" "^7.14.5"
+    "@babel/helper-split-export-declaration" "^7.14.5"
+
+"@babel/helper-create-regexp-features-plugin@^7.14.5":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/helper-create-regexp-features-plugin/-/helper-create-regexp-features-plugin-7.14.5.tgz#c7d5ac5e9cf621c26057722fb7a8a4c5889358c4"
+  integrity sha512-TLawwqpOErY2HhWbGJ2nZT5wSkR192QpN+nBg1THfBfftrlvOh+WbhrxXCH4q4xJ9Gl16BGPR/48JA+Ryiho/A==
   dependencies:
-    "@babel/helper-function-name" "^7.1.0"
-    "@babel/types" "^7.5.5"
-    lodash "^4.17.13"
+    "@babel/helper-annotate-as-pure" "^7.14.5"
+    regexpu-core "^4.7.1"
 
 "@babel/helper-explode-assignable-expression@^7.1.0":
   version "7.1.0"
   resolved "https://registry.yarnpkg.com/@babel/helper-explode-assignable-expression/-/helper-explode-assignable-expression-7.1.0.tgz#537fa13f6f1674df745b0c00ec8fe4e99681c8f6"
   integrity sha512-NRQpfHrJ1msCHtKjbzs9YcMmJZOg6mQMmGRB+hbamEdG5PNpaSm95275VD92DvJKuyl0s2sFiDmMZ+EnnvufqA==
   dependencies:
     "@babel/traverse" "^7.1.0"
@@ -130,323 +177,507 @@
   resolved "https://registry.yarnpkg.com/@babel/helper-function-name/-/helper-function-name-7.1.0.tgz#a0ceb01685f73355d4360c1247f582bfafc8ff53"
   integrity sha512-A95XEoCpb3TO+KZzJ4S/5uW5fNe26DjBGqf1o9ucyLyCmi1dXq/B3c8iaWTfBk3VvetUxl16e8tIrd5teOCfGw==
   dependencies:
     "@babel/helper-get-function-arity" "^7.0.0"
     "@babel/template" "^7.1.0"
     "@babel/types" "^7.0.0"
 
+"@babel/helper-function-name@^7.14.5":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/helper-function-name/-/helper-function-name-7.14.5.tgz#89e2c474972f15d8e233b52ee8c480e2cfcd50c4"
+  integrity sha512-Gjna0AsXWfFvrAuX+VKcN/aNNWonizBj39yGwUzVDVTlMYJMK2Wp6xdpy72mfArFq5uK+NOuexfzZlzI1z9+AQ==
+  dependencies:
+    "@babel/helper-get-function-arity" "^7.14.5"
+    "@babel/template" "^7.14.5"
+    "@babel/types" "^7.14.5"
+
 "@babel/helper-get-function-arity@^7.0.0":
   version "7.0.0"
   resolved "https://registry.yarnpkg.com/@babel/helper-get-function-arity/-/helper-get-function-arity-7.0.0.tgz#83572d4320e2a4657263734113c42868b64e49c3"
   integrity sha512-r2DbJeg4svYvt3HOS74U4eWKsUAMRH01Z1ds1zx8KNTPtpTL5JAsdFv8BNyOpVqdFhHkkRDIg5B4AsxmkjAlmQ==
   dependencies:
     "@babel/types" "^7.0.0"
 
+"@babel/helper-get-function-arity@^7.14.5":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/helper-get-function-arity/-/helper-get-function-arity-7.14.5.tgz#25fbfa579b0937eee1f3b805ece4ce398c431815"
+  integrity sha512-I1Db4Shst5lewOM4V+ZKJzQ0JGGaZ6VY1jYvMghRjqs6DWgxLCIyFt30GlnKkfUeFLpJt2vzbMVEXVSXlIFYUg==
+  dependencies:
+    "@babel/types" "^7.14.5"
+
+"@babel/helper-hoist-variables@^7.14.5":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/helper-hoist-variables/-/helper-hoist-variables-7.14.5.tgz#e0dd27c33a78e577d7c8884916a3e7ef1f7c7f8d"
+  integrity sha512-R1PXiz31Uc0Vxy4OEOm07x0oSjKAdPPCh3tPivn/Eo8cvz6gveAeuyUUPB21Hoiif0uoPQSSdhIPS3352nvdyQ==
+  dependencies:
+    "@babel/types" "^7.14.5"
+
 "@babel/helper-hoist-variables@^7.4.4":
   version "7.4.4"
   resolved "https://registry.yarnpkg.com/@babel/helper-hoist-variables/-/helper-hoist-variables-7.4.4.tgz#0298b5f25c8c09c53102d52ac4a98f773eb2850a"
   integrity sha512-VYk2/H/BnYbZDDg39hr3t2kKyifAm1W6zHRfhx8jGjIHpQEBv9dry7oQ2f3+J703TLu69nYdxsovl0XYfcnK4w==
   dependencies:
     "@babel/types" "^7.4.4"
 
-"@babel/helper-member-expression-to-functions@^7.5.5":
-  version "7.5.5"
-  resolved "https://registry.yarnpkg.com/@babel/helper-member-expression-to-functions/-/helper-member-expression-to-functions-7.5.5.tgz#1fb5b8ec4453a93c439ee9fe3aeea4a84b76b590"
-  integrity sha512-5qZ3D1uMclSNqYcXqiHoA0meVdv+xUEex9em2fqMnrk/scphGlGgg66zjMrPJESPwrFJ6sbfFQYUSa0Mz7FabA==
+"@babel/helper-member-expression-to-functions@^7.14.5", "@babel/helper-member-expression-to-functions@^7.14.7":
+  version "7.14.7"
+  resolved "https://registry.yarnpkg.com/@babel/helper-member-expression-to-functions/-/helper-member-expression-to-functions-7.14.7.tgz#97e56244beb94211fe277bd818e3a329c66f7970"
+  integrity sha512-TMUt4xKxJn6ccjcOW7c4hlwyJArizskAhoSTOCkA0uZ+KghIaci0Qg9R043kUMWI9mtQfgny+NQ5QATnZ+paaA==
   dependencies:
-    "@babel/types" "^7.5.5"
+    "@babel/types" "^7.14.5"
 
 "@babel/helper-module-imports@^7.0.0":
   version "7.0.0"
   resolved "https://registry.yarnpkg.com/@babel/helper-module-imports/-/helper-module-imports-7.0.0.tgz#96081b7111e486da4d2cd971ad1a4fe216cc2e3d"
   integrity sha512-aP/hlLq01DWNEiDg4Jn23i+CXxW/owM4WpDLFUbpjxe4NS3BhLVZQ5i7E0ZrxuQ/vwekIeciyamgB1UIYxxM6A==
   dependencies:
     "@babel/types" "^7.0.0"
 
-"@babel/helper-module-transforms@^7.1.0", "@babel/helper-module-transforms@^7.4.4":
-  version "7.5.5"
-  resolved "https://registry.yarnpkg.com/@babel/helper-module-transforms/-/helper-module-transforms-7.5.5.tgz#f84ff8a09038dcbca1fd4355661a500937165b4a"
-  integrity sha512-jBeCvETKuJqeiaCdyaheF40aXnnU1+wkSiUs/IQg3tB85up1LyL8x77ClY8qJpuRJUcXQo+ZtdNESmZl4j56Pw==
-  dependencies:
-    "@babel/helper-module-imports" "^7.0.0"
-    "@babel/helper-simple-access" "^7.1.0"
-    "@babel/helper-split-export-declaration" "^7.4.4"
-    "@babel/template" "^7.4.4"
-    "@babel/types" "^7.5.5"
-    lodash "^4.17.13"
-
-"@babel/helper-optimise-call-expression@^7.0.0":
-  version "7.0.0"
-  resolved "https://registry.yarnpkg.com/@babel/helper-optimise-call-expression/-/helper-optimise-call-expression-7.0.0.tgz#a2920c5702b073c15de51106200aa8cad20497d5"
-  integrity sha512-u8nd9NQePYNQV8iPWu/pLLYBqZBa4ZaY1YWRFMuxrid94wKI1QNt67NEZ7GAe5Kc/0LLScbim05xZFWkAdrj9g==
+"@babel/helper-module-imports@^7.14.5", "@babel/helper-module-imports@^7.7.0":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/helper-module-imports/-/helper-module-imports-7.14.5.tgz#6d1a44df6a38c957aa7c312da076429f11b422f3"
+  integrity sha512-SwrNHu5QWS84XlHwGYPDtCxcA0hrSlL2yhWYLgeOc0w7ccOl2qv4s/nARI0aYZW+bSwAL5CukeXA47B/1NKcnQ==
+  dependencies:
+    "@babel/types" "^7.14.5"
+
+"@babel/helper-module-transforms@^7.14.5", "@babel/helper-module-transforms@^7.7.0":
+  version "7.14.8"
+  resolved "https://registry.yarnpkg.com/@babel/helper-module-transforms/-/helper-module-transforms-7.14.8.tgz#d4279f7e3fd5f4d5d342d833af36d4dd87d7dc49"
+  integrity sha512-RyE+NFOjXn5A9YU1dkpeBaduagTlZ0+fccnIcAGbv1KGUlReBj7utF7oEth8IdIBQPcux0DDgW5MFBH2xu9KcA==
+  dependencies:
+    "@babel/helper-module-imports" "^7.14.5"
+    "@babel/helper-replace-supers" "^7.14.5"
+    "@babel/helper-simple-access" "^7.14.8"
+    "@babel/helper-split-export-declaration" "^7.14.5"
+    "@babel/helper-validator-identifier" "^7.14.8"
+    "@babel/template" "^7.14.5"
+    "@babel/traverse" "^7.14.8"
+    "@babel/types" "^7.14.8"
+
+"@babel/helper-optimise-call-expression@^7.14.5":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/helper-optimise-call-expression/-/helper-optimise-call-expression-7.14.5.tgz#f27395a8619e0665b3f0364cddb41c25d71b499c"
+  integrity sha512-IqiLIrODUOdnPU9/F8ib1Fx2ohlgDhxnIDU7OEVi+kAbEZcyiF7BLU8W6PfvPi9LzztjS7kcbzbmL7oG8kD6VA==
   dependencies:
-    "@babel/types" "^7.0.0"
+    "@babel/types" "^7.14.5"
 
 "@babel/helper-plugin-utils@^7.0.0":
   version "7.0.0"
   resolved "https://registry.yarnpkg.com/@babel/helper-plugin-utils/-/helper-plugin-utils-7.0.0.tgz#bbb3fbee98661c569034237cc03967ba99b4f250"
   integrity sha512-CYAOUCARwExnEixLdB6sDm2dIJ/YgEAKDM1MOeMeZu9Ld/bDgVo8aiWrXwcY7OBh+1Ea2uUcVRcxKk0GJvW7QA==
 
+"@babel/helper-plugin-utils@^7.10.4", "@babel/helper-plugin-utils@^7.14.5", "@babel/helper-plugin-utils@^7.8.0", "@babel/helper-plugin-utils@^7.8.3":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/helper-plugin-utils/-/helper-plugin-utils-7.14.5.tgz#5ac822ce97eec46741ab70a517971e443a70c5a9"
+  integrity sha512-/37qQCE3K0vvZKwoK4XU/irIJQdIfCJuhU5eKnNxpFDsOkgFaUAwbv+RYw6eYgsC0E4hS7r5KqGULUogqui0fQ==
+
 "@babel/helper-regex@^7.0.0", "@babel/helper-regex@^7.4.4":
   version "7.5.5"
   resolved "https://registry.yarnpkg.com/@babel/helper-regex/-/helper-regex-7.5.5.tgz#0aa6824f7100a2e0e89c1527c23936c152cab351"
   integrity sha512-CkCYQLkfkiugbRDO8eZn6lRuR8kzZoGXCg3149iTk5se7g6qykSpy3+hELSwquhu+TgHn8nkLiBwHvNX8Hofcw==
   dependencies:
     lodash "^4.17.13"
 
-"@babel/helper-remap-async-to-generator@^7.1.0":
-  version "7.1.0"
-  resolved "https://registry.yarnpkg.com/@babel/helper-remap-async-to-generator/-/helper-remap-async-to-generator-7.1.0.tgz#361d80821b6f38da75bd3f0785ece20a88c5fe7f"
-  integrity sha512-3fOK0L+Fdlg8S5al8u/hWE6vhufGSn0bN09xm2LXMy//REAF8kDCrYoOBKYmA8m5Nom+sV9LyLCwrFynA8/slg==
-  dependencies:
-    "@babel/helper-annotate-as-pure" "^7.0.0"
-    "@babel/helper-wrap-function" "^7.1.0"
-    "@babel/template" "^7.1.0"
-    "@babel/traverse" "^7.1.0"
-    "@babel/types" "^7.0.0"
-
-"@babel/helper-replace-supers@^7.5.5":
-  version "7.5.5"
-  resolved "https://registry.yarnpkg.com/@babel/helper-replace-supers/-/helper-replace-supers-7.5.5.tgz#f84ce43df031222d2bad068d2626cb5799c34bc2"
-  integrity sha512-XvRFWrNnlsow2u7jXDuH4jDDctkxbS7gXssrP4q2nUD606ukXHRvydj346wmNg+zAgpFx4MWf4+usfC93bElJg==
-  dependencies:
-    "@babel/helper-member-expression-to-functions" "^7.5.5"
-    "@babel/helper-optimise-call-expression" "^7.0.0"
-    "@babel/traverse" "^7.5.5"
-    "@babel/types" "^7.5.5"
-
-"@babel/helper-simple-access@^7.1.0":
-  version "7.1.0"
-  resolved "https://registry.yarnpkg.com/@babel/helper-simple-access/-/helper-simple-access-7.1.0.tgz#65eeb954c8c245beaa4e859da6188f39d71e585c"
-  integrity sha512-Vk+78hNjRbsiu49zAPALxTb+JUQCz1aolpd8osOF16BGnLtseD21nbHgLPGUwrXEurZgiCOUmvs3ExTu4F5x6w==
+"@babel/helper-remap-async-to-generator@^7.14.5":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/helper-remap-async-to-generator/-/helper-remap-async-to-generator-7.14.5.tgz#51439c913612958f54a987a4ffc9ee587a2045d6"
+  integrity sha512-rLQKdQU+HYlxBwQIj8dk4/0ENOUEhA/Z0l4hN8BexpvmSMN9oA9EagjnhnDpNsRdWCfjwa4mn/HyBXO9yhQP6A==
+  dependencies:
+    "@babel/helper-annotate-as-pure" "^7.14.5"
+    "@babel/helper-wrap-function" "^7.14.5"
+    "@babel/types" "^7.14.5"
+
+"@babel/helper-replace-supers@^7.14.5":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/helper-replace-supers/-/helper-replace-supers-7.14.5.tgz#0ecc0b03c41cd567b4024ea016134c28414abb94"
+  integrity sha512-3i1Qe9/8x/hCHINujn+iuHy+mMRLoc77b2nI9TB0zjH1hvn9qGlXjWlggdwUcju36PkPCy/lpM7LLUdcTyH4Ow==
+  dependencies:
+    "@babel/helper-member-expression-to-functions" "^7.14.5"
+    "@babel/helper-optimise-call-expression" "^7.14.5"
+    "@babel/traverse" "^7.14.5"
+    "@babel/types" "^7.14.5"
+
+"@babel/helper-simple-access@^7.14.5", "@babel/helper-simple-access@^7.14.8", "@babel/helper-simple-access@^7.7.0":
+  version "7.14.8"
+  resolved "https://registry.yarnpkg.com/@babel/helper-simple-access/-/helper-simple-access-7.14.8.tgz#82e1fec0644a7e775c74d305f212c39f8fe73924"
+  integrity sha512-TrFN4RHh9gnWEU+s7JloIho2T76GPwRHhdzOWLqTrMnlas8T9O7ec+oEDNsRXndOmru9ymH9DFrEOxpzPoSbdg==
+  dependencies:
+    "@babel/types" "^7.14.8"
+
+"@babel/helper-skip-transparent-expression-wrappers@^7.14.5":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/helper-skip-transparent-expression-wrappers/-/helper-skip-transparent-expression-wrappers-7.14.5.tgz#96f486ac050ca9f44b009fbe5b7d394cab3a0ee4"
+  integrity sha512-dmqZB7mrb94PZSAOYtr+ZN5qt5owZIAgqtoTuqiFbHFtxgEcmQlRJVI+bO++fciBunXtB6MK7HrzrfcAzIz2NQ==
+  dependencies:
+    "@babel/types" "^7.14.5"
+
+"@babel/helper-split-export-declaration@^7.14.5":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/helper-split-export-declaration/-/helper-split-export-declaration-7.14.5.tgz#22b23a54ef51c2b7605d851930c1976dd0bc693a"
+  integrity sha512-hprxVPu6e5Kdp2puZUmvOGjaLv9TCe58E/Fl6hRq4YiVQxIcNvuq6uTM2r1mT/oPskuS9CgR+I94sqAYv0NGKA==
   dependencies:
-    "@babel/template" "^7.1.0"
-    "@babel/types" "^7.0.0"
+    "@babel/types" "^7.14.5"
 
 "@babel/helper-split-export-declaration@^7.4.4":
   version "7.4.4"
   resolved "https://registry.yarnpkg.com/@babel/helper-split-export-declaration/-/helper-split-export-declaration-7.4.4.tgz#ff94894a340be78f53f06af038b205c49d993677"
   integrity sha512-Ro/XkzLf3JFITkW6b+hNxzZ1n5OQ80NvIUdmHspih1XAhtN3vPTuUFT4eQnela+2MaZ5ulH+iyP513KJrxbN7Q==
   dependencies:
     "@babel/types" "^7.4.4"
 
-"@babel/helper-wrap-function@^7.1.0":
-  version "7.2.0"
-  resolved "https://registry.yarnpkg.com/@babel/helper-wrap-function/-/helper-wrap-function-7.2.0.tgz#c4e0012445769e2815b55296ead43a958549f6fa"
-  integrity sha512-o9fP1BZLLSrYlxYEYyl2aS+Flun5gtjTIG8iln+XuEzQTs0PLagAGSXUcqruJwD5fM48jzIEggCKpIfWTcR7pQ==
-  dependencies:
-    "@babel/helper-function-name" "^7.1.0"
-    "@babel/template" "^7.1.0"
-    "@babel/traverse" "^7.1.0"
-    "@babel/types" "^7.2.0"
-
-"@babel/helpers@^7.4.4":
-  version "7.6.2"
-  resolved "https://registry.yarnpkg.com/@babel/helpers/-/helpers-7.6.2.tgz#681ffe489ea4dcc55f23ce469e58e59c1c045153"
-  integrity sha512-3/bAUL8zZxYs1cdX2ilEE0WobqbCmKWr/889lf2SS0PpDcpEIY8pb1CCyz0pEcX3pEb+MCbks1jIokz2xLtGTA==
-  dependencies:
-    "@babel/template" "^7.6.0"
-    "@babel/traverse" "^7.6.2"
-    "@babel/types" "^7.6.0"
+"@babel/helper-validator-identifier@^7.14.5", "@babel/helper-validator-identifier@^7.14.8":
+  version "7.14.8"
+  resolved "https://registry.yarnpkg.com/@babel/helper-validator-identifier/-/helper-validator-identifier-7.14.8.tgz#32be33a756f29e278a0d644fa08a2c9e0f88a34c"
+  integrity sha512-ZGy6/XQjllhYQrNw/3zfWRwZCTVSiBLZ9DHVZxn9n2gip/7ab8mv2TWlKPIBk26RwedCBoWdjLmn+t9na2Gcow==
+
+"@babel/helper-validator-option@^7.14.5":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/helper-validator-option/-/helper-validator-option-7.14.5.tgz#6e72a1fff18d5dfcb878e1e62f1a021c4b72d5a3"
+  integrity sha512-OX8D5eeX4XwcroVW45NMvoYaIuFI+GQpA2a8Gi+X/U/cDUIRsV37qQfF905F0htTRCREQIB4KqPeaveRJUl3Ow==
+
+"@babel/helper-wrap-function@^7.14.5":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/helper-wrap-function/-/helper-wrap-function-7.14.5.tgz#5919d115bf0fe328b8a5d63bcb610f51601f2bff"
+  integrity sha512-YEdjTCq+LNuNS1WfxsDCNpgXkJaIyqco6DAelTUjT4f2KIWC1nBcaCaSdHTBqQVLnTBexBcVcFhLSU1KnYuePQ==
+  dependencies:
+    "@babel/helper-function-name" "^7.14.5"
+    "@babel/template" "^7.14.5"
+    "@babel/traverse" "^7.14.5"
+    "@babel/types" "^7.14.5"
+
+"@babel/helpers@^7.7.0":
+  version "7.14.8"
+  resolved "https://registry.yarnpkg.com/@babel/helpers/-/helpers-7.14.8.tgz#839f88f463025886cff7f85a35297007e2da1b77"
+  integrity sha512-ZRDmI56pnV+p1dH6d+UN6GINGz7Krps3+270qqI9UJ4wxYThfAIcI5i7j5vXC4FJ3Wap+S9qcebxeYiqn87DZw==
+  dependencies:
+    "@babel/template" "^7.14.5"
+    "@babel/traverse" "^7.14.8"
+    "@babel/types" "^7.14.8"
 
 "@babel/highlight@^7.0.0":
   version "7.5.0"
   resolved "https://registry.yarnpkg.com/@babel/highlight/-/highlight-7.5.0.tgz#56d11312bd9248fa619591d02472be6e8cb32540"
   integrity sha512-7dV4eu9gBxoM0dAnj/BCFDW9LFU0zvTrkq0ugM7pnHEgguOEeOz1so2ZghEdzviYzQEED0r4EAgpsBChKy1TRQ==
   dependencies:
     chalk "^2.0.0"
     esutils "^2.0.2"
     js-tokens "^4.0.0"
 
-"@babel/parser@^7.4.5", "@babel/parser@^7.6.0", "@babel/parser@^7.6.2":
+"@babel/highlight@^7.14.5":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/highlight/-/highlight-7.14.5.tgz#6861a52f03966405001f6aa534a01a24d99e8cd9"
+  integrity sha512-qf9u2WFWVV0MppaL877j2dBtQIDgmidgjGk5VIMw3OadXvYaXn66U1BFlH2t4+t3i+8PhedppRv+i40ABzd+gg==
+  dependencies:
+    "@babel/helper-validator-identifier" "^7.14.5"
+    chalk "^2.0.0"
+    js-tokens "^4.0.0"
+
+"@babel/parser@^7.14.5", "@babel/parser@^7.14.8", "@babel/parser@^7.7.2":
+  version "7.14.8"
+  resolved "https://registry.yarnpkg.com/@babel/parser/-/parser-7.14.8.tgz#66fd41666b2d7b840bd5ace7f7416d5ac60208d4"
+  integrity sha512-syoCQFOoo/fzkWDeM0dLEZi5xqurb5vuyzwIMNZRNun+N/9A4cUZeQaE7dTrB8jGaKuJRBtEOajtnmw0I5hvvA==
+
+"@babel/parser@^7.6.0", "@babel/parser@^7.6.2":
   version "7.6.2"
   resolved "https://registry.yarnpkg.com/@babel/parser/-/parser-7.6.2.tgz#205e9c95e16ba3b8b96090677a67c9d6075b70a1"
   integrity sha512-mdFqWrSPCmikBoaBYMuBulzTIKuXVPtEISFbRRVNwMWpCms/hmE2kRq0bblUHaNRKrjRlmVbx1sDHmjmRgD2Xg==
 
-"@babel/plugin-proposal-async-generator-functions@^7.2.0":
-  version "7.2.0"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-proposal-async-generator-functions/-/plugin-proposal-async-generator-functions-7.2.0.tgz#b289b306669dce4ad20b0252889a15768c9d417e"
-  integrity sha512-+Dfo/SCQqrwx48ptLVGLdE39YtWRuKc/Y9I5Fy0P1DDBB9lsAHpjcEJQt+4IifuSOSTLBKJObJqMvaO1pIE8LQ==
+"@babel/plugin-proposal-async-generator-functions@^7.7.0":
+  version "7.14.7"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-proposal-async-generator-functions/-/plugin-proposal-async-generator-functions-7.14.7.tgz#784a48c3d8ed073f65adcf30b57bcbf6c8119ace"
+  integrity sha512-RK8Wj7lXLY3bqei69/cc25gwS5puEc3dknoFPFbqfy3XxYQBQFvu4ioWpafMBAB+L9NyptQK4nMOa5Xz16og8Q==
+  dependencies:
+    "@babel/helper-plugin-utils" "^7.14.5"
+    "@babel/helper-remap-async-to-generator" "^7.14.5"
+    "@babel/plugin-syntax-async-generators" "^7.8.4"
+
+"@babel/plugin-proposal-class-properties@7.7.0":
+  version "7.7.0"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-proposal-class-properties/-/plugin-proposal-class-properties-7.7.0.tgz#ac54e728ecf81d90e8f4d2a9c05a890457107917"
+  integrity sha512-tufDcFA1Vj+eWvwHN+jvMN6QsV5o+vUlytNKrbMiCeDL0F2j92RURzUsUMWE5EJkLyWxjdUslCsMQa9FWth16A==
+  dependencies:
+    "@babel/helper-create-class-features-plugin" "^7.7.0"
+    "@babel/helper-plugin-utils" "^7.0.0"
+
+"@babel/plugin-proposal-dynamic-import@^7.7.0":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-proposal-dynamic-import/-/plugin-proposal-dynamic-import-7.14.5.tgz#0c6617df461c0c1f8fff3b47cd59772360101d2c"
+  integrity sha512-ExjiNYc3HDN5PXJx+bwC50GIx/KKanX2HiggnIUAYedbARdImiCU4RhhHfdf0Kd7JNXGpsBBBCOm+bBVy3Gb0g==
   dependencies:
-    "@babel/helper-plugin-utils" "^7.0.0"
-    "@babel/helper-remap-async-to-generator" "^7.1.0"
-    "@babel/plugin-syntax-async-generators" "^7.2.0"
-
-"@babel/plugin-proposal-class-properties@7.4.4":
-  version "7.4.4"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-proposal-class-properties/-/plugin-proposal-class-properties-7.4.4.tgz#93a6486eed86d53452ab9bab35e368e9461198ce"
-  integrity sha512-WjKTI8g8d5w1Bc9zgwSz2nfrsNQsXcCf9J9cdCvrJV6RF56yztwm4TmJC0MgJ9tvwO9gUA/mcYe89bLdGfiXFg==
-  dependencies:
-    "@babel/helper-create-class-features-plugin" "^7.4.4"
-    "@babel/helper-plugin-utils" "^7.0.0"
+    "@babel/helper-plugin-utils" "^7.14.5"
+    "@babel/plugin-syntax-dynamic-import" "^7.8.3"
 
 "@babel/plugin-proposal-json-strings@^7.2.0":
   version "7.2.0"
   resolved "https://registry.yarnpkg.com/@babel/plugin-proposal-json-strings/-/plugin-proposal-json-strings-7.2.0.tgz#568ecc446c6148ae6b267f02551130891e29f317"
   integrity sha512-MAFV1CA/YVmYwZG0fBQyXhmj0BHCB5egZHCKWIFVv/XCxAeVGIHfos3SwDck4LvCllENIAg7xMKOG5kH0dzyUg==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
     "@babel/plugin-syntax-json-strings" "^7.2.0"
 
-"@babel/plugin-proposal-object-rest-spread@7.4.4":
-  version "7.4.4"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-proposal-object-rest-spread/-/plugin-proposal-object-rest-spread-7.4.4.tgz#1ef173fcf24b3e2df92a678f027673b55e7e3005"
-  integrity sha512-dMBG6cSPBbHeEBdFXeQ2QLc5gUpg4Vkaz8octD4aoW/ISO+jBOcsuxYL7bsb5WSu8RLP6boxrBIALEHgoHtO9g==
+"@babel/plugin-proposal-nullish-coalescing-operator@7.7.4":
+  version "7.7.4"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-proposal-nullish-coalescing-operator/-/plugin-proposal-nullish-coalescing-operator-7.7.4.tgz#7db302c83bc30caa89e38fee935635ef6bd11c28"
+  integrity sha512-TbYHmr1Gl1UC7Vo2HVuj/Naci5BEGNZ0AJhzqD2Vpr6QPFWpUmBRLrIDjedzx7/CShq0bRDS2gI4FIs77VHLVQ==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
-    "@babel/plugin-syntax-object-rest-spread" "^7.2.0"
+    "@babel/plugin-syntax-nullish-coalescing-operator" "^7.7.4"
+
+"@babel/plugin-proposal-numeric-separator@7.8.3":
+  version "7.8.3"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-proposal-numeric-separator/-/plugin-proposal-numeric-separator-7.8.3.tgz#5d6769409699ec9b3b68684cd8116cedff93bad8"
+  integrity sha512-jWioO1s6R/R+wEHizfaScNsAx+xKgwTLNXSh7tTC4Usj3ItsPEhYkEpU4h+lpnBwq7NBVOJXfO6cRFYcX69JUQ==
+  dependencies:
+    "@babel/helper-plugin-utils" "^7.8.3"
+    "@babel/plugin-syntax-numeric-separator" "^7.8.3"
 
-"@babel/plugin-proposal-object-rest-spread@^7.4.4":
+"@babel/plugin-proposal-object-rest-spread@7.6.2":
   version "7.6.2"
   resolved "https://registry.yarnpkg.com/@babel/plugin-proposal-object-rest-spread/-/plugin-proposal-object-rest-spread-7.6.2.tgz#8ffccc8f3a6545e9f78988b6bf4fe881b88e8096"
   integrity sha512-LDBXlmADCsMZV1Y9OQwMc0MyGZ8Ta/zlD9N67BfQT8uYwkRswiu2hU6nJKrjrt/58aH/vqfQlR/9yId/7A2gWw==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
     "@babel/plugin-syntax-object-rest-spread" "^7.2.0"
 
+"@babel/plugin-proposal-object-rest-spread@^7.6.2":
+  version "7.14.7"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-proposal-object-rest-spread/-/plugin-proposal-object-rest-spread-7.14.7.tgz#5920a2b3df7f7901df0205974c0641b13fd9d363"
+  integrity sha512-082hsZz+sVabfmDWo1Oct1u1AgbKbUAyVgmX4otIc7bdsRgHBXwTwb3DpDmD4Eyyx6DNiuz5UAATT655k+kL5g==
+  dependencies:
+    "@babel/compat-data" "^7.14.7"
+    "@babel/helper-compilation-targets" "^7.14.5"
+    "@babel/helper-plugin-utils" "^7.14.5"
+    "@babel/plugin-syntax-object-rest-spread" "^7.8.3"
+    "@babel/plugin-transform-parameters" "^7.14.5"
+
 "@babel/plugin-proposal-optional-catch-binding@^7.2.0":
   version "7.2.0"
   resolved "https://registry.yarnpkg.com/@babel/plugin-proposal-optional-catch-binding/-/plugin-proposal-optional-catch-binding-7.2.0.tgz#135d81edb68a081e55e56ec48541ece8065c38f5"
   integrity sha512-mgYj3jCcxug6KUcX4OBoOJz3CMrwRfQELPQ5560F70YQUBZB7uac9fqaWamKR1iWUzGiK2t0ygzjTScZnVz75g==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
     "@babel/plugin-syntax-optional-catch-binding" "^7.2.0"
 
+"@babel/plugin-proposal-optional-chaining@7.7.4":
+  version "7.7.4"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-proposal-optional-chaining/-/plugin-proposal-optional-chaining-7.7.4.tgz#3f04c2de1a942cbd3008324df8144b9cbc0ca0ba"
+  integrity sha512-JmgaS+ygAWDR/STPe3/7y0lNlHgS+19qZ9aC06nYLwQ/XB7c0q5Xs+ksFU3EDnp9EiEsO0dnRAOKeyLHTZuW3A==
+  dependencies:
+    "@babel/helper-plugin-utils" "^7.0.0"
+    "@babel/plugin-syntax-optional-chaining" "^7.7.4"
+
 "@babel/plugin-proposal-unicode-property-regex@^7.4.4":
   version "7.6.2"
   resolved "https://registry.yarnpkg.com/@babel/plugin-proposal-unicode-property-regex/-/plugin-proposal-unicode-property-regex-7.6.2.tgz#05413762894f41bfe42b9a5e80919bd575dcc802"
   integrity sha512-NxHETdmpeSCtiatMRYWVJo7266rrvAC3DTeG5exQBIH/fMIUK7ejDNznBbn3HQl/o9peymRRg7Yqkx6PdUXmMw==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
     "@babel/helper-regex" "^7.4.4"
     regexpu-core "^4.6.0"
 
+"@babel/plugin-proposal-unicode-property-regex@^7.7.0":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-proposal-unicode-property-regex/-/plugin-proposal-unicode-property-regex-7.14.5.tgz#0f95ee0e757a5d647f378daa0eca7e93faa8bbe8"
+  integrity sha512-6axIeOU5LnY471KenAB9vI8I5j7NQ2d652hIYwVyRfgaZT5UpiqFKCuVXCDMSrU+3VFafnu2c5m3lrWIlr6A5Q==
+  dependencies:
+    "@babel/helper-create-regexp-features-plugin" "^7.14.5"
+    "@babel/helper-plugin-utils" "^7.14.5"
+
 "@babel/plugin-syntax-async-generators@^7.2.0":
   version "7.2.0"
   resolved "https://registry.yarnpkg.com/@babel/plugin-syntax-async-generators/-/plugin-syntax-async-generators-7.2.0.tgz#69e1f0db34c6f5a0cf7e2b3323bf159a76c8cb7f"
   integrity sha512-1ZrIRBv2t0GSlcwVoQ6VgSLpLgiN/FVQUzt9znxo7v2Ov4jJrs8RY8tv0wvDmFN3qIdMKWrmMMW6yZ0G19MfGg==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
 
+"@babel/plugin-syntax-async-generators@^7.8.4":
+  version "7.8.4"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-syntax-async-generators/-/plugin-syntax-async-generators-7.8.4.tgz#a983fb1aeb2ec3f6ed042a210f640e90e786fe0d"
+  integrity sha512-tycmZxkGfZaxhMRbXlPXuVFpdWlXpir2W4AMhSJgRKzk/eDlIXOhb2LHWoLpDF7TEHylV5zNhykX6KAgHJmTNw==
+  dependencies:
+    "@babel/helper-plugin-utils" "^7.8.0"
+
+"@babel/plugin-syntax-bigint@7.8.3":
+  version "7.8.3"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-syntax-bigint/-/plugin-syntax-bigint-7.8.3.tgz#4c9a6f669f5d0cdf1b90a1671e9a146be5300cea"
+  integrity sha512-wnTnFlG+YxQm3vDxpGE57Pj0srRU4sHE/mDkt1qv2YJJSeUAec2ma4WLUnUPeKjyrfntVwe/N6dCXpU+zL3Npg==
+  dependencies:
+    "@babel/helper-plugin-utils" "^7.8.0"
+
 "@babel/plugin-syntax-dynamic-import@7.2.0":
   version "7.2.0"
   resolved "https://registry.yarnpkg.com/@babel/plugin-syntax-dynamic-import/-/plugin-syntax-dynamic-import-7.2.0.tgz#69c159ffaf4998122161ad8ebc5e6d1f55df8612"
   integrity sha512-mVxuJ0YroI/h/tbFTPGZR8cv6ai+STMKNBq0f8hFxsxWjl94qqhsb+wXbpNMDPU3cfR1TIsVFzU3nXyZMqyK4w==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
 
+"@babel/plugin-syntax-dynamic-import@^7.2.0", "@babel/plugin-syntax-dynamic-import@^7.8.3":
+  version "7.8.3"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-syntax-dynamic-import/-/plugin-syntax-dynamic-import-7.8.3.tgz#62bf98b2da3cd21d626154fc96ee5b3cb68eacb3"
+  integrity sha512-5gdGbFon+PszYzqs83S3E5mpi7/y/8M9eC90MRTZfduQOYW76ig6SOSPNe41IG5LoP3FGBn2N0RjVDSQiS94kQ==
+  dependencies:
+    "@babel/helper-plugin-utils" "^7.8.0"
+
 "@babel/plugin-syntax-json-strings@^7.2.0":
   version "7.2.0"
   resolved "https://registry.yarnpkg.com/@babel/plugin-syntax-json-strings/-/plugin-syntax-json-strings-7.2.0.tgz#72bd13f6ffe1d25938129d2a186b11fd62951470"
   integrity sha512-5UGYnMSLRE1dqqZwug+1LISpA403HzlSfsg6P9VXU6TBjcSHeNlw4DxDx7LgpF+iKZoOG/+uzqoRHTdcUpiZNg==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
 
+"@babel/plugin-syntax-jsx@^7.14.5":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-syntax-jsx/-/plugin-syntax-jsx-7.14.5.tgz#000e2e25d8673cce49300517a3eda44c263e4201"
+  integrity sha512-ohuFIsOMXJnbOMRfX7/w7LocdR6R7whhuRD4ax8IipLcLPlZGJKkBxgHp++U4N/vKyU16/YDQr2f5seajD3jIw==
+  dependencies:
+    "@babel/helper-plugin-utils" "^7.14.5"
+
 "@babel/plugin-syntax-jsx@^7.2.0":
   version "7.2.0"
   resolved "https://registry.yarnpkg.com/@babel/plugin-syntax-jsx/-/plugin-syntax-jsx-7.2.0.tgz#0b85a3b4bc7cdf4cc4b8bf236335b907ca22e7c7"
   integrity sha512-VyN4QANJkRW6lDBmENzRszvZf3/4AXaj9YR7GwrWeeN9tEBPuXbmDYVU9bYBN0D70zCWVwUy0HWq2553VCb6Hw==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
 
+"@babel/plugin-syntax-nullish-coalescing-operator@^7.7.4":
+  version "7.8.3"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-syntax-nullish-coalescing-operator/-/plugin-syntax-nullish-coalescing-operator-7.8.3.tgz#167ed70368886081f74b5c36c65a88c03b66d1a9"
+  integrity sha512-aSff4zPII1u2QD7y+F8oDsz19ew4IGEJg9SVW+bqwpwtfFleiQDMdzA/R+UlWDzfnHFCxxleFT0PMIrR36XLNQ==
+  dependencies:
+    "@babel/helper-plugin-utils" "^7.8.0"
+
+"@babel/plugin-syntax-numeric-separator@^7.8.3":
+  version "7.10.4"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-syntax-numeric-separator/-/plugin-syntax-numeric-separator-7.10.4.tgz#b9b070b3e33570cd9fd07ba7fa91c0dd37b9af97"
+  integrity sha512-9H6YdfkcK/uOnY/K7/aA2xpzaAgkQn37yzWUMRK7OaPOqOpGS1+n0H5hxT9AUw9EsSjPW8SVyMJwYRtWs3X3ug==
+  dependencies:
+    "@babel/helper-plugin-utils" "^7.10.4"
+
 "@babel/plugin-syntax-object-rest-spread@^7.2.0":
   version "7.2.0"
   resolved "https://registry.yarnpkg.com/@babel/plugin-syntax-object-rest-spread/-/plugin-syntax-object-rest-spread-7.2.0.tgz#3b7a3e733510c57e820b9142a6579ac8b0dfad2e"
   integrity sha512-t0JKGgqk2We+9may3t0xDdmneaXmyxq0xieYcKHxIsrJO64n1OiMWNUtc5gQK1PA0NpdCRrtZp4z+IUaKugrSA==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
 
+"@babel/plugin-syntax-object-rest-spread@^7.8.3":
+  version "7.8.3"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-syntax-object-rest-spread/-/plugin-syntax-object-rest-spread-7.8.3.tgz#60e225edcbd98a640332a2e72dd3e66f1af55871"
+  integrity sha512-XoqMijGZb9y3y2XskN+P1wUGiVwWZ5JmoDRwx5+3GmEplNyVM2s2Dg8ILFQm8rWM48orGy5YpI5Bl8U1y7ydlA==
+  dependencies:
+    "@babel/helper-plugin-utils" "^7.8.0"
+
 "@babel/plugin-syntax-optional-catch-binding@^7.2.0":
   version "7.2.0"
   resolved "https://registry.yarnpkg.com/@babel/plugin-syntax-optional-catch-binding/-/plugin-syntax-optional-catch-binding-7.2.0.tgz#a94013d6eda8908dfe6a477e7f9eda85656ecf5c"
   integrity sha512-bDe4xKNhb0LI7IvZHiA13kff0KEfaGX/Hv4lMA9+7TEc63hMNvfKo6ZFpXhKuEp+II/q35Gc4NoMeDZyaUbj9w==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
 
-"@babel/plugin-syntax-typescript@^7.2.0":
-  version "7.3.3"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-syntax-typescript/-/plugin-syntax-typescript-7.3.3.tgz#a7cc3f66119a9f7ebe2de5383cce193473d65991"
-  integrity sha512-dGwbSMA1YhVS8+31CnPR7LB4pcbrzcV99wQzby4uAfrkZPYZlQ7ImwdpzLqi6Z6IL02b8IAL379CaMwo0x5Lag==
+"@babel/plugin-syntax-optional-chaining@^7.7.4":
+  version "7.8.3"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-syntax-optional-chaining/-/plugin-syntax-optional-chaining-7.8.3.tgz#4f69c2ab95167e0180cd5336613f8c5788f7d48a"
+  integrity sha512-KoK9ErH1MBlCPxV0VANkXW2/dw4vlbGDrFgz8bmUsBGYkFRcbRwMh6cIJubdPrkxRwuGdtCk0v/wPTKbQgBjkg==
+  dependencies:
+    "@babel/helper-plugin-utils" "^7.8.0"
+
+"@babel/plugin-syntax-top-level-await@^7.7.0":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-syntax-top-level-await/-/plugin-syntax-top-level-await-7.14.5.tgz#c1cfdadc35a646240001f06138247b741c34d94c"
+  integrity sha512-hx++upLv5U1rgYfwe1xBQUhRmU41NEvpUvrp8jkrSCdvGSnM5/qdRMtylJ6PG5OFkBaHkbTAKTnd3/YyESRHFw==
+  dependencies:
+    "@babel/helper-plugin-utils" "^7.14.5"
+
+"@babel/plugin-syntax-typescript@^7.14.5":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-syntax-typescript/-/plugin-syntax-typescript-7.14.5.tgz#b82c6ce471b165b5ce420cf92914d6fb46225716"
+  integrity sha512-u6OXzDaIXjEstBRRoBCQ/uKQKlbuaeE5in0RvWdA4pN6AhqxTIwUsnHPU1CFZA/amYObMsuWhYfRl3Ch90HD0Q==
   dependencies:
-    "@babel/helper-plugin-utils" "^7.0.0"
+    "@babel/helper-plugin-utils" "^7.14.5"
 
 "@babel/plugin-transform-arrow-functions@^7.2.0":
   version "7.2.0"
   resolved "https://registry.yarnpkg.com/@babel/plugin-transform-arrow-functions/-/plugin-transform-arrow-functions-7.2.0.tgz#9aeafbe4d6ffc6563bf8f8372091628f00779550"
   integrity sha512-ER77Cax1+8/8jCB9fo4Ud161OZzWN5qawi4GusDuRLcDbDG+bIGYY20zb2dfAFdTRGzrfq2xZPvF0R64EHnimg==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
 
-"@babel/plugin-transform-async-to-generator@^7.4.4":
-  version "7.5.0"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-async-to-generator/-/plugin-transform-async-to-generator-7.5.0.tgz#89a3848a0166623b5bc481164b5936ab947e887e"
-  integrity sha512-mqvkzwIGkq0bEF1zLRRiTdjfomZJDV33AH3oQzHVGkI2VzEmXLpKKOBvEVaFZBJdN0XTyH38s9j/Kiqr68dggg==
-  dependencies:
-    "@babel/helper-module-imports" "^7.0.0"
-    "@babel/helper-plugin-utils" "^7.0.0"
-    "@babel/helper-remap-async-to-generator" "^7.1.0"
+"@babel/plugin-transform-async-to-generator@^7.7.0":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-async-to-generator/-/plugin-transform-async-to-generator-7.14.5.tgz#72c789084d8f2094acb945633943ef8443d39e67"
+  integrity sha512-szkbzQ0mNk0rpu76fzDdqSyPu0MuvpXgC+6rz5rpMb5OIRxdmHfQxrktL8CYolL2d8luMCZTR0DpIMIdL27IjA==
+  dependencies:
+    "@babel/helper-module-imports" "^7.14.5"
+    "@babel/helper-plugin-utils" "^7.14.5"
+    "@babel/helper-remap-async-to-generator" "^7.14.5"
 
 "@babel/plugin-transform-block-scoped-functions@^7.2.0":
   version "7.2.0"
   resolved "https://registry.yarnpkg.com/@babel/plugin-transform-block-scoped-functions/-/plugin-transform-block-scoped-functions-7.2.0.tgz#5d3cc11e8d5ddd752aa64c9148d0db6cb79fd190"
   integrity sha512-ntQPR6q1/NKuphly49+QiQiTN0O63uOwjdD6dhIjSWBI5xlrbUFh720TIpzBhpnrLfv2tNH/BXvLIab1+BAI0w==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
 
-"@babel/plugin-transform-block-scoping@^7.4.4":
-  version "7.6.2"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-block-scoping/-/plugin-transform-block-scoping-7.6.2.tgz#96c33ab97a9ae500cc6f5b19e04a7e6553360a79"
-  integrity sha512-zZT8ivau9LOQQaOGC7bQLQOT4XPkPXgN2ERfUgk1X8ql+mVkLc4E8eKk+FO3o0154kxzqenWCorfmEXpEZcrSQ==
-  dependencies:
-    "@babel/helper-plugin-utils" "^7.0.0"
-    lodash "^4.17.13"
-
-"@babel/plugin-transform-classes@^7.4.4":
-  version "7.5.5"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-classes/-/plugin-transform-classes-7.5.5.tgz#d094299d9bd680a14a2a0edae38305ad60fb4de9"
-  integrity sha512-U2htCNK/6e9K7jGyJ++1p5XRU+LJjrwtoiVn9SzRlDT2KubcZ11OOwy3s24TjHxPgxNwonCYP7U2K51uVYCMDg==
-  dependencies:
-    "@babel/helper-annotate-as-pure" "^7.0.0"
-    "@babel/helper-define-map" "^7.5.5"
-    "@babel/helper-function-name" "^7.1.0"
-    "@babel/helper-optimise-call-expression" "^7.0.0"
-    "@babel/helper-plugin-utils" "^7.0.0"
-    "@babel/helper-replace-supers" "^7.5.5"
-    "@babel/helper-split-export-declaration" "^7.4.4"
+"@babel/plugin-transform-block-scoping@^7.6.3":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-block-scoping/-/plugin-transform-block-scoping-7.14.5.tgz#8cc63e61e50f42e078e6f09be775a75f23ef9939"
+  integrity sha512-LBYm4ZocNgoCqyxMLoOnwpsmQ18HWTQvql64t3GvMUzLQrNoV1BDG0lNftC8QKYERkZgCCT/7J5xWGObGAyHDw==
+  dependencies:
+    "@babel/helper-plugin-utils" "^7.14.5"
+
+"@babel/plugin-transform-classes@^7.7.0":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-classes/-/plugin-transform-classes-7.14.5.tgz#0e98e82097b38550b03b483f9b51a78de0acb2cf"
+  integrity sha512-J4VxKAMykM06K/64z9rwiL6xnBHgB1+FVspqvlgCdwD1KUbQNfszeKVVOMh59w3sztHYIZDgnhOC4WbdEfHFDA==
+  dependencies:
+    "@babel/helper-annotate-as-pure" "^7.14.5"
+    "@babel/helper-function-name" "^7.14.5"
+    "@babel/helper-optimise-call-expression" "^7.14.5"
+    "@babel/helper-plugin-utils" "^7.14.5"
+    "@babel/helper-replace-supers" "^7.14.5"
+    "@babel/helper-split-export-declaration" "^7.14.5"
     globals "^11.1.0"
 
 "@babel/plugin-transform-computed-properties@^7.2.0":
   version "7.2.0"
   resolved "https://registry.yarnpkg.com/@babel/plugin-transform-computed-properties/-/plugin-transform-computed-properties-7.2.0.tgz#83a7df6a658865b1c8f641d510c6f3af220216da"
   integrity sha512-kP/drqTxY6Xt3NNpKiMomfgkNn4o7+vKxK2DDKcBG9sHj51vHqMBGy8wbDS/J4lMxnqs153/T3+DmCEAkC5cpA==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
 
-"@babel/plugin-transform-destructuring@^7.4.4":
-  version "7.6.0"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-destructuring/-/plugin-transform-destructuring-7.6.0.tgz#44bbe08b57f4480094d57d9ffbcd96d309075ba6"
-  integrity sha512-2bGIS5P1v4+sWTCnKNDZDxbGvEqi0ijeqM/YqHtVGrvG2y0ySgnEEhXErvE9dA0bnIzY9bIzdFK0jFA46ASIIQ==
+"@babel/plugin-transform-destructuring@^7.6.0":
+  version "7.14.7"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-destructuring/-/plugin-transform-destructuring-7.14.7.tgz#0ad58ed37e23e22084d109f185260835e5557576"
+  integrity sha512-0mDE99nK+kVh3xlc5vKwB6wnP9ecuSj+zQCa/n0voENtP/zymdT4HH6QEb65wjjcbqr1Jb/7z9Qp7TF5FtwYGw==
   dependencies:
-    "@babel/helper-plugin-utils" "^7.0.0"
+    "@babel/helper-plugin-utils" "^7.14.5"
 
 "@babel/plugin-transform-dotall-regex@^7.4.4":
   version "7.6.2"
   resolved "https://registry.yarnpkg.com/@babel/plugin-transform-dotall-regex/-/plugin-transform-dotall-regex-7.6.2.tgz#44abb948b88f0199a627024e1508acaf8dc9b2f9"
   integrity sha512-KGKT9aqKV+9YMZSkowzYoYEiHqgaDhGmPNZlZxX6UeHC4z30nC1J9IrZuGqbYFB1jaIGdv91ujpze0exiVK8bA==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
     "@babel/helper-regex" "^7.4.4"
     regexpu-core "^4.6.0"
 
-"@babel/plugin-transform-duplicate-keys@^7.2.0":
-  version "7.5.0"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-duplicate-keys/-/plugin-transform-duplicate-keys-7.5.0.tgz#c5dbf5106bf84cdf691222c0974c12b1df931853"
-  integrity sha512-igcziksHizyQPlX9gfSjHkE2wmoCH3evvD2qR5w29/Dk0SMKE/eOI7f1HhBdNhR/zxJDqrgpoDTq5YSLH/XMsQ==
+"@babel/plugin-transform-dotall-regex@^7.7.0":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-dotall-regex/-/plugin-transform-dotall-regex-7.14.5.tgz#2f6bf76e46bdf8043b4e7e16cf24532629ba0c7a"
+  integrity sha512-loGlnBdj02MDsFaHhAIJzh7euK89lBrGIdM9EAtHFo6xKygCUGuuWe07o1oZVk287amtW1n0808sQM99aZt3gw==
+  dependencies:
+    "@babel/helper-create-regexp-features-plugin" "^7.14.5"
+    "@babel/helper-plugin-utils" "^7.14.5"
+
+"@babel/plugin-transform-duplicate-keys@^7.5.0":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-duplicate-keys/-/plugin-transform-duplicate-keys-7.14.5.tgz#365a4844881bdf1501e3a9f0270e7f0f91177954"
+  integrity sha512-iJjbI53huKbPDAsJ8EmVmvCKeeq21bAze4fu9GBQtSLqfvzj2oRuHVx4ZkDwEhg1htQ+5OBZh/Ab0XDf5iBZ7A==
   dependencies:
-    "@babel/helper-plugin-utils" "^7.0.0"
+    "@babel/helper-plugin-utils" "^7.14.5"
 
 "@babel/plugin-transform-exponentiation-operator@^7.2.0":
   version "7.2.0"
   resolved "https://registry.yarnpkg.com/@babel/plugin-transform-exponentiation-operator/-/plugin-transform-exponentiation-operator-7.2.0.tgz#a63868289e5b4007f7054d46491af51435766008"
   integrity sha512-umh4hR6N7mu4Elq9GG8TOu9M0bakvlsREEC+ialrQN6ABS4oDQ69qJv1VtR3uxlKMCQMCvzk7vr17RHKcjx68A==
   dependencies:
     "@babel/helper-builder-binary-assignment-operator-visitor" "^7.1.0"
@@ -455,21 +686,21 @@
 "@babel/plugin-transform-for-of@^7.4.4":
   version "7.4.4"
   resolved "https://registry.yarnpkg.com/@babel/plugin-transform-for-of/-/plugin-transform-for-of-7.4.4.tgz#0267fc735e24c808ba173866c6c4d1440fc3c556"
   integrity sha512-9T/5Dlr14Z9TIEXLXkt8T1DU7F24cbhwhMNUziN3hB1AXoZcdzPcTiKGRn/6iOymDqtTKWnr/BtRKN9JwbKtdQ==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
 
-"@babel/plugin-transform-function-name@^7.4.4":
-  version "7.4.4"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-function-name/-/plugin-transform-function-name-7.4.4.tgz#e1436116abb0610c2259094848754ac5230922ad"
-  integrity sha512-iU9pv7U+2jC9ANQkKeNF6DrPy4GBa4NWQtl6dHB4Pb3izX2JOEvDTFarlNsBj/63ZEzNNIAMs3Qw4fNCcSOXJA==
+"@babel/plugin-transform-function-name@^7.7.0":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-function-name/-/plugin-transform-function-name-7.14.5.tgz#e81c65ecb900746d7f31802f6bed1f52d915d6f2"
+  integrity sha512-vbO6kv0fIzZ1GpmGQuvbwwm+O4Cbm2NrPzwlup9+/3fdkuzo1YqOZcXw26+YUJB84Ja7j9yURWposEHLYwxUfQ==
   dependencies:
-    "@babel/helper-function-name" "^7.1.0"
-    "@babel/helper-plugin-utils" "^7.0.0"
+    "@babel/helper-function-name" "^7.14.5"
+    "@babel/helper-plugin-utils" "^7.14.5"
 
 "@babel/plugin-transform-literals@^7.2.0":
   version "7.2.0"
   resolved "https://registry.yarnpkg.com/@babel/plugin-transform-literals/-/plugin-transform-literals-7.2.0.tgz#690353e81f9267dad4fd8cfd77eafa86aba53ea1"
   integrity sha512-2ThDhm4lI4oV7fVQ6pNNK+sx+c/GM5/SaML0w/r4ZB7sAneD/piDJtwdKlNckXeyGK7wlwg2E2w33C/Hh+VFCg==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
@@ -477,80 +708,90 @@
 "@babel/plugin-transform-member-expression-literals@^7.2.0":
   version "7.2.0"
   resolved "https://registry.yarnpkg.com/@babel/plugin-transform-member-expression-literals/-/plugin-transform-member-expression-literals-7.2.0.tgz#fa10aa5c58a2cb6afcf2c9ffa8cb4d8b3d489a2d"
   integrity sha512-HiU3zKkSU6scTidmnFJ0bMX8hz5ixC93b4MHMiYebmk2lUVNGOboPsqQvx5LzooihijUoLR/v7Nc1rbBtnc7FA==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
 
-"@babel/plugin-transform-modules-amd@^7.2.0":
-  version "7.5.0"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-modules-amd/-/plugin-transform-modules-amd-7.5.0.tgz#ef00435d46da0a5961aa728a1d2ecff063e4fb91"
-  integrity sha512-n20UsQMKnWrltocZZm24cRURxQnWIvsABPJlw/fvoy9c6AgHZzoelAIzajDHAQrDpuKFFPPcFGd7ChsYuIUMpg==
-  dependencies:
-    "@babel/helper-module-transforms" "^7.1.0"
-    "@babel/helper-plugin-utils" "^7.0.0"
-    babel-plugin-dynamic-import-node "^2.3.0"
-
-"@babel/plugin-transform-modules-commonjs@7.4.4":
-  version "7.4.4"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-modules-commonjs/-/plugin-transform-modules-commonjs-7.4.4.tgz#0bef4713d30f1d78c2e59b3d6db40e60192cac1e"
-  integrity sha512-4sfBOJt58sEo9a2BQXnZq+Q3ZTSAUXyK3E30o36BOGnJ+tvJ6YSxF0PG6kERvbeISgProodWuI9UVG3/FMY6iw==
-  dependencies:
-    "@babel/helper-module-transforms" "^7.4.4"
-    "@babel/helper-plugin-utils" "^7.0.0"
-    "@babel/helper-simple-access" "^7.1.0"
-
-"@babel/plugin-transform-modules-commonjs@^7.4.4":
-  version "7.6.0"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-modules-commonjs/-/plugin-transform-modules-commonjs-7.6.0.tgz#39dfe957de4420445f1fcf88b68a2e4aa4515486"
-  integrity sha512-Ma93Ix95PNSEngqomy5LSBMAQvYKVe3dy+JlVJSHEXZR5ASL9lQBedMiCyVtmTLraIDVRE3ZjTZvmXXD2Ozw3g==
+"@babel/plugin-transform-modules-amd@^7.5.0":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-modules-amd/-/plugin-transform-modules-amd-7.14.5.tgz#4fd9ce7e3411cb8b83848480b7041d83004858f7"
+  integrity sha512-3lpOU8Vxmp3roC4vzFpSdEpGUWSMsHFreTWOMMLzel2gNGfHE5UWIh/LN6ghHs2xurUp4jRFYMUIZhuFbody1g==
+  dependencies:
+    "@babel/helper-module-transforms" "^7.14.5"
+    "@babel/helper-plugin-utils" "^7.14.5"
+    babel-plugin-dynamic-import-node "^2.3.3"
+
+"@babel/plugin-transform-modules-commonjs@7.7.0":
+  version "7.7.0"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-modules-commonjs/-/plugin-transform-modules-commonjs-7.7.0.tgz#3e5ffb4fd8c947feede69cbe24c9554ab4113fe3"
+  integrity sha512-KEMyWNNWnjOom8vR/1+d+Ocz/mILZG/eyHHO06OuBQ2aNhxT62fr4y6fGOplRx+CxCSp3IFwesL8WdINfY/3kg==
   dependencies:
-    "@babel/helper-module-transforms" "^7.4.4"
+    "@babel/helper-module-transforms" "^7.7.0"
     "@babel/helper-plugin-utils" "^7.0.0"
-    "@babel/helper-simple-access" "^7.1.0"
+    "@babel/helper-simple-access" "^7.7.0"
     babel-plugin-dynamic-import-node "^2.3.0"
 
-"@babel/plugin-transform-modules-systemjs@^7.4.4":
-  version "7.5.0"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-modules-systemjs/-/plugin-transform-modules-systemjs-7.5.0.tgz#e75266a13ef94202db2a0620977756f51d52d249"
-  integrity sha512-Q2m56tyoQWmuNGxEtUyeEkm6qJYFqs4c+XyXH5RAuYxObRNz9Zgj/1g2GMnjYp2EUyEy7YTrxliGCXzecl/vJg==
+"@babel/plugin-transform-modules-commonjs@^7.7.0":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-modules-commonjs/-/plugin-transform-modules-commonjs-7.14.5.tgz#7aaee0ea98283de94da98b28f8c35701429dad97"
+  integrity sha512-en8GfBtgnydoao2PS+87mKyw62k02k7kJ9ltbKe0fXTHrQmG6QZZflYuGI1VVG7sVpx4E1n7KBpNlPb8m78J+A==
+  dependencies:
+    "@babel/helper-module-transforms" "^7.14.5"
+    "@babel/helper-plugin-utils" "^7.14.5"
+    "@babel/helper-simple-access" "^7.14.5"
+    babel-plugin-dynamic-import-node "^2.3.3"
+
+"@babel/plugin-transform-modules-systemjs@^7.7.0":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-modules-systemjs/-/plugin-transform-modules-systemjs-7.14.5.tgz#c75342ef8b30dcde4295d3401aae24e65638ed29"
+  integrity sha512-mNMQdvBEE5DcMQaL5LbzXFMANrQjd2W7FPzg34Y4yEz7dBgdaC+9B84dSO+/1Wba98zoDbInctCDo4JGxz1VYA==
+  dependencies:
+    "@babel/helper-hoist-variables" "^7.14.5"
+    "@babel/helper-module-transforms" "^7.14.5"
+    "@babel/helper-plugin-utils" "^7.14.5"
+    "@babel/helper-validator-identifier" "^7.14.5"
+    babel-plugin-dynamic-import-node "^2.3.3"
+
+"@babel/plugin-transform-modules-umd@^7.7.0":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-modules-umd/-/plugin-transform-modules-umd-7.14.5.tgz#fb662dfee697cce274a7cda525190a79096aa6e0"
+  integrity sha512-RfPGoagSngC06LsGUYyM9QWSXZ8MysEjDJTAea1lqRjNECE3y0qIJF/qbvJxc4oA4s99HumIMdXOrd+TdKaAAA==
+  dependencies:
+    "@babel/helper-module-transforms" "^7.14.5"
+    "@babel/helper-plugin-utils" "^7.14.5"
+
+"@babel/plugin-transform-named-capturing-groups-regex@^7.7.0":
+  version "7.14.7"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-named-capturing-groups-regex/-/plugin-transform-named-capturing-groups-regex-7.14.7.tgz#60c06892acf9df231e256c24464bfecb0908fd4e"
+  integrity sha512-DTNOTaS7TkW97xsDMrp7nycUVh6sn/eq22VaxWfEdzuEbRsiaOU0pqU7DlyUGHVsbQbSghvjKRpEl+nUCKGQSg==
   dependencies:
-    "@babel/helper-hoist-variables" "^7.4.4"
-    "@babel/helper-plugin-utils" "^7.0.0"
-    babel-plugin-dynamic-import-node "^2.3.0"
-
-"@babel/plugin-transform-modules-umd@^7.2.0":
-  version "7.2.0"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-modules-umd/-/plugin-transform-modules-umd-7.2.0.tgz#7678ce75169f0877b8eb2235538c074268dd01ae"
-  integrity sha512-BV3bw6MyUH1iIsGhXlOK6sXhmSarZjtJ/vMiD9dNmpY8QXFFQTj+6v92pcfy1iqa8DeAfJFwoxcrS/TUZda6sw==
-  dependencies:
-    "@babel/helper-module-transforms" "^7.1.0"
-    "@babel/helper-plugin-utils" "^7.0.0"
-
-"@babel/plugin-transform-named-capturing-groups-regex@^7.4.5":
-  version "7.6.2"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-named-capturing-groups-regex/-/plugin-transform-named-capturing-groups-regex-7.6.2.tgz#c1ca0bb84b94f385ca302c3932e870b0fb0e522b"
-  integrity sha512-xBdB+XOs+lgbZc2/4F5BVDVcDNS4tcSKQc96KmlqLEAwz6tpYPEvPdmDfvVG0Ssn8lAhronaRs6Z6KSexIpK5g==
-  dependencies:
-    regexpu-core "^4.6.0"
+    "@babel/helper-create-regexp-features-plugin" "^7.14.5"
 
 "@babel/plugin-transform-new-target@^7.4.4":
   version "7.4.4"
   resolved "https://registry.yarnpkg.com/@babel/plugin-transform-new-target/-/plugin-transform-new-target-7.4.4.tgz#18d120438b0cc9ee95a47f2c72bc9768fbed60a5"
   integrity sha512-r1z3T2DNGQwwe2vPGZMBNjioT2scgWzK9BCnDEh+46z8EEwXBq24uRzd65I7pjtugzPSj921aM15RpESgzsSuA==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
 
-"@babel/plugin-transform-object-super@^7.2.0":
-  version "7.5.5"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-object-super/-/plugin-transform-object-super-7.5.5.tgz#c70021df834073c65eb613b8679cc4a381d1a9f9"
-  integrity sha512-un1zJQAhSosGFBduPgN/YFNvWVpRuHKU7IHBglLoLZsGmruJPOo6pbInneflUdmq7YvSVqhpPs5zdBvLnteltQ==
+"@babel/plugin-transform-object-super@^7.5.5":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-object-super/-/plugin-transform-object-super-7.14.5.tgz#d0b5faeac9e98597a161a9cf78c527ed934cdc45"
+  integrity sha512-MKfOBWzK0pZIrav9z/hkRqIk/2bTv9qvxHzPQc12RcVkMOzpIKnFCNYJip00ssKWYkd8Sf5g0Wr7pqJ+cmtuFg==
+  dependencies:
+    "@babel/helper-plugin-utils" "^7.14.5"
+    "@babel/helper-replace-supers" "^7.14.5"
+
+"@babel/plugin-transform-parameters@^7.14.5":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-parameters/-/plugin-transform-parameters-7.14.5.tgz#49662e86a1f3ddccac6363a7dfb1ff0a158afeb3"
+  integrity sha512-Tl7LWdr6HUxTmzQtzuU14SqbgrSKmaR77M0OKyq4njZLQTPfOvzblNKyNkGwOfEFCEx7KeYHQHDI0P3F02IVkA==
   dependencies:
-    "@babel/helper-plugin-utils" "^7.0.0"
-    "@babel/helper-replace-supers" "^7.5.5"
+    "@babel/helper-plugin-utils" "^7.14.5"
 
 "@babel/plugin-transform-parameters@^7.4.4":
   version "7.4.4"
   resolved "https://registry.yarnpkg.com/@babel/plugin-transform-parameters/-/plugin-transform-parameters-7.4.4.tgz#7556cf03f318bd2719fe4c922d2d808be5571e16"
   integrity sha512-oMh5DUO1V63nZcu/ZVLQFqiihBGo4OpxJxR1otF50GMeCLiRx5nUdtokd+u9SuVJrvvuIh9OosRFPP4pIPnwmw==
   dependencies:
     "@babel/helper-call-delegate" "^7.4.4"
@@ -583,60 +824,63 @@
   version "7.5.0"
   resolved "https://registry.yarnpkg.com/@babel/plugin-transform-react-jsx-source/-/plugin-transform-react-jsx-source-7.5.0.tgz#583b10c49cf057e237085bcbd8cc960bd83bd96b"
   integrity sha512-58Q+Jsy4IDCZx7kqEZuSDdam/1oW8OdDX8f+Loo6xyxdfg1yF0GE2XNJQSTZCaMol93+FBzpWiPEwtbMloAcPg==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
     "@babel/plugin-syntax-jsx" "^7.2.0"
 
-"@babel/plugin-transform-react-jsx@^7.0.0":
-  version "7.3.0"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-react-jsx/-/plugin-transform-react-jsx-7.3.0.tgz#f2cab99026631c767e2745a5368b331cfe8f5290"
-  integrity sha512-a/+aRb7R06WcKvQLOu4/TpjKOdvVEKRLWFpKcNuHhiREPgGRB4TQJxq07+EZLS8LFVYpfq1a5lDUnuMdcCpBKg==
-  dependencies:
-    "@babel/helper-builder-react-jsx" "^7.3.0"
-    "@babel/helper-plugin-utils" "^7.0.0"
-    "@babel/plugin-syntax-jsx" "^7.2.0"
-
-"@babel/plugin-transform-regenerator@^7.4.5":
-  version "7.4.5"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-regenerator/-/plugin-transform-regenerator-7.4.5.tgz#629dc82512c55cee01341fb27bdfcb210354680f"
-  integrity sha512-gBKRh5qAaCWntnd09S8QC7r3auLCqq5DI6O0DlfoyDjslSBVqBibrMdsqO+Uhmx3+BlOmE/Kw1HFxmGbv0N9dA==
+"@babel/plugin-transform-react-jsx@^7.7.0":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-react-jsx/-/plugin-transform-react-jsx-7.14.5.tgz#39749f0ee1efd8a1bd729152cf5f78f1d247a44a"
+  integrity sha512-7RylxNeDnxc1OleDm0F5Q/BSL+whYRbOAR+bwgCxIr0L32v7UFh/pz1DLMZideAUxKT6eMoS2zQH6fyODLEi8Q==
+  dependencies:
+    "@babel/helper-annotate-as-pure" "^7.14.5"
+    "@babel/helper-module-imports" "^7.14.5"
+    "@babel/helper-plugin-utils" "^7.14.5"
+    "@babel/plugin-syntax-jsx" "^7.14.5"
+    "@babel/types" "^7.14.5"
+
+"@babel/plugin-transform-regenerator@^7.7.0":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-regenerator/-/plugin-transform-regenerator-7.14.5.tgz#9676fd5707ed28f522727c5b3c0aa8544440b04f"
+  integrity sha512-NVIY1W3ITDP5xQl50NgTKlZ0GrotKtLna08/uGY6ErQt6VEQZXla86x/CTddm5gZdcr+5GSsvMeTmWA5Ii6pkg==
   dependencies:
-    regenerator-transform "^0.14.0"
+    regenerator-transform "^0.14.2"
 
 "@babel/plugin-transform-reserved-words@^7.2.0":
   version "7.2.0"
   resolved "https://registry.yarnpkg.com/@babel/plugin-transform-reserved-words/-/plugin-transform-reserved-words-7.2.0.tgz#4792af87c998a49367597d07fedf02636d2e1634"
   integrity sha512-fz43fqW8E1tAB3DKF19/vxbpib1fuyCwSPE418ge5ZxILnBhWyhtPgz8eh1RCGGJlwvksHkyxMxh0eenFi+kFw==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
 
-"@babel/plugin-transform-runtime@7.4.4":
-  version "7.4.4"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-runtime/-/plugin-transform-runtime-7.4.4.tgz#a50f5d16e9c3a4ac18a1a9f9803c107c380bce08"
-  integrity sha512-aMVojEjPszvau3NRg+TIH14ynZLvPewH4xhlCW1w6A3rkxTS1m4uwzRclYR9oS+rl/dr+kT+pzbfHuAWP/lc7Q==
+"@babel/plugin-transform-runtime@7.6.2":
+  version "7.6.2"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-runtime/-/plugin-transform-runtime-7.6.2.tgz#2669f67c1fae0ae8d8bf696e4263ad52cb98b6f8"
+  integrity sha512-cqULw/QB4yl73cS5Y0TZlQSjDvNkzDbu0FurTZyHlJpWE5T3PCMdnyV+xXoH1opr1ldyHODe3QAX3OMAii5NxA==
   dependencies:
     "@babel/helper-module-imports" "^7.0.0"
     "@babel/helper-plugin-utils" "^7.0.0"
     resolve "^1.8.1"
     semver "^5.5.1"
 
 "@babel/plugin-transform-shorthand-properties@^7.2.0":
   version "7.2.0"
   resolved "https://registry.yarnpkg.com/@babel/plugin-transform-shorthand-properties/-/plugin-transform-shorthand-properties-7.2.0.tgz#6333aee2f8d6ee7e28615457298934a3b46198f0"
   integrity sha512-QP4eUM83ha9zmYtpbnyjTLAGKQritA5XW/iG9cjtuOI8s1RuL/3V6a3DeSHfKutJQ+ayUfeZJPcnCYEQzaPQqg==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
 
-"@babel/plugin-transform-spread@^7.2.0":
-  version "7.6.2"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-spread/-/plugin-transform-spread-7.6.2.tgz#fc77cf798b24b10c46e1b51b1b88c2bf661bb8dd"
-  integrity sha512-DpSvPFryKdK1x+EDJYCy28nmAaIMdxmhot62jAXF/o99iA33Zj2Lmcp3vDmz+MUh0LNYVPvfj5iC3feb3/+PFg==
+"@babel/plugin-transform-spread@^7.6.2":
+  version "7.14.6"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-spread/-/plugin-transform-spread-7.14.6.tgz#6bd40e57fe7de94aa904851963b5616652f73144"
+  integrity sha512-Zr0x0YroFJku7n7+/HH3A2eIrGMjbmAIbJSVv0IZ+t3U2WUQUA64S/oeied2e+MaGSjmt4alzBCsK9E8gh+fag==
   dependencies:
-    "@babel/helper-plugin-utils" "^7.0.0"
+    "@babel/helper-plugin-utils" "^7.14.5"
+    "@babel/helper-skip-transparent-expression-wrappers" "^7.14.5"
 
 "@babel/plugin-transform-sticky-regex@^7.2.0":
   version "7.2.0"
   resolved "https://registry.yarnpkg.com/@babel/plugin-transform-sticky-regex/-/plugin-transform-sticky-regex-7.2.0.tgz#a1e454b5995560a9c1e0d537dfc15061fd2687e1"
   integrity sha512-KKYCoGaRAf+ckH8gEL3JHUaFVyNHKe3ASNsZ+AlktgHevvxGigoIttrEJb8iKN03Q7Eazlv1s6cx2B2cQ3Jabw==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
@@ -653,153 +897,211 @@
 "@babel/plugin-transform-typeof-symbol@^7.2.0":
   version "7.2.0"
   resolved "https://registry.yarnpkg.com/@babel/plugin-transform-typeof-symbol/-/plugin-transform-typeof-symbol-7.2.0.tgz#117d2bcec2fbf64b4b59d1f9819894682d29f2b2"
   integrity sha512-2LNhETWYxiYysBtrBTqL8+La0jIoQQnIScUJc74OYvUGRmkskNY4EzLCnjHBzdmb38wqtTaixpo1NctEcvMDZw==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
 
-"@babel/plugin-transform-typescript@^7.3.2":
-  version "7.6.0"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-typescript/-/plugin-transform-typescript-7.6.0.tgz#48d78405f1aa856ebeea7288a48a19ed8da377a6"
-  integrity sha512-yzw7EopOOr6saONZ3KA3lpizKnWRTe+rfBqg4AmQbSow7ik7fqmzrfIqt053osLwLE2AaTqGinLM2tl6+M/uog==
-  dependencies:
-    "@babel/helper-create-class-features-plugin" "^7.6.0"
-    "@babel/helper-plugin-utils" "^7.0.0"
-    "@babel/plugin-syntax-typescript" "^7.2.0"
-
-"@babel/plugin-transform-unicode-regex@^7.4.4":
-  version "7.6.2"
-  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-unicode-regex/-/plugin-transform-unicode-regex-7.6.2.tgz#b692aad888a7e8d8b1b214be6b9dc03d5031f698"
-  integrity sha512-orZI6cWlR3nk2YmYdb0gImrgCUwb5cBUwjf6Ks6dvNVvXERkwtJWOQaEOjPiu0Gu1Tq6Yq/hruCZZOOi9F34Dw==
-  dependencies:
-    "@babel/helper-plugin-utils" "^7.0.0"
-    "@babel/helper-regex" "^7.4.4"
-    regexpu-core "^4.6.0"
-
-"@babel/preset-env@7.4.5":
-  version "7.4.5"
-  resolved "https://registry.yarnpkg.com/@babel/preset-env/-/preset-env-7.4.5.tgz#2fad7f62983d5af563b5f3139242755884998a58"
-  integrity sha512-f2yNVXM+FsR5V8UwcFeIHzHWgnhXg3NpRmy0ADvALpnhB0SLbCvrCRr4BLOUYbQNLS+Z0Yer46x9dJXpXewI7w==
+"@babel/plugin-transform-typescript@^7.7.2":
+  version "7.14.6"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-typescript/-/plugin-transform-typescript-7.14.6.tgz#6e9c2d98da2507ebe0a883b100cde3c7279df36c"
+  integrity sha512-XlTdBq7Awr4FYIzqhmYY80WN0V0azF74DMPyFqVHBvf81ZUgc4X7ZOpx6O8eLDK6iM5cCQzeyJw0ynTaefixRA==
+  dependencies:
+    "@babel/helper-create-class-features-plugin" "^7.14.6"
+    "@babel/helper-plugin-utils" "^7.14.5"
+    "@babel/plugin-syntax-typescript" "^7.14.5"
+
+"@babel/plugin-transform-unicode-regex@^7.7.0":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/plugin-transform-unicode-regex/-/plugin-transform-unicode-regex-7.14.5.tgz#4cd09b6c8425dd81255c7ceb3fb1836e7414382e"
+  integrity sha512-UygduJpC5kHeCiRw/xDVzC+wj8VaYSoKl5JNVmbP7MadpNinAm3SvZCxZ42H37KZBKztz46YC73i9yV34d0Tzw==
+  dependencies:
+    "@babel/helper-create-regexp-features-plugin" "^7.14.5"
+    "@babel/helper-plugin-utils" "^7.14.5"
+
+"@babel/preset-env@7.7.1":
+  version "7.7.1"
+  resolved "https://registry.yarnpkg.com/@babel/preset-env/-/preset-env-7.7.1.tgz#04a2ff53552c5885cf1083e291c8dd5490f744bb"
+  integrity sha512-/93SWhi3PxcVTDpSqC+Dp4YxUu3qZ4m7I76k0w73wYfn7bGVuRIO4QUz95aJksbS+AD1/mT1Ie7rbkT0wSplaA==
   dependencies:
-    "@babel/helper-module-imports" "^7.0.0"
+    "@babel/helper-module-imports" "^7.7.0"
     "@babel/helper-plugin-utils" "^7.0.0"
-    "@babel/plugin-proposal-async-generator-functions" "^7.2.0"
+    "@babel/plugin-proposal-async-generator-functions" "^7.7.0"
+    "@babel/plugin-proposal-dynamic-import" "^7.7.0"
     "@babel/plugin-proposal-json-strings" "^7.2.0"
-    "@babel/plugin-proposal-object-rest-spread" "^7.4.4"
+    "@babel/plugin-proposal-object-rest-spread" "^7.6.2"
     "@babel/plugin-proposal-optional-catch-binding" "^7.2.0"
-    "@babel/plugin-proposal-unicode-property-regex" "^7.4.4"
+    "@babel/plugin-proposal-unicode-property-regex" "^7.7.0"
     "@babel/plugin-syntax-async-generators" "^7.2.0"
+    "@babel/plugin-syntax-dynamic-import" "^7.2.0"
     "@babel/plugin-syntax-json-strings" "^7.2.0"
     "@babel/plugin-syntax-object-rest-spread" "^7.2.0"
     "@babel/plugin-syntax-optional-catch-binding" "^7.2.0"
+    "@babel/plugin-syntax-top-level-await" "^7.7.0"
     "@babel/plugin-transform-arrow-functions" "^7.2.0"
-    "@babel/plugin-transform-async-to-generator" "^7.4.4"
+    "@babel/plugin-transform-async-to-generator" "^7.7.0"
     "@babel/plugin-transform-block-scoped-functions" "^7.2.0"
-    "@babel/plugin-transform-block-scoping" "^7.4.4"
-    "@babel/plugin-transform-classes" "^7.4.4"
+    "@babel/plugin-transform-block-scoping" "^7.6.3"
+    "@babel/plugin-transform-classes" "^7.7.0"
     "@babel/plugin-transform-computed-properties" "^7.2.0"
-    "@babel/plugin-transform-destructuring" "^7.4.4"
-    "@babel/plugin-transform-dotall-regex" "^7.4.4"
-    "@babel/plugin-transform-duplicate-keys" "^7.2.0"
+    "@babel/plugin-transform-destructuring" "^7.6.0"
+    "@babel/plugin-transform-dotall-regex" "^7.7.0"
+    "@babel/plugin-transform-duplicate-keys" "^7.5.0"
     "@babel/plugin-transform-exponentiation-operator" "^7.2.0"
     "@babel/plugin-transform-for-of" "^7.4.4"
-    "@babel/plugin-transform-function-name" "^7.4.4"
+    "@babel/plugin-transform-function-name" "^7.7.0"
     "@babel/plugin-transform-literals" "^7.2.0"
     "@babel/plugin-transform-member-expression-literals" "^7.2.0"
-    "@babel/plugin-transform-modules-amd" "^7.2.0"
-    "@babel/plugin-transform-modules-commonjs" "^7.4.4"
-    "@babel/plugin-transform-modules-systemjs" "^7.4.4"
-    "@babel/plugin-transform-modules-umd" "^7.2.0"
-    "@babel/plugin-transform-named-capturing-groups-regex" "^7.4.5"
+    "@babel/plugin-transform-modules-amd" "^7.5.0"
+    "@babel/plugin-transform-modules-commonjs" "^7.7.0"
+    "@babel/plugin-transform-modules-systemjs" "^7.7.0"
+    "@babel/plugin-transform-modules-umd" "^7.7.0"
+    "@babel/plugin-transform-named-capturing-groups-regex" "^7.7.0"
     "@babel/plugin-transform-new-target" "^7.4.4"
-    "@babel/plugin-transform-object-super" "^7.2.0"
+    "@babel/plugin-transform-object-super" "^7.5.5"
     "@babel/plugin-transform-parameters" "^7.4.4"
     "@babel/plugin-transform-property-literals" "^7.2.0"
-    "@babel/plugin-transform-regenerator" "^7.4.5"
+    "@babel/plugin-transform-regenerator" "^7.7.0"
     "@babel/plugin-transform-reserved-words" "^7.2.0"
     "@babel/plugin-transform-shorthand-properties" "^7.2.0"
-    "@babel/plugin-transform-spread" "^7.2.0"
+    "@babel/plugin-transform-spread" "^7.6.2"
     "@babel/plugin-transform-sticky-regex" "^7.2.0"
     "@babel/plugin-transform-template-literals" "^7.4.4"
     "@babel/plugin-transform-typeof-symbol" "^7.2.0"
-    "@babel/plugin-transform-unicode-regex" "^7.4.4"
-    "@babel/types" "^7.4.4"
+    "@babel/plugin-transform-unicode-regex" "^7.7.0"
+    "@babel/types" "^7.7.1"
     browserslist "^4.6.0"
     core-js-compat "^3.1.1"
     invariant "^2.2.2"
     js-levenshtein "^1.1.3"
     semver "^5.5.0"
 
-"@babel/preset-react@7.0.0":
-  version "7.0.0"
-  resolved "https://registry.yarnpkg.com/@babel/preset-react/-/preset-react-7.0.0.tgz#e86b4b3d99433c7b3e9e91747e2653958bc6b3c0"
-  integrity sha512-oayxyPS4Zj+hF6Et11BwuBkmpgT/zMxyuZgFrMeZID6Hdh3dGlk4sHCAhdBCpuCKW2ppBfl2uCCetlrUIJRY3w==
+"@babel/preset-modules@0.1.1":
+  version "0.1.1"
+  resolved "https://registry.yarnpkg.com/@babel/preset-modules/-/preset-modules-0.1.1.tgz#add61473e3182771b36930c1312f3c56c114e406"
+  integrity sha512-x/kt2aAZlgcFnP3P851fkkb2s4FmTiyGic58pkWMaRK9Am3u9KkH1ttHGjwlsKu7/TVJsLEBXZnjUxqsid3tww==
+  dependencies:
+    "@babel/helper-plugin-utils" "^7.0.0"
+    "@babel/plugin-proposal-unicode-property-regex" "^7.4.4"
+    "@babel/plugin-transform-dotall-regex" "^7.4.4"
+    "@babel/types" "^7.4.4"
+    esutils "^2.0.2"
+
+"@babel/preset-react@7.7.0":
+  version "7.7.0"
+  resolved "https://registry.yarnpkg.com/@babel/preset-react/-/preset-react-7.7.0.tgz#8ab0c4787d98cf1f5f22dabf115552bf9e4e406c"
+  integrity sha512-IXXgSUYBPHUGhUkH+89TR6faMcBtuMW0h5OHbMuVbL3/5wK2g6a2M2BBpkLa+Kw0sAHiZ9dNVgqJMDP/O4GRBA==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
     "@babel/plugin-transform-react-display-name" "^7.0.0"
-    "@babel/plugin-transform-react-jsx" "^7.0.0"
+    "@babel/plugin-transform-react-jsx" "^7.7.0"
     "@babel/plugin-transform-react-jsx-self" "^7.0.0"
     "@babel/plugin-transform-react-jsx-source" "^7.0.0"
 
-"@babel/preset-typescript@7.3.3":
-  version "7.3.3"
-  resolved "https://registry.yarnpkg.com/@babel/preset-typescript/-/preset-typescript-7.3.3.tgz#88669911053fa16b2b276ea2ede2ca603b3f307a"
-  integrity sha512-mzMVuIP4lqtn4du2ynEfdO0+RYcslwrZiJHXu4MGaC1ctJiW2fyaeDrtjJGs7R/KebZ1sgowcIoWf4uRpEfKEg==
+"@babel/preset-typescript@7.7.2":
+  version "7.7.2"
+  resolved "https://registry.yarnpkg.com/@babel/preset-typescript/-/preset-typescript-7.7.2.tgz#f71c8bba2ae02f11b29dbf7d6a35f47bbe011632"
+  integrity sha512-1B4HthAelaLGfNRyrWqJtBEjXX1ulThCrLQ5B2VOtEAznWFIFXFJahgXImqppy66lx/Oh+cOSCQdJzZqh2Jh5g==
   dependencies:
     "@babel/helper-plugin-utils" "^7.0.0"
-    "@babel/plugin-transform-typescript" "^7.3.2"
-
-"@babel/runtime-corejs2@7.4.5":
-  version "7.4.5"
-  resolved "https://registry.yarnpkg.com/@babel/runtime-corejs2/-/runtime-corejs2-7.4.5.tgz#3d892f0560df21bafb384dd7727e33853e95d3c9"
-  integrity sha512-5yLuwzvIDecKwYMzJtiarky4Fb5643H3Ao5jwX0HrMR5oM5mn2iHH9wSZonxwNK0oAjAFUQAiOd4jT7/9Y2jMQ==
+    "@babel/plugin-transform-typescript" "^7.7.2"
+
+"@babel/runtime@7.7.2":
+  version "7.7.2"
+  resolved "https://registry.yarnpkg.com/@babel/runtime/-/runtime-7.7.2.tgz#111a78002a5c25fc8e3361bedc9529c696b85a6a"
+  integrity sha512-JONRbXbTXc9WQE2mAZd1p0Z3DZ/6vaQIkgYMSTP3KjRCyd7rCZCcfhCyX+YjwcKxcZ82UrxbRD358bpExNgrjw==
   dependencies:
-    core-js "^2.6.5"
     regenerator-runtime "^0.13.2"
 
-"@babel/runtime@7.4.5":
-  version "7.4.5"
-  resolved "https://registry.yarnpkg.com/@babel/runtime/-/runtime-7.4.5.tgz#582bb531f5f9dc67d2fcb682979894f75e253f12"
-  integrity sha512-TuI4qpWZP6lGOGIuGWtp9sPluqYICmbk8T/1vpSysqJxRPkudh/ofFWyqdcMsDf2s7KvDL4/YHgKyvcS3g9CJQ==
+"@babel/runtime@^7.8.4":
+  version "7.14.8"
+  resolved "https://registry.yarnpkg.com/@babel/runtime/-/runtime-7.14.8.tgz#7119a56f421018852694290b9f9148097391b446"
+  integrity sha512-twj3L8Og5SaCRCErB4x4ajbvBIVV77CGeFglHpeg5WC5FF8TZzBWXtTJ4MqaD9QszLYTtr+IsaAL2rEUevb+eg==
   dependencies:
-    regenerator-runtime "^0.13.2"
+    regenerator-runtime "^0.13.4"
 
-"@babel/template@^7.1.0", "@babel/template@^7.4.4", "@babel/template@^7.6.0":
+"@babel/template@^7.1.0":
   version "7.6.0"
   resolved "https://registry.yarnpkg.com/@babel/template/-/template-7.6.0.tgz#7f0159c7f5012230dad64cca42ec9bdb5c9536e6"
   integrity sha512-5AEH2EXD8euCk446b7edmgFdub/qfH1SN6Nii3+fyXP807QRx9Q73A2N5hNwRRslC2H9sNzaFhsPubkS4L8oNQ==
   dependencies:
     "@babel/code-frame" "^7.0.0"
     "@babel/parser" "^7.6.0"
     "@babel/types" "^7.6.0"
 
-"@babel/traverse@^7.0.0", "@babel/traverse@^7.1.0", "@babel/traverse@^7.4.4", "@babel/traverse@^7.4.5", "@babel/traverse@^7.5.5", "@babel/traverse@^7.6.2":
+"@babel/template@^7.14.5", "@babel/template@^7.7.0":
+  version "7.14.5"
+  resolved "https://registry.yarnpkg.com/@babel/template/-/template-7.14.5.tgz#a9bc9d8b33354ff6e55a9c60d1109200a68974f4"
+  integrity sha512-6Z3Po85sfxRGachLULUhOmvAaOo7xCvqGQtxINai2mEGPFm6pQ4z5QInFnUrRpfoSV60BnjyF5F3c+15fxFV1g==
+  dependencies:
+    "@babel/code-frame" "^7.14.5"
+    "@babel/parser" "^7.14.5"
+    "@babel/types" "^7.14.5"
+
+"@babel/traverse@^7.0.0", "@babel/traverse@^7.1.0", "@babel/traverse@^7.4.4":
   version "7.6.2"
   resolved "https://registry.yarnpkg.com/@babel/traverse/-/traverse-7.6.2.tgz#b0e2bfd401d339ce0e6c05690206d1e11502ce2c"
   integrity sha512-8fRE76xNwNttVEF2TwxJDGBLWthUkHWSldmfuBzVRmEDWOtu4XdINTgN7TDWzuLg4bbeIMLvfMFD9we5YcWkRQ==
   dependencies:
     "@babel/code-frame" "^7.5.5"
     "@babel/generator" "^7.6.2"
     "@babel/helper-function-name" "^7.1.0"
     "@babel/helper-split-export-declaration" "^7.4.4"
     "@babel/parser" "^7.6.2"
     "@babel/types" "^7.6.0"
     debug "^4.1.0"
     globals "^11.1.0"
     lodash "^4.17.13"
 
-"@babel/types@^7.0.0", "@babel/types@^7.2.0", "@babel/types@^7.3.0", "@babel/types@^7.4.4", "@babel/types@^7.5.5", "@babel/types@^7.6.0":
-  version "7.6.1"
-  resolved "https://registry.yarnpkg.com/@babel/types/-/types-7.6.1.tgz#53abf3308add3ac2a2884d539151c57c4b3ac648"
-  integrity sha512-X7gdiuaCmA0uRjCmRtYJNAVCc/q+5xSgsfKJHqMN4iNLILX39677fJE1O40arPMh0TTtS9ItH67yre6c7k6t0g==
+"@babel/traverse@^7.14.5", "@babel/traverse@^7.14.8", "@babel/traverse@^7.7.2":
+  version "7.14.8"
+  resolved "https://registry.yarnpkg.com/@babel/traverse/-/traverse-7.14.8.tgz#c0253f02677c5de1a8ff9df6b0aacbec7da1a8ce"
+  integrity sha512-kexHhzCljJcFNn1KYAQ6A5wxMRzq9ebYpEDV4+WdNyr3i7O44tanbDOR/xjiG2F3sllan+LgwK+7OMk0EmydHg==
+  dependencies:
+    "@babel/code-frame" "^7.14.5"
+    "@babel/generator" "^7.14.8"
+    "@babel/helper-function-name" "^7.14.5"
+    "@babel/helper-hoist-variables" "^7.14.5"
+    "@babel/helper-split-export-declaration" "^7.14.5"
+    "@babel/parser" "^7.14.8"
+    "@babel/types" "^7.14.8"
+    debug "^4.1.0"
+    globals "^11.1.0"
+
+"@babel/types@7.7.4":
+  version "7.7.4"
+  resolved "https://registry.yarnpkg.com/@babel/types/-/types-7.7.4.tgz#516570d539e44ddf308c07569c258ff94fde9193"
+  integrity sha512-cz5Ji23KCi4T+YIE/BolWosrJuSmoZeN1EFnRtBwF+KKLi8GG/Z2c2hOJJeCXPk4mwk4QFvTmwIodJowXgttRA==
+  dependencies:
+    esutils "^2.0.2"
+    lodash "^4.17.13"
+    to-fast-properties "^2.0.0"
+
+"@babel/types@7.8.3":
+  version "7.8.3"
+  resolved "https://registry.yarnpkg.com/@babel/types/-/types-7.8.3.tgz#5a383dffa5416db1b73dedffd311ffd0788fb31c"
+  integrity sha512-jBD+G8+LWpMBBWvVcdr4QysjUE4mU/syrhN17o1u3gx0/WzJB1kwiVZAXRtWbsIPOwW8pF/YJV5+nmetPzepXg==
   dependencies:
     esutils "^2.0.2"
     lodash "^4.17.13"
     to-fast-properties "^2.0.0"
 
+"@babel/types@^7.0.0", "@babel/types@^7.14.5", "@babel/types@^7.14.8", "@babel/types@^7.4.4", "@babel/types@^7.6.0", "@babel/types@^7.7.1", "@babel/types@^7.7.2":
+  version "7.14.8"
+  resolved "https://registry.yarnpkg.com/@babel/types/-/types-7.14.8.tgz#38109de8fcadc06415fbd9b74df0065d4d41c728"
+  integrity sha512-iob4soQa7dZw8nodR/KlOQkPh9S4I8RwCxwRIFuiMRYjOzH/KJzdUfDgz6cGi5dDaclXF4P2PAhCdrBJNIg68Q==
+  dependencies:
+    "@babel/helper-validator-identifier" "^7.14.8"
+    to-fast-properties "^2.0.0"
+
+"@csstools/convert-colors@^1.4.0":
+  version "1.4.0"
+  resolved "https://registry.yarnpkg.com/@csstools/convert-colors/-/convert-colors-1.4.0.tgz#ad495dc41b12e75d588c6db8b9834f08fa131eb7"
+  integrity sha512-5a6wqoJV/xEdbRNKVo6I4hO3VjyDq//8q2f9I6PBAvMesJHFauXDorcNCsr9RzvsZnaWi5NYCcfyqP1QeFHFbw==
+
 "@emotion/is-prop-valid@^0.8.1":
   version "0.8.3"
   resolved "https://registry.yarnpkg.com/@emotion/is-prop-valid/-/is-prop-valid-0.8.3.tgz#cbe62ddbea08aa022cdf72da3971570a33190d29"
   integrity sha512-We7VBiltAJ70KQA0dWkdPMXnYoizlxOXpvtjmu5/MBnExd+u0PGgV27WCYanmLAbCwAU30Le/xA0CQs/F/Otig==
   dependencies:
     "@emotion/memoize" "0.7.3"
 
@@ -809,14 +1111,24 @@
   integrity sha512-2Md9mH6mvo+ygq1trTeVp2uzAKwE2P7In0cRpD/M9Q70aH8L+rxMLbb3JCN2JoSWsV2O+DdFjfbbXoMoLBczow==
 
 "@emotion/unitless@^0.7.0":
   version "0.7.4"
   resolved "https://registry.yarnpkg.com/@emotion/unitless/-/unitless-0.7.4.tgz#a87b4b04e5ae14a88d48ebef15015f6b7d1f5677"
   integrity sha512-kBa+cDHOR9jpRJ+kcGMsysrls0leukrm68DmFQoMIWQcXdr2cZvyvypWuGYT7U+9kAExUE7+T7r6G3C3A6L8MQ==
 
+"@next/polyfill-nomodule@9.3.2":
+  version "9.3.2"
+  resolved "https://registry.yarnpkg.com/@next/polyfill-nomodule/-/polyfill-nomodule-9.3.2.tgz#f8e282fdeb448eb6b70bcc18c5d46c911072687a"
+  integrity sha512-kEa7v3trZmW6iWeTJrhg+ZsE9njae7mLkgyZB5M1r975JHr5PQ69B5aX7hrEAj7aAJYvCKETgAczx4gGR8MOzQ==
+
+"@types/json-schema@^7.0.5":
+  version "7.0.8"
+  resolved "https://registry.yarnpkg.com/@types/json-schema/-/json-schema-7.0.8.tgz#edf1bf1dbf4e04413ca8e5b17b3b7d7d54b59818"
+  integrity sha512-YSBPTLTVm2e2OoQIDYx8HaeWJ5tTToLH67kXR7zYNGupXMEHa2++G8k+DczX2cFVgalypqtyZIcU19AFcmOpmg==
+
 "@types/node@^12.7.8":
   version "12.7.8"
   resolved "https://registry.yarnpkg.com/@types/node/-/node-12.7.8.tgz#cb1bf6800238898bc2ff6ffa5702c3cadd350708"
   integrity sha512-FMdVn84tJJdV+xe+53sYiZS4R5yn1mAIxfj+DVoNiQjTYz1+OYmjwEZr1ev9nU0axXwda0QDbYl06QHanRVH3A==
 
 "@types/prop-types@*":
   version "15.7.3"
@@ -1001,42 +1313,68 @@
     negotiator "0.6.2"
 
 acorn@^6.2.1:
   version "6.4.1"
   resolved "https://registry.yarnpkg.com/acorn/-/acorn-6.4.1.tgz#531e58ba3f51b9dacb9a6646ca4debf5b14ca474"
   integrity sha512-ZVA9k326Nwrj3Cj9jlh3wGFutC2ZornPNARZwsNYqQYgN0EsV2d53w5RN/co65Ohn4sUAUtb1rSUAOD6XN9idA==
 
+adjust-sourcemap-loader@2.0.0:
+  version "2.0.0"
+  resolved "https://registry.yarnpkg.com/adjust-sourcemap-loader/-/adjust-sourcemap-loader-2.0.0.tgz#6471143af75ec02334b219f54bc7970c52fb29a4"
+  integrity sha512-4hFsTsn58+YjrU9qKzML2JSSDqKvN8mUGQ0nNIrfPi8hmIONT4L3uUaT6MKdMsZ9AjsU6D2xDkZxCkbQPxChrA==
+  dependencies:
+    assert "1.4.1"
+    camelcase "5.0.0"
+    loader-utils "1.2.3"
+    object-path "0.11.4"
+    regex-parser "2.2.10"
+
 ajv-errors@^1.0.0:
   version "1.0.1"
   resolved "https://registry.yarnpkg.com/ajv-errors/-/ajv-errors-1.0.1.tgz#f35986aceb91afadec4102fbd85014950cefa64d"
   integrity sha512-DCRfO/4nQ+89p/RK43i8Ezd41EqdGIU4ld7nGF8OQ14oc/we5rEntLCUa7+jrn3nn83BosfwZA0wb4pon2o8iQ==
 
 ajv-keywords@^3.1.0, ajv-keywords@^3.4.1:
   version "3.4.1"
   resolved "https://registry.yarnpkg.com/ajv-keywords/-/ajv-keywords-3.4.1.tgz#ef916e271c64ac12171fd8384eaae6b2345854da"
   integrity sha512-RO1ibKvd27e6FEShVFfPALuHI3WjSVNeK5FIsmme/LYRNxjKuNj+Dt7bucLa6NdSv3JcVTyMlm9kGR84z1XpaQ==
 
+ajv-keywords@^3.5.2:
+  version "3.5.2"
+  resolved "https://registry.yarnpkg.com/ajv-keywords/-/ajv-keywords-3.5.2.tgz#31f29da5ab6e00d1c2d329acf7b5929614d5014d"
+  integrity sha512-5p6WTN0DdTGVQk6VjcEju19IgaHudalcfabD7yhDGeA6bcQnmL+CpveLJq/3hvfwd1aof6L386Ougkx6RfyMIQ==
+
 ajv@^6.1.0, ajv@^6.10.0, ajv@^6.10.2:
   version "6.10.2"
   resolved "https://registry.yarnpkg.com/ajv/-/ajv-6.10.2.tgz#d3cea04d6b017b2894ad69040fec8b623eb4bd52"
   integrity sha512-TXtUUEYHuaTEbLZWIKUr5pmBuhDLy+8KYtPYdcV8qC+pOZL+NKqYwvWSRrVXHn+ZmRRAu8vJTAznH7Oag6RVRw==
   dependencies:
     fast-deep-equal "^2.0.1"
     fast-json-stable-stringify "^2.0.0"
     json-schema-traverse "^0.4.1"
     uri-js "^4.2.2"
 
-amphtml-validator@1.0.23:
-  version "1.0.23"
-  resolved "https://registry.yarnpkg.com/amphtml-validator/-/amphtml-validator-1.0.23.tgz#dba0c3854289563c0adaac292cd4d6096ee4d7c8"
-  integrity sha1-26DDhUKJVjwK2qwpLNTWCW7k18g=
-  dependencies:
-    colors "1.1.2"
-    commander "2.9.0"
-    promise "7.1.1"
+ajv@^6.12.4:
+  version "6.12.6"
+  resolved "https://registry.yarnpkg.com/ajv/-/ajv-6.12.6.tgz#baf5a62e802b07d977034586f8c3baf5adf26df4"
+  integrity sha512-j3fVLgvTo527anyYyJOGTYJbG+vnnQYvE0m5mmkc1TK+nxAppkCLMIL0aZ4dblVCNoGShhm+kzE4ZUykBoMg4g==
+  dependencies:
+    fast-deep-equal "^3.1.1"
+    fast-json-stable-stringify "^2.0.0"
+    json-schema-traverse "^0.4.1"
+    uri-js "^4.2.2"
+
+amphtml-validator@1.0.30:
+  version "1.0.30"
+  resolved "https://registry.yarnpkg.com/amphtml-validator/-/amphtml-validator-1.0.30.tgz#b722ea5e965d0cc028cbdc360fc76b97e669715e"
+  integrity sha512-CaEm2ivIi4M4QTiFnCE9t4MRgawCf88iAV/+VsS0zEw6T4VBU6zoXcgn4L+dt6/WZ/NYxKpc38duSoRLqZJhNQ==
+  dependencies:
+    colors "1.2.5"
+    commander "2.15.1"
+    promise "8.0.1"
 
 ansi-colors@^3.0.0:
   version "3.2.4"
   resolved "https://registry.yarnpkg.com/ansi-colors/-/ansi-colors-3.2.4.tgz#e3a3da4bfbae6c86a9c285625de124a234026fbf"
   integrity sha512-hHUXGagefjN2iRrID63xckIvotOXOojhQKWIPUZ4mNUZ9nLZW+7FMNoE1lOkEhNWYsx/7ysGIuJYCiMAA9FnrA==
 
 ansi-html@0.0.7:
@@ -1075,27 +1413,47 @@
   version "2.0.0"
   resolved "https://registry.yarnpkg.com/anymatch/-/anymatch-2.0.0.tgz#bcb24b4f37934d9aa7ac17b4adaf89e7c76ef2eb"
   integrity sha512-5teOsQWABXHHBFP9y3skS5P3d/WfWXpv3FUpy+LorMrNYaT9pI4oLMQX7jzQ2KklNpGpWHzdCXTDT2Y3XGlZBw==
   dependencies:
     micromatch "^3.1.4"
     normalize-path "^2.1.1"
 
+anymatch@~3.1.2:
+  version "3.1.2"
+  resolved "https://registry.yarnpkg.com/anymatch/-/anymatch-3.1.2.tgz#c0557c096af32f106198f4f4e2a383537e378716"
+  integrity sha512-P43ePfOAIupkguHUycrc4qJ9kz8ZiuOUijaETwX7THt0Y/GNK7v0aa8rY816xWjZ7rJdA5XdMcpVFTKMq+RvWg==
+  dependencies:
+    normalize-path "^3.0.0"
+    picomatch "^2.0.4"
+
 aproba@^1.0.3, aproba@^1.1.1:
   version "1.2.0"
   resolved "https://registry.yarnpkg.com/aproba/-/aproba-1.2.0.tgz#6802e6264efd18c790a1b0d517f0f2627bf2c94a"
   integrity sha512-Y9J6ZjXtoYh8RnXVCMOU/ttDmk1aBjunq9vO0ta5x85WDQiQfUF9sIPBITdbiiIVcBo03Hi3jMxigBtsddlXRw==
 
 are-we-there-yet@~1.1.2:
   version "1.1.5"
   resolved "https://registry.yarnpkg.com/are-we-there-yet/-/are-we-there-yet-1.1.5.tgz#4b35c2944f062a8bfcda66410760350fe9ddfc21"
   integrity sha512-5hYdAkZlcG8tOLujVDTgCT+uPX0VnpAH28gWsLfzpXYm7wP6mp5Q/gYyR7YQ0cKVJcXJnl3j2kpBan13PtQf6w==
   dependencies:
     delegates "^1.0.0"
     readable-stream "^2.0.6"
 
+argparse@^1.0.7:
+  version "1.0.10"
+  resolved "https://registry.yarnpkg.com/argparse/-/argparse-1.0.10.tgz#bcd6791ea5ae09725e17e5ad988134cd40b3d911"
+  integrity sha512-o5Roy6tNG4SL/FOkCAN6RzjiakZS25RLYFrcMttJqbdd8BWrnA+fGz57iN5Pb06pvBGvl5gQ0B48dJlslXvoTg==
+  dependencies:
+    sprintf-js "~1.0.2"
+
+arity-n@^1.0.4:
+  version "1.0.4"
+  resolved "https://registry.yarnpkg.com/arity-n/-/arity-n-1.0.4.tgz#d9e76b11733e08569c0847ae7b39b2860b30b745"
+  integrity sha1-2edrEXM+CFacCEeuezmyhgswt0U=
+
 arr-diff@^4.0.0:
   version "4.0.0"
   resolved "https://registry.yarnpkg.com/arr-diff/-/arr-diff-4.0.0.tgz#d6461074febfec71e7e15235761a329a5dc7c520"
   integrity sha1-1kYQdP6/7HHn4VI1dhoyml3HxSA=
 
 arr-flatten@^1.1.0:
   version "1.1.0"
@@ -1134,27 +1492,39 @@
   resolved "https://registry.yarnpkg.com/asn1.js/-/asn1.js-4.10.1.tgz#b9c2bf5805f1e64aadeed6df3a2bfafb5a73f5a0"
   integrity sha512-p32cOF5q0Zqs9uBiONKYLm6BClCoBCM5O9JfeUSlnQLBTxYdTK+pW+nXflm8UkKd2UYlEbYz5qEi0JuZR9ckSw==
   dependencies:
     bn.js "^4.0.0"
     inherits "^2.0.1"
     minimalistic-assert "^1.0.0"
 
+assert@1.4.1:
+  version "1.4.1"
+  resolved "https://registry.yarnpkg.com/assert/-/assert-1.4.1.tgz#99912d591836b5a6f5b345c0f07eefc08fc65d91"
+  integrity sha1-mZEtWRg2tab1s0XA8H7vwI/GXZE=
+  dependencies:
+    util "0.10.3"
+
 assert@^1.1.1:
   version "1.5.0"
   resolved "https://registry.yarnpkg.com/assert/-/assert-1.5.0.tgz#55c109aaf6e0aefdb3dc4b71240c70bf574b18eb"
   integrity sha512-EDsgawzwoun2CZkCgtxJbv392v4nbk9XDD06zI+kQYoBM/3RBWLlEyJARDOmhAAosBjWACEkKL6S+lIZtcAubA==
   dependencies:
     object-assign "^4.1.1"
     util "0.10.3"
 
 assign-symbols@^1.0.0:
   version "1.0.0"
   resolved "https://registry.yarnpkg.com/assign-symbols/-/assign-symbols-1.0.0.tgz#59667f41fadd4f20ccbc2bb96b8d4f7f78ec0367"
   integrity sha1-WWZ/QfrdTyDMvCu5a41Pf3jsA2c=
 
+ast-types@0.13.2:
+  version "0.13.2"
+  resolved "https://registry.yarnpkg.com/ast-types/-/ast-types-0.13.2.tgz#df39b677a911a83f3a049644fb74fdded23cea48"
+  integrity sha512-uWMHxJxtfj/1oZClOxDEV1sQ1HCDkA4MG8Gr69KKeBjEVH0R84WlejZ0y2DcwyBlpAEMltmVYkVgqfLFb2oyiA==
+
 async-each@^1.0.1:
   version "1.0.3"
   resolved "https://registry.yarnpkg.com/async-each/-/async-each-1.0.3.tgz#b727dbf87d7651602f06f4d4ac387f47d91b0cbf"
   integrity sha512-z/WhQ5FPySLdvREByI2vZiTWwCnF0moMJ1hK9YQwDTHKh6I7/uSckMetoRGb5UBZPC1z0jlw+n/XCgjeH7y1AQ==
 
 async-retry@1.2.3:
   version "1.2.3"
@@ -1185,14 +1555,27 @@
     make-dir "^1.0.0"
     memory-fs "^0.4.1"
     read-pkg "^2.0.0"
     tapable "^1.0.0"
     webpack-merge "^4.1.0"
     webpack-sources "^1.0.1"
 
+autoprefixer@^9.6.1:
+  version "9.8.6"
+  resolved "https://registry.yarnpkg.com/autoprefixer/-/autoprefixer-9.8.6.tgz#3b73594ca1bf9266320c5acf1588d74dea74210f"
+  integrity sha512-XrvP4VVHdRBCdX1S3WXVD8+RyG9qeb1D5Sn1DeLiG2xfSpzellk5k54xbUERJ3M5DggQxes39UGOTP8CFrEGbg==
+  dependencies:
+    browserslist "^4.12.0"
+    caniuse-lite "^1.0.30001109"
+    colorette "^1.2.1"
+    normalize-range "^0.1.2"
+    num2fraction "^1.2.2"
+    postcss "^7.0.32"
+    postcss-value-parser "^4.1.0"
+
 babel-code-frame@^6.22.0:
   version "6.26.0"
   resolved "https://registry.yarnpkg.com/babel-code-frame/-/babel-code-frame-6.26.0.tgz#63fd43f7dc1e3bb7ce35947db8fe369a3f58c74b"
   integrity sha1-Y/1D99weO7fONZR9uP42mj9Yx0s=
   dependencies:
     chalk "^1.1.3"
     esutils "^2.0.2"
@@ -1216,14 +1599,21 @@
 babel-plugin-dynamic-import-node@^2.3.0:
   version "2.3.0"
   resolved "https://registry.yarnpkg.com/babel-plugin-dynamic-import-node/-/babel-plugin-dynamic-import-node-2.3.0.tgz#f00f507bdaa3c3e3ff6e7e5e98d90a7acab96f7f"
   integrity sha512-o6qFkpeQEBxcqt0XYlWzAVxNCSCZdUgcR8IRlhD/8DylxjjO4foPcvTW0GGKa/cVt3rvxZ7o5ippJ+/0nvLhlQ==
   dependencies:
     object.assign "^4.1.0"
 
+babel-plugin-dynamic-import-node@^2.3.3:
+  version "2.3.3"
+  resolved "https://registry.yarnpkg.com/babel-plugin-dynamic-import-node/-/babel-plugin-dynamic-import-node-2.3.3.tgz#84fda19c976ec5c6defef57f9427b3def66e17a3"
+  integrity sha512-jZVI+s9Zg3IqA/kdi0i6UDCybUI3aSBLnglhYbSSjKlV7yF1F/5LWv8MakQmvYpnbJDS6fcBL2KzHSxNCMtWSQ==
+  dependencies:
+    object.assign "^4.1.0"
+
 "babel-plugin-styled-components@>= 1":
   version "1.10.6"
   resolved "https://registry.yarnpkg.com/babel-plugin-styled-components/-/babel-plugin-styled-components-1.10.6.tgz#f8782953751115faf09a9f92431436912c34006b"
   integrity sha512-gyQj/Zf1kQti66100PhrCRjI5ldjaze9O0M3emXRPAN80Zsf8+e1thpTpaXJXVHXtaM4/+dJEgZHyS9Its+8SA==
   dependencies:
     "@babel/helper-annotate-as-pure" "^7.0.0"
     "@babel/helper-module-imports" "^7.0.0"
@@ -1231,45 +1621,27 @@
     lodash "^4.17.11"
 
 babel-plugin-syntax-jsx@6.18.0, babel-plugin-syntax-jsx@^6.18.0:
   version "6.18.0"
   resolved "https://registry.yarnpkg.com/babel-plugin-syntax-jsx/-/babel-plugin-syntax-jsx-6.18.0.tgz#0af32a9a6e13ca7a3fd5069e62d7b0f58d0d8946"
   integrity sha1-CvMqmm4Tyno/1QaeYtew9Y0NiUY=
 
-babel-plugin-transform-define@1.3.1:
-  version "1.3.1"
-  resolved "https://registry.yarnpkg.com/babel-plugin-transform-define/-/babel-plugin-transform-define-1.3.1.tgz#b21b7bad3b84cf8e3f07cdc8c660b99cbbc01213"
-  integrity sha512-JXZ1xE9jIbKCGYZ4wbSMPSI5mdS4DRLi5+SkTHgZqWn5YIf/EucykkzUsPmzJlpkX8fsMVdLnA5vt/LvT97Zbg==
+babel-plugin-transform-define@2.0.0:
+  version "2.0.0"
+  resolved "https://registry.yarnpkg.com/babel-plugin-transform-define/-/babel-plugin-transform-define-2.0.0.tgz#79c3536635f899aabaf830b194b25519465675a4"
+  integrity sha512-0dv5RNRUlUKxGYIIErl01lpvi8b7W2R04Qcl1mCj70ahwZcgiklfXnFlh4FGnRh6aayCfSZKdhiMryVzcq5Dmg==
   dependencies:
     lodash "^4.17.11"
     traverse "0.6.6"
 
 babel-plugin-transform-react-remove-prop-types@0.4.24:
   version "0.4.24"
   resolved "https://registry.yarnpkg.com/babel-plugin-transform-react-remove-prop-types/-/babel-plugin-transform-react-remove-prop-types-0.4.24.tgz#f2edaf9b4c6a5fbe5c1d678bfb531078c1555f3a"
   integrity sha512-eqj0hVcJUR57/Ug2zE1Yswsw4LhuqqHhD+8v120T1cl3kjg76QwtyBrdIk4WVwK+lAhBJVYCd/v+4nc4y+8JsA==
 
-babel-runtime@^6.26.0:
-  version "6.26.0"
-  resolved "https://registry.yarnpkg.com/babel-runtime/-/babel-runtime-6.26.0.tgz#965c7058668e82b55d7bfe04ff2337bc8b5647fe"
-  integrity sha1-llxwWGaOgrVde/4E/yM3vItWR/4=
-  dependencies:
-    core-js "^2.4.0"
-    regenerator-runtime "^0.11.0"
-
-babel-types@6.26.0:
-  version "6.26.0"
-  resolved "https://registry.yarnpkg.com/babel-types/-/babel-types-6.26.0.tgz#a3b073f94ab49eb6fa55cd65227a334380632497"
-  integrity sha1-o7Bz+Uq0nrb6Vc1lInozQ4BjJJc=
-  dependencies:
-    babel-runtime "^6.26.0"
-    esutils "^2.0.2"
-    lodash "^4.17.4"
-    to-fast-properties "^1.0.3"
-
 balanced-match@^1.0.0:
   version "1.0.0"
   resolved "https://registry.yarnpkg.com/balanced-match/-/balanced-match-1.0.0.tgz#89b4d199ab2bee49de164ea02b89ce462d71b767"
   integrity sha1-ibTRmasr7kneFk6gK4nORi1xt2c=
 
 base64-js@^1.0.2:
   version "1.3.1"
@@ -1295,23 +1667,28 @@
   integrity sha512-vyL2OymJxmarO8gxMr0mhChsO9QGwhynfuu4+MHTAW6czfq9humCB7rKpUjDd9YUiDPU4mzpyupFSvOClAwbmQ==
 
 binary-extensions@^1.0.0:
   version "1.13.1"
   resolved "https://registry.yarnpkg.com/binary-extensions/-/binary-extensions-1.13.1.tgz#598afe54755b2868a5330d2aff9d4ebb53209b65"
   integrity sha512-Un7MIEDdUC5gNpcGDV97op1Ywk748MpHcFTHoYs6qnj1Z3j7I53VG3nwZhKzoBZmbdRNnb6WRdFlwl7tSDuZGw==
 
+binary-extensions@^2.0.0:
+  version "2.2.0"
+  resolved "https://registry.yarnpkg.com/binary-extensions/-/binary-extensions-2.2.0.tgz#75f502eeaf9ffde42fc98829645be4ea76bd9e2d"
+  integrity sha512-jDctJ/IVQbZoJykoeHbhXpOlNBqGNcwXJKJog42E5HDPUwQTSdjCHdihjj0DlnheQ7blbT6dHOafNAiS8ooQKA==
+
 bluebird@^3.5.0, bluebird@^3.5.5:
   version "3.5.5"
   resolved "https://registry.yarnpkg.com/bluebird/-/bluebird-3.5.5.tgz#a8d0afd73251effbbd5fe384a77d73003c17a71f"
   integrity sha512-5am6HnnfN+urzt4yfg7IgTbotDjIT/u8AJpEt0sIU9FtXfVeezXAPKswrG+xKUCOYAINpSdgZVDU6QFh+cuH3w==
 
-bn.js@^4.0.0, bn.js@^4.1.0, bn.js@^4.1.1, bn.js@^4.4.0:
-  version "4.11.9"
-  resolved "https://registry.yarnpkg.com/bn.js/-/bn.js-4.11.9.tgz#26d556829458f9d1e81fc48952493d0ba3507828"
-  integrity sha512-E6QoYqCKZfgatHTdHzs1RRKP7ip4vvm+EyRUeE2RF0NblwVvb0p6jSVeNTOFxPn26QXN2o6SMfNxKp6kU8zQaw==
+bn.js@^4.0.0, bn.js@^4.1.0, bn.js@^4.1.1, bn.js@^4.11.9:
+  version "4.12.0"
+  resolved "https://registry.yarnpkg.com/bn.js/-/bn.js-4.12.0.tgz#775b3f278efbb9718eec7361f483fb36fbbfea88"
+  integrity sha512-c98Bf3tPniI+scsdk237ku1Dc3ujXQTSgyiPUDEOe7tRkhrqridvh8klBv0HCEso1OLOYcHuCv/cS6DNxKH+ZA==
 
 brace-expansion@^1.1.7:
   version "1.1.11"
   resolved "https://registry.yarnpkg.com/brace-expansion/-/brace-expansion-1.1.11.tgz#3c7fcbf529d87226f3d2f52b966ff5271eb441dd"
   integrity sha512-iCuPHDFgrHX7H2vEI/5xpz07zSHB00TpugqhmYtVmMO6518mCuRMoOYFldEBl0g187ufozdaHgWKcYFb61qGiA==
   dependencies:
     balanced-match "^1.0.0"
@@ -1329,15 +1706,22 @@
     isobject "^3.0.1"
     repeat-element "^1.1.2"
     snapdragon "^0.8.1"
     snapdragon-node "^2.0.1"
     split-string "^3.0.2"
     to-regex "^3.0.1"
 
-brorand@^1.0.1:
+braces@~3.0.2:
+  version "3.0.2"
+  resolved "https://registry.yarnpkg.com/braces/-/braces-3.0.2.tgz#3454e1a462ee8d599e236df336cd9ea4f8afe107"
+  integrity sha512-b8um+L1RzM3WDSzvhm6gIz1yfTbBt6YTlcEKAvsmqCZZFw46z626lVj9j1yEPW33H5H+lBQpZMP1k8l+78Ha0A==
+  dependencies:
+    fill-range "^7.0.1"
+
+brorand@^1.0.1, brorand@^1.1.0:
   version "1.1.0"
   resolved "https://registry.yarnpkg.com/brorand/-/brorand-1.1.0.tgz#12c25efe40a45e3c323eb8675a0a0ce57b22371f"
   integrity sha1-EsJe/kCkXjwyPrhnWgoM5XsiNx8=
 
 browserify-aes@^1.0.0, browserify-aes@^1.0.4:
   version "1.2.0"
   resolved "https://registry.yarnpkg.com/browserify-aes/-/browserify-aes-1.2.0.tgz#326734642f403dabc3003209853bb70ad428ef48"
@@ -1393,28 +1777,49 @@
 browserify-zlib@^0.2.0:
   version "0.2.0"
   resolved "https://registry.yarnpkg.com/browserify-zlib/-/browserify-zlib-0.2.0.tgz#2869459d9aa3be245fe8fe2ca1f46e2e7f54d73f"
   integrity sha512-Z942RysHXmJrhqk88FmKBVq/v5tqmSkDz7p54G/MGyjMnCFFnC79XWNbg+Vta8W6Wb2qtSZTSxIGkJrRpCFEiA==
   dependencies:
     pako "~1.0.5"
 
-browserslist@^4.6.0, browserslist@^4.6.6:
-  version "4.7.0"
-  resolved "https://registry.yarnpkg.com/browserslist/-/browserslist-4.7.0.tgz#9ee89225ffc07db03409f2fee524dc8227458a17"
-  integrity sha512-9rGNDtnj+HaahxiVV38Gn8n8Lr8REKsel68v1sPFfIGEK6uSXTY3h9acgiT1dZVtOOUtifo/Dn8daDQ5dUgVsA==
-  dependencies:
-    caniuse-lite "^1.0.30000989"
-    electron-to-chromium "^1.3.247"
-    node-releases "^1.1.29"
+browserslist@4.8.3:
+  version "4.8.3"
+  resolved "https://registry.yarnpkg.com/browserslist/-/browserslist-4.8.3.tgz#65802fcd77177c878e015f0e3189f2c4f627ba44"
+  integrity sha512-iU43cMMknxG1ClEZ2MDKeonKE1CCrFVkQK2AqO2YWFmvIrx4JWrvQ4w4hQez6EpVI8rHTtqh/ruHHDHSOKxvUg==
+  dependencies:
+    caniuse-lite "^1.0.30001017"
+    electron-to-chromium "^1.3.322"
+    node-releases "^1.1.44"
+
+browserslist@^4.12.0, browserslist@^4.16.6, browserslist@^4.6.0, browserslist@^4.6.4, browserslist@^4.6.6:
+  version "4.16.6"
+  resolved "https://registry.yarnpkg.com/browserslist/-/browserslist-4.16.6.tgz#d7901277a5a88e554ed305b183ec9b0c08f66fa2"
+  integrity sha512-Wspk/PqO+4W9qp5iUTJsa1B/QrYn1keNCcEP5OvP7WBwT4KaDly0uONYmC6Xa3Z5IqnUgS0KcgLYu1l74x0ZXQ==
+  dependencies:
+    caniuse-lite "^1.0.30001219"
+    colorette "^1.2.2"
+    electron-to-chromium "^1.3.723"
+    escalade "^3.1.1"
+    node-releases "^1.1.71"
+
+buffer-equal-constant-time@1.0.1:
+  version "1.0.1"
+  resolved "https://registry.yarnpkg.com/buffer-equal-constant-time/-/buffer-equal-constant-time-1.0.1.tgz#f8e71132f7ffe6e01a5c9697a4c6f3e48d5cc819"
+  integrity sha1-+OcRMvf/5uAaXJaXpMbz5I1cyBk=
 
 buffer-from@^1.0.0:
   version "1.1.1"
   resolved "https://registry.yarnpkg.com/buffer-from/-/buffer-from-1.1.1.tgz#32713bc028f75c02fdb710d7c7bcec1f2c6070ef"
   integrity sha512-MQcXEUbCKtEo7bhqEs6560Hyd4XaovZlO/k9V3hjVUF/zwW7KBVdSK4gIt/bzwS9MbR5qob+F5jusZsb0YQK2A==
 
+buffer-json@^2.0.0:
+  version "2.0.0"
+  resolved "https://registry.yarnpkg.com/buffer-json/-/buffer-json-2.0.0.tgz#f73e13b1e42f196fe2fd67d001c7d7107edd7c23"
+  integrity sha512-+jjPFVqyfF1esi9fvfUs3NqM0pH1ziZ36VP4hmA/y/Ssfo/5w5xHKfTw9BwQjoJ1w/oVtpLomqwUHKdefGyuHw==
+
 buffer-xor@^1.0.3:
   version "1.0.3"
   resolved "https://registry.yarnpkg.com/buffer-xor/-/buffer-xor-1.0.3.tgz#26e61ed1422fb70dd42e6e36729ed51d855fe8d9"
   integrity sha1-JuYe0UIvtw3ULm42cp7VHYVf6Nk=
 
 buffer@^4.3.0:
   version "4.9.1"
@@ -1472,23 +1877,69 @@
     has-value "^1.0.0"
     isobject "^3.0.1"
     set-value "^2.0.0"
     to-object-path "^0.3.0"
     union-value "^1.0.0"
     unset-value "^1.0.0"
 
+cache-loader@4.1.0:
+  version "4.1.0"
+  resolved "https://registry.yarnpkg.com/cache-loader/-/cache-loader-4.1.0.tgz#9948cae353aec0a1fcb1eafda2300816ec85387e"
+  integrity sha512-ftOayxve0PwKzBF/GLsZNC9fJBXl8lkZE3TOsjkboHfVHVkL39iUEs1FO07A33mizmci5Dudt38UZrrYXDtbhw==
+  dependencies:
+    buffer-json "^2.0.0"
+    find-cache-dir "^3.0.0"
+    loader-utils "^1.2.3"
+    mkdirp "^0.5.1"
+    neo-async "^2.6.1"
+    schema-utils "^2.0.0"
+
+caller-callsite@^2.0.0:
+  version "2.0.0"
+  resolved "https://registry.yarnpkg.com/caller-callsite/-/caller-callsite-2.0.0.tgz#847e0fce0a223750a9a027c54b33731ad3154134"
+  integrity sha1-hH4PzgoiN1CpoCfFSzNzGtMVQTQ=
+  dependencies:
+    callsites "^2.0.0"
+
+caller-path@^2.0.0:
+  version "2.0.0"
+  resolved "https://registry.yarnpkg.com/caller-path/-/caller-path-2.0.0.tgz#468f83044e369ab2010fac5f06ceee15bb2cb1f4"
+  integrity sha1-Ro+DBE42mrIBD6xfBs7uFbsssfQ=
+  dependencies:
+    caller-callsite "^2.0.0"
+
+callsites@^2.0.0:
+  version "2.0.0"
+  resolved "https://registry.yarnpkg.com/callsites/-/callsites-2.0.0.tgz#06eb84f00eea413da86affefacbffb36093b3c50"
+  integrity sha1-BuuE8A7qQT2oav/vrL/7Ngk7PFA=
+
+camelcase@5.0.0:
+  version "5.0.0"
+  resolved "https://registry.yarnpkg.com/camelcase/-/camelcase-5.0.0.tgz#03295527d58bd3cd4aa75363f35b2e8d97be2f42"
+  integrity sha512-faqwZqnWxbxn+F1d399ygeamQNy3lPp/H9H6rNrqYh4FSVCtcY+3cub1MxA8o9mDd55mM8Aghuu/kuyYA6VTsA==
+
+camelcase@5.3.1, camelcase@^5.3.1:
+  version "5.3.1"
+  resolved "https://registry.yarnpkg.com/camelcase/-/camelcase-5.3.1.tgz#e3c9b31569e106811df242f715725a1f4c494320"
+  integrity sha512-L28STB170nwWS63UjtlEOE3dldQApaJXZkOI1uMFfzf3rRuPegHaHesyee+YxQ+W6SvRDQV6UrdOdRiR153wJg==
+
 camelize@^1.0.0:
   version "1.0.0"
   resolved "https://registry.yarnpkg.com/camelize/-/camelize-1.0.0.tgz#164a5483e630fa4321e5af07020e531831b2609b"
   integrity sha1-FkpUg+Yw+kMh5a8HAg5TGDGyYJs=
 
-caniuse-lite@^1.0.30000989:
-  version "1.0.30000997"
-  resolved "https://registry.yarnpkg.com/caniuse-lite/-/caniuse-lite-1.0.30000997.tgz#ba44a606804f8680894b7042612c2c7f65685b7e"
-  integrity sha512-BQLFPIdj2ntgBNWp9Q64LGUIEmvhKkzzHhUHR3CD5A9Lb7ZKF20/+sgadhFap69lk5XmK1fTUleDclaRFvgVUA==
+caniuse-lite@^1.0.30000981, caniuse-lite@^1.0.30001017, caniuse-lite@^1.0.30001109, caniuse-lite@^1.0.30001179:
+  version "1.0.30001246"
+  resolved "https://registry.yarnpkg.com/caniuse-lite/-/caniuse-lite-1.0.30001246.tgz#fe17d9919f87124d6bb416ef7b325356d69dc76c"
+  integrity sha512-Tc+ff0Co/nFNbLOrziBXmMVtpt9S2c2Y+Z9Nk9Khj09J+0zR9ejvIW5qkZAErCbOrVODCx/MN+GpB5FNBs5GFA==
+
+caniuse-lite@^1.0.30001219:
+  version "1.0.30001230"
+  resolved "https://registry.yarnpkg.com/caniuse-lite/-/caniuse-lite-1.0.30001230.tgz#8135c57459854b2240b57a4a6786044bdc5a9f71"
+  integrity sha512-5yBd5nWCBS+jWKTcHOzXwo5xzcj4ePE/yjtkZyUV1BTUmrBaA9MRGC+e7mxnqXSA90CmCA8L3eKLaSUkt099IQ==
 
 chalk@2.4.2, chalk@^2.0.0, chalk@^2.0.1, chalk@^2.3.0, chalk@^2.4.1, chalk@^2.4.2:
   version "2.4.2"
   resolved "https://registry.yarnpkg.com/chalk/-/chalk-2.4.2.tgz#cd42541677a54333cf541a49108c1432b44c9424"
   integrity sha512-Mti+f9lpJNcwF4tWV8/OrTTtF1gZi+f8FqlyAdouralcFWFQWF2+NgCHShjkCb+IFBLq9buZwE1xckQU4peSuQ==
   dependencies:
     ansi-styles "^3.2.1"
@@ -1502,15 +1953,15 @@
   dependencies:
     ansi-styles "^2.2.1"
     escape-string-regexp "^1.0.2"
     has-ansi "^2.0.0"
     strip-ansi "^3.0.0"
     supports-color "^2.0.0"
 
-chokidar@^2.0.2, chokidar@^2.0.4:
+chokidar@^2.0.2:
   version "2.1.8"
   resolved "https://registry.yarnpkg.com/chokidar/-/chokidar-2.1.8.tgz#804b3a7b6a99358c3c5c61e71d8728f041cff917"
   integrity sha512-ZmZUazfOzf0Nve7duiCKD23PFSCs4JPoYyccjUFF3aQkQadqBhfzhjkwBH2mNOG9cTBwhamM37EIsIkZw3nRgg==
   dependencies:
     anymatch "^2.0.0"
     async-each "^1.0.1"
     braces "^2.3.2"
@@ -1521,14 +1972,29 @@
     normalize-path "^3.0.0"
     path-is-absolute "^1.0.0"
     readdirp "^2.2.1"
     upath "^1.1.1"
   optionalDependencies:
     fsevents "^1.2.7"
 
+chokidar@^3.3.0:
+  version "3.5.2"
+  resolved "https://registry.yarnpkg.com/chokidar/-/chokidar-3.5.2.tgz#dba3976fcadb016f66fd365021d91600d01c1e75"
+  integrity sha512-ekGhOnNVPgT77r4K/U3GDhu+FQ2S8TnK/s2KbIGXi0SZWuwkZ2QNyfWdZW+TVfn84DpEP7rLeCt2UI6bJ8GwbQ==
+  dependencies:
+    anymatch "~3.1.2"
+    braces "~3.0.2"
+    glob-parent "~5.1.2"
+    is-binary-path "~2.1.0"
+    is-glob "~4.0.1"
+    normalize-path "~3.0.0"
+    readdirp "~3.6.0"
+  optionalDependencies:
+    fsevents "~2.3.2"
+
 chownr@^1.1.1:
   version "1.1.3"
   resolved "https://registry.yarnpkg.com/chownr/-/chownr-1.1.3.tgz#42d837d5239688d55f303003a508230fa6727142"
   integrity sha512-i70fVHhmV3DtTl6nqvZOnIjbY0Pe4kAUjwHj8z0zAdgBtYrJyYwLKCCuRBQ5ppkyL0AkN7HKRnETdmdp1zqNXw==
 
 chrome-trace-event@^1.0.2:
   version "1.0.2"
@@ -1568,14 +2034,23 @@
     restore-cursor "^2.0.0"
 
 cli-spinners@^2.0.0:
   version "2.2.0"
   resolved "https://registry.yarnpkg.com/cli-spinners/-/cli-spinners-2.2.0.tgz#e8b988d9206c692302d8ee834e7a85c0144d8f77"
   integrity sha512-tgU3fKwzYjiLEQgPMD9Jt+JjHVL9kW93FiIMX/l7rivvOD4/LL0Mf7gda3+4U2KJBloybwgj5KEoQgGRioMiKQ==
 
+clone-deep@^4.0.1:
+  version "4.0.1"
+  resolved "https://registry.yarnpkg.com/clone-deep/-/clone-deep-4.0.1.tgz#c19fd9bdbbf85942b4fd979c84dcf7d5f07c2387"
+  integrity sha512-neHB9xuzh/wk0dIHweyAXv2aPGZIVk3pLMe+/RNzINf17fe0OG96QroktYAUm7SM1PBnzTabaLboqqxDyMU+SQ==
+  dependencies:
+    is-plain-object "^2.0.4"
+    kind-of "^6.0.2"
+    shallow-clone "^3.0.0"
+
 clone@^1.0.2:
   version "1.0.4"
   resolved "https://registry.yarnpkg.com/clone/-/clone-1.0.4.tgz#da309cc263df15994c688ca902179ca3c7cd7c7e"
   integrity sha1-2jCcwmPfFZlMaIypAheco8fNfH4=
 
 code-point-at@^1.0.0:
   version "1.1.0"
@@ -1598,41 +2073,51 @@
     color-name "1.1.3"
 
 color-name@1.1.3:
   version "1.1.3"
   resolved "https://registry.yarnpkg.com/color-name/-/color-name-1.1.3.tgz#a7d0558bd89c42f795dd42328f740831ca53bc25"
   integrity sha1-p9BVi9icQveV3UIyj3QIMcpTvCU=
 
-colors@1.1.2:
-  version "1.1.2"
-  resolved "https://registry.yarnpkg.com/colors/-/colors-1.1.2.tgz#168a4701756b6a7f51a12ce0c97bfa28c084ed63"
-  integrity sha1-FopHAXVran9RoSzgyXv6KMCE7WM=
+colorette@^1.2.1, colorette@^1.2.2:
+  version "1.2.2"
+  resolved "https://registry.yarnpkg.com/colorette/-/colorette-1.2.2.tgz#cbcc79d5e99caea2dbf10eb3a26fd8b3e6acfa94"
+  integrity sha512-MKGMzyfeuutC/ZJ1cba9NqcNpfeqMUcYmyF1ZFY6/Cn7CNSAKx6a+s48sqLqyAiZuaP2TcqMhoo+dlwFnVxT9w==
 
-commander@2.9.0:
-  version "2.9.0"
-  resolved "https://registry.yarnpkg.com/commander/-/commander-2.9.0.tgz#9c99094176e12240cb22d6c5146098400fe0f7d4"
-  integrity sha1-nJkJQXbhIkDLItbFFGCYQA/g99Q=
-  dependencies:
-    graceful-readlink ">= 1.0.0"
+colors@1.2.5:
+  version "1.2.5"
+  resolved "https://registry.yarnpkg.com/colors/-/colors-1.2.5.tgz#89c7ad9a374bc030df8013241f68136ed8835afc"
+  integrity sha512-erNRLao/Y3Fv54qUa0LBB+//Uf3YwMUmdJinN20yMXm9zdKKqH9wt7R9IIVZ+K7ShzfpLV/Zg8+VyrBJYB4lpg==
+
+commander@2.15.1:
+  version "2.15.1"
+  resolved "https://registry.yarnpkg.com/commander/-/commander-2.15.1.tgz#df46e867d0fc2aec66a34662b406a9ccafff5b0f"
+  integrity sha512-VlfT9F3V0v+jr4yxPc5gg9s62/fIVWsd2Bk2iD435um1NlGMYdVCq+MjcXnhYq2icNOizHr1kK+5TI6H0Hy0ag==
 
-commander@^2.19.0, commander@^2.20.0:
+commander@^2.20.0:
   version "2.20.0"
   resolved "https://registry.yarnpkg.com/commander/-/commander-2.20.0.tgz#d58bb2b5c1ee8f87b0d340027e9e94e222c5a422"
   integrity sha512-7j2y+40w61zy6YC2iRNpUe/NwhNyoXrYpHMrSunaMG64nRnaf96zO/KMQR4OyN/UnE5KLyEBnKHd4aG3rskjpQ==
 
 commondir@^1.0.1:
   version "1.0.1"
   resolved "https://registry.yarnpkg.com/commondir/-/commondir-1.0.1.tgz#ddd800da0c66127393cca5950ea968a3aaf1253b"
   integrity sha1-3dgA2gxmEnOTzKWVDqloo6rxJTs=
 
 component-emitter@^1.2.1:
   version "1.3.0"
   resolved "https://registry.yarnpkg.com/component-emitter/-/component-emitter-1.3.0.tgz#16e4070fba8ae29b679f2215853ee181ab2eabc0"
   integrity sha512-Rd3se6QB+sO1TwqZjscQrurpEPIfO0/yYnSin6Q/rD3mOutHvUrCAhJub3r90uNb+SESBuE0QYoB90YdfatsRg==
 
+compose-function@3.0.3:
+  version "3.0.3"
+  resolved "https://registry.yarnpkg.com/compose-function/-/compose-function-3.0.3.tgz#9ed675f13cc54501d30950a486ff6a7ba3ab185f"
+  integrity sha1-ntZ18TzFRQHTCVCkhv9qe6OrGF8=
+  dependencies:
+    arity-n "^1.0.4"
+
 compressible@~2.0.16:
   version "2.0.17"
   resolved "https://registry.yarnpkg.com/compressible/-/compressible-2.0.17.tgz#6e8c108a16ad58384a977f3a482ca20bff2f38c1"
   integrity sha512-BGHeLCK1GV7j1bSmQQAi26X+GgWcTjLr/0tzSvMCl3LH1w1IJ4PFSPoV5316b30cneTziC+B1a+3OjoSUcQYmw==
   dependencies:
     mime-db ">= 1.40.0 < 2"
 
@@ -1695,18 +2180,30 @@
   integrity sha1-wguW2MYXdIqvHBYCF2DNJ/y4y3U=
 
 content-type@1.0.4:
   version "1.0.4"
   resolved "https://registry.yarnpkg.com/content-type/-/content-type-1.0.4.tgz#e138cc75e040c727b1966fe5e5f8c9aee256fe3b"
   integrity sha512-hIP3EEPs8tB9AT1L+NUqtwOAps4mk2Zob89MWXMHjHWg9milF/j4osnnQLXBCBFBk/tvIG/tUc9mOUJiPBhPXA==
 
-convert-source-map@1.6.0, convert-source-map@^1.1.0:
-  version "1.6.0"
-  resolved "https://registry.yarnpkg.com/convert-source-map/-/convert-source-map-1.6.0.tgz#51b537a8c43e0f04dec1993bffcdd504e758ac20"
-  integrity sha512-eFu7XigvxdZ1ETfbgPBohgyQ/Z++C0eEhTor0qRwBw9unw+L0/6V8wkSuGgzdThkiS5lSpdptOQPD8Ak40a+7A==
+convert-source-map@1.7.0:
+  version "1.7.0"
+  resolved "https://registry.yarnpkg.com/convert-source-map/-/convert-source-map-1.7.0.tgz#17a2cb882d7f77d3490585e2ce6c524424a3a442"
+  integrity sha512-4FJkXzKXEDB1snCFZlLP4gpC3JILicCpGbzG9f9G7tGqGCzETQ2hWPrcinA9oU4wtf2biUaEH5065UnMeR33oA==
+  dependencies:
+    safe-buffer "~5.1.1"
+
+convert-source-map@^0.3.3:
+  version "0.3.5"
+  resolved "https://registry.yarnpkg.com/convert-source-map/-/convert-source-map-0.3.5.tgz#f1d802950af7dd2631a1febe0596550c86ab3190"
+  integrity sha1-8dgClQr33SYxof6+BZZVDIarMZA=
+
+convert-source-map@^1.7.0:
+  version "1.8.0"
+  resolved "https://registry.yarnpkg.com/convert-source-map/-/convert-source-map-1.8.0.tgz#f3373c32d21b4d780dd8004514684fb791ca4369"
+  integrity sha512-+OQdjP49zViI/6i7nIJpA8rAl4sV/JdPfU9nZs3VqOwGIgizICvuN2ru6fMd+4llL0tar18UYJXfZ/TWtmhUjA==
   dependencies:
     safe-buffer "~5.1.1"
 
 cookie@0.4.0:
   version "0.4.0"
   resolved "https://registry.yarnpkg.com/cookie/-/cookie-0.4.0.tgz#beb437e7022b3b6d49019d088665303ebe9c14ba"
   integrity sha512-+Hp8fLp57wnUSt0tY0tHEXh4voZRDnoIrZPqlo3DPiI4y9lwg/jqx+1Om94/W6ZaPDOUbnjOt/99w66zk+l1Xg==
@@ -1732,24 +2229,29 @@
   version "3.2.1"
   resolved "https://registry.yarnpkg.com/core-js-compat/-/core-js-compat-3.2.1.tgz#0cbdbc2e386e8e00d3b85dc81c848effec5b8150"
   integrity sha512-MwPZle5CF9dEaMYdDeWm73ao/IflDH+FjeJCWEADcEgFSE9TLimFKwJsfmkwzI8eC0Aj0mgvMDjeQjrElkz4/A==
   dependencies:
     browserslist "^4.6.6"
     semver "^6.3.0"
 
-core-js@^2.4.0, core-js@^2.6.5:
-  version "2.6.9"
-  resolved "https://registry.yarnpkg.com/core-js/-/core-js-2.6.9.tgz#6b4b214620c834152e179323727fc19741b084f2"
-  integrity sha512-HOpZf6eXmnl7la+cUdMnLvUxKNqLUzJvgIziQ0DiF3JwSImNphIqdGqzj6hIKyX04MmV0poclQ7+wjWvxQyR2A==
-
 core-util-is@~1.0.0:
   version "1.0.2"
   resolved "https://registry.yarnpkg.com/core-util-is/-/core-util-is-1.0.2.tgz#b5fd54220aa2bc5ab57aab7140c940754503c1a7"
   integrity sha1-tf1UIgqivFq1eqtxQMlAdUUDwac=
 
+cosmiconfig@^5.0.0:
+  version "5.2.1"
+  resolved "https://registry.yarnpkg.com/cosmiconfig/-/cosmiconfig-5.2.1.tgz#040f726809c591e77a17c0a3626ca45b4f168b1a"
+  integrity sha512-H65gsXo1SKjf8zmrJ67eJk8aIRKV5ff2D4uKZIBZShbhGSpEmsQOPW/SKMKYhSTrqR7ufy6RP69rPogdaPh/kA==
+  dependencies:
+    import-fresh "^2.0.0"
+    is-directory "^0.3.1"
+    js-yaml "^3.13.1"
+    parse-json "^4.0.0"
+
 create-ecdh@^4.0.0:
   version "4.0.3"
   resolved "https://registry.yarnpkg.com/create-ecdh/-/create-ecdh-4.0.3.tgz#c9111b6f33045c4697f144787f9254cdc77c45ff"
   integrity sha512-GbEHQPMOswGpKXM9kCWVrremUcBmjteUaQ01T9rkKCPDXfUHX0IoP9LpHYo2NPFampa4e+/pFDc3jQdxrxQLaw==
   dependencies:
     bn.js "^4.1.0"
     elliptic "^6.0.0"
@@ -1773,14 +2275,21 @@
     cipher-base "^1.0.3"
     create-hash "^1.1.0"
     inherits "^2.0.1"
     ripemd160 "^2.0.0"
     safe-buffer "^5.0.1"
     sha.js "^2.4.8"
 
+cross-fetch@3.1.2:
+  version "3.1.2"
+  resolved "https://registry.yarnpkg.com/cross-fetch/-/cross-fetch-3.1.2.tgz#ee0c2f18844c4fde36150c2a4ddc068d20c1bc41"
+  integrity sha512-+JhD65rDNqLbGmB3Gzs3HrEKC0aQnD+XA3SY6RjgkF88jV2q5cTc5+CwxlS3sdmLk98gpPt5CF9XRnPdlxZe6w==
+  dependencies:
+    node-fetch "2.6.1"
+
 crypto-browserify@^3.11.0:
   version "3.12.0"
   resolved "https://registry.yarnpkg.com/crypto-browserify/-/crypto-browserify-3.12.0.tgz#396cf9f3137f03e4b8e532c58f698254e00f80ec"
   integrity sha512-fz4spIh+znjO2VjL+IdhEpRJ3YN6sMzITSBijk6FK2UvTqruSQW+/cCZTSNsMiZNvUeq0CqurF+dAbyiGOY6Wg==
   dependencies:
     browserify-cipher "^1.0.0"
     browserify-sign "^4.0.0"
@@ -1790,48 +2299,127 @@
     diffie-hellman "^5.0.0"
     inherits "^2.0.1"
     pbkdf2 "^3.0.3"
     public-encrypt "^4.0.0"
     randombytes "^2.0.0"
     randomfill "^1.0.3"
 
+css-blank-pseudo@^0.1.4:
+  version "0.1.4"
+  resolved "https://registry.yarnpkg.com/css-blank-pseudo/-/css-blank-pseudo-0.1.4.tgz#dfdefd3254bf8a82027993674ccf35483bfcb3c5"
+  integrity sha512-LHz35Hr83dnFeipc7oqFDmsjHdljj3TQtxGGiNWSOsTLIAubSm4TEz8qCaKFpk7idaQ1GfWscF4E6mgpBysA1w==
+  dependencies:
+    postcss "^7.0.5"
+
 css-color-keywords@^1.0.0:
   version "1.0.0"
   resolved "https://registry.yarnpkg.com/css-color-keywords/-/css-color-keywords-1.0.0.tgz#fea2616dc676b2962686b3af8dbdbe180b244e05"
   integrity sha1-/qJhbcZ2spYmhrOvjb2+GAskTgU=
 
+css-has-pseudo@^0.10.0:
+  version "0.10.0"
+  resolved "https://registry.yarnpkg.com/css-has-pseudo/-/css-has-pseudo-0.10.0.tgz#3c642ab34ca242c59c41a125df9105841f6966ee"
+  integrity sha512-Z8hnfsZu4o/kt+AuFzeGpLVhFOGO9mluyHBaA2bA8aCGTwah5sT3WV/fTHH8UNZUytOIImuGPrl/prlb4oX4qQ==
+  dependencies:
+    postcss "^7.0.6"
+    postcss-selector-parser "^5.0.0-rc.4"
+
+css-loader@3.3.0:
+  version "3.3.0"
+  resolved "https://registry.yarnpkg.com/css-loader/-/css-loader-3.3.0.tgz#65f889807baec3197313965d6cda9899f936734d"
+  integrity sha512-x9Y1vvHe5RR+4tzwFdWExPueK00uqFTCw7mZy+9aE/X1SKWOArm5luaOrtJ4d05IpOwJ6S86b/tVcIdhw1Bu4A==
+  dependencies:
+    camelcase "^5.3.1"
+    cssesc "^3.0.0"
+    icss-utils "^4.1.1"
+    loader-utils "^1.2.3"
+    normalize-path "^3.0.0"
+    postcss "^7.0.23"
+    postcss-modules-extract-imports "^2.0.0"
+    postcss-modules-local-by-default "^3.0.2"
+    postcss-modules-scope "^2.1.1"
+    postcss-modules-values "^3.0.0"
+    postcss-value-parser "^4.0.2"
+    schema-utils "^2.6.0"
+
+css-prefers-color-scheme@^3.1.1:
+  version "3.1.1"
+  resolved "https://registry.yarnpkg.com/css-prefers-color-scheme/-/css-prefers-color-scheme-3.1.1.tgz#6f830a2714199d4f0d0d0bb8a27916ed65cff1f4"
+  integrity sha512-MTu6+tMs9S3EUqzmqLXEcgNRbNkkD/TGFvowpeoWJn5Vfq7FMgsmRQs9X5NXAURiOBmOxm/lLjsDNXDE6k9bhg==
+  dependencies:
+    postcss "^7.0.5"
+
 css-to-react-native@^2.2.2:
   version "2.3.2"
   resolved "https://registry.yarnpkg.com/css-to-react-native/-/css-to-react-native-2.3.2.tgz#e75e2f8f7aa385b4c3611c52b074b70a002f2e7d"
   integrity sha512-VOFaeZA053BqvvvqIA8c9n0+9vFppVBAHCp6JgFTtTMU3Mzi+XnelJ9XC9ul3BqFzZyQ5N+H0SnwsWT2Ebchxw==
   dependencies:
     camelize "^1.0.0"
     css-color-keywords "^1.0.0"
     postcss-value-parser "^3.3.0"
 
-css@2.2.4:
+css@2.2.4, css@^2.0.0:
   version "2.2.4"
   resolved "https://registry.yarnpkg.com/css/-/css-2.2.4.tgz#c646755c73971f2bba6a601e2cf2fd71b1298929"
   integrity sha512-oUnjmWpy0niI3x/mPL8dVEI1l7MnG3+HHyRPHf+YFSbK+svOhXpmSOcDURUh2aOCgl2grzrOPt1nHLuCVFULLw==
   dependencies:
     inherits "^2.0.3"
     source-map "^0.6.1"
     source-map-resolve "^0.5.2"
     urix "^0.1.0"
 
+cssdb@^4.4.0:
+  version "4.4.0"
+  resolved "https://registry.yarnpkg.com/cssdb/-/cssdb-4.4.0.tgz#3bf2f2a68c10f5c6a08abd92378331ee803cddb0"
+  integrity sha512-LsTAR1JPEM9TpGhl/0p3nQecC2LJ0kD8X5YARu1hk/9I1gril5vDtMZyNxcEpxxDj34YNck/ucjuoUd66K03oQ==
+
+cssesc@^2.0.0:
+  version "2.0.0"
+  resolved "https://registry.yarnpkg.com/cssesc/-/cssesc-2.0.0.tgz#3b13bd1bb1cb36e1bcb5a4dcd27f54c5dcb35703"
+  integrity sha512-MsCAG1z9lPdoO/IUMLSBWBSVxVtJ1395VGIQ+Fc2gNdkQ1hNDnQdw3YhA71WJCBW1vdwA0cAnk/DnW6bqoEUYg==
+
+cssesc@^3.0.0:
+  version "3.0.0"
+  resolved "https://registry.yarnpkg.com/cssesc/-/cssesc-3.0.0.tgz#37741919903b868565e1c09ea747445cd18983ee"
+  integrity sha512-/Tb/JcjK111nNScGob5MNtsntNM1aCNUDipB/TkwZFhyDrrE47SOx/18wF2bbjgc3ZzCSKW1T5nt5EbFoAz/Vg==
+
+cssnano-preset-simple@^1.0.0:
+  version "1.3.1"
+  resolved "https://registry.yarnpkg.com/cssnano-preset-simple/-/cssnano-preset-simple-1.3.1.tgz#25517b278735abe4cf197faccc74c95c03fa1300"
+  integrity sha512-T3a4FACX9h9WRw3Gmiut9T+Pjrj/ScEPk5EclCzHz6Ya6WjMu8oZ3IOtIB32bQOJmkefa1LAI9G7ZDy5psvw2A==
+  dependencies:
+    caniuse-lite "^1.0.30001179"
+    postcss "^7.0.32"
+
+cssnano-simple@1.0.0:
+  version "1.0.0"
+  resolved "https://registry.yarnpkg.com/cssnano-simple/-/cssnano-simple-1.0.0.tgz#a9322f7f4c192fad29c6d48afcb7927a9c5c597b"
+  integrity sha512-B7u9vvtXEqeU2rzdt+Kfw5O9Nd46R7KNjJoP7Y5lGQs6c7n1Et5Ilofh2W9OjBV/ZiJV5+7j9ShWgiYNtH/57A==
+  dependencies:
+    cssnano-preset-simple "^1.0.0"
+    postcss "^7.0.18"
+
 csstype@^2.2.0:
   version "2.6.6"
   resolved "https://registry.yarnpkg.com/csstype/-/csstype-2.6.6.tgz#c34f8226a94bbb10c32cc0d714afdf942291fc41"
   integrity sha512-RpFbQGUE74iyPgvr46U9t1xoQBM8T4BL8SxrN66Le2xYAPSaDJJKeztV3awugusb3g3G9iL8StmkBBXhcbbXhg==
 
 cyclist@^1.0.1:
   version "1.0.1"
   resolved "https://registry.yarnpkg.com/cyclist/-/cyclist-1.0.1.tgz#596e9698fd0c80e12038c2b82d6eb1b35b6224d9"
   integrity sha1-WW6WmP0MgOEgOMK4LW6xs1tiJNk=
 
+d@1, d@^1.0.1:
+  version "1.0.1"
+  resolved "https://registry.yarnpkg.com/d/-/d-1.0.1.tgz#8698095372d58dbee346ffd0c7093f99f8f9eb5a"
+  integrity sha512-m62ShEObQ39CfralilEQRjH6oAMtNCV1xJyEx5LpRYUVN+EviphDgUc/F3hnYbADmkiNs67Y+3ylmlG7Lnu+FA==
+  dependencies:
+    es5-ext "^0.10.50"
+    type "^1.0.1"
+
 date-now@^0.1.4:
   version "0.1.4"
   resolved "https://registry.yarnpkg.com/date-now/-/date-now-0.1.4.tgz#eaf439fd4d4848ad74e5cc7dbef200672b9e345b"
   integrity sha1-6vQ5/U1ISK105cx9vvIAZyueNFs=
 
 debug@2.6.9, debug@^2.2.0, debug@^2.3.3:
   version "2.6.9"
@@ -1936,78 +2524,161 @@
   integrity sha1-l4hXRCxEdJ5CBmE+N5RiBYJqvYA=
 
 detect-libc@^1.0.2:
   version "1.0.3"
   resolved "https://registry.yarnpkg.com/detect-libc/-/detect-libc-1.0.3.tgz#fa137c4bd698edf55cd5cd02ac559f91a4c4ba9b"
   integrity sha1-+hN8S9aY7fVc1c0CrFWfkaTEups=
 
-devalue@2.0.0:
-  version "2.0.0"
-  resolved "https://registry.yarnpkg.com/devalue/-/devalue-2.0.0.tgz#2afa0b7c1bb35bebbef792498150663fdcd33c68"
-  integrity sha512-6H2FBD5DPnQS75UWJtQjoVeKZlmXoa765UgYS5RQnx6Ay9LUhUld0w1/D6cYdrY+wnu6XQNlpEBfnJUZK0YyPQ==
+devalue@2.0.1:
+  version "2.0.1"
+  resolved "https://registry.yarnpkg.com/devalue/-/devalue-2.0.1.tgz#5d368f9adc0928e47b77eea53ca60d2f346f9762"
+  integrity sha512-I2TiqT5iWBEyB8GRfTDP0hiLZ0YeDJZ+upDxjBfOC2lebO5LezQMv7QvIUTzdb64jQyAKLf1AHADtGN+jw6v8Q==
 
 diffie-hellman@^5.0.0:
   version "5.0.3"
   resolved "https://registry.yarnpkg.com/diffie-hellman/-/diffie-hellman-5.0.3.tgz#40e8ee98f55a2149607146921c63e1ae5f3d2875"
   integrity sha512-kqag/Nl+f3GwyK25fhUMYj81BUOrZ9IuJsjIcDE5icNM9FJHAVm3VcUDxdLPoQtTuUylWm6ZIknYJwwaPxsUzg==
   dependencies:
     bn.js "^4.1.0"
     miller-rabin "^4.0.0"
     randombytes "^2.0.0"
 
+dom-serializer@^0.2.1:
+  version "0.2.2"
+  resolved "https://registry.yarnpkg.com/dom-serializer/-/dom-serializer-0.2.2.tgz#1afb81f533717175d478655debc5e332d9f9bb51"
+  integrity sha512-2/xPb3ORsQ42nHYiSunXkDjPLBaEj/xTwUO4B7XCZQTRk7EBtTOPaygh10YAAh2OI1Qrp6NWfpAhzswj0ydt9g==
+  dependencies:
+    domelementtype "^2.0.1"
+    entities "^2.0.0"
+
+dom-serializer@^1.0.1:
+  version "1.3.2"
+  resolved "https://registry.yarnpkg.com/dom-serializer/-/dom-serializer-1.3.2.tgz#6206437d32ceefaec7161803230c7a20bc1b4d91"
+  integrity sha512-5c54Bk5Dw4qAxNOI1pFEizPSjVsx5+bpJKmL2kPn8JhBUq2q09tTCa3mjijun2NfK78NMouDYNMBkOrPZiS+ig==
+  dependencies:
+    domelementtype "^2.0.1"
+    domhandler "^4.2.0"
+    entities "^2.0.0"
+
 domain-browser@^1.1.1:
   version "1.2.0"
   resolved "https://registry.yarnpkg.com/domain-browser/-/domain-browser-1.2.0.tgz#3d31f50191a6749dd1375a7f522e823d42e54eda"
   integrity sha512-jnjyiM6eRyZl2H+W8Q/zLMA481hzi0eszAaBUzIVnmYVDBbnLxVNnfu1HgEBvCbL+71FrxMl3E6lpKH7Ge3OXA==
 
+domelementtype@^2.0.1, domelementtype@^2.2.0:
+  version "2.2.0"
+  resolved "https://registry.yarnpkg.com/domelementtype/-/domelementtype-2.2.0.tgz#9a0b6c2782ed6a1c7323d42267183df9bd8b1d57"
+  integrity sha512-DtBMo82pv1dFtUmHyr48beiuq792Sxohr+8Hm9zoxklYPfa6n0Z3Byjj2IV7bmr2IyqClnqEQhfgHJJ5QF0R5A==
+
+domhandler@3.0.0:
+  version "3.0.0"
+  resolved "https://registry.yarnpkg.com/domhandler/-/domhandler-3.0.0.tgz#51cd13efca31da95bbb0c5bee3a48300e333b3e9"
+  integrity sha512-eKLdI5v9m67kbXQbJSNn1zjh0SDzvzWVWtX+qEI3eMjZw8daH9k8rlj1FZY9memPwjiskQFbe7vHVVJIAqoEhw==
+  dependencies:
+    domelementtype "^2.0.1"
+
+domhandler@^3.0.0:
+  version "3.3.0"
+  resolved "https://registry.yarnpkg.com/domhandler/-/domhandler-3.3.0.tgz#6db7ea46e4617eb15cf875df68b2b8524ce0037a"
+  integrity sha512-J1C5rIANUbuYK+FuFL98650rihynUOEzRLxW+90bKZRWB6A1X1Tf82GxR1qAWLyfNPRvjqfip3Q5tdYlmAa9lA==
+  dependencies:
+    domelementtype "^2.0.1"
+
+domhandler@^4.2.0:
+  version "4.2.0"
+  resolved "https://registry.yarnpkg.com/domhandler/-/domhandler-4.2.0.tgz#f9768a5f034be60a89a27c2e4d0f74eba0d8b059"
+  integrity sha512-zk7sgt970kzPks2Bf+dwT/PLzghLnsivb9CcxkvR8Mzr66Olr0Ofd8neSbglHJHaHa2MadfoSdNlKYAaafmWfA==
+  dependencies:
+    domelementtype "^2.2.0"
+
+domutils@2.0.0:
+  version "2.0.0"
+  resolved "https://registry.yarnpkg.com/domutils/-/domutils-2.0.0.tgz#15b8278e37bfa8468d157478c58c367718133c08"
+  integrity sha512-n5SelJ1axbO636c2yUtOGia/IcJtVtlhQbFiVDBZHKV5ReJO1ViX7sFEemtuyoAnBxk5meNSYgA8V4s0271efg==
+  dependencies:
+    dom-serializer "^0.2.1"
+    domelementtype "^2.0.1"
+    domhandler "^3.0.0"
+
+domutils@^2.0.0:
+  version "2.7.0"
+  resolved "https://registry.yarnpkg.com/domutils/-/domutils-2.7.0.tgz#8ebaf0c41ebafcf55b0b72ec31c56323712c5442"
+  integrity sha512-8eaHa17IwJUPAiB+SoTYBo5mCdeMgdcAoXJ59m6DT1vw+5iLS3gNoqYaRowaBKtGVrOF1Jz4yDTgYKLK2kvfJg==
+  dependencies:
+    dom-serializer "^1.0.1"
+    domelementtype "^2.2.0"
+    domhandler "^4.2.0"
+
 dot-prop@^5.0.0:
   version "5.2.0"
   resolved "https://registry.yarnpkg.com/dot-prop/-/dot-prop-5.2.0.tgz#c34ecc29556dc45f1f4c22697b6f4904e0cc4fcb"
   integrity sha512-uEUyaDKoSQ1M4Oq8l45hSE26SnTxL6snNnqvK/VWx5wJhmff5z0FUVJDKDanor/6w3kzE3i7XZOk+7wC0EXr1A==
   dependencies:
     is-obj "^2.0.0"
 
+duplexer@^0.1.1:
+  version "0.1.2"
+  resolved "https://registry.yarnpkg.com/duplexer/-/duplexer-0.1.2.tgz#3abe43aef3835f8ae077d136ddce0f276b0400e6"
+  integrity sha512-jtD6YG370ZCIi/9GTaJKQxWTZD045+4R4hTk/x1UyoqadyJ9x9CgSi1RlVDQF8U2sxLLSnFkCaMihqljHIWgMg==
+
 duplexify@^3.4.2, duplexify@^3.6.0:
   version "3.7.1"
   resolved "https://registry.yarnpkg.com/duplexify/-/duplexify-3.7.1.tgz#2a4df5317f6ccfd91f86d6fd25d8d8a103b88309"
   integrity sha512-07z8uv2wMyS51kKhD1KsdXJg5WQ6t93RneqRxUHnskXVtlYYkLqM0gqStQZ3pj073g687jPCHrqNfCzawLYh5g==
   dependencies:
     end-of-stream "^1.0.0"
     inherits "^2.0.1"
     readable-stream "^2.0.0"
     stream-shift "^1.0.0"
 
+ecdsa-sig-formatter@1.0.11:
+  version "1.0.11"
+  resolved "https://registry.yarnpkg.com/ecdsa-sig-formatter/-/ecdsa-sig-formatter-1.0.11.tgz#ae0f0fa2d85045ef14a817daa3ce9acd0489e5bf"
+  integrity sha512-nagl3RYrbNv6kQkeJIpt6NJZy8twLB/2vtz6yN9Z4vRKHN4/QZJIEbqohALSgwKdnksuY3k5Addp5lg8sVoVcQ==
+  dependencies:
+    safe-buffer "^5.0.1"
+
 ee-first@1.1.1:
   version "1.1.1"
   resolved "https://registry.yarnpkg.com/ee-first/-/ee-first-1.1.1.tgz#590c61156b0ae2f4f0255732a158b266bc56b21d"
   integrity sha1-WQxhFWsK4vTwJVcyoViyZrxWsh0=
 
-electron-to-chromium@^1.3.247:
-  version "1.3.266"
-  resolved "https://registry.yarnpkg.com/electron-to-chromium/-/electron-to-chromium-1.3.266.tgz#a33fb529c75f8d133e75ea7cbedb73a62f2158d2"
-  integrity sha512-UTuTZ4v8T0gLPHI7U75PXLQePWI65MTS3mckRrnLCkNljHvsutbYs+hn2Ua/RFul3Jt/L3Ht2rLP+dU/AlBfrQ==
+electron-to-chromium@^1.3.322:
+  version "1.3.785"
+  resolved "https://registry.yarnpkg.com/electron-to-chromium/-/electron-to-chromium-1.3.785.tgz#79f546c69a6be4f30913aaace361bc746f26df48"
+  integrity sha512-WmCgAeURsMFiyoJ646eUaJQ7GNfvMRLXo+GamUyKVNEM4MqTAsXyC0f38JEB4N3BtbD0tlAKozGP5E2T9K3YGg==
+
+electron-to-chromium@^1.3.723:
+  version "1.3.739"
+  resolved "https://registry.yarnpkg.com/electron-to-chromium/-/electron-to-chromium-1.3.739.tgz#f07756aa92cabd5a6eec6f491525a64fe62f98b9"
+  integrity sha512-+LPJVRsN7hGZ9EIUUiWCpO7l4E3qBYHNadazlucBfsXBbccDFNKUBAgzE68FnkWGJPwD/AfKhSzL+G+Iqb8A4A==
 
 elliptic@^6.0.0:
-  version "6.5.3"
-  resolved "https://registry.yarnpkg.com/elliptic/-/elliptic-6.5.3.tgz#cb59eb2efdaf73a0bd78ccd7015a62ad6e0f93d6"
-  integrity sha512-IMqzv5wNQf+E6aHeIqATs0tOLeOTwj1QKbRcS3jBbYkl5oLAserA8yJTT7/VyHUYG91PRmPyeQDObKLPpeS4dw==
+  version "6.5.4"
+  resolved "https://registry.yarnpkg.com/elliptic/-/elliptic-6.5.4.tgz#da37cebd31e79a1367e941b592ed1fbebd58abbb"
+  integrity sha512-iLhC6ULemrljPZb+QutR5TQGB+pdW6KGD5RSegS+8sorOZT+rdQFbsQFJgvN3eRqNALqJer4oQ16YvJHlU8hzQ==
   dependencies:
-    bn.js "^4.4.0"
-    brorand "^1.0.1"
+    bn.js "^4.11.9"
+    brorand "^1.1.0"
     hash.js "^1.0.0"
-    hmac-drbg "^1.0.0"
-    inherits "^2.0.1"
-    minimalistic-assert "^1.0.0"
-    minimalistic-crypto-utils "^1.0.0"
+    hmac-drbg "^1.0.1"
+    inherits "^2.0.4"
+    minimalistic-assert "^1.0.1"
+    minimalistic-crypto-utils "^1.0.1"
 
 emojis-list@^2.0.0:
   version "2.1.0"
   resolved "https://registry.yarnpkg.com/emojis-list/-/emojis-list-2.1.0.tgz#4daa4d9db00f9819880c79fa457ae5b09a1fd389"
   integrity sha1-TapNnbAPmBmIDHn6RXrlsJof04k=
 
+emojis-list@^3.0.0:
+  version "3.0.0"
+  resolved "https://registry.yarnpkg.com/emojis-list/-/emojis-list-3.0.0.tgz#5570662046ad29e2e916e71aae260abdff4f6a78"
+  integrity sha512-/kyM18EfinwXZbno9FyUGeFh87KC8HRQBQGildHZbEuRyWFOmv1U10o9BBp8XVZDVNNuQKyIGIu5ZYAAXJ0V2Q==
+
 encodeurl@~1.0.2:
   version "1.0.2"
   resolved "https://registry.yarnpkg.com/encodeurl/-/encodeurl-1.0.2.tgz#ad3ff4c86ec2d029322f5a02c3a9a606c95b3f59"
   integrity sha1-rT/0yG7C0CkyL1oCw6mmBslbP1k=
 
 end-of-stream@^1.0.0, end-of-stream@^1.1.0:
   version "1.4.3"
@@ -2021,73 +2692,124 @@
   resolved "https://registry.yarnpkg.com/enhanced-resolve/-/enhanced-resolve-4.1.0.tgz#41c7e0bfdfe74ac1ffe1e57ad6a5c6c9f3742a7f"
   integrity sha512-F/7vkyTtyc/llOIn8oWclcB25KdRaiPBpZYDgJHgh/UHtpgT2p2eldQgtQnLtUvfMKPKxbRaQM/hHkvLHt1Vng==
   dependencies:
     graceful-fs "^4.1.2"
     memory-fs "^0.4.0"
     tapable "^1.0.0"
 
+entities@^2.0.0:
+  version "2.2.0"
+  resolved "https://registry.yarnpkg.com/entities/-/entities-2.2.0.tgz#098dc90ebb83d8dffa089d55256b351d34c4da55"
+  integrity sha512-p92if5Nz619I0w+akJrLZH0MX0Pb5DX39XOwQTtXSdQQOaYH03S1uIQp4mhOZtAXrxq4ViO67YTiLBo2638o9A==
+
 env-paths@^2.2.0:
   version "2.2.0"
   resolved "https://registry.yarnpkg.com/env-paths/-/env-paths-2.2.0.tgz#cdca557dc009152917d6166e2febe1f039685e43"
   integrity sha512-6u0VYSCo/OW6IoD5WCLLy9JUGARbamfSavcNXry/eu8aHVFei6CD3Sw+VGX5alea1i9pgPHW0mbu6Xj0uBh7gA==
 
 errno@^0.1.3, errno@~0.1.7:
   version "0.1.7"
   resolved "https://registry.yarnpkg.com/errno/-/errno-0.1.7.tgz#4684d71779ad39af177e3f007996f7c67c852618"
   integrity sha512-MfrRBDWzIWifgq6tJj60gkAwtLNb6sQPlcFrSOflcP1aFmmruKQ2wRnze/8V6kgyz7H3FF8Npzv78mZ7XLLflg==
   dependencies:
     prr "~1.0.1"
 
-error-ex@^1.2.0:
+error-ex@^1.2.0, error-ex@^1.3.1:
   version "1.3.2"
   resolved "https://registry.yarnpkg.com/error-ex/-/error-ex-1.3.2.tgz#b4ac40648107fdcdcfae242f428bea8a14d4f1bf"
   integrity sha512-7dFHNmqeFSEt2ZBsCriorKnn3Z2pj+fd9kmI6QoWw4//DL+icEBfc0U7qJCisqrTsKTjw4fNFy2pW9OqStD84g==
   dependencies:
     is-arrayish "^0.2.1"
 
+es5-ext@^0.10.35, es5-ext@^0.10.50:
+  version "0.10.53"
+  resolved "https://registry.yarnpkg.com/es5-ext/-/es5-ext-0.10.53.tgz#93c5a3acfdbef275220ad72644ad02ee18368de1"
+  integrity sha512-Xs2Stw6NiNHWypzRTY1MtaG/uJlwCk8kH81920ma8mvN8Xq1gsfhZvpkImLQArw8AHnv8MT2I45J3c0R8slE+Q==
+  dependencies:
+    es6-iterator "~2.0.3"
+    es6-symbol "~3.1.3"
+    next-tick "~1.0.0"
+
+es6-iterator@2.0.3, es6-iterator@~2.0.3:
+  version "2.0.3"
+  resolved "https://registry.yarnpkg.com/es6-iterator/-/es6-iterator-2.0.3.tgz#a7de889141a05a94b0854403b2d0a0fbfa98f3b7"
+  integrity sha1-p96IkUGgWpSwhUQDstCg+/qY87c=
+  dependencies:
+    d "1"
+    es5-ext "^0.10.35"
+    es6-symbol "^3.1.1"
+
+es6-symbol@^3.1.1, es6-symbol@~3.1.3:
+  version "3.1.3"
+  resolved "https://registry.yarnpkg.com/es6-symbol/-/es6-symbol-3.1.3.tgz#bad5d3c1bcdac28269f4cb331e431c78ac705d18"
+  integrity sha512-NJ6Yn3FuDinBaBRWl/q5X/s4koRHBrgKAu+yGI6JCBeiu3qrcbJhwT2GeR/EXVfylRk8dpQVJoLEFhK+Mu31NA==
+  dependencies:
+    d "^1.0.1"
+    ext "^1.1.2"
+
+escalade@^3.1.1:
+  version "3.1.1"
+  resolved "https://registry.yarnpkg.com/escalade/-/escalade-3.1.1.tgz#d8cfdc7000965c5a0174b4a82eaa5c0552742e40"
+  integrity sha512-k0er2gUkLf8O0zKJiAhmkTnJlTvINGv7ygDNPbeIsX/TJjGJZHuh9B2UxbsaEkmlEo9MfhrSzmhIlhRlI2GXnw==
+
 escape-html@~1.0.3:
   version "1.0.3"
   resolved "https://registry.yarnpkg.com/escape-html/-/escape-html-1.0.3.tgz#0258eae4d3d0c0974de1c169188ef0051d1d1988"
   integrity sha1-Aljq5NPQwJdN4cFpGI7wBR0dGYg=
 
+escape-string-regexp@2.0.0:
+  version "2.0.0"
+  resolved "https://registry.yarnpkg.com/escape-string-regexp/-/escape-string-regexp-2.0.0.tgz#a30304e99daa32e23b2fd20f51babd07cffca344"
+  integrity sha512-UpzcLCXolUWcNu5HtVMHYdXJjArjsF9C0aNnquZYY4uW/Vu0miy5YoWvbV345HauVvcAUnpRuhMMcqTcGOY2+w==
+
 escape-string-regexp@^1.0.2, escape-string-regexp@^1.0.5:
   version "1.0.5"
   resolved "https://registry.yarnpkg.com/escape-string-regexp/-/escape-string-regexp-1.0.5.tgz#1b61c0562190a8dff6ae3bb2cf0200ca130b86d4"
   integrity sha1-G2HAViGQqN/2rjuyzwIAyhMLhtQ=
 
 eslint-scope@^4.0.3:
   version "4.0.3"
   resolved "https://registry.yarnpkg.com/eslint-scope/-/eslint-scope-4.0.3.tgz#ca03833310f6889a3264781aa82e63eb9cfe7848"
   integrity sha512-p7VutNr1O/QrxysMo3E45FjYDTeXBy0iTltPFNSqKAIfjDSXC+4dj+qfyuD8bfAXrW/y6lW3O76VaYNPKfpKrg==
   dependencies:
     esrecurse "^4.1.0"
     estraverse "^4.1.1"
 
+esprima@^4.0.0, esprima@~4.0.0:
+  version "4.0.1"
+  resolved "https://registry.yarnpkg.com/esprima/-/esprima-4.0.1.tgz#13b04cdb3e6c5d19df91ab6987a8695619b0aa71"
+  integrity sha512-eGuFFw7Upda+g4p+QHvnW0RyTX/SVeJBDM/gCtMARO0cLuT2HcEKnTPvhjV6aGeqrCB/sbNop0Kszm0jsaWU4A==
+
 esrecurse@^4.1.0:
   version "4.2.1"
   resolved "https://registry.yarnpkg.com/esrecurse/-/esrecurse-4.2.1.tgz#007a3b9fdbc2b3bb87e4879ea19c92fdbd3942cf"
   integrity sha512-64RBB++fIOAXPw3P9cy89qfMlvZEXZkqqJkjqqXIvzP5ezRZjW+lPWjw35UX/3EhUPFYbg5ER4JYgDw4007/DQ==
   dependencies:
     estraverse "^4.1.0"
 
 estraverse@^4.1.0, estraverse@^4.1.1:
   version "4.3.0"
   resolved "https://registry.yarnpkg.com/estraverse/-/estraverse-4.3.0.tgz#398ad3f3c5a24948be7725e83d11a7de28cdbd1d"
   integrity sha512-39nnKffWz8xN1BU/2c79n9nB9HDzo0niYUqx6xyqUnyoAnQyyWpOTdZEeiCch8BBu515t4wp9ZmgVfVhn9EBpw==
 
-esutils@^2.0.0, esutils@^2.0.2:
+esutils@^2.0.2:
   version "2.0.3"
   resolved "https://registry.yarnpkg.com/esutils/-/esutils-2.0.3.tgz#74d2eb4de0b8da1293711910d50775b9b710ef64"
   integrity sha512-kVscqXk4OCp68SZ0dkgEKVi6/8ij300KBWTJq32P/dYeWTSwK41WyTxalN1eRmA5Z9UU/LX9D7FWSmV9SAYx6g==
 
 etag@1.8.1, etag@~1.8.1:
   version "1.8.1"
   resolved "https://registry.yarnpkg.com/etag/-/etag-1.8.1.tgz#41ae2eeb65efa62268aebfea83ac7d79299b0887"
   integrity sha1-Qa4u62XvpiJorr/qg6x9eSmbCIc=
 
+eventemitter3@^4.0.0:
+  version "4.0.7"
+  resolved "https://registry.yarnpkg.com/eventemitter3/-/eventemitter3-4.0.7.tgz#2de9b68f6528d5644ef5c59526a1b4a07306169f"
+  integrity sha512-8guHBZCwKnFhYdHr2ysuRWErTwhoN2X8XELRlrRwpmfeY2jjuUN4taQMsULKUVo1K4DvZl+0pgfyoysHxvmvEw==
+
 events@^3.0.0:
   version "3.0.0"
   resolved "https://registry.yarnpkg.com/events/-/events-3.0.0.tgz#9a0a0dfaf62893d92b875b8f2698ca4114973e88"
   integrity sha512-Dc381HFWJzEOhQ+d8pkNon++bk9h6cdAoAj4iE6Q4y6xgTzySWXlKn05/TVNpjnfRqi/X0EpJEJohPjNI3zpVA==
 
 evp_bytestokey@^1.0.0, evp_bytestokey@^1.0.3:
   version "1.0.3"
@@ -2106,14 +2828,21 @@
     define-property "^0.2.5"
     extend-shallow "^2.0.1"
     posix-character-classes "^0.1.0"
     regex-not "^1.0.0"
     snapdragon "^0.8.1"
     to-regex "^3.0.1"
 
+ext@^1.1.2:
+  version "1.4.0"
+  resolved "https://registry.yarnpkg.com/ext/-/ext-1.4.0.tgz#89ae7a07158f79d35517882904324077e4379244"
+  integrity sha512-Key5NIsUxdqKg3vIsdw9dSuXpPCQ297y6wBjL30edxwPgt2E44WcWBZey/ZvUc6sERLTxKdyCu4gZFmUbk1Q7A==
+  dependencies:
+    type "^2.0.0"
+
 extend-shallow@^2.0.1:
   version "2.0.1"
   resolved "https://registry.yarnpkg.com/extend-shallow/-/extend-shallow-2.0.1.tgz#51af7d614ad9a9f610ea1bafbb989d6b1c56890f"
   integrity sha1-Ua99YUrZqfYQ6huvu5idaxxWiQ8=
   dependencies:
     is-extendable "^0.1.0"
 
@@ -2140,34 +2869,59 @@
     to-regex "^3.0.1"
 
 fast-deep-equal@^2.0.1:
   version "2.0.1"
   resolved "https://registry.yarnpkg.com/fast-deep-equal/-/fast-deep-equal-2.0.1.tgz#7b05218ddf9667bf7f370bf7fdb2cb15fdd0aa49"
   integrity sha1-ewUhjd+WZ79/Nwv3/bLLFf3Qqkk=
 
+fast-deep-equal@^3.1.1:
+  version "3.1.3"
+  resolved "https://registry.yarnpkg.com/fast-deep-equal/-/fast-deep-equal-3.1.3.tgz#3a7d56b559d6cbc3eb512325244e619a65c6c525"
+  integrity sha512-f3qQ9oQy9j2AhBe/H9VC91wLmKBCCU/gDOnKNAYG5hswO7BLKj09Hc5HYNz9cGI++xlpDCIgDaitVs03ATR84Q==
+
 fast-json-stable-stringify@^2.0.0:
   version "2.0.0"
   resolved "https://registry.yarnpkg.com/fast-json-stable-stringify/-/fast-json-stable-stringify-2.0.0.tgz#d5142c0caee6b1189f87d3a76111064f86c8bbf2"
   integrity sha1-1RQsDK7msRifh9OnYREGT4bIu/I=
 
 figgy-pudding@^3.5.1:
-  version "3.5.1"
-  resolved "https://registry.yarnpkg.com/figgy-pudding/-/figgy-pudding-3.5.1.tgz#862470112901c727a0e495a80744bd5baa1d6790"
-  integrity sha512-vNKxJHTEKNThjfrdJwHc7brvM6eVevuO5nTj6ez8ZQ1qbXTvGthucRF7S4vf2cr71QVnT70V34v0S1DyQsti0w==
+  version "3.5.2"
+  resolved "https://registry.yarnpkg.com/figgy-pudding/-/figgy-pudding-3.5.2.tgz#b4eee8148abb01dcf1d1ac34367d59e12fa61d6e"
+  integrity sha512-0btnI/H8f2pavGMN8w40mlSKOfTK2SVJmBfBeVIj3kNw0swwgzyRq0d5TJVOwodFmtvpPeWPN/MCcfuWF0Ezbw==
+
+file-loader@4.2.0:
+  version "4.2.0"
+  resolved "https://registry.yarnpkg.com/file-loader/-/file-loader-4.2.0.tgz#5fb124d2369d7075d70a9a5abecd12e60a95215e"
+  integrity sha512-+xZnaK5R8kBJrHK0/6HRlrKNamvVS5rjyuju+rnyxRGuwUJwpAMsVzUl5dz6rK8brkzjV6JpcFNjp6NqV0g1OQ==
+  dependencies:
+    loader-utils "^1.2.3"
+    schema-utils "^2.0.0"
 
 fill-range@^4.0.0:
   version "4.0.0"
   resolved "https://registry.yarnpkg.com/fill-range/-/fill-range-4.0.0.tgz#d544811d428f98eb06a63dc402d2403c328c38f7"
   integrity sha1-1USBHUKPmOsGpj3EAtJAPDKMOPc=
   dependencies:
     extend-shallow "^2.0.1"
     is-number "^3.0.0"
     repeat-string "^1.6.1"
     to-regex-range "^2.1.0"
 
+fill-range@^7.0.1:
+  version "7.0.1"
+  resolved "https://registry.yarnpkg.com/fill-range/-/fill-range-7.0.1.tgz#1919a6a7c75fe38b2c7c77e5198535da9acdda40"
+  integrity sha512-qOo9F+dMUmC2Lcb4BbVvnKJxTPjCm+RRpe4gDuGrzkL7mEVl/djYSu2OdQ2Pa302N4oqkSg9ir6jaLWJ2USVpQ==
+  dependencies:
+    to-regex-range "^5.0.1"
+
+finally-polyfill@0.1.0:
+  version "0.1.0"
+  resolved "https://registry.yarnpkg.com/finally-polyfill/-/finally-polyfill-0.1.0.tgz#2a17b16581d9477db16a703c7b79a898ac0b7d50"
+  integrity sha512-J1LEcZ5VXe1l3sEO+S//WqL5wcJ/ep7QeKJA6HhNZrcEEFj0eyC8IW3DEZhxySI2bx3r85dwAXz+vYPGuHx5UA==
+
 find-cache-dir@^1.0.0:
   version "1.0.0"
   resolved "https://registry.yarnpkg.com/find-cache-dir/-/find-cache-dir-1.0.0.tgz#9288e3e9e3cc3748717d39eade17cf71fc30ee6f"
   integrity sha1-kojj6ePMN0hxfTnq3hfPcfww7m8=
   dependencies:
     commondir "^1.0.1"
     make-dir "^1.0.0"
@@ -2178,14 +2932,23 @@
   resolved "https://registry.yarnpkg.com/find-cache-dir/-/find-cache-dir-2.1.0.tgz#8d0f94cd13fe43c6c7c261a0d86115ca918c05f7"
   integrity sha512-Tq6PixE0w/VMFfCgbONnkiQIVol/JJL7nRMi20fqzA4NRs9AfeqMGeRdPi3wIhYkxjeBaWh2rxwapn5Tu3IqOQ==
   dependencies:
     commondir "^1.0.1"
     make-dir "^2.0.0"
     pkg-dir "^3.0.0"
 
+find-cache-dir@^3.0.0:
+  version "3.3.1"
+  resolved "https://registry.yarnpkg.com/find-cache-dir/-/find-cache-dir-3.3.1.tgz#89b33fad4a4670daa94f855f7fbe31d6d84fe880"
+  integrity sha512-t2GDMt3oGC/v+BMwzmllWDuJF/xcDtE5j/fCGbqDD7OLuJkj0cfh1YSA5VKPvwMeLFLNDBkwOKZ2X85jGLVftQ==
+  dependencies:
+    commondir "^1.0.1"
+    make-dir "^3.0.2"
+    pkg-dir "^4.1.0"
+
 find-up@4.0.0:
   version "4.0.0"
   resolved "https://registry.yarnpkg.com/find-up/-/find-up-4.0.0.tgz#c367f8024de92efb75f2d4906536d24682065c3a"
   integrity sha512-zoH7ZWPkRdgwYCDVoQTzqjG8JSPANhtvLhh4KVUHyKnaUJJrNeFmWIkTcNuJmR3GLMEmGYEf2S2bjgx26JTF+Q==
   dependencies:
     locate-path "^5.0.0"
 
@@ -2199,35 +2962,53 @@
 find-up@^3.0.0:
   version "3.0.0"
   resolved "https://registry.yarnpkg.com/find-up/-/find-up-3.0.0.tgz#49169f1d7993430646da61ecc5ae355c21c97b73"
   integrity sha512-1yD6RmLI1XBfxugvORwlck6f75tYL+iR0jqwsOrOxMZyGYqUuDhJ0l4AXdO1iX/FTs9cBAMEk1gWSEx1kSbylg==
   dependencies:
     locate-path "^3.0.0"
 
+find-up@^4.0.0:
+  version "4.1.0"
+  resolved "https://registry.yarnpkg.com/find-up/-/find-up-4.1.0.tgz#97afe7d6cdc0bc5928584b7c8d7b16e8a9aa5d19"
+  integrity sha512-PpOwAdQ/YlXQ2vj8a3h8IipDuYRi3wceVQQGYWxNINccq40Anw7BlsEXCMbt1Zt+OLA6Fq9suIpIWD0OsnISlw==
+  dependencies:
+    locate-path "^5.0.0"
+    path-exists "^4.0.0"
+
+flatten@^1.0.2:
+  version "1.0.3"
+  resolved "https://registry.yarnpkg.com/flatten/-/flatten-1.0.3.tgz#c1283ac9f27b368abc1e36d1ff7b04501a30356b"
+  integrity sha512-dVsPA/UwQ8+2uoFe5GHtiBMu48dWLTdsuEd7CKGlZlD78r1TTWBvDuFaFGKCo/ZfEr95Uk56vZoX86OsHkUeIg==
+
 flush-write-stream@^1.0.0:
   version "1.1.1"
   resolved "https://registry.yarnpkg.com/flush-write-stream/-/flush-write-stream-1.1.1.tgz#8dd7d873a1babc207d94ead0c2e0e44276ebf2e8"
   integrity sha512-3Z4XhFZ3992uIq0XOqb9AreonueSYphE6oYbpt5+3u06JWklbsPkNv3ZKkP9Bz/r+1MWCaMoSQ28P85+1Yc77w==
   dependencies:
     inherits "^2.0.3"
     readable-stream "^2.3.6"
 
+follow-redirects@^1.0.0:
+  version "1.14.1"
+  resolved "https://registry.yarnpkg.com/follow-redirects/-/follow-redirects-1.14.1.tgz#d9114ded0a1cfdd334e164e6662ad02bfd91ff43"
+  integrity sha512-HWqDgT7ZEkqRzBvc2s64vSZ/hfOceEol3ac/7tKwzuvEyWx3/4UegXh5oBOIotkGsObyk3xznnSRVADBgWSQVg==
+
 for-in@^1.0.2:
   version "1.0.2"
   resolved "https://registry.yarnpkg.com/for-in/-/for-in-1.0.2.tgz#81068d295a8142ec0ac726c6e2200c30fb6d5e80"
   integrity sha1-gQaNKVqBQuwKxybG4iAMMPttXoA=
 
-fork-ts-checker-webpack-plugin@1.3.4:
-  version "1.3.4"
-  resolved "https://registry.yarnpkg.com/fork-ts-checker-webpack-plugin/-/fork-ts-checker-webpack-plugin-1.3.4.tgz#a75b6fe8d3db0089555f083c4f77372227704244"
-  integrity sha512-2QDXnI2mbbly/OHx/ivtspi2l4K2g+IB0LTQ3AwsBfxyHtMFXtojlsJqGyhUggX08BC+F02CoCG0hRSPOLU2dQ==
+fork-ts-checker-webpack-plugin@3.1.1:
+  version "3.1.1"
+  resolved "https://registry.yarnpkg.com/fork-ts-checker-webpack-plugin/-/fork-ts-checker-webpack-plugin-3.1.1.tgz#a1642c0d3e65f50c2cc1742e9c0a80f441f86b19"
+  integrity sha512-DuVkPNrM12jR41KM2e+N+styka0EgLkTnXmNcXdgOM37vtGeY+oCBK/Jx0hzSeEU6memFCtWb4htrHPMDfwwUQ==
   dependencies:
     babel-code-frame "^6.22.0"
     chalk "^2.4.1"
-    chokidar "^2.0.4"
+    chokidar "^3.3.0"
     micromatch "^3.1.10"
     minimatch "^3.0.4"
     semver "^5.6.0"
     tapable "^1.0.0"
     worker-rpc "^0.1.0"
 
 fragment-cache@^0.2.1:
@@ -2276,14 +3057,19 @@
   version "1.2.9"
   resolved "https://registry.yarnpkg.com/fsevents/-/fsevents-1.2.9.tgz#3f5ed66583ccd6f400b5a00db6f7e861363e388f"
   integrity sha512-oeyj2H3EjjonWcFjD5NvZNE9Rqe4UW+nQBU2HNeKw0koVLEFIhtyETyAakeAM3de7Z/SW5kcA+fZUait9EApnw==
   dependencies:
     nan "^2.12.1"
     node-pre-gyp "^0.12.0"
 
+fsevents@~2.3.2:
+  version "2.3.2"
+  resolved "https://registry.yarnpkg.com/fsevents/-/fsevents-2.3.2.tgz#8a526f78b8fdf4623b709e0b975c52c24c02fd1a"
+  integrity sha512-xiqMQR4xAeHTuB9uWm+fFRcIOgKBMiOBP+eXiyT7jsgVCq1bkVygt00oASowB7EdtpOHaaPgKt812P9ab+DDKA==
+
 function-bind@^1.1.1:
   version "1.1.1"
   resolved "https://registry.yarnpkg.com/function-bind/-/function-bind-1.1.1.tgz#a56899d3ea3c9bab874bb9773b7c5ede92f4895d"
   integrity sha512-yIovAzMX49sF8Yl58fSCWJ5svSLuaibPxXQJFLmBObTuCr0Mf1KiPopGM9NiFjiYBCbfaa2Fh6breQ6ANVTI0A==
 
 gauge@~2.7.3:
   version "2.7.4"
@@ -2308,14 +3094,21 @@
   version "3.1.0"
   resolved "https://registry.yarnpkg.com/glob-parent/-/glob-parent-3.1.0.tgz#9e6af6299d8d3bd2bd40430832bd113df906c5ae"
   integrity sha1-nmr2KZ2NO9K9QEMIMr0RPfkGxa4=
   dependencies:
     is-glob "^3.1.0"
     path-dirname "^1.0.0"
 
+glob-parent@~5.1.2:
+  version "5.1.2"
+  resolved "https://registry.yarnpkg.com/glob-parent/-/glob-parent-5.1.2.tgz#869832c58034fe68a4093c17dc15e8340d8401c4"
+  integrity sha512-AOIgSQCepiJYwP3ARnGx+5VnTu2HBYdzbGP45eLw1vr3zB3vZLeyed1sC9hnbcOc9/SrMyM5RPQrkGz4aS9Zow==
+  dependencies:
+    is-glob "^4.0.1"
+
 glob-to-regexp@^0.4.1:
   version "0.4.1"
   resolved "https://registry.yarnpkg.com/glob-to-regexp/-/glob-to-regexp-0.4.1.tgz#c75297087c851b9a578bd217dd59a92f59fe546e"
   integrity sha512-lkX1HJXwyMcprw/5YUZc2s7DrpAiHB21/V+E1rHUrVNokkvB6bqMzT0VfV6/86ZNabt1k14YOIaT7nDvOX3Iiw==
 
 glob@^7.0.3, glob@^7.1.3, glob@^7.1.4:
   version "7.1.4"
@@ -2346,18 +3139,21 @@
     pinkie-promise "^2.0.0"
 
 graceful-fs@^4.1.11, graceful-fs@^4.1.15, graceful-fs@^4.1.2:
   version "4.2.2"
   resolved "https://registry.yarnpkg.com/graceful-fs/-/graceful-fs-4.2.2.tgz#6f0952605d0140c1cfdb138ed005775b92d67b02"
   integrity sha512-IItsdsea19BoLC7ELy13q1iJFNmd7ofZH5+X/pJr90/nRoPEX0DJo1dHDbgtYWOhJhcCgMDTOw84RZ72q6lB+Q==
 
-"graceful-readlink@>= 1.0.0":
-  version "1.0.1"
-  resolved "https://registry.yarnpkg.com/graceful-readlink/-/graceful-readlink-1.0.1.tgz#4cafad76bc62f02fa039b2f94e9a3dd3a391a725"
-  integrity sha1-TK+tdrxi8C+gObL5Tpo906ORpyU=
+gzip-size@5.1.1:
+  version "5.1.1"
+  resolved "https://registry.yarnpkg.com/gzip-size/-/gzip-size-5.1.1.tgz#cb9bee692f87c0612b232840a873904e4c135274"
+  integrity sha512-FNHi6mmoHvs1mxZAds4PpdCS6QG8B4C1krxJsMutgxl5t3+GlRTzzI3NEkifXx2pVsOvJdOGSmIgDhQ55FwdPA==
+  dependencies:
+    duplexer "^0.1.1"
+    pify "^4.0.1"
 
 has-ansi@^2.0.0:
   version "2.0.0"
   resolved "https://registry.yarnpkg.com/has-ansi/-/has-ansi-2.0.0.tgz#34f5049ce1ecdf2b0649af3ef24e45ed35416d91"
   integrity sha1-NPUEnOHs3ysGSa8+8k5F7TVBbZE=
   dependencies:
     ansi-regex "^2.0.0"
@@ -2427,33 +3223,43 @@
   version "1.1.7"
   resolved "https://registry.yarnpkg.com/hash.js/-/hash.js-1.1.7.tgz#0babca538e8d4ee4a0f8988d68866537a003cf42"
   integrity sha512-taOaskGt4z4SOANNseOviYDvjEJinIkRgmp7LbKP2YTTmVxWBl87s/uzK9r+44BclBSp2X7K1hqeNfz9JbBeXA==
   dependencies:
     inherits "^2.0.3"
     minimalistic-assert "^1.0.1"
 
-hmac-drbg@^1.0.0:
+hmac-drbg@^1.0.1:
   version "1.0.1"
   resolved "https://registry.yarnpkg.com/hmac-drbg/-/hmac-drbg-1.0.1.tgz#d2745701025a6c775a6c545793ed502fc0c649a1"
   integrity sha1-0nRXAQJabHdabFRXk+1QL8DGSaE=
   dependencies:
     hash.js "^1.0.3"
     minimalistic-assert "^1.0.0"
     minimalistic-crypto-utils "^1.0.1"
 
 hosted-git-info@^2.1.4:
-  version "2.8.4"
-  resolved "https://registry.yarnpkg.com/hosted-git-info/-/hosted-git-info-2.8.4.tgz#44119abaf4bc64692a16ace34700fed9c03e2546"
-  integrity sha512-pzXIvANXEFrc5oFFXRMkbLPQ2rXRoDERwDLyrcUxGhaZhgP54BBSl9Oheh7Vv0T090cszWBxPjkQQ5Sq1PbBRQ==
+  version "2.8.9"
+  resolved "https://registry.yarnpkg.com/hosted-git-info/-/hosted-git-info-2.8.9.tgz#dffc0bf9a21c02209090f2aa69429e1414daf3f9"
+  integrity sha512-mxIDAb9Lsm6DoOJ7xH+5+X4y1LU/4Hi50L9C5sIswK3JzULS4bwk1FvjdBgvYR4bzT4tuUQiC15FE2f5HbLvYw==
 
 html-entities@^1.2.0:
   version "1.2.1"
   resolved "https://registry.yarnpkg.com/html-entities/-/html-entities-1.2.1.tgz#0df29351f0721163515dfb9e5543e5f6eed5162f"
   integrity sha1-DfKTUfByEWNRXfueVUPl9u7VFi8=
 
+htmlparser2@4.1.0:
+  version "4.1.0"
+  resolved "https://registry.yarnpkg.com/htmlparser2/-/htmlparser2-4.1.0.tgz#9a4ef161f2e4625ebf7dfbe6c0a2f52d18a59e78"
+  integrity sha512-4zDq1a1zhE4gQso/c5LP1OtrhYTncXNSpvJYtWJBtXAETPlMfi3IFNjGuQbYLuVY4ZR0QMqRVvo4Pdy9KLyP8Q==
+  dependencies:
+    domelementtype "^2.0.1"
+    domhandler "^3.0.0"
+    domutils "^2.0.0"
+    entities "^2.0.0"
+
 http-errors@1.7.2:
   version "1.7.2"
   resolved "https://registry.yarnpkg.com/http-errors/-/http-errors-1.7.2.tgz#4f5029cf13239f31036e5b2e55292bcfbcc85c8f"
   integrity sha512-uUQBt3H/cSIVfch6i1EuPNy/YsRSOUBXTVfZ+yR7Zjez3qjBz6i9+i4zjNaoqcoFVI4lQJ5plg63TvGfRSDCRg==
   dependencies:
     depd "~1.1.2"
     inherits "2.0.3"
@@ -2468,62 +3274,110 @@
   dependencies:
     depd "~1.1.2"
     inherits "2.0.4"
     setprototypeof "1.1.1"
     statuses ">= 1.5.0 < 2"
     toidentifier "1.0.0"
 
+http-proxy@1.18.0:
+  version "1.18.0"
+  resolved "https://registry.yarnpkg.com/http-proxy/-/http-proxy-1.18.0.tgz#dbe55f63e75a347db7f3d99974f2692a314a6a3a"
+  integrity sha512-84I2iJM/n1d4Hdgc6y2+qY5mDaz2PUVjlg9znE9byl+q0uC3DeByqBGReQu5tpLK0TAqTIXScRUV+dg7+bUPpQ==
+  dependencies:
+    eventemitter3 "^4.0.0"
+    follow-redirects "^1.0.0"
+    requires-port "^1.0.0"
+
 https-browserify@^1.0.0:
   version "1.0.0"
   resolved "https://registry.yarnpkg.com/https-browserify/-/https-browserify-1.0.0.tgz#ec06c10e0a34c0f2faf199f7fd7fc78fffd03c73"
   integrity sha1-7AbBDgo0wPL68Zn3/X/Hj//QPHM=
 
 iconv-lite@0.4.24, iconv-lite@^0.4.4:
   version "0.4.24"
   resolved "https://registry.yarnpkg.com/iconv-lite/-/iconv-lite-0.4.24.tgz#2022b4b25fbddc21d2f524974a474aafe733908b"
   integrity sha512-v3MXnZAcvnywkTUEZomIActle7RXXeedOR31wwl7VlyoXO4Qi9arvSenNQWne1TcRwhCL1HwLI21bEqdpj8/rA==
   dependencies:
     safer-buffer ">= 2.1.2 < 3"
 
+icss-utils@^4.0.0, icss-utils@^4.1.1:
+  version "4.1.1"
+  resolved "https://registry.yarnpkg.com/icss-utils/-/icss-utils-4.1.1.tgz#21170b53789ee27447c2f47dd683081403f9a467"
+  integrity sha512-4aFq7wvWyMHKgxsH8QQtGpvbASCf+eM3wPRLI6R+MgAnTCZ6STYsRvttLvRWK0Nfif5piF394St3HeJDaljGPA==
+  dependencies:
+    postcss "^7.0.14"
+
 ieee754@^1.1.4:
   version "1.1.13"
   resolved "https://registry.yarnpkg.com/ieee754/-/ieee754-1.1.13.tgz#ec168558e95aa181fd87d37f55c32bbcb6708b84"
   integrity sha512-4vf7I2LYV/HaWerSo3XmlMkp5eZ83i+/CDluXi/IGTs/O1sejBNhTtnxzmRZfvOUqj7lZjqHkeTvpgSFDlWZTg==
 
 iferr@^0.1.5:
   version "0.1.5"
   resolved "https://registry.yarnpkg.com/iferr/-/iferr-0.1.5.tgz#c60eed69e6d8fdb6b3104a1fcbca1c192dc5b501"
   integrity sha1-xg7taebY/bazEEofy8ocGS3FtQE=
 
+ignore-loader@0.1.2:
+  version "0.1.2"
+  resolved "https://registry.yarnpkg.com/ignore-loader/-/ignore-loader-0.1.2.tgz#d81f240376d0ba4f0d778972c3ad25874117a463"
+  integrity sha1-2B8kA3bQuk8Nd4lyw60lh0EXpGM=
+
 ignore-walk@^3.0.1:
   version "3.0.2"
   resolved "https://registry.yarnpkg.com/ignore-walk/-/ignore-walk-3.0.2.tgz#99d83a246c196ea5c93ef9315ad7b0819c35069b"
   integrity sha512-EXyErtpHbn75ZTsOADsfx6J/FPo6/5cjev46PXrcTpd8z3BoRkXgYu9/JVqrI7tusjmwCZutGeRJeU0Wo1e4Cw==
   dependencies:
     minimatch "^3.0.4"
 
+import-cwd@^2.0.0:
+  version "2.1.0"
+  resolved "https://registry.yarnpkg.com/import-cwd/-/import-cwd-2.1.0.tgz#aa6cf36e722761285cb371ec6519f53e2435b0a9"
+  integrity sha1-qmzzbnInYShcs3HsZRn1PiQ1sKk=
+  dependencies:
+    import-from "^2.1.0"
+
+import-fresh@^2.0.0:
+  version "2.0.0"
+  resolved "https://registry.yarnpkg.com/import-fresh/-/import-fresh-2.0.0.tgz#d81355c15612d386c61f9ddd3922d4304822a546"
+  integrity sha1-2BNVwVYS04bGH53dOSLUMEgipUY=
+  dependencies:
+    caller-path "^2.0.0"
+    resolve-from "^3.0.0"
+
+import-from@^2.1.0:
+  version "2.1.0"
+  resolved "https://registry.yarnpkg.com/import-from/-/import-from-2.1.0.tgz#335db7f2a7affd53aaa471d4b8021dee36b7f3b1"
+  integrity sha1-M1238qev/VOqpHHUuAId7ja387E=
+  dependencies:
+    resolve-from "^3.0.0"
+
 imurmurhash@^0.1.4:
   version "0.1.4"
   resolved "https://registry.yarnpkg.com/imurmurhash/-/imurmurhash-0.1.4.tgz#9218b9b2b928a238b13dc4fb6b6d576f231453ea"
   integrity sha1-khi5srkoojixPcT7a21XbyMUU+o=
 
+indexes-of@^1.0.1:
+  version "1.0.1"
+  resolved "https://registry.yarnpkg.com/indexes-of/-/indexes-of-1.0.1.tgz#f30f716c8e2bd346c7b67d3df3915566a7c05607"
+  integrity sha1-8w9xbI4r00bHtn0985FVZqfAVgc=
+
 infer-owner@^1.0.3:
   version "1.0.4"
   resolved "https://registry.yarnpkg.com/infer-owner/-/infer-owner-1.0.4.tgz#c4cefcaa8e51051c2a40ba2ce8a3d27295af9467"
   integrity sha512-IClj+Xz94+d7irH5qRyfJonOdfTzuDaifE6ZPWfx0N0+/ATZCbuTPq2prFl526urkQd90WyUKIh1DfBQ2hMz9A==
 
 inflight@^1.0.4:
   version "1.0.6"
   resolved "https://registry.yarnpkg.com/inflight/-/inflight-1.0.6.tgz#49bd6331d7d02d0c09bc910a1075ba8165b56df9"
   integrity sha1-Sb1jMdfQLQwJvJEKEHW6gWW1bfk=
   dependencies:
     once "^1.3.0"
     wrappy "1"
 
-inherits@2, inherits@2.0.4, inherits@^2.0.1, inherits@^2.0.3, inherits@~2.0.1, inherits@~2.0.3:
+inherits@2, inherits@2.0.4, inherits@^2.0.1, inherits@^2.0.3, inherits@^2.0.4, inherits@~2.0.1, inherits@~2.0.3:
   version "2.0.4"
   resolved "https://registry.yarnpkg.com/inherits/-/inherits-2.0.4.tgz#0fa2c64f932917c3433a0ded55363aae37416b7c"
   integrity sha512-k/vGaX4/Yla3WzyMCvTQOXYeIHvqOKtnqBduzTHpzpQZzAskKMhZ2K+EnBiSM9zGSoIFeMpXKxa4dYeZIQqewQ==
 
 inherits@2.0.1:
   version "2.0.1"
   resolved "https://registry.yarnpkg.com/inherits/-/inherits-2.0.1.tgz#b17d08d326b4423e568eff719f91b0b1cbdf69f1"
@@ -2568,14 +3422,21 @@
 is-binary-path@^1.0.0:
   version "1.0.1"
   resolved "https://registry.yarnpkg.com/is-binary-path/-/is-binary-path-1.0.1.tgz#75f16642b480f187a711c814161fd3a4a7655898"
   integrity sha1-dfFmQrSA8YenEcgUFh/TpKdlWJg=
   dependencies:
     binary-extensions "^1.0.0"
 
+is-binary-path@~2.1.0:
+  version "2.1.0"
+  resolved "https://registry.yarnpkg.com/is-binary-path/-/is-binary-path-2.1.0.tgz#ea1f7f3b80f064236e83470f86c09c254fb45b09"
+  integrity sha512-ZMERYes6pDydyuGidse7OsHxtbI7WVeUEozgR/g7rd0xUimYNlvZRE/K2MgZTjWy725IfelLeVcEM97mmtRGXw==
+  dependencies:
+    binary-extensions "^2.0.0"
+
 is-buffer@^1.1.5:
   version "1.1.6"
   resolved "https://registry.yarnpkg.com/is-buffer/-/is-buffer-1.1.6.tgz#efaa2ea9daa0d7ab2ea13a97b2b8ad51fefbe8be"
   integrity sha512-NcdALwpXkTm5Zvvbk7owOUSvVvBKDgKP5/ewfXEznmQFfs4ZRmanOeKBTjRVjka3QFoN6XJ+9F3USqfHqTaU5w==
 
 is-data-descriptor@^0.1.4:
   version "0.1.4"
@@ -2605,14 +3466,19 @@
   resolved "https://registry.yarnpkg.com/is-descriptor/-/is-descriptor-1.0.2.tgz#3b159746a66604b04f8c81524ba365c5f14d86ec"
   integrity sha512-2eis5WqQGV7peooDyLmNEPUrps9+SXX5c9pL3xEB+4e9HnGuDa7mB7kHxHw4CbqS9k1T2hOH3miL8n8WtiYVtg==
   dependencies:
     is-accessor-descriptor "^1.0.0"
     is-data-descriptor "^1.0.0"
     kind-of "^6.0.2"
 
+is-directory@^0.3.1:
+  version "0.3.1"
+  resolved "https://registry.yarnpkg.com/is-directory/-/is-directory-0.3.1.tgz#61339b6f2475fc772fd9c9d83f5c8575dc154ae1"
+  integrity sha1-YTObbyR1/Hcv2cnYP1yFddwVSuE=
+
 is-docker@2.0.0:
   version "2.0.0"
   resolved "https://registry.yarnpkg.com/is-docker/-/is-docker-2.0.0.tgz#2cb0df0e75e2d064fe1864c37cdeacb7b2dcf25b"
   integrity sha512-pJEdRugimx4fBMra5z2/5iRdZ63OhYV0vr0Dwm5+xtW4D1FvRkB8hamMIhnWfyJeDdyr/aa7BDyNbtG38VxgoQ==
 
 is-extendable@^0.1.0, is-extendable@^0.1.1:
   version "0.1.1"
@@ -2646,28 +3512,33 @@
 is-glob@^3.1.0:
   version "3.1.0"
   resolved "https://registry.yarnpkg.com/is-glob/-/is-glob-3.1.0.tgz#7ba5ae24217804ac70707b96922567486cc3e84a"
   integrity sha1-e6WuJCF4BKxwcHuWkiVnSGzD6Eo=
   dependencies:
     is-extglob "^2.1.0"
 
-is-glob@^4.0.0:
+is-glob@^4.0.0, is-glob@^4.0.1, is-glob@~4.0.1:
   version "4.0.1"
   resolved "https://registry.yarnpkg.com/is-glob/-/is-glob-4.0.1.tgz#7567dbe9f2f5e2467bc77ab83c4a29482407a5dc"
   integrity sha512-5G0tKtBTFImOqDnLB2hG6Bp2qcKEFduo4tZu9MT/H6NQv/ghhy30o55ufafxJ/LdH79LLs2Kfrn85TLKyA7BUg==
   dependencies:
     is-extglob "^2.1.1"
 
 is-number@^3.0.0:
   version "3.0.0"
   resolved "https://registry.yarnpkg.com/is-number/-/is-number-3.0.0.tgz#24fd6201a4782cf50561c810276afc7d12d71195"
   integrity sha1-JP1iAaR4LPUFYcgQJ2r8fRLXEZU=
   dependencies:
     kind-of "^3.0.2"
 
+is-number@^7.0.0:
+  version "7.0.0"
+  resolved "https://registry.yarnpkg.com/is-number/-/is-number-7.0.0.tgz#7535345b896734d5f80c4d06c50955527a14f12b"
+  integrity sha512-41Cifkg6e8TylSpdtTpeLVMqvSBEVzTttHvERD741+pnZ8ANv0004MRL43QKPDlK9cGvNp6NZWZUBlbGXYxxng==
+
 is-obj@^2.0.0:
   version "2.0.0"
   resolved "https://registry.yarnpkg.com/is-obj/-/is-obj-2.0.0.tgz#473fb05d973705e3fd9620545018ca8e22ef4982"
   integrity sha512-drqDG3cbczxxEJRoOXcOjtdp1J/lyp1mNn0xaznRs8+muBhgQcrnbspox5X5fOw0HnMnbfDzvnEMEtqDEJEo8w==
 
 is-path-cwd@^1.0.0:
   version "1.0.0"
@@ -2684,14 +3555,19 @@
 is-path-inside@^1.0.0:
   version "1.0.1"
   resolved "https://registry.yarnpkg.com/is-path-inside/-/is-path-inside-1.0.1.tgz#8ef5b7de50437a3fdca6b4e865ef7aa55cb48036"
   integrity sha1-jvW33lBDej/cprToZe96pVy0gDY=
   dependencies:
     path-is-inside "^1.0.1"
 
+is-plain-obj@^1.0.0:
+  version "1.1.0"
+  resolved "https://registry.yarnpkg.com/is-plain-obj/-/is-plain-obj-1.1.0.tgz#71a50c8429dfca773c92a390a4a03b39fcd51d3e"
+  integrity sha1-caUMhCnfync8kqOQpKA7OfzVHT4=
+
 is-plain-object@^2.0.3, is-plain-object@^2.0.4:
   version "2.0.4"
   resolved "https://registry.yarnpkg.com/is-plain-object/-/is-plain-object-2.0.4.tgz#2c163b3fafb1b606d9d17928f05c2a1c38e07677"
   integrity sha512-h5PpgXkWitc38BBMYawTYMWJHFZJVnBquFE57xFpjB8pJFiF6gZ+bU+WyI/yqXiFR5mdLsgYNaPe8uao6Uv9Og==
   dependencies:
     isobject "^3.0.1"
 
@@ -2706,14 +3582,19 @@
   integrity sha512-seFn10yAXy+yJlTRO+8VfiafC+0QJanGLMPTBWLrJm/QPauuchy0UXh8B6H5o9VA8BAzk0iYievt6mNp6gfaqA==
 
 is-windows@^1.0.2:
   version "1.0.2"
   resolved "https://registry.yarnpkg.com/is-windows/-/is-windows-1.0.2.tgz#d1850eb9791ecd18e6182ce12a30f396634bb19d"
   integrity sha512-eXK1UInq2bPmjyX6e3VHIzMLobc4J94i4AWn+Hpq3OU5KkrRC96OAcR3PRJ/pGu6m8TRnBHP9dkXQVsT/COVIA==
 
+is-wsl@2.1.1:
+  version "2.1.1"
+  resolved "https://registry.yarnpkg.com/is-wsl/-/is-wsl-2.1.1.tgz#4a1c152d429df3d441669498e2486d3596ebaf1d"
+  integrity sha512-umZHcSrwlDHo2TGMXv0DZ8dIUGunZ2Iv68YZnrmCiBPkZ4aaOhtv7pXJKeki9k3qJ3RJr0cDyitcl5wEH3AYog==
+
 is-wsl@^1.1.0:
   version "1.1.0"
   resolved "https://registry.yarnpkg.com/is-wsl/-/is-wsl-1.1.0.tgz#1f16e4aa22b04d1336b66188a66af3c600c3a66d"
   integrity sha1-HxbkqiKwTRM2tmGIpmrzxgDDpm0=
 
 isarray@1.0.0, isarray@^1.0.0, isarray@~1.0.0:
   version "1.0.0"
@@ -2759,52 +3640,100 @@
   integrity sha512-RdJUflcE3cUzKiMqQgsCu06FPu9UdIJO0beYbPhHN4k6apgJtifcoCtT9bcxOpYBtpD2kCM6Sbzg4CausW/PKQ==
 
 js-tokens@^3.0.2:
   version "3.0.2"
   resolved "https://registry.yarnpkg.com/js-tokens/-/js-tokens-3.0.2.tgz#9866df395102130e38f7f996bceb65443209c25b"
   integrity sha1-mGbfOVECEw449/mWvOtlRDIJwls=
 
+js-yaml@^3.13.1:
+  version "3.14.1"
+  resolved "https://registry.yarnpkg.com/js-yaml/-/js-yaml-3.14.1.tgz#dae812fdb3825fa306609a8717383c50c36a0537"
+  integrity sha512-okMH7OXXJ7YrN9Ok3/SXrnu4iX9yOk+25nqX4imS2npuvTYDmo/QEZoqwZkYaIDk3jVvBOTOIEgEhaLOynBS9g==
+  dependencies:
+    argparse "^1.0.7"
+    esprima "^4.0.0"
+
 jsesc@^2.5.1:
   version "2.5.2"
   resolved "https://registry.yarnpkg.com/jsesc/-/jsesc-2.5.2.tgz#80564d2e483dacf6e8ef209650a67df3f0c283a4"
   integrity sha512-OYu7XEzjkCQ3C5Ps3QIZsQfNpqoJyZZA99wd9aWd05NCtC5pWOkShK2mkL6HXQR6/Cy2lbNdPlZBpuQHXE63gA==
 
 jsesc@~0.5.0:
   version "0.5.0"
   resolved "https://registry.yarnpkg.com/jsesc/-/jsesc-0.5.0.tgz#e7dee66e35d6fc16f710fe91d5cf69f70f08911d"
   integrity sha1-597mbjXW/Bb3EP6R1c9p9w8IkR0=
 
-json-parse-better-errors@^1.0.2:
+json-parse-better-errors@^1.0.1, json-parse-better-errors@^1.0.2:
   version "1.0.2"
   resolved "https://registry.yarnpkg.com/json-parse-better-errors/-/json-parse-better-errors-1.0.2.tgz#bb867cfb3450e69107c131d1c514bab3dc8bcaa9"
   integrity sha512-mrqyZKfX5EhL7hvqcV6WG1yYjnjeuYDzDhhcAAUrq8Po85NBQBJP+ZDUT75qZQ98IkUoBqdkExkukOU7Ts2wrw==
 
 json-schema-traverse@^0.4.1:
   version "0.4.1"
   resolved "https://registry.yarnpkg.com/json-schema-traverse/-/json-schema-traverse-0.4.1.tgz#69f6a87d9513ab8bb8fe63bdb0979c448e684660"
   integrity sha512-xbbCH5dCYU5T8LcEhhuh7HJ88HXuW3qsI3Y0zOZFKfZEHcpWiHU/Jxzk629Brsab/mMiHQti9wMP+845RPe3Vg==
 
 json-schema-typed@^7.0.0:
   version "7.0.1"
   resolved "https://registry.yarnpkg.com/json-schema-typed/-/json-schema-typed-7.0.1.tgz#5e56564b5a0950423e22b285a30ade219e38084d"
   integrity sha512-IqUK+Cqc8/MqHsCvv1TMccbKdBzoATOLHXZAF5UDu70/CCxo648cHUig24hc+XTK53TyeNk1UeVTlc2Haovtsw==
 
+json5@2.1.1:
+  version "2.1.1"
+  resolved "https://registry.yarnpkg.com/json5/-/json5-2.1.1.tgz#81b6cb04e9ba496f1c7005d07b4368a2638f90b6"
+  integrity sha512-l+3HXD0GEI3huGq1njuqtzYK8OYJyXMkOLtQ53pjWh89tvWS2h6l+1zMkYWqlb57+SiQodKZyvMEFb2X+KrFhQ==
+  dependencies:
+    minimist "^1.2.0"
+
 json5@^1.0.1:
   version "1.0.1"
   resolved "https://registry.yarnpkg.com/json5/-/json5-1.0.1.tgz#779fb0018604fa854eacbf6252180d83543e3dbe"
   integrity sha512-aKS4WQjPenRxiQsC93MNfjx+nbF4PAdYzmd/1JIj8HYzqfbu86beTuNgXDzPknWk0n0uARlyewZo4s++ES36Ow==
   dependencies:
     minimist "^1.2.0"
 
-json5@^2.1.0:
-  version "2.1.0"
-  resolved "https://registry.yarnpkg.com/json5/-/json5-2.1.0.tgz#e7a0c62c48285c628d20a10b85c89bb807c32850"
-  integrity sha512-8Mh9h6xViijj36g7Dxi+Y4S6hNGV96vcJZr/SrlHh1LR/pEn/8j/+qIBbs44YKl69Lrfctp4QD+AdWLTMqEZAQ==
+json5@^2.1.0, json5@^2.1.2:
+  version "2.2.0"
+  resolved "https://registry.yarnpkg.com/json5/-/json5-2.2.0.tgz#2dfefe720c6ba525d9ebd909950f0515316c89a3"
+  integrity sha512-f+8cldu7X/y7RAJurMEJmdoKXGB/X550w2Nr3tTbezL6RwEE/iMcm+tZnXeoZtKuOq6ft8+CqzEkrIgx1fPoQA==
   dependencies:
-    minimist "^1.2.0"
+    minimist "^1.2.5"
+
+jsonwebtoken@8.5.1:
+  version "8.5.1"
+  resolved "https://registry.yarnpkg.com/jsonwebtoken/-/jsonwebtoken-8.5.1.tgz#00e71e0b8df54c2121a1f26137df2280673bcc0d"
+  integrity sha512-XjwVfRS6jTMsqYs0EsuJ4LGxXV14zQybNd4L2r0UvbVnSF9Af8x7p5MzbJ90Ioz/9TI41/hTCvznF/loiSzn8w==
+  dependencies:
+    jws "^3.2.2"
+    lodash.includes "^4.3.0"
+    lodash.isboolean "^3.0.3"
+    lodash.isinteger "^4.0.4"
+    lodash.isnumber "^3.0.3"
+    lodash.isplainobject "^4.0.6"
+    lodash.isstring "^4.0.1"
+    lodash.once "^4.0.0"
+    ms "^2.1.1"
+    semver "^5.6.0"
+
+jwa@^1.4.1:
+  version "1.4.1"
+  resolved "https://registry.yarnpkg.com/jwa/-/jwa-1.4.1.tgz#743c32985cb9e98655530d53641b66c8645b039a"
+  integrity sha512-qiLX/xhEEFKUAJ6FiBMbes3w9ATzyk5W7Hvzpa/SLYdxNtng+gcurvrI7TbACjIXlsJyr05/S1oUhZrc63evQA==
+  dependencies:
+    buffer-equal-constant-time "1.0.1"
+    ecdsa-sig-formatter "1.0.11"
+    safe-buffer "^5.0.1"
+
+jws@^3.2.2:
+  version "3.2.2"
+  resolved "https://registry.yarnpkg.com/jws/-/jws-3.2.2.tgz#001099f3639468c9414000e99995fa52fb478304"
+  integrity sha512-YHlZCB6lMTllWDtSPHz/ZXTsi8S00usEV6v1tjq8tOUZzw7DpSDWVXjXDre6ed1w/pd495ODpHZYSdkRTsa0HA==
+  dependencies:
+    jwa "^1.4.1"
+    safe-buffer "^5.0.1"
 
 kind-of@^3.0.2, kind-of@^3.0.3, kind-of@^3.2.0:
   version "3.2.2"
   resolved "https://registry.yarnpkg.com/kind-of/-/kind-of-3.2.2.tgz#31ea21a734bab9bbb0f32466d893aea51e4a3c64"
   integrity sha1-MeohpzS6ubuw8yRm2JOupR5KPGQ=
   dependencies:
     is-buffer "^1.1.5"
@@ -2840,28 +3769,46 @@
   integrity sha1-eUfkIUmvgNaWy/eXvKq8/h/inKg=
   dependencies:
     graceful-fs "^4.1.2"
     parse-json "^2.2.0"
     pify "^2.0.0"
     strip-bom "^3.0.0"
 
-loader-runner@^2.4.0:
+loader-runner@^2.3.1, loader-runner@^2.4.0:
   version "2.4.0"
   resolved "https://registry.yarnpkg.com/loader-runner/-/loader-runner-2.4.0.tgz#ed47066bfe534d7e84c4c7b9998c2a75607d9357"
   integrity sha512-Jsmr89RcXGIwivFY21FcRrisYZfvLMTWx5kOLc+JTxtpBOG6xML0vzbc6SEQG2FO9/4Fc3wW4LVcB5DmGflaRw==
 
-loader-utils@1.2.3, loader-utils@^1.0.2, loader-utils@^1.2.3:
+loader-utils@1.2.3:
   version "1.2.3"
   resolved "https://registry.yarnpkg.com/loader-utils/-/loader-utils-1.2.3.tgz#1ff5dc6911c9f0a062531a4c04b609406108c2c7"
   integrity sha512-fkpz8ejdnEMG3s37wGL07iSBDg99O9D5yflE9RGNH3hRdx9SOwYfnGYdZOUIZitN8E+E2vkq3MUMYMvPYl5ZZA==
   dependencies:
     big.js "^5.2.2"
     emojis-list "^2.0.0"
     json5 "^1.0.1"
 
+loader-utils@2.0.0:
+  version "2.0.0"
+  resolved "https://registry.yarnpkg.com/loader-utils/-/loader-utils-2.0.0.tgz#e4cace5b816d425a166b5f097e10cd12b36064b0"
+  integrity sha512-rP4F0h2RaWSvPEkD7BLDFQnvSf+nK+wr3ESUjNTyAGobqrijmW92zc+SO6d4p4B1wh7+B/Jg1mkQe5NYUEHtHQ==
+  dependencies:
+    big.js "^5.2.2"
+    emojis-list "^3.0.0"
+    json5 "^2.1.2"
+
+loader-utils@^1.0.2, loader-utils@^1.1.0, loader-utils@^1.2.3:
+  version "1.4.0"
+  resolved "https://registry.yarnpkg.com/loader-utils/-/loader-utils-1.4.0.tgz#c579b5e34cb34b1a74edc6c1fb36bfa371d5a613"
+  integrity sha512-qH0WSMBtn/oHuwjy/NucEgbx5dbxxnxup9s4PVXJUDHZBQY+s0NWA9rJf53RBnQZxfch7euUui7hpoAPvALZdA==
+  dependencies:
+    big.js "^5.2.2"
+    emojis-list "^3.0.0"
+    json5 "^1.0.1"
+
 locate-path@^2.0.0:
   version "2.0.0"
   resolved "https://registry.yarnpkg.com/locate-path/-/locate-path-2.0.0.tgz#2b568b265eec944c6d9c0de9c3dbbbca0354cd8e"
   integrity sha1-K1aLJl7slExtnA3pw9u7ygNUzY4=
   dependencies:
     p-locate "^2.0.0"
     path-exists "^3.0.0"
@@ -2877,18 +3824,58 @@
 locate-path@^5.0.0:
   version "5.0.0"
   resolved "https://registry.yarnpkg.com/locate-path/-/locate-path-5.0.0.tgz#1afba396afd676a6d42504d0a67a3a7eb9f62aa0"
   integrity sha512-t7hw9pI+WvuwNJXwk5zVHpyhIqzg2qTlklJOf0mVxGSbe3Fp2VieZcduNYjaLDoy6p9uGpQEGWG87WpMKlNq8g==
   dependencies:
     p-locate "^4.1.0"
 
+lodash.curry@4.1.1:
+  version "4.1.1"
+  resolved "https://registry.yarnpkg.com/lodash.curry/-/lodash.curry-4.1.1.tgz#248e36072ede906501d75966200a86dab8b23170"
+  integrity sha1-JI42By7ekGUB11lmIAqG2riyMXA=
+
+lodash.includes@^4.3.0:
+  version "4.3.0"
+  resolved "https://registry.yarnpkg.com/lodash.includes/-/lodash.includes-4.3.0.tgz#60bb98a87cb923c68ca1e51325483314849f553f"
+  integrity sha1-YLuYqHy5I8aMoeUTJUgzFISfVT8=
+
+lodash.isboolean@^3.0.3:
+  version "3.0.3"
+  resolved "https://registry.yarnpkg.com/lodash.isboolean/-/lodash.isboolean-3.0.3.tgz#6c2e171db2a257cd96802fd43b01b20d5f5870f6"
+  integrity sha1-bC4XHbKiV82WgC/UOwGyDV9YcPY=
+
+lodash.isinteger@^4.0.4:
+  version "4.0.4"
+  resolved "https://registry.yarnpkg.com/lodash.isinteger/-/lodash.isinteger-4.0.4.tgz#619c0af3d03f8b04c31f5882840b77b11cd68343"
+  integrity sha1-YZwK89A/iwTDH1iChAt3sRzWg0M=
+
+lodash.isnumber@^3.0.3:
+  version "3.0.3"
+  resolved "https://registry.yarnpkg.com/lodash.isnumber/-/lodash.isnumber-3.0.3.tgz#3ce76810c5928d03352301ac287317f11c0b1ffc"
+  integrity sha1-POdoEMWSjQM1IwGsKHMX8RwLH/w=
+
+lodash.isplainobject@^4.0.6:
+  version "4.0.6"
+  resolved "https://registry.yarnpkg.com/lodash.isplainobject/-/lodash.isplainobject-4.0.6.tgz#7c526a52d89b45c45cc690b88163be0497f550cb"
+  integrity sha1-fFJqUtibRcRcxpC4gWO+BJf1UMs=
+
+lodash.isstring@^4.0.1:
+  version "4.0.1"
+  resolved "https://registry.yarnpkg.com/lodash.isstring/-/lodash.isstring-4.0.1.tgz#d527dfb5456eca7cc9bb95d5daeaf88ba54a5451"
+  integrity sha1-1SfftUVuynzJu5XV2ur4i6VKVFE=
+
+lodash.once@^4.0.0:
+  version "4.1.1"
+  resolved "https://registry.yarnpkg.com/lodash.once/-/lodash.once-4.1.1.tgz#0dd3971213c7c56df880977d504c88fb471a97ac"
+  integrity sha1-DdOXEhPHxW34gJd9UEyI+0cal6w=
+
 lodash@^4.17.11, lodash@^4.17.13, lodash@^4.17.15, lodash@^4.17.4:
-  version "4.17.20"
-  resolved "https://registry.yarnpkg.com/lodash/-/lodash-4.17.20.tgz#b44a9b6297bcb698f1c51a3545a2b3b368d59c52"
-  integrity sha512-PlhdFcillOINfeV7Ni6oF1TAEayyZBoZ8bcshTHqOYJYlrqzRK5hagpagky5o4HfCzzd1TRkXPMFq6cKk9rGmA==
+  version "4.17.21"
+  resolved "https://registry.yarnpkg.com/lodash/-/lodash-4.17.21.tgz#679591c564c3bffaae8454cf0b3df370c3d6911c"
+  integrity sha512-v2kDEe57lecTulaDIuNTPy3Ry4gLGJ6Z1O3vE1krgXZNrsQ+LFTGHVxVjcXPs17LhbZVGedAJv8XZ1tvj5FvSg==
 
 log-symbols@^2.2.0:
   version "2.2.0"
   resolved "https://registry.yarnpkg.com/log-symbols/-/log-symbols-2.2.0.tgz#5740e1c5d6f0dfda4ad9323b5332107ef6b4c40a"
   integrity sha512-VeIAFslyIerEJLXHziedo2basKbMKtTw3vfn5IzG0XTjhAVEJyNHnL2p7vc+wBDSdQuUpNw3M2u6xb9QsAY5Eg==
   dependencies:
     chalk "^2.0.1"
@@ -2896,21 +3883,28 @@
 loose-envify@^1.0.0, loose-envify@^1.1.0, loose-envify@^1.4.0:
   version "1.4.0"
   resolved "https://registry.yarnpkg.com/loose-envify/-/loose-envify-1.4.0.tgz#71ee51fa7be4caec1a63839f7e682d8132d30caf"
   integrity sha512-lyuxPGr/Wfhrlem2CL/UcnUc1zcqKAImBDzukY7Y5F/yQiNdko6+fRLevlw1HgMySw7f611UIY408EtxRSoK3Q==
   dependencies:
     js-tokens "^3.0.0 || ^4.0.0"
 
-lru-cache@^5.1.1:
+lru-cache@5.1.1, lru-cache@^5.1.1:
   version "5.1.1"
   resolved "https://registry.yarnpkg.com/lru-cache/-/lru-cache-5.1.1.tgz#1da27e6710271947695daf6848e847f01d84b920"
   integrity sha512-KpNARQA3Iwv+jTA0utUVVbrh+Jlrr1Fv0e56GGzAFOXN7dk/FviaDW8LHmK52DlcH4WP2n6gI8vN1aesBFgo9w==
   dependencies:
     yallist "^3.0.2"
 
+lru-cache@6.0.0:
+  version "6.0.0"
+  resolved "https://registry.yarnpkg.com/lru-cache/-/lru-cache-6.0.0.tgz#6d6fe6570ebd96aaf90fcad1dafa3b2566db3a94"
+  integrity sha512-Jo6dJ04CmSjuznwJSS3pUeWmd/H0ffTlkXXgwZi+eq1UCmqQwCh+eLsYOYCwY991i2Fah4h1BEMCx4qThGbsiA==
+  dependencies:
+    yallist "^4.0.0"
+
 make-dir@^1.0.0:
   version "1.3.0"
   resolved "https://registry.yarnpkg.com/make-dir/-/make-dir-1.3.0.tgz#79c1033b80515bd6d24ec9933e860ca75ee27f0c"
   integrity sha512-2w31R7SJtieJJnQtGc7RVL2StM2vGYVfqUOvUDxH6bC6aJTxPxTF0GnIgCyu7tjockiUWAYQRbxa7vKn34s5sQ==
   dependencies:
     pify "^3.0.0"
 
@@ -2925,14 +3919,21 @@
 make-dir@^3.0.0:
   version "3.0.0"
   resolved "https://registry.yarnpkg.com/make-dir/-/make-dir-3.0.0.tgz#1b5f39f6b9270ed33f9f054c5c0f84304989f801"
   integrity sha512-grNJDhb8b1Jm1qeqW5R/O63wUo4UXo2v2HMic6YT9i/HBlF93S8jkMgH7yugvY9ABDShH4VZMn8I+U8+fCNegw==
   dependencies:
     semver "^6.0.0"
 
+make-dir@^3.0.2:
+  version "3.1.0"
+  resolved "https://registry.yarnpkg.com/make-dir/-/make-dir-3.1.0.tgz#415e967046b3a7f1d185277d84aa58203726a13f"
+  integrity sha512-g3FeP20LNwhALb/6Cz6Dd4F2ngze0jz7tbzrD2wAV+o9FeNHe4rL+yK2md0J/fiSf1sa1ADhXqi5+oVwOM/eGw==
+  dependencies:
+    semver "^6.0.0"
+
 mamacro@^0.0.3:
   version "0.0.3"
   resolved "https://registry.yarnpkg.com/mamacro/-/mamacro-0.0.3.tgz#ad2c9576197c9f1abf308d0787865bd975a3f3e4"
   integrity sha512-qMEwh+UujcQ+kbz3T6V+wAmO2U8veoq2w+3wY8MquqwVA3jChfwY+Tk52GZKDfACEPjuZ7r2oJLejwpt8jtwTA==
 
 map-cache@^0.2.2:
   version "0.2.2"
@@ -3040,20 +4041,30 @@
   integrity sha512-LRxmNwziLPT828z+4YkNzloCFC2YM4wrB99k+AV5ZbEyfGNWfG8SO1FUXLmLDBSo89NrJZ4DIWeLjy1CHGhMGA==
 
 mimic-fn@^1.0.0:
   version "1.2.0"
   resolved "https://registry.yarnpkg.com/mimic-fn/-/mimic-fn-1.2.0.tgz#820c86a39334640e99516928bd03fca88057d022"
   integrity sha512-jf84uxzwiuiIVKiOLpfYk7N46TSy8ubTonmneY9vrpHNAnp0QBt2BxWV9dO3/j+BoVAb+a5G6YDPW3M5HOdMWQ==
 
+mini-css-extract-plugin@0.8.0:
+  version "0.8.0"
+  resolved "https://registry.yarnpkg.com/mini-css-extract-plugin/-/mini-css-extract-plugin-0.8.0.tgz#81d41ec4fe58c713a96ad7c723cdb2d0bd4d70e1"
+  integrity sha512-MNpRGbNA52q6U92i0qbVpQNsgk7LExy41MdAlG84FeytfDOtRIf/mCHdEgG8rpTKOaNKiqUnZdlptF469hxqOw==
+  dependencies:
+    loader-utils "^1.1.0"
+    normalize-url "1.9.1"
+    schema-utils "^1.0.0"
+    webpack-sources "^1.1.0"
+
 minimalistic-assert@^1.0.0, minimalistic-assert@^1.0.1:
   version "1.0.1"
   resolved "https://registry.yarnpkg.com/minimalistic-assert/-/minimalistic-assert-1.0.1.tgz#2e194de044626d4a10e7f7fbc00ce73e83e4d5c7"
   integrity sha512-UtJcAD4yEaGtjPezWuO9wC4nwUnVH/8/Im3yEHQP4b67cXlD/Qr9hdITCU1xDbSEXg2XKNaP8jsReV7vQd00/A==
 
-minimalistic-crypto-utils@^1.0.0, minimalistic-crypto-utils@^1.0.1:
+minimalistic-crypto-utils@^1.0.1:
   version "1.0.1"
   resolved "https://registry.yarnpkg.com/minimalistic-crypto-utils/-/minimalistic-crypto-utils-1.0.1.tgz#f6c00c1c0b082246e5c4d99dfb8c7c083b2b582a"
   integrity sha1-9sAMHAsIIkblxNmd+4x8CDsrWCo=
 
 minimatch@^3.0.4:
   version "3.0.4"
   resolved "https://registry.yarnpkg.com/minimatch/-/minimatch-3.0.4.tgz#5166e286457f03306064be5497e8dbb0c3d32083"
@@ -3067,14 +4078,19 @@
   integrity sha1-hX/Kv8M5fSYluCKCYuhqp6ARsF0=
 
 minimist@^1.2.0:
   version "1.2.0"
   resolved "https://registry.yarnpkg.com/minimist/-/minimist-1.2.0.tgz#a35008b20f41383eec1fb914f4cd5df79a264284"
   integrity sha1-o1AIsg9BOD7sH7kU9M1d95omQoQ=
 
+minimist@^1.2.5:
+  version "1.2.5"
+  resolved "https://registry.yarnpkg.com/minimist/-/minimist-1.2.5.tgz#67d66014b66a6a8aaa0c083c5fd58df4e4e97602"
+  integrity sha512-FM9nNUYrRBAELZQT3xeZQ7fmMOBg6nWNmJKTcgsJeaLstP/UODVpGsr5OhXhhXg6f+qtJ8uiZ+PUxkDWcgIXLw==
+
 minipass@^2.2.1, minipass@^2.6.0, minipass@^2.8.6:
   version "2.8.6"
   resolved "https://registry.yarnpkg.com/minipass/-/minipass-2.8.6.tgz#620d889ace26356391d010ecb9458749df9b6db5"
   integrity sha512-lFG7d6g3+/UaFDCOtqPiKAC9zngWWsQZl1g5q6gaONqrjq61SX2xFqXMleQiFVyDpYwa018E9hmlAFY22PCb+A==
   dependencies:
     safe-buffer "^5.1.2"
     yallist "^3.0.0"
@@ -3106,15 +4122,15 @@
   version "1.3.2"
   resolved "https://registry.yarnpkg.com/mixin-deep/-/mixin-deep-1.3.2.tgz#1120b43dc359a785dce65b55b82e257ccf479566"
   integrity sha512-WRoDn//mXBiJ1H40rqa3vH0toePwSsGb45iInWlTySa+Uu4k3tYUSxa2v1KqAiLtvlrSzaExqS1gtk96A9zvEA==
   dependencies:
     for-in "^1.0.2"
     is-extendable "^1.0.1"
 
-mkdirp@0.5.1, mkdirp@^0.5.0, mkdirp@^0.5.1:
+mkdirp@^0.5.0, mkdirp@^0.5.1:
   version "0.5.1"
   resolved "https://registry.yarnpkg.com/mkdirp/-/mkdirp-0.5.1.tgz#30057438eac6cf7f8c4767f38648d6697d75c903"
   integrity sha1-MAV0OOrGz3+MR2fzhkjWaX11yQM=
   dependencies:
     minimist "0.0.8"
 
 move-concurrently@^1.0.1:
@@ -3162,14 +4178,21 @@
     is-windows "^1.0.2"
     kind-of "^6.0.2"
     object.pick "^1.3.0"
     regex-not "^1.0.0"
     snapdragon "^0.8.1"
     to-regex "^3.0.1"
 
+native-url@0.2.6:
+  version "0.2.6"
+  resolved "https://registry.yarnpkg.com/native-url/-/native-url-0.2.6.tgz#ca1258f5ace169c716ff44eccbddb674e10399ae"
+  integrity sha512-k4bDC87WtgrdD362gZz6zoiXQrl40kYlBmpfmSjwRO1VU0V5ccwJTlxuE72F6m3V0vc1xOf6n3UCP9QyerRqmA==
+  dependencies:
+    querystring "^0.2.0"
+
 needle@^2.2.1:
   version "2.4.0"
   resolved "https://registry.yarnpkg.com/needle/-/needle-2.4.0.tgz#6833e74975c444642590e15a750288c5f939b57c"
   integrity sha512-4Hnwzr3mi5L97hMYeNl8wRW/Onhy4nUKR/lVemJ8gJedxxUyBLm9kkrDColJvoSfwi0jCNhD+xCdOtiGDQiRZg==
   dependencies:
     debug "^3.2.6"
     iconv-lite "^0.4.4"
@@ -3181,84 +4204,130 @@
   integrity sha512-hZXc7K2e+PgeI1eDBe/10Ard4ekbfrrqG8Ep+8Jmf4JID2bNg7NvCPOZN+kfF574pFQI7mum2AUqDidoKqcTOw==
 
 neo-async@^2.5.0, neo-async@^2.6.1:
   version "2.6.1"
   resolved "https://registry.yarnpkg.com/neo-async/-/neo-async-2.6.1.tgz#ac27ada66167fa8849a6addd837f6b189ad2081c"
   integrity sha512-iyam8fBuCUpWeKPGpaNMetEocMt364qkCsfL9JuhjXX6dRnguRVOfk2GZaDpPjcOKiiXCPINZC1GczQ7iTq3Zw==
 
-next@^9.0.2:
-  version "9.0.6"
-  resolved "https://registry.yarnpkg.com/next/-/next-9.0.6.tgz#cf6e84fdae20699033cb4603863a4dc297f5d002"
-  integrity sha512-kXq+AbgB/Pi5UtMkEkJbDW1ObdsrTKhcP48Bw8BQP4GNzWI9icDRqTQoa7hf+7SKCu1IMshDKan60T6UnZpJ+w==
-  dependencies:
-    "@ampproject/toolbox-optimizer" "1.0.1"
-    "@babel/core" "7.4.5"
-    "@babel/plugin-proposal-class-properties" "7.4.4"
-    "@babel/plugin-proposal-object-rest-spread" "7.4.4"
+neo-async@^2.6.0:
+  version "2.6.2"
+  resolved "https://registry.yarnpkg.com/neo-async/-/neo-async-2.6.2.tgz#b4aafb93e3aeb2d8174ca53cf163ab7d7308305f"
+  integrity sha512-Yd3UES5mWCSqR+qNT93S3UoYUkqAZ9lLg8a7g9rimsWmYGK8cVToA4/sF3RrshdyV3sAGMXVUmpMYOw+dLpOuw==
+
+next-tick@~1.0.0:
+  version "1.0.0"
+  resolved "https://registry.yarnpkg.com/next-tick/-/next-tick-1.0.0.tgz#ca86d1fe8828169b0120208e3dc8424b9db8342c"
+  integrity sha1-yobR/ogoFpsBICCOPchCS524NCw=
+
+next@^9.3.2:
+  version "9.3.2"
+  resolved "https://registry.yarnpkg.com/next/-/next-9.3.2.tgz#fabf907f5397ae3581d4227789f625533fb4e64e"
+  integrity sha512-KVNnnFyvtO1DwSEyMgt3wtxpkprnGCldEOyMXbt9Zxf8RcCw3YnRImbg8mVgrcXJRzkQpve4bdJKYY5MVwT/RA==
+  dependencies:
+    "@ampproject/toolbox-optimizer" "2.0.1"
+    "@babel/core" "7.7.2"
+    "@babel/plugin-proposal-class-properties" "7.7.0"
+    "@babel/plugin-proposal-nullish-coalescing-operator" "7.7.4"
+    "@babel/plugin-proposal-numeric-separator" "7.8.3"
+    "@babel/plugin-proposal-object-rest-spread" "7.6.2"
+    "@babel/plugin-proposal-optional-chaining" "7.7.4"
+    "@babel/plugin-syntax-bigint" "7.8.3"
     "@babel/plugin-syntax-dynamic-import" "7.2.0"
-    "@babel/plugin-transform-modules-commonjs" "7.4.4"
-    "@babel/plugin-transform-runtime" "7.4.4"
-    "@babel/preset-env" "7.4.5"
-    "@babel/preset-react" "7.0.0"
-    "@babel/preset-typescript" "7.3.3"
-    "@babel/runtime" "7.4.5"
-    "@babel/runtime-corejs2" "7.4.5"
-    amphtml-validator "1.0.23"
+    "@babel/plugin-transform-modules-commonjs" "7.7.0"
+    "@babel/plugin-transform-runtime" "7.6.2"
+    "@babel/preset-env" "7.7.1"
+    "@babel/preset-modules" "0.1.1"
+    "@babel/preset-react" "7.7.0"
+    "@babel/preset-typescript" "7.7.2"
+    "@babel/runtime" "7.7.2"
+    "@babel/types" "7.7.4"
+    "@next/polyfill-nomodule" "9.3.2"
+    amphtml-validator "1.0.30"
     async-retry "1.2.3"
     async-sema "3.0.0"
     autodll-webpack-plugin "0.4.2"
     babel-core "7.0.0-bridge.0"
     babel-loader "8.0.6"
     babel-plugin-syntax-jsx "6.18.0"
-    babel-plugin-transform-define "1.3.1"
+    babel-plugin-transform-define "2.0.0"
     babel-plugin-transform-react-remove-prop-types "0.4.24"
+    browserslist "4.8.3"
+    cache-loader "4.1.0"
     chalk "2.4.2"
     ci-info "2.0.0"
     compression "1.7.4"
     conf "5.0.0"
     content-type "1.0.4"
     cookie "0.4.0"
-    devalue "2.0.0"
+    css-loader "3.3.0"
+    cssnano-simple "1.0.0"
+    devalue "2.0.1"
+    escape-string-regexp "2.0.0"
     etag "1.8.1"
+    file-loader "4.2.0"
+    finally-polyfill "0.1.0"
     find-up "4.0.0"
-    fork-ts-checker-webpack-plugin "1.3.4"
+    fork-ts-checker-webpack-plugin "3.1.1"
     fresh "0.5.2"
+    gzip-size "5.1.1"
+    http-proxy "1.18.0"
+    ignore-loader "0.1.2"
     is-docker "2.0.0"
+    is-wsl "2.1.1"
     jest-worker "24.9.0"
+    json5 "2.1.1"
+    jsonwebtoken "8.5.1"
     launch-editor "2.2.1"
-    loader-utils "1.2.3"
-    mkdirp "0.5.1"
+    loader-utils "2.0.0"
+    lodash.curry "4.1.1"
+    lru-cache "5.1.1"
+    mini-css-extract-plugin "0.8.0"
+    native-url "0.2.6"
     node-fetch "2.6.0"
     ora "3.4.0"
-    path-to-regexp "2.1.0"
+    path-to-regexp "6.1.0"
     pnp-webpack-plugin "1.5.0"
+    postcss-flexbugs-fixes "4.2.0"
+    postcss-loader "3.0.0"
+    postcss-preset-env "6.7.0"
     prop-types "15.7.2"
     prop-types-exact "1.2.0"
     raw-body "2.4.0"
     react-error-overlay "5.1.6"
     react-is "16.8.6"
+    recast "0.18.5"
+    resolve-url-loader "3.1.1"
+    sass-loader "8.0.2"
     send "0.17.1"
     source-map "0.6.1"
     string-hash "1.1.3"
     strip-ansi "5.2.0"
-    styled-jsx "3.2.2"
-    terser "4.0.0"
+    style-loader "1.0.0"
+    styled-jsx "3.2.5"
+    terser "4.4.2"
+    thread-loader "2.1.3"
     unfetch "4.1.0"
     url "0.11.0"
-    watchpack "2.0.0-beta.5"
-    webpack "4.39.0"
+    use-subscription "1.1.1"
+    watchpack "2.0.0-beta.13"
+    webpack "4.42.0"
     webpack-dev-middleware "3.7.0"
     webpack-hot-middleware "2.25.0"
-    webpack-sources "1.3.0"
+    webpack-sources "1.4.3"
 
-node-fetch@2.6.0, node-fetch@^2.2.0:
+node-fetch@2.6.0:
   version "2.6.0"
   resolved "https://registry.yarnpkg.com/node-fetch/-/node-fetch-2.6.0.tgz#e633456386d4aa55863f676a7ab0daa8fdecb0fd"
   integrity sha512-8dG4H5ujfvFiqDmVu9fQ5bOHUC15JMjMY/Zumv26oOvvVJjM67KF8koCWIabKQ1GJIa9r2mMZscBq/TbdOcmNA==
 
+node-fetch@2.6.1, node-fetch@^2.2.0:
+  version "2.6.1"
+  resolved "https://registry.yarnpkg.com/node-fetch/-/node-fetch-2.6.1.tgz#045bd323631f76ed2e2b55573394416b639a0052"
+  integrity sha512-V4aYg89jEoVRxRb2fJdAg8FHvI7cEyYdVAh94HH0UIK8oJxUfkjlDQN9RbMx+bEjP7+ggMiFRprSti032Oipxw==
+
 node-libs-browser@^2.2.1:
   version "2.2.1"
   resolved "https://registry.yarnpkg.com/node-libs-browser/-/node-libs-browser-2.2.1.tgz#b64f513d18338625f90346d27b0d235e631f6425"
   integrity sha512-h/zcD8H9kaDZ9ALUWwlBUDo6TKF8a7qBSCSEGfjTVIYeqsioSKaAX+BN7NgiMGp6iSIXZ3PxgCu8KS3b71YK5Q==
   dependencies:
     assert "^1.1.1"
     browserify-zlib "^0.2.0"
@@ -3296,29 +4365,37 @@
     npm-packlist "^1.1.6"
     npmlog "^4.0.2"
     rc "^1.2.7"
     rimraf "^2.6.1"
     semver "^5.3.0"
     tar "^4"
 
-node-releases@^1.1.29:
-  version "1.1.32"
-  resolved "https://registry.yarnpkg.com/node-releases/-/node-releases-1.1.32.tgz#485b35c1bf9b4d8baa105d782f8ca731e518276e"
-  integrity sha512-VhVknkitq8dqtWoluagsGPn3dxTvN9fwgR59fV3D7sLBHe0JfDramsMI8n8mY//ccq/Kkrf8ZRHRpsyVZ3qw1A==
-  dependencies:
-    semver "^5.3.0"
+node-releases@^1.1.44:
+  version "1.1.73"
+  resolved "https://registry.yarnpkg.com/node-releases/-/node-releases-1.1.73.tgz#dd4e81ddd5277ff846b80b52bb40c49edf7a7b20"
+  integrity sha512-uW7fodD6pyW2FZNZnp/Z3hvWKeEW1Y8R1+1CnErE8cXFXzl5blBOoVB41CvMer6P6Q0S5FXDwcHgFd1Wj0U9zg==
+
+node-releases@^1.1.71:
+  version "1.1.72"
+  resolved "https://registry.yarnpkg.com/node-releases/-/node-releases-1.1.72.tgz#14802ab6b1039a79a0c7d662b610a5bbd76eacbe"
+  integrity sha512-LLUo+PpH3dU6XizX3iVoubUNheF/owjXCZZ5yACDxNnPtgFuludV1ZL3ayK1kVep42Rmm0+R9/Y60NQbZ2bifw==
 
 nopt@^4.0.1:
   version "4.0.1"
   resolved "https://registry.yarnpkg.com/nopt/-/nopt-4.0.1.tgz#d0d4685afd5415193c8c7505602d0d17cd64474d"
   integrity sha1-0NRoWv1UFRk8jHUFYC0NF81kR00=
   dependencies:
     abbrev "1"
     osenv "^0.1.4"
 
+normalize-html-whitespace@1.0.0:
+  version "1.0.0"
+  resolved "https://registry.yarnpkg.com/normalize-html-whitespace/-/normalize-html-whitespace-1.0.0.tgz#5e3c8e192f1b06c3b9eee4b7e7f28854c7601e34"
+  integrity sha512-9ui7CGtOOlehQu0t/OhhlmDyc71mKVlv+4vF+me4iZLPrNtRL2xoquEdfZxasC/bdQi/Hr3iTrpyRKIG+ocabA==
+
 normalize-package-data@^2.3.2:
   version "2.5.0"
   resolved "https://registry.yarnpkg.com/normalize-package-data/-/normalize-package-data-2.5.0.tgz#e66db1838b200c1dfc233225d12cb36520e234a8"
   integrity sha512-/5CMN3T0R4XTj4DcGaexo+roZSdSFW/0AOOTROrjxzCG1wrWXEsGbRKevjlIL+ZDE4sZlJr5ED4YW0yqmkK+eA==
   dependencies:
     hosted-git-info "^2.1.4"
     resolve "^1.10.0"
@@ -3328,19 +4405,34 @@
 normalize-path@^2.1.1:
   version "2.1.1"
   resolved "https://registry.yarnpkg.com/normalize-path/-/normalize-path-2.1.1.tgz#1ab28b556e198363a8c1a6f7e6fa20137fe6aed9"
   integrity sha1-GrKLVW4Zg2Oowab35vogE3/mrtk=
   dependencies:
     remove-trailing-separator "^1.0.1"
 
-normalize-path@^3.0.0:
+normalize-path@^3.0.0, normalize-path@~3.0.0:
   version "3.0.0"
   resolved "https://registry.yarnpkg.com/normalize-path/-/normalize-path-3.0.0.tgz#0dcd69ff23a1c9b11fd0978316644a0388216a65"
   integrity sha512-6eZs5Ls3WtCisHWp9S2GUy8dqkpGi4BVSz3GaqiE6ezub0512ESztXUwUB6C6IKbQkY2Pnb/mD4WYojCRwcwLA==
 
+normalize-range@^0.1.2:
+  version "0.1.2"
+  resolved "https://registry.yarnpkg.com/normalize-range/-/normalize-range-0.1.2.tgz#2d10c06bdfd312ea9777695a4d28439456b75942"
+  integrity sha1-LRDAa9/TEuqXd2laTShDlFa3WUI=
+
+normalize-url@1.9.1:
+  version "1.9.1"
+  resolved "https://registry.yarnpkg.com/normalize-url/-/normalize-url-1.9.1.tgz#2cc0d66b31ea23036458436e3620d85954c66c3c"
+  integrity sha1-LMDWazHqIwNkWENuNiDYWVTGbDw=
+  dependencies:
+    object-assign "^4.0.1"
+    prepend-http "^1.0.0"
+    query-string "^4.1.0"
+    sort-keys "^1.0.0"
+
 npm-bundled@^1.0.1:
   version "1.0.6"
   resolved "https://registry.yarnpkg.com/npm-bundled/-/npm-bundled-1.0.6.tgz#e7ba9aadcef962bb61248f91721cd932b3fe6bdd"
   integrity sha512-8/JCaftHwbd//k6y2rEWp6k1wxVfpFzB6t1p825+cUb7Ym2XQfhwIC5KwhrvzZRJu+LtDE585zVaS32+CGtf0g==
 
 npm-packlist@^1.1.6:
   version "1.4.4"
@@ -3356,14 +4448,19 @@
   integrity sha512-2uUqazuKlTaSI/dC8AzicUck7+IrEaOnN/e0jd3Xtt1KcGpwx30v50mL7oPyr/h9bL3E4aZccVwpwP+5W9Vjkg==
   dependencies:
     are-we-there-yet "~1.1.2"
     console-control-strings "~1.1.0"
     gauge "~2.7.3"
     set-blocking "~2.0.0"
 
+num2fraction@^1.2.2:
+  version "1.2.2"
+  resolved "https://registry.yarnpkg.com/num2fraction/-/num2fraction-1.2.2.tgz#6f682b6a027a4e9ddfa4564cd2589d1d4e669ede"
+  integrity sha1-b2gragJ6Tp3fpFZM0lidHU5mnt4=
+
 number-is-nan@^1.0.0:
   version "1.0.1"
   resolved "https://registry.yarnpkg.com/number-is-nan/-/number-is-nan-1.0.1.tgz#097b602b53422a522c1afb8790318336941a011d"
   integrity sha1-CXtgK1NCKlIsGvuHkDGDNpQaAR0=
 
 object-assign@^4.0.1, object-assign@^4.1.0, object-assign@^4.1.1:
   version "4.1.1"
@@ -3380,14 +4477,19 @@
     kind-of "^3.0.3"
 
 object-keys@^1.0.11, object-keys@^1.0.12:
   version "1.1.1"
   resolved "https://registry.yarnpkg.com/object-keys/-/object-keys-1.1.1.tgz#1c47f272df277f3b1daf061677d9c82e2322c60e"
   integrity sha512-NuAESUOUMrlIXOfHKzD6bpPu3tYt3xvjNdRIQ+FeT0lNb4K8WR70CaDxhuNguS2XG+GjkyMwOzsN5ZktImfhLA==
 
+object-path@0.11.4:
+  version "0.11.4"
+  resolved "https://registry.yarnpkg.com/object-path/-/object-path-0.11.4.tgz#370ae752fbf37de3ea70a861c23bba8915691949"
+  integrity sha1-NwrnUvvzfePqcKhhwju6iRVpGUk=
+
 object-visit@^1.0.0:
   version "1.0.1"
   resolved "https://registry.yarnpkg.com/object-visit/-/object-visit-1.0.1.tgz#f79c4493af0c5377b59fe39d395e41042dd045bb"
   integrity sha1-95xEk68MU3e1n+OdOV5BBC3QRbs=
   dependencies:
     isobject "^3.0.0"
 
@@ -3548,25 +4650,21 @@
 parse-json@^2.2.0:
   version "2.2.0"
   resolved "https://registry.yarnpkg.com/parse-json/-/parse-json-2.2.0.tgz#f480f40434ef80741f8469099f8dea18f55a4dc9"
   integrity sha1-9ID0BDTvgHQfhGkJn43qGPVaTck=
   dependencies:
     error-ex "^1.2.0"
 
-parse5-htmlparser2-tree-adapter@5.1.0:
-  version "5.1.0"
-  resolved "https://registry.yarnpkg.com/parse5-htmlparser2-tree-adapter/-/parse5-htmlparser2-tree-adapter-5.1.0.tgz#a8244ee12bbd6b8937ad2a16ea43fe348aebcc86"
-  integrity sha512-OrI4DNmghGcwDB3XN8FKKN7g5vBmau91uqj+VYuwuj/r6GhFBMBNymsM+Z9z+Z1p4HHgI0UuQirQRgh3W5d88g==
+parse-json@^4.0.0:
+  version "4.0.0"
+  resolved "https://registry.yarnpkg.com/parse-json/-/parse-json-4.0.0.tgz#be35f5425be1f7f6c747184f98a788cb99477ee0"
+  integrity sha1-vjX1Qlvh9/bHRxhPmKeIy5lHfuA=
   dependencies:
-    parse5 "^5.1.0"
-
-parse5@5.1.0, parse5@^5.1.0:
-  version "5.1.0"
-  resolved "https://registry.yarnpkg.com/parse5/-/parse5-5.1.0.tgz#c59341c9723f414c452975564c7c00a68d58acd2"
-  integrity sha512-fxNG2sQjHvlVAYmzBZS9YlDp6PTSSDwa98vkD4QgVDDCAo84z5X1t5XyJQ62ImdLXx5NdIIfihey6xpum9/gRQ==
+    error-ex "^1.3.1"
+    json-parse-better-errors "^1.0.1"
 
 pascalcase@^0.1.1:
   version "0.1.1"
   resolved "https://registry.yarnpkg.com/pascalcase/-/pascalcase-0.1.1.tgz#b363e55e8006ca6fe21784d2db22bd15d7917f14"
   integrity sha1-s2PlXoAGym/iF4TS2yK9FdeRfxQ=
 
 path-browserify@0.0.1:
@@ -3580,14 +4678,19 @@
   integrity sha1-zDPSTVJeCZpTiMAzbG4yuRYGCeA=
 
 path-exists@^3.0.0:
   version "3.0.0"
   resolved "https://registry.yarnpkg.com/path-exists/-/path-exists-3.0.0.tgz#ce0ebeaa5f78cb18925ea7d810d7b59b010fd515"
   integrity sha1-zg6+ql94yxiSXqfYENe1mwEP1RU=
 
+path-exists@^4.0.0:
+  version "4.0.0"
+  resolved "https://registry.yarnpkg.com/path-exists/-/path-exists-4.0.0.tgz#513bdbe2d3b95d7762e8c1137efa195c6c61b5b3"
+  integrity sha512-ak9Qy5Q7jYb2Wwcey5Fpvg2KoAc/ZIhLSLOSBmRmygPsGwkVVt0fZa0qrtMz+m6tJTAHfZQ8FnmB4MG4LWy7/w==
+
 path-is-absolute@^1.0.0:
   version "1.0.1"
   resolved "https://registry.yarnpkg.com/path-is-absolute/-/path-is-absolute-1.0.1.tgz#174b9268735534ffbc7ace6bf53a5a9e1b5c5f5f"
   integrity sha1-F0uSaHNVNP+8es5r9TpanhtcX18=
 
 path-is-inside@^1.0.1:
   version "1.0.2"
@@ -3595,18 +4698,18 @@
   integrity sha1-NlQX3t5EQw0cEa9hAn+s8HS9/FM=
 
 path-parse@^1.0.6:
   version "1.0.6"
   resolved "https://registry.yarnpkg.com/path-parse/-/path-parse-1.0.6.tgz#d62dbb5679405d72c4737ec58600e9ddcf06d24c"
   integrity sha512-GSmOT2EbHrINBf9SR7CDELwlJ8AENk3Qn7OikK4nFYAu3Ote2+JYNVvkpAEQm3/TLNEJFD/xZJjzyxg3KBWOzw==
 
-path-to-regexp@2.1.0:
-  version "2.1.0"
-  resolved "https://registry.yarnpkg.com/path-to-regexp/-/path-to-regexp-2.1.0.tgz#7e30f9f5b134bd6a28ffc2e3ef1e47075ac5259b"
-  integrity sha512-dZY7QPCPp5r9cnNuQ955mOv4ZFVDXY/yvqeV7Y1W2PJA3PEFcuow9xKFfJxbBj1pIjOAP+M2B4/7xubmykLrXw==
+path-to-regexp@6.1.0:
+  version "6.1.0"
+  resolved "https://registry.yarnpkg.com/path-to-regexp/-/path-to-regexp-6.1.0.tgz#0b18f88b7a0ce0bfae6a25990c909ab86f512427"
+  integrity sha512-h9DqehX3zZZDCEm+xbfU0ZmwCGFCAAraPJWMXJ4+v32NjZJilVg3k1TcKsRgIb8IQ/izZSaydDc1OhJCZvs2Dw==
 
 path-type@^2.0.0:
   version "2.0.0"
   resolved "https://registry.yarnpkg.com/path-type/-/path-type-2.0.0.tgz#f012ccb8415b7096fc2daa1054c3d72389594c73"
   integrity sha1-8BLMuEFbcJb8LaoQVMPXI4lZTHM=
   dependencies:
     pify "^2.0.0"
@@ -3618,14 +4721,19 @@
   dependencies:
     create-hash "^1.1.2"
     create-hmac "^1.1.4"
     ripemd160 "^2.0.1"
     safe-buffer "^5.0.1"
     sha.js "^2.4.8"
 
+picomatch@^2.0.4, picomatch@^2.2.1:
+  version "2.3.0"
+  resolved "https://registry.yarnpkg.com/picomatch/-/picomatch-2.3.0.tgz#f1f061de8f6a4bf022892e2d128234fb98302972"
+  integrity sha512-lY1Q/PiJGC2zOv/z391WOTD+Z02bCgsFfvxoXXf6h7kv9o+WmsmzYqrAwY63sNgOxE4xEdq0WyUnXfKeBrSvYw==
+
 pify@^2.0.0:
   version "2.3.0"
   resolved "https://registry.yarnpkg.com/pify/-/pify-2.3.0.tgz#ed141a6ac043a849ea588498e7dca8b15330e90c"
   integrity sha1-7RQaasBDqEnqWISY59yosVMw6Qw=
 
 pify@^3.0.0:
   version "3.0.0"
@@ -3659,14 +4767,21 @@
 pkg-dir@^3.0.0:
   version "3.0.0"
   resolved "https://registry.yarnpkg.com/pkg-dir/-/pkg-dir-3.0.0.tgz#2749020f239ed990881b1f71210d51eb6523bea3"
   integrity sha512-/E57AYkoeQ25qkxMj5PBOVgF8Kiu/h7cYS30Z5+R7WaiCCBfLq58ZI/dSeaEKb9WVJV5n/03QwrN3IeWIFllvw==
   dependencies:
     find-up "^3.0.0"
 
+pkg-dir@^4.1.0:
+  version "4.2.0"
+  resolved "https://registry.yarnpkg.com/pkg-dir/-/pkg-dir-4.2.0.tgz#f099133df7ede422e81d1d8448270eeb3e4261f3"
+  integrity sha512-HRDzbaKjC+AOWVXxAU/x54COGeIv9eb+6CkDSQoNTt4XyWoIJvuPsXizxu/Fr23EiekbtZwmh1IcIG/l/a10GQ==
+  dependencies:
+    find-up "^4.0.0"
+
 pkg-up@^3.0.1:
   version "3.1.0"
   resolved "https://registry.yarnpkg.com/pkg-up/-/pkg-up-3.1.0.tgz#100ec235cc150e4fd42519412596a28512a0def5"
   integrity sha512-nDywThFk1i4BQK4twPQ6TA4RT8bDY96yeuCVBWL3ePARCiEKDRSrNGbFIgUJpLp+XeIR65v8ra7WuJOFUBtkMA==
   dependencies:
     find-up "^3.0.0"
 
@@ -3678,20 +4793,398 @@
     ts-pnp "^1.1.2"
 
 posix-character-classes@^0.1.0:
   version "0.1.1"
   resolved "https://registry.yarnpkg.com/posix-character-classes/-/posix-character-classes-0.1.1.tgz#01eac0fe3b5af71a2a6c02feabb8c1fef7e00eab"
   integrity sha1-AerA/jta9xoqbAL+q7jB/vfgDqs=
 
+postcss-attribute-case-insensitive@^4.0.1:
+  version "4.0.2"
+  resolved "https://registry.yarnpkg.com/postcss-attribute-case-insensitive/-/postcss-attribute-case-insensitive-4.0.2.tgz#d93e46b504589e94ac7277b0463226c68041a880"
+  integrity sha512-clkFxk/9pcdb4Vkn0hAHq3YnxBQ2p0CGD1dy24jN+reBck+EWxMbxSUqN4Yj7t0w8csl87K6p0gxBe1utkJsYA==
+  dependencies:
+    postcss "^7.0.2"
+    postcss-selector-parser "^6.0.2"
+
+postcss-color-functional-notation@^2.0.1:
+  version "2.0.1"
+  resolved "https://registry.yarnpkg.com/postcss-color-functional-notation/-/postcss-color-functional-notation-2.0.1.tgz#5efd37a88fbabeb00a2966d1e53d98ced93f74e0"
+  integrity sha512-ZBARCypjEDofW4P6IdPVTLhDNXPRn8T2s1zHbZidW6rPaaZvcnCS2soYFIQJrMZSxiePJ2XIYTlcb2ztr/eT2g==
+  dependencies:
+    postcss "^7.0.2"
+    postcss-values-parser "^2.0.0"
+
+postcss-color-gray@^5.0.0:
+  version "5.0.0"
+  resolved "https://registry.yarnpkg.com/postcss-color-gray/-/postcss-color-gray-5.0.0.tgz#532a31eb909f8da898ceffe296fdc1f864be8547"
+  integrity sha512-q6BuRnAGKM/ZRpfDascZlIZPjvwsRye7UDNalqVz3s7GDxMtqPY6+Q871liNxsonUw8oC61OG+PSaysYpl1bnw==
+  dependencies:
+    "@csstools/convert-colors" "^1.4.0"
+    postcss "^7.0.5"
+    postcss-values-parser "^2.0.0"
+
+postcss-color-hex-alpha@^5.0.3:
+  version "5.0.3"
+  resolved "https://registry.yarnpkg.com/postcss-color-hex-alpha/-/postcss-color-hex-alpha-5.0.3.tgz#a8d9ca4c39d497c9661e374b9c51899ef0f87388"
+  integrity sha512-PF4GDel8q3kkreVXKLAGNpHKilXsZ6xuu+mOQMHWHLPNyjiUBOr75sp5ZKJfmv1MCus5/DWUGcK9hm6qHEnXYw==
+  dependencies:
+    postcss "^7.0.14"
+    postcss-values-parser "^2.0.1"
+
+postcss-color-mod-function@^3.0.3:
+  version "3.0.3"
+  resolved "https://registry.yarnpkg.com/postcss-color-mod-function/-/postcss-color-mod-function-3.0.3.tgz#816ba145ac11cc3cb6baa905a75a49f903e4d31d"
+  integrity sha512-YP4VG+xufxaVtzV6ZmhEtc+/aTXH3d0JLpnYfxqTvwZPbJhWqp8bSY3nfNzNRFLgB4XSaBA82OE4VjOOKpCdVQ==
+  dependencies:
+    "@csstools/convert-colors" "^1.4.0"
+    postcss "^7.0.2"
+    postcss-values-parser "^2.0.0"
+
+postcss-color-rebeccapurple@^4.0.1:
+  version "4.0.1"
+  resolved "https://registry.yarnpkg.com/postcss-color-rebeccapurple/-/postcss-color-rebeccapurple-4.0.1.tgz#c7a89be872bb74e45b1e3022bfe5748823e6de77"
+  integrity sha512-aAe3OhkS6qJXBbqzvZth2Au4V3KieR5sRQ4ptb2b2O8wgvB3SJBsdG+jsn2BZbbwekDG8nTfcCNKcSfe/lEy8g==
+  dependencies:
+    postcss "^7.0.2"
+    postcss-values-parser "^2.0.0"
+
+postcss-custom-media@^7.0.8:
+  version "7.0.8"
+  resolved "https://registry.yarnpkg.com/postcss-custom-media/-/postcss-custom-media-7.0.8.tgz#fffd13ffeffad73621be5f387076a28b00294e0c"
+  integrity sha512-c9s5iX0Ge15o00HKbuRuTqNndsJUbaXdiNsksnVH8H4gdc+zbLzr/UasOwNG6CTDpLFekVY4672eWdiiWu2GUg==
+  dependencies:
+    postcss "^7.0.14"
+
+postcss-custom-properties@^8.0.11:
+  version "8.0.11"
+  resolved "https://registry.yarnpkg.com/postcss-custom-properties/-/postcss-custom-properties-8.0.11.tgz#2d61772d6e92f22f5e0d52602df8fae46fa30d97"
+  integrity sha512-nm+o0eLdYqdnJ5abAJeXp4CEU1c1k+eB2yMCvhgzsds/e0umabFrN6HoTy/8Q4K5ilxERdl/JD1LO5ANoYBeMA==
+  dependencies:
+    postcss "^7.0.17"
+    postcss-values-parser "^2.0.1"
+
+postcss-custom-selectors@^5.1.2:
+  version "5.1.2"
+  resolved "https://registry.yarnpkg.com/postcss-custom-selectors/-/postcss-custom-selectors-5.1.2.tgz#64858c6eb2ecff2fb41d0b28c9dd7b3db4de7fba"
+  integrity sha512-DSGDhqinCqXqlS4R7KGxL1OSycd1lydugJ1ky4iRXPHdBRiozyMHrdu0H3o7qNOCiZwySZTUI5MV0T8QhCLu+w==
+  dependencies:
+    postcss "^7.0.2"
+    postcss-selector-parser "^5.0.0-rc.3"
+
+postcss-dir-pseudo-class@^5.0.0:
+  version "5.0.0"
+  resolved "https://registry.yarnpkg.com/postcss-dir-pseudo-class/-/postcss-dir-pseudo-class-5.0.0.tgz#6e3a4177d0edb3abcc85fdb6fbb1c26dabaeaba2"
+  integrity sha512-3pm4oq8HYWMZePJY+5ANriPs3P07q+LW6FAdTlkFH2XqDdP4HeeJYMOzn0HYLhRSjBO3fhiqSwwU9xEULSrPgw==
+  dependencies:
+    postcss "^7.0.2"
+    postcss-selector-parser "^5.0.0-rc.3"
+
+postcss-double-position-gradients@^1.0.0:
+  version "1.0.0"
+  resolved "https://registry.yarnpkg.com/postcss-double-position-gradients/-/postcss-double-position-gradients-1.0.0.tgz#fc927d52fddc896cb3a2812ebc5df147e110522e"
+  integrity sha512-G+nV8EnQq25fOI8CH/B6krEohGWnF5+3A6H/+JEpOncu5dCnkS1QQ6+ct3Jkaepw1NGVqqOZH6lqrm244mCftA==
+  dependencies:
+    postcss "^7.0.5"
+    postcss-values-parser "^2.0.0"
+
+postcss-env-function@^2.0.2:
+  version "2.0.2"
+  resolved "https://registry.yarnpkg.com/postcss-env-function/-/postcss-env-function-2.0.2.tgz#0f3e3d3c57f094a92c2baf4b6241f0b0da5365d7"
+  integrity sha512-rwac4BuZlITeUbiBq60h/xbLzXY43qOsIErngWa4l7Mt+RaSkT7QBjXVGTcBHupykkblHMDrBFh30zchYPaOUw==
+  dependencies:
+    postcss "^7.0.2"
+    postcss-values-parser "^2.0.0"
+
+postcss-flexbugs-fixes@4.2.0:
+  version "4.2.0"
+  resolved "https://registry.yarnpkg.com/postcss-flexbugs-fixes/-/postcss-flexbugs-fixes-4.2.0.tgz#662b3dcb6354638b9213a55eed8913bcdc8d004a"
+  integrity sha512-QRE0n3hpkxxS/OGvzOa+PDuy4mh/Jg4o9ui22/ko5iGYOG3M5dfJabjnAZjTdh2G9F85c7Hv8hWcEDEKW/xceQ==
+  dependencies:
+    postcss "^7.0.26"
+
+postcss-focus-visible@^4.0.0:
+  version "4.0.0"
+  resolved "https://registry.yarnpkg.com/postcss-focus-visible/-/postcss-focus-visible-4.0.0.tgz#477d107113ade6024b14128317ade2bd1e17046e"
+  integrity sha512-Z5CkWBw0+idJHSV6+Bgf2peDOFf/x4o+vX/pwcNYrWpXFrSfTkQ3JQ1ojrq9yS+upnAlNRHeg8uEwFTgorjI8g==
+  dependencies:
+    postcss "^7.0.2"
+
+postcss-focus-within@^3.0.0:
+  version "3.0.0"
+  resolved "https://registry.yarnpkg.com/postcss-focus-within/-/postcss-focus-within-3.0.0.tgz#763b8788596cee9b874c999201cdde80659ef680"
+  integrity sha512-W0APui8jQeBKbCGZudW37EeMCjDeVxKgiYfIIEo8Bdh5SpB9sxds/Iq8SEuzS0Q4YFOlG7EPFulbbxujpkrV2w==
+  dependencies:
+    postcss "^7.0.2"
+
+postcss-font-variant@^4.0.0:
+  version "4.0.1"
+  resolved "https://registry.yarnpkg.com/postcss-font-variant/-/postcss-font-variant-4.0.1.tgz#42d4c0ab30894f60f98b17561eb5c0321f502641"
+  integrity sha512-I3ADQSTNtLTTd8uxZhtSOrTCQ9G4qUVKPjHiDk0bV75QSxXjVWiJVJ2VLdspGUi9fbW9BcjKJoRvxAH1pckqmA==
+  dependencies:
+    postcss "^7.0.2"
+
+postcss-gap-properties@^2.0.0:
+  version "2.0.0"
+  resolved "https://registry.yarnpkg.com/postcss-gap-properties/-/postcss-gap-properties-2.0.0.tgz#431c192ab3ed96a3c3d09f2ff615960f902c1715"
+  integrity sha512-QZSqDaMgXCHuHTEzMsS2KfVDOq7ZFiknSpkrPJY6jmxbugUPTuSzs/vuE5I3zv0WAS+3vhrlqhijiprnuQfzmg==
+  dependencies:
+    postcss "^7.0.2"
+
+postcss-image-set-function@^3.0.1:
+  version "3.0.1"
+  resolved "https://registry.yarnpkg.com/postcss-image-set-function/-/postcss-image-set-function-3.0.1.tgz#28920a2f29945bed4c3198d7df6496d410d3f288"
+  integrity sha512-oPTcFFip5LZy8Y/whto91L9xdRHCWEMs3e1MdJxhgt4jy2WYXfhkng59fH5qLXSCPN8k4n94p1Czrfe5IOkKUw==
+  dependencies:
+    postcss "^7.0.2"
+    postcss-values-parser "^2.0.0"
+
+postcss-initial@^3.0.0:
+  version "3.0.4"
+  resolved "https://registry.yarnpkg.com/postcss-initial/-/postcss-initial-3.0.4.tgz#9d32069a10531fe2ecafa0b6ac750ee0bc7efc53"
+  integrity sha512-3RLn6DIpMsK1l5UUy9jxQvoDeUN4gP939tDcKUHD/kM8SGSKbFAnvkpFpj3Bhtz3HGk1jWY5ZNWX6mPta5M9fg==
+  dependencies:
+    postcss "^7.0.2"
+
+postcss-lab-function@^2.0.1:
+  version "2.0.1"
+  resolved "https://registry.yarnpkg.com/postcss-lab-function/-/postcss-lab-function-2.0.1.tgz#bb51a6856cd12289ab4ae20db1e3821ef13d7d2e"
+  integrity sha512-whLy1IeZKY+3fYdqQFuDBf8Auw+qFuVnChWjmxm/UhHWqNHZx+B99EwxTvGYmUBqe3Fjxs4L1BoZTJmPu6usVg==
+  dependencies:
+    "@csstools/convert-colors" "^1.4.0"
+    postcss "^7.0.2"
+    postcss-values-parser "^2.0.0"
+
+postcss-load-config@^2.0.0:
+  version "2.1.2"
+  resolved "https://registry.yarnpkg.com/postcss-load-config/-/postcss-load-config-2.1.2.tgz#c5ea504f2c4aef33c7359a34de3573772ad7502a"
+  integrity sha512-/rDeGV6vMUo3mwJZmeHfEDvwnTKKqQ0S7OHUi/kJvvtx3aWtyWG2/0ZWnzCt2keEclwN6Tf0DST2v9kITdOKYw==
+  dependencies:
+    cosmiconfig "^5.0.0"
+    import-cwd "^2.0.0"
+
+postcss-loader@3.0.0:
+  version "3.0.0"
+  resolved "https://registry.yarnpkg.com/postcss-loader/-/postcss-loader-3.0.0.tgz#6b97943e47c72d845fa9e03f273773d4e8dd6c2d"
+  integrity sha512-cLWoDEY5OwHcAjDnkyRQzAXfs2jrKjXpO/HQFcc5b5u/r7aa471wdmChmwfnv7x2u840iat/wi0lQ5nbRgSkUA==
+  dependencies:
+    loader-utils "^1.1.0"
+    postcss "^7.0.0"
+    postcss-load-config "^2.0.0"
+    schema-utils "^1.0.0"
+
+postcss-logical@^3.0.0:
+  version "3.0.0"
+  resolved "https://registry.yarnpkg.com/postcss-logical/-/postcss-logical-3.0.0.tgz#2495d0f8b82e9f262725f75f9401b34e7b45d5b5"
+  integrity sha512-1SUKdJc2vuMOmeItqGuNaC+N8MzBWFWEkAnRnLpFYj1tGGa7NqyVBujfRtgNa2gXR+6RkGUiB2O5Vmh7E2RmiA==
+  dependencies:
+    postcss "^7.0.2"
+
+postcss-media-minmax@^4.0.0:
+  version "4.0.0"
+  resolved "https://registry.yarnpkg.com/postcss-media-minmax/-/postcss-media-minmax-4.0.0.tgz#b75bb6cbc217c8ac49433e12f22048814a4f5ed5"
+  integrity sha512-fo9moya6qyxsjbFAYl97qKO9gyre3qvbMnkOZeZwlsW6XYFsvs2DMGDlchVLfAd8LHPZDxivu/+qW2SMQeTHBw==
+  dependencies:
+    postcss "^7.0.2"
+
+postcss-modules-extract-imports@^2.0.0:
+  version "2.0.0"
+  resolved "https://registry.yarnpkg.com/postcss-modules-extract-imports/-/postcss-modules-extract-imports-2.0.0.tgz#818719a1ae1da325f9832446b01136eeb493cd7e"
+  integrity sha512-LaYLDNS4SG8Q5WAWqIJgdHPJrDDr/Lv775rMBFUbgjTz6j34lUznACHcdRWroPvXANP2Vj7yNK57vp9eFqzLWQ==
+  dependencies:
+    postcss "^7.0.5"
+
+postcss-modules-local-by-default@^3.0.2:
+  version "3.0.3"
+  resolved "https://registry.yarnpkg.com/postcss-modules-local-by-default/-/postcss-modules-local-by-default-3.0.3.tgz#bb14e0cc78279d504dbdcbfd7e0ca28993ffbbb0"
+  integrity sha512-e3xDq+LotiGesympRlKNgaJ0PCzoUIdpH0dj47iWAui/kyTgh3CiAr1qP54uodmJhl6p9rN6BoNcdEDVJx9RDw==
+  dependencies:
+    icss-utils "^4.1.1"
+    postcss "^7.0.32"
+    postcss-selector-parser "^6.0.2"
+    postcss-value-parser "^4.1.0"
+
+postcss-modules-scope@^2.1.1:
+  version "2.2.0"
+  resolved "https://registry.yarnpkg.com/postcss-modules-scope/-/postcss-modules-scope-2.2.0.tgz#385cae013cc7743f5a7d7602d1073a89eaae62ee"
+  integrity sha512-YyEgsTMRpNd+HmyC7H/mh3y+MeFWevy7V1evVhJWewmMbjDHIbZbOXICC2y+m1xI1UVfIT1HMW/O04Hxyu9oXQ==
+  dependencies:
+    postcss "^7.0.6"
+    postcss-selector-parser "^6.0.0"
+
+postcss-modules-values@^3.0.0:
+  version "3.0.0"
+  resolved "https://registry.yarnpkg.com/postcss-modules-values/-/postcss-modules-values-3.0.0.tgz#5b5000d6ebae29b4255301b4a3a54574423e7f10"
+  integrity sha512-1//E5jCBrZ9DmRX+zCtmQtRSV6PV42Ix7Bzj9GbwJceduuf7IqP8MgeTXuRDHOWj2m0VzZD5+roFWDuU8RQjcg==
+  dependencies:
+    icss-utils "^4.0.0"
+    postcss "^7.0.6"
+
+postcss-nesting@^7.0.0:
+  version "7.0.1"
+  resolved "https://registry.yarnpkg.com/postcss-nesting/-/postcss-nesting-7.0.1.tgz#b50ad7b7f0173e5b5e3880c3501344703e04c052"
+  integrity sha512-FrorPb0H3nuVq0Sff7W2rnc3SmIcruVC6YwpcS+k687VxyxO33iE1amna7wHuRVzM8vfiYofXSBHNAZ3QhLvYg==
+  dependencies:
+    postcss "^7.0.2"
+
+postcss-overflow-shorthand@^2.0.0:
+  version "2.0.0"
+  resolved "https://registry.yarnpkg.com/postcss-overflow-shorthand/-/postcss-overflow-shorthand-2.0.0.tgz#31ecf350e9c6f6ddc250a78f0c3e111f32dd4c30"
+  integrity sha512-aK0fHc9CBNx8jbzMYhshZcEv8LtYnBIRYQD5i7w/K/wS9c2+0NSR6B3OVMu5y0hBHYLcMGjfU+dmWYNKH0I85g==
+  dependencies:
+    postcss "^7.0.2"
+
+postcss-page-break@^2.0.0:
+  version "2.0.0"
+  resolved "https://registry.yarnpkg.com/postcss-page-break/-/postcss-page-break-2.0.0.tgz#add52d0e0a528cabe6afee8b46e2abb277df46bf"
+  integrity sha512-tkpTSrLpfLfD9HvgOlJuigLuk39wVTbbd8RKcy8/ugV2bNBUW3xU+AIqyxhDrQr1VUj1RmyJrBn1YWrqUm9zAQ==
+  dependencies:
+    postcss "^7.0.2"
+
+postcss-place@^4.0.1:
+  version "4.0.1"
+  resolved "https://registry.yarnpkg.com/postcss-place/-/postcss-place-4.0.1.tgz#e9f39d33d2dc584e46ee1db45adb77ca9d1dcc62"
+  integrity sha512-Zb6byCSLkgRKLODj/5mQugyuj9bvAAw9LqJJjgwz5cYryGeXfFZfSXoP1UfveccFmeq0b/2xxwcTEVScnqGxBg==
+  dependencies:
+    postcss "^7.0.2"
+    postcss-values-parser "^2.0.0"
+
+postcss-preset-env@6.7.0:
+  version "6.7.0"
+  resolved "https://registry.yarnpkg.com/postcss-preset-env/-/postcss-preset-env-6.7.0.tgz#c34ddacf8f902383b35ad1e030f178f4cdf118a5"
+  integrity sha512-eU4/K5xzSFwUFJ8hTdTQzo2RBLbDVt83QZrAvI07TULOkmyQlnYlpwep+2yIK+K+0KlZO4BvFcleOCCcUtwchg==
+  dependencies:
+    autoprefixer "^9.6.1"
+    browserslist "^4.6.4"
+    caniuse-lite "^1.0.30000981"
+    css-blank-pseudo "^0.1.4"
+    css-has-pseudo "^0.10.0"
+    css-prefers-color-scheme "^3.1.1"
+    cssdb "^4.4.0"
+    postcss "^7.0.17"
+    postcss-attribute-case-insensitive "^4.0.1"
+    postcss-color-functional-notation "^2.0.1"
+    postcss-color-gray "^5.0.0"
+    postcss-color-hex-alpha "^5.0.3"
+    postcss-color-mod-function "^3.0.3"
+    postcss-color-rebeccapurple "^4.0.1"
+    postcss-custom-media "^7.0.8"
+    postcss-custom-properties "^8.0.11"
+    postcss-custom-selectors "^5.1.2"
+    postcss-dir-pseudo-class "^5.0.0"
+    postcss-double-position-gradients "^1.0.0"
+    postcss-env-function "^2.0.2"
+    postcss-focus-visible "^4.0.0"
+    postcss-focus-within "^3.0.0"
+    postcss-font-variant "^4.0.0"
+    postcss-gap-properties "^2.0.0"
+    postcss-image-set-function "^3.0.1"
+    postcss-initial "^3.0.0"
+    postcss-lab-function "^2.0.1"
+    postcss-logical "^3.0.0"
+    postcss-media-minmax "^4.0.0"
+    postcss-nesting "^7.0.0"
+    postcss-overflow-shorthand "^2.0.0"
+    postcss-page-break "^2.0.0"
+    postcss-place "^4.0.1"
+    postcss-pseudo-class-any-link "^6.0.0"
+    postcss-replace-overflow-wrap "^3.0.0"
+    postcss-selector-matches "^4.0.0"
+    postcss-selector-not "^4.0.0"
+
+postcss-pseudo-class-any-link@^6.0.0:
+  version "6.0.0"
+  resolved "https://registry.yarnpkg.com/postcss-pseudo-class-any-link/-/postcss-pseudo-class-any-link-6.0.0.tgz#2ed3eed393b3702879dec4a87032b210daeb04d1"
+  integrity sha512-lgXW9sYJdLqtmw23otOzrtbDXofUdfYzNm4PIpNE322/swES3VU9XlXHeJS46zT2onFO7V1QFdD4Q9LiZj8mew==
+  dependencies:
+    postcss "^7.0.2"
+    postcss-selector-parser "^5.0.0-rc.3"
+
+postcss-replace-overflow-wrap@^3.0.0:
+  version "3.0.0"
+  resolved "https://registry.yarnpkg.com/postcss-replace-overflow-wrap/-/postcss-replace-overflow-wrap-3.0.0.tgz#61b360ffdaedca84c7c918d2b0f0d0ea559ab01c"
+  integrity sha512-2T5hcEHArDT6X9+9dVSPQdo7QHzG4XKclFT8rU5TzJPDN7RIRTbO9c4drUISOVemLj03aezStHCR2AIcr8XLpw==
+  dependencies:
+    postcss "^7.0.2"
+
+postcss-selector-matches@^4.0.0:
+  version "4.0.0"
+  resolved "https://registry.yarnpkg.com/postcss-selector-matches/-/postcss-selector-matches-4.0.0.tgz#71c8248f917ba2cc93037c9637ee09c64436fcff"
+  integrity sha512-LgsHwQR/EsRYSqlwdGzeaPKVT0Ml7LAT6E75T8W8xLJY62CE4S/l03BWIt3jT8Taq22kXP08s2SfTSzaraoPww==
+  dependencies:
+    balanced-match "^1.0.0"
+    postcss "^7.0.2"
+
+postcss-selector-not@^4.0.0:
+  version "4.0.1"
+  resolved "https://registry.yarnpkg.com/postcss-selector-not/-/postcss-selector-not-4.0.1.tgz#263016eef1cf219e0ade9a913780fc1f48204cbf"
+  integrity sha512-YolvBgInEK5/79C+bdFMyzqTg6pkYqDbzZIST/PDMqa/o3qtXenD05apBG2jLgT0/BQ77d4U2UK12jWpilqMAQ==
+  dependencies:
+    balanced-match "^1.0.0"
+    postcss "^7.0.2"
+
+postcss-selector-parser@^5.0.0-rc.3, postcss-selector-parser@^5.0.0-rc.4:
+  version "5.0.0"
+  resolved "https://registry.yarnpkg.com/postcss-selector-parser/-/postcss-selector-parser-5.0.0.tgz#249044356697b33b64f1a8f7c80922dddee7195c"
+  integrity sha512-w+zLE5Jhg6Liz8+rQOWEAwtwkyqpfnmsinXjXg6cY7YIONZZtgvE0v2O0uhQBs0peNomOJwWRKt6JBfTdTd3OQ==
+  dependencies:
+    cssesc "^2.0.0"
+    indexes-of "^1.0.1"
+    uniq "^1.0.1"
+
+postcss-selector-parser@^6.0.0, postcss-selector-parser@^6.0.2:
+  version "6.0.6"
+  resolved "https://registry.yarnpkg.com/postcss-selector-parser/-/postcss-selector-parser-6.0.6.tgz#2c5bba8174ac2f6981ab631a42ab0ee54af332ea"
+  integrity sha512-9LXrvaaX3+mcv5xkg5kFwqSzSH1JIObIx51PrndZwlmznwXRfxMddDvo9gve3gVR8ZTKgoFDdWkbRFmEhT4PMg==
+  dependencies:
+    cssesc "^3.0.0"
+    util-deprecate "^1.0.2"
+
 postcss-value-parser@^3.3.0:
   version "3.3.1"
   resolved "https://registry.yarnpkg.com/postcss-value-parser/-/postcss-value-parser-3.3.1.tgz#9ff822547e2893213cf1c30efa51ac5fd1ba8281"
   integrity sha512-pISE66AbVkp4fDQ7VHBwRNXzAAKJjw4Vw7nWI/+Q3vuly7SNfgYXvm6i5IgFylHGK5sP/xHAbB7N49OS4gWNyQ==
 
-private@^0.1.6:
+postcss-value-parser@^4.0.2, postcss-value-parser@^4.1.0:
+  version "4.1.0"
+  resolved "https://registry.yarnpkg.com/postcss-value-parser/-/postcss-value-parser-4.1.0.tgz#443f6a20ced6481a2bda4fa8532a6e55d789a2cb"
+  integrity sha512-97DXOFbQJhk71ne5/Mt6cOu6yxsSfM0QGQyl0L25Gca4yGWEGJaig7l7gbCX623VqTBNGLRLaVUCnNkcedlRSQ==
+
+postcss-values-parser@^2.0.0, postcss-values-parser@^2.0.1:
+  version "2.0.1"
+  resolved "https://registry.yarnpkg.com/postcss-values-parser/-/postcss-values-parser-2.0.1.tgz#da8b472d901da1e205b47bdc98637b9e9e550e5f"
+  integrity sha512-2tLuBsA6P4rYTNKCXYG/71C7j1pU6pK503suYOmn4xYrQIzW+opD+7FAFNuGSdZC/3Qfy334QbeMu7MEb8gOxg==
+  dependencies:
+    flatten "^1.0.2"
+    indexes-of "^1.0.1"
+    uniq "^1.0.1"
+
+postcss@7.0.21:
+  version "7.0.21"
+  resolved "https://registry.yarnpkg.com/postcss/-/postcss-7.0.21.tgz#06bb07824c19c2021c5d056d5b10c35b989f7e17"
+  integrity sha512-uIFtJElxJo29QC753JzhidoAhvp/e/Exezkdhfmt8AymWT6/5B7W1WmponYWkHk2eg6sONyTch0A3nkMPun3SQ==
+  dependencies:
+    chalk "^2.4.2"
+    source-map "^0.6.1"
+    supports-color "^6.1.0"
+
+postcss@^7.0.0, postcss@^7.0.14, postcss@^7.0.17, postcss@^7.0.18, postcss@^7.0.2, postcss@^7.0.23, postcss@^7.0.26, postcss@^7.0.32, postcss@^7.0.5, postcss@^7.0.6:
+  version "7.0.36"
+  resolved "https://registry.yarnpkg.com/postcss/-/postcss-7.0.36.tgz#056f8cffa939662a8f5905950c07d5285644dfcb"
+  integrity sha512-BebJSIUMwJHRH0HAQoxN4u1CN86glsrwsW0q7T+/m44eXOUAxSNdHRkNZPYz5vVUbg17hFgOQDE7fZk7li3pZw==
+  dependencies:
+    chalk "^2.4.2"
+    source-map "^0.6.1"
+    supports-color "^6.1.0"
+
+prepend-http@^1.0.0:
+  version "1.0.4"
+  resolved "https://registry.yarnpkg.com/prepend-http/-/prepend-http-1.0.4.tgz#d4f4562b0ce3696e41ac52d0e002e57a635dc6dc"
+  integrity sha1-1PRWKwzjaW5BrFLQ4ALlemNdxtw=
+
+private@^0.1.8:
   version "0.1.8"
   resolved "https://registry.yarnpkg.com/private/-/private-0.1.8.tgz#2381edb3689f7a53d653190060fcf822d2f368ff"
   integrity sha512-VvivMrbvd2nKkiG38qjULzlc+4Vx4wm/whI9pQD35YrARNnhxeiRktSOhSukRLFNlzg6Br/cJPet5J/u19r/mg==
 
 process-nextick-args@~2.0.0:
   version "2.0.1"
   resolved "https://registry.yarnpkg.com/process-nextick-args/-/process-nextick-args-2.0.1.tgz#7820d9b16120cc55ca9ae7792680ae7dba6d7fe2"
@@ -3703,18 +5196,18 @@
   integrity sha1-czIwDoQBYb2j5podHZGn1LwW8YI=
 
 promise-inflight@^1.0.1:
   version "1.0.1"
   resolved "https://registry.yarnpkg.com/promise-inflight/-/promise-inflight-1.0.1.tgz#98472870bf228132fcbdd868129bad12c3c029e3"
   integrity sha1-mEcocL8igTL8vdhoEputEsPAKeM=
 
-promise@7.1.1:
-  version "7.1.1"
-  resolved "https://registry.yarnpkg.com/promise/-/promise-7.1.1.tgz#489654c692616b8aa55b0724fa809bb7db49c5bf"
-  integrity sha1-SJZUxpJha4qlWwck+oCbt9tJxb8=
+promise@8.0.1:
+  version "8.0.1"
+  resolved "https://registry.yarnpkg.com/promise/-/promise-8.0.1.tgz#e45d68b00a17647b6da711bf85ed6ed47208f450"
+  integrity sha1-5F1osAoXZHttpxG/he1u1HII9FA=
   dependencies:
     asap "~2.0.3"
 
 prop-types-exact@1.2.0:
   version "1.2.0"
   resolved "https://registry.yarnpkg.com/prop-types-exact/-/prop-types-exact-1.2.0.tgz#825d6be46094663848237e3925a98c6e944e9869"
   integrity sha512-K+Tk3Kd9V0odiXFP9fwDHUYRyvK3Nun3GVyPapSIs5OBkITAm15W0CPFD/YKTkMUAbc0b9CUwRQp2ybiBIq+eA==
@@ -3785,25 +5278,33 @@
   integrity sha1-wNWmOycYgArY4esPpSachN1BhF4=
 
 punycode@^2.1.0:
   version "2.1.1"
   resolved "https://registry.yarnpkg.com/punycode/-/punycode-2.1.1.tgz#b58b010ac40c22c5657616c8d2c2c02c7bf479ec"
   integrity sha512-XRsRjdf+j5ml+y/6GKHPZbrF/8p2Yga0JPtdqTIY2Xe5ohJPD9saDJJLPvp9+NSBprVvevdXZybnj2cv8OEd0A==
 
+query-string@^4.1.0:
+  version "4.3.4"
+  resolved "https://registry.yarnpkg.com/query-string/-/query-string-4.3.4.tgz#bbb693b9ca915c232515b228b1a02b609043dbeb"
+  integrity sha1-u7aTucqRXCMlFbIosaArYJBD2+s=
+  dependencies:
+    object-assign "^4.1.0"
+    strict-uri-encode "^1.0.0"
+
 querystring-es3@^0.2.0:
   version "0.2.1"
   resolved "https://registry.yarnpkg.com/querystring-es3/-/querystring-es3-0.2.1.tgz#9ec61f79049875707d69414596fd907a4d711e73"
   integrity sha1-nsYfeQSYdXB9aUFFlv2Qek1xHnM=
 
 querystring@0.2.0, querystring@^0.2.0:
   version "0.2.0"
   resolved "https://registry.yarnpkg.com/querystring/-/querystring-0.2.0.tgz#b209849203bb25df820da756e747005878521620"
   integrity sha1-sgmEkgO7Jd+CDadW50cAWHhSFiA=
 
-randombytes@^2.0.0, randombytes@^2.0.1, randombytes@^2.0.5:
+randombytes@^2.0.0, randombytes@^2.0.1, randombytes@^2.0.5, randombytes@^2.1.0:
   version "2.1.0"
   resolved "https://registry.yarnpkg.com/randombytes/-/randombytes-2.1.0.tgz#df6f84372f0270dc65cdf6291349ab7a473d4f2a"
   integrity sha512-vYl3iOX+4CKUWuxGi9Ukhie6fsqXqS9FE2Zaic4tNFD2N2QQaXOMFbuKK4QmDHC0JO6B1Zp41J0LpT0oR68amQ==
   dependencies:
     safe-buffer "^5.1.0"
 
 randomfill@^1.0.3:
@@ -3900,80 +5401,133 @@
   resolved "https://registry.yarnpkg.com/readdirp/-/readdirp-2.2.1.tgz#0e87622a3325aa33e892285caf8b4e846529a525"
   integrity sha512-1JU/8q+VgFZyxwrJ+SVIOsh+KywWGpds3NTqikiKpDMZWScmAYyKIgqkO+ARvNWJfXeXR1zxz7aHF4u4CyH6vQ==
   dependencies:
     graceful-fs "^4.1.11"
     micromatch "^3.1.10"
     readable-stream "^2.0.2"
 
+readdirp@~3.6.0:
+  version "3.6.0"
+  resolved "https://registry.yarnpkg.com/readdirp/-/readdirp-3.6.0.tgz#74a370bd857116e245b29cc97340cd431a02a6c7"
+  integrity sha512-hOS089on8RduqdbhvQ5Z37A0ESjsqz6qnRcffsMU3495FuTdqSm+7bhJ29JvIOsBDEEnan5DPu9t3To9VRlMzA==
+  dependencies:
+    picomatch "^2.2.1"
+
+recast@0.18.5:
+  version "0.18.5"
+  resolved "https://registry.yarnpkg.com/recast/-/recast-0.18.5.tgz#9d5adbc07983a3c8145f3034812374a493e0fe4d"
+  integrity sha512-sD1WJrpLQAkXGyQZyGzTM75WJvyAd98II5CHdK3IYbt/cZlU0UzCRVU11nUFNXX9fBVEt4E9ajkMjBlUlG+Oog==
+  dependencies:
+    ast-types "0.13.2"
+    esprima "~4.0.0"
+    private "^0.1.8"
+    source-map "~0.6.1"
+
 reflect.ownkeys@^0.2.0:
   version "0.2.0"
   resolved "https://registry.yarnpkg.com/reflect.ownkeys/-/reflect.ownkeys-0.2.0.tgz#749aceec7f3fdf8b63f927a04809e90c5c0b3460"
   integrity sha1-dJrO7H8/34tj+SegSAnpDFwLNGA=
 
 regenerate-unicode-properties@^8.1.0:
   version "8.1.0"
   resolved "https://registry.yarnpkg.com/regenerate-unicode-properties/-/regenerate-unicode-properties-8.1.0.tgz#ef51e0f0ea4ad424b77bf7cb41f3e015c70a3f0e"
   integrity sha512-LGZzkgtLY79GeXLm8Dp0BVLdQlWICzBnJz/ipWUgo59qBaZ+BHtq51P2q1uVZlppMuUAT37SDk39qUbjTWB7bA==
   dependencies:
     regenerate "^1.4.0"
 
+regenerate-unicode-properties@^8.2.0:
+  version "8.2.0"
+  resolved "https://registry.yarnpkg.com/regenerate-unicode-properties/-/regenerate-unicode-properties-8.2.0.tgz#e5de7111d655e7ba60c057dbe9ff37c87e65cdec"
+  integrity sha512-F9DjY1vKLo/tPePDycuH3dn9H1OTPIkVD9Kz4LODu+F2C75mgjAJ7x/gwy6ZcSNRAAkhNlJSOHRe8k3p+K9WhA==
+  dependencies:
+    regenerate "^1.4.0"
+
 regenerate@^1.4.0:
   version "1.4.0"
   resolved "https://registry.yarnpkg.com/regenerate/-/regenerate-1.4.0.tgz#4a856ec4b56e4077c557589cae85e7a4c8869a11"
   integrity sha512-1G6jJVDWrt0rK99kBjvEtziZNCICAuvIPkSiUFIQxVP06RCVpq3dmDo2oi6ABpYaDYaTRr67BEhL8r1wgEZZKg==
 
-regenerator-runtime@^0.11.0:
-  version "0.11.1"
-  resolved "https://registry.yarnpkg.com/regenerator-runtime/-/regenerator-runtime-0.11.1.tgz#be05ad7f9bf7d22e056f9726cee5017fbf19e2e9"
-  integrity sha512-MguG95oij0fC3QV3URf4V2SDYGJhJnJGqvIIgdECeODCT98wSWDAJ94SSuVpYQUoTcGUIL6L4yNB7j1DFFHSBg==
-
 regenerator-runtime@^0.13.2:
   version "0.13.3"
   resolved "https://registry.yarnpkg.com/regenerator-runtime/-/regenerator-runtime-0.13.3.tgz#7cf6a77d8f5c6f60eb73c5fc1955b2ceb01e6bf5"
   integrity sha512-naKIZz2GQ8JWh///G7L3X6LaQUAMp2lvb1rvwwsURe/VXwD6VMfr+/1NuNw3ag8v2kY1aQ/go5SNn79O9JU7yw==
 
-regenerator-transform@^0.14.0:
-  version "0.14.1"
-  resolved "https://registry.yarnpkg.com/regenerator-transform/-/regenerator-transform-0.14.1.tgz#3b2fce4e1ab7732c08f665dfdb314749c7ddd2fb"
-  integrity sha512-flVuee02C3FKRISbxhXl9mGzdbWUVHubl1SMaknjxkFB1/iqpJhArQUvRxOOPEc/9tAiX0BaQ28FJH10E4isSQ==
+regenerator-runtime@^0.13.4:
+  version "0.13.9"
+  resolved "https://registry.yarnpkg.com/regenerator-runtime/-/regenerator-runtime-0.13.9.tgz#8925742a98ffd90814988d7566ad30ca3b263b52"
+  integrity sha512-p3VT+cOEgxFsRRA9X4lkI1E+k2/CtnKtU4gcxyaCUreilL/vqI6CdZ3wxVUx3UOUg+gnUOQQcRI7BmSI656MYA==
+
+regenerator-transform@^0.14.2:
+  version "0.14.5"
+  resolved "https://registry.yarnpkg.com/regenerator-transform/-/regenerator-transform-0.14.5.tgz#c98da154683671c9c4dcb16ece736517e1b7feb4"
+  integrity sha512-eOf6vka5IO151Jfsw2NO9WpGX58W6wWmefK3I1zEGr0lOD0u8rwPaNqQL1aRxUaxLeKO3ArNh3VYg1KbaD+FFw==
   dependencies:
-    private "^0.1.6"
+    "@babel/runtime" "^7.8.4"
 
 regex-not@^1.0.0, regex-not@^1.0.2:
   version "1.0.2"
   resolved "https://registry.yarnpkg.com/regex-not/-/regex-not-1.0.2.tgz#1f4ece27e00b0b65e0247a6810e6a85d83a5752c"
   integrity sha512-J6SDjUgDxQj5NusnOtdFxDwN/+HWykR8GELwctJ7mdqhcyy1xEc4SRFHUXvxTp661YaVKAjfRLZ9cCqS6tn32A==
   dependencies:
     extend-shallow "^3.0.2"
     safe-regex "^1.1.0"
 
+regex-parser@2.2.10:
+  version "2.2.10"
+  resolved "https://registry.yarnpkg.com/regex-parser/-/regex-parser-2.2.10.tgz#9e66a8f73d89a107616e63b39d4deddfee912b37"
+  integrity sha512-8t6074A68gHfU8Neftl0Le6KTDwfGAj7IyjPIMSfikI2wJUTHDMaIq42bUsfVnj8mhx0R+45rdUXHGpN164avA==
+
 regexpu-core@^4.6.0:
   version "4.6.0"
   resolved "https://registry.yarnpkg.com/regexpu-core/-/regexpu-core-4.6.0.tgz#2037c18b327cfce8a6fea2a4ec441f2432afb8b6"
   integrity sha512-YlVaefl8P5BnFYOITTNzDvan1ulLOiXJzCNZxduTIosN17b87h3bvG9yHMoHaRuo88H4mQ06Aodj5VtYGGGiTg==
   dependencies:
     regenerate "^1.4.0"
     regenerate-unicode-properties "^8.1.0"
     regjsgen "^0.5.0"
     regjsparser "^0.6.0"
     unicode-match-property-ecmascript "^1.0.4"
     unicode-match-property-value-ecmascript "^1.1.0"
 
+regexpu-core@^4.7.1:
+  version "4.7.1"
+  resolved "https://registry.yarnpkg.com/regexpu-core/-/regexpu-core-4.7.1.tgz#2dea5a9a07233298fbf0db91fa9abc4c6e0f8ad6"
+  integrity sha512-ywH2VUraA44DZQuRKzARmw6S66mr48pQVva4LBeRhcOltJ6hExvWly5ZjFLYo67xbIxb6W1q4bAGtgfEl20zfQ==
+  dependencies:
+    regenerate "^1.4.0"
+    regenerate-unicode-properties "^8.2.0"
+    regjsgen "^0.5.1"
+    regjsparser "^0.6.4"
+    unicode-match-property-ecmascript "^1.0.4"
+    unicode-match-property-value-ecmascript "^1.2.0"
+
 regjsgen@^0.5.0:
   version "0.5.0"
   resolved "https://registry.yarnpkg.com/regjsgen/-/regjsgen-0.5.0.tgz#a7634dc08f89209c2049adda3525711fb97265dd"
   integrity sha512-RnIrLhrXCX5ow/E5/Mh2O4e/oa1/jW0eaBKTSy3LaCj+M3Bqvm97GWDp2yUtzIs4LEn65zR2yiYGFqb2ApnzDA==
 
+regjsgen@^0.5.1:
+  version "0.5.2"
+  resolved "https://registry.yarnpkg.com/regjsgen/-/regjsgen-0.5.2.tgz#92ff295fb1deecbf6ecdab2543d207e91aa33733"
+  integrity sha512-OFFT3MfrH90xIW8OOSyUrk6QHD5E9JOTeGodiJeBS3J6IwlgzJMNE/1bZklWz5oTg+9dCMyEetclvCVXOPoN3A==
+
 regjsparser@^0.6.0:
   version "0.6.0"
   resolved "https://registry.yarnpkg.com/regjsparser/-/regjsparser-0.6.0.tgz#f1e6ae8b7da2bae96c99399b868cd6c933a2ba9c"
   integrity sha512-RQ7YyokLiQBomUJuUG8iGVvkgOLxwyZM8k6d3q5SAXpg4r5TZJZigKFvC6PpD+qQ98bCDC5YelPeA3EucDoNeQ==
   dependencies:
     jsesc "~0.5.0"
 
+regjsparser@^0.6.4:
+  version "0.6.9"
+  resolved "https://registry.yarnpkg.com/regjsparser/-/regjsparser-0.6.9.tgz#b489eef7c9a2ce43727627011429cf833a7183e6"
+  integrity sha512-ZqbNRz1SNjLAiYuwY0zoXW8Ne675IX5q+YHioAGbCw4X96Mjl2+dcX9B2ciaeyYjViDAfvIjFpQjJgLttTEERQ==
+  dependencies:
+    jsesc "~0.5.0"
+
 remove-trailing-separator@^1.0.1:
   version "1.1.0"
   resolved "https://registry.yarnpkg.com/remove-trailing-separator/-/remove-trailing-separator-1.1.0.tgz#c24bce2a283adad5bc3f58e0d48249b92379d8ef"
   integrity sha1-wkvOKig62tW8P1jg1IJJuSN52O8=
 
 repeat-element@^1.1.2:
   version "1.1.3"
@@ -3981,14 +5535,40 @@
   integrity sha512-ahGq0ZnV5m5XtZLMb+vP76kcAM5nkLqk0lpqAuojSKGgQtn4eRi4ZZGm2olo2zKFH+sMsWaqOCW1dqAnOru72g==
 
 repeat-string@^1.6.1:
   version "1.6.1"
   resolved "https://registry.yarnpkg.com/repeat-string/-/repeat-string-1.6.1.tgz#8dcae470e1c88abc2d600fff4a776286da75e637"
   integrity sha1-jcrkcOHIirwtYA//Sndihtp15jc=
 
+requires-port@^1.0.0:
+  version "1.0.0"
+  resolved "https://registry.yarnpkg.com/requires-port/-/requires-port-1.0.0.tgz#925d2601d39ac485e091cf0da5c6e694dc3dcaff"
+  integrity sha1-kl0mAdOaxIXgkc8NpcbmlNw9yv8=
+
+resolve-from@^3.0.0:
+  version "3.0.0"
+  resolved "https://registry.yarnpkg.com/resolve-from/-/resolve-from-3.0.0.tgz#b22c7af7d9d6881bc8b6e653335eebcb0a188748"
+  integrity sha1-six699nWiBvItuZTM17rywoYh0g=
+
+resolve-url-loader@3.1.1:
+  version "3.1.1"
+  resolved "https://registry.yarnpkg.com/resolve-url-loader/-/resolve-url-loader-3.1.1.tgz#28931895fa1eab9be0647d3b2958c100ae3c0bf0"
+  integrity sha512-K1N5xUjj7v0l2j/3Sgs5b8CjrrgtC70SmdCuZiJ8tSyb5J+uk3FoeZ4b7yTnH6j7ngI+Bc5bldHJIa8hYdu2gQ==
+  dependencies:
+    adjust-sourcemap-loader "2.0.0"
+    camelcase "5.3.1"
+    compose-function "3.0.3"
+    convert-source-map "1.7.0"
+    es6-iterator "2.0.3"
+    loader-utils "1.2.3"
+    postcss "7.0.21"
+    rework "1.0.1"
+    rework-visit "1.0.0"
+    source-map "0.6.1"
+
 resolve-url@^0.2.1:
   version "0.2.1"
   resolved "https://registry.yarnpkg.com/resolve-url/-/resolve-url-0.2.1.tgz#2c637fe77c893afd2a663fe21aa9080068e2052a"
   integrity sha1-LGN/53yJOv0qZj/iGqkIAGjiBSo=
 
 resolve@^1.10.0, resolve@^1.3.2, resolve@^1.8.1:
   version "1.12.0"
@@ -4011,14 +5591,27 @@
   integrity sha512-TTlYpa+OL+vMMNG24xSlQGEJ3B/RzEfUlLct7b5G/ytav+wPrplCpVMFuwzXbkecJrb6IYo1iFb0S9v37754mg==
 
 retry@0.12.0:
   version "0.12.0"
   resolved "https://registry.yarnpkg.com/retry/-/retry-0.12.0.tgz#1b42a6266a21f07421d1b0b54b7dc167b01c013b"
   integrity sha1-G0KmJmoh8HQh0bC1S33BZ7AcATs=
 
+rework-visit@1.0.0:
+  version "1.0.0"
+  resolved "https://registry.yarnpkg.com/rework-visit/-/rework-visit-1.0.0.tgz#9945b2803f219e2f7aca00adb8bc9f640f842c9a"
+  integrity sha1-mUWygD8hni96ygCtuLyfZA+ELJo=
+
+rework@1.0.1:
+  version "1.0.1"
+  resolved "https://registry.yarnpkg.com/rework/-/rework-1.0.1.tgz#30806a841342b54510aa4110850cd48534144aa7"
+  integrity sha1-MIBqhBNCtUUQqkEQhQzUhTQUSqc=
+  dependencies:
+    convert-source-map "^0.3.3"
+    css "^2.0.0"
+
 rimraf@^2.2.8, rimraf@^2.5.4, rimraf@^2.6.1, rimraf@^2.6.3:
   version "2.7.1"
   resolved "https://registry.yarnpkg.com/rimraf/-/rimraf-2.7.1.tgz#35797f13a7fdadc566142c29d4f07ccad483e3ec"
   integrity sha512-uWjbaKIK3T1OSVptzX7Nl6PvQ3qAGtKEtVRjRuazjfL3Bx5eI409VZSqgND+4UNnmzLVdPj9FqFJNPqBZFve4w==
   dependencies:
     glob "^7.1.3"
 
@@ -4055,14 +5648,25 @@
     ret "~0.1.10"
 
 "safer-buffer@>= 2.1.2 < 3":
   version "2.1.2"
   resolved "https://registry.yarnpkg.com/safer-buffer/-/safer-buffer-2.1.2.tgz#44fa161b0187b9549dd84bb91802f9bd8385cd6a"
   integrity sha512-YZo3K82SD7Riyi0E1EQPojLz7kpepnSQI9IyPbHHg1XXXevb5dJI7tpyN2ADxGcQbHG7vcyRHk0cbwqcQriUtg==
 
+sass-loader@8.0.2:
+  version "8.0.2"
+  resolved "https://registry.yarnpkg.com/sass-loader/-/sass-loader-8.0.2.tgz#debecd8c3ce243c76454f2e8290482150380090d"
+  integrity sha512-7o4dbSK8/Ol2KflEmSco4jTjQoV988bM82P9CZdmo9hR3RLnvNc0ufMNdMrB0caq38JQ/FgF4/7RcbcfKzxoFQ==
+  dependencies:
+    clone-deep "^4.0.1"
+    loader-utils "^1.2.3"
+    neo-async "^2.6.1"
+    schema-utils "^2.6.1"
+    semver "^6.3.0"
+
 sax@^1.2.4:
   version "1.2.4"
   resolved "https://registry.yarnpkg.com/sax/-/sax-1.2.4.tgz#2816234e2378bddc4e5354fab5caa895df7100d9"
   integrity sha512-NqVDv9TpANUjFm0N8uM5GxL36UgKi9/atZw+x7YFnQ8ckwFGKrl4xX4yWtrey3UJm5nP1kUbnYgLopqWNSRhWw==
 
 scheduler@^0.15.0:
   version "0.15.0"
@@ -4077,14 +5681,23 @@
   resolved "https://registry.yarnpkg.com/schema-utils/-/schema-utils-1.0.0.tgz#0b79a93204d7b600d4b2850d1f66c2a34951c770"
   integrity sha512-i27Mic4KovM/lnGsy8whRCHhc7VicJajAjTrYg11K9zfZXnYIt4k5F+kZkwjnrhKzLic/HLU4j11mjsz2G/75g==
   dependencies:
     ajv "^6.1.0"
     ajv-errors "^1.0.0"
     ajv-keywords "^3.1.0"
 
+schema-utils@^2.0.0, schema-utils@^2.0.1, schema-utils@^2.6.0, schema-utils@^2.6.1:
+  version "2.7.1"
+  resolved "https://registry.yarnpkg.com/schema-utils/-/schema-utils-2.7.1.tgz#1ca4f32d1b24c590c203b8e7a50bf0ea4cd394d7"
+  integrity sha512-SHiNtMOUGWBQJwzISiVYKu82GiV4QYGePp3odlY1tuKO7gPtphAT5R/py0fA6xtbgLL/RvtJZnU9b8s0F1q0Xg==
+  dependencies:
+    "@types/json-schema" "^7.0.5"
+    ajv "^6.12.4"
+    ajv-keywords "^3.5.2"
+
 "semver@2 || 3 || 4 || 5", semver@^5.3.0, semver@^5.4.1, semver@^5.5.0, semver@^5.5.1, semver@^5.6.0:
   version "5.7.1"
   resolved "https://registry.yarnpkg.com/semver/-/semver-5.7.1.tgz#a954f931aeba508d307bbf069eff0c01c96116f7"
   integrity sha512-sauaDf/PZdVgrLTNYHRtpXa1iRiKcaebiKQ1BJdpQlWH2lCvexQdX55snPFyK7QzpudqbCI0qXFfOasHdyNDGQ==
 
 semver@^6.0.0, semver@^6.3.0:
   version "6.3.0"
@@ -4106,18 +5719,20 @@
     http-errors "~1.7.2"
     mime "1.6.0"
     ms "2.1.1"
     on-finished "~2.3.0"
     range-parser "~1.2.1"
     statuses "~1.5.0"
 
-serialize-javascript@^1.7.0:
-  version "1.9.1"
-  resolved "https://registry.yarnpkg.com/serialize-javascript/-/serialize-javascript-1.9.1.tgz#cfc200aef77b600c47da9bb8149c943e798c2fdb"
-  integrity sha512-0Vb/54WJ6k5v8sSWN09S0ora+Hnr+cX40r9F170nT+mSkaxltoE/7R3OrIdBSUv1OoiobH1QoWQbCnAO+e8J1A==
+serialize-javascript@^4.0.0:
+  version "4.0.0"
+  resolved "https://registry.yarnpkg.com/serialize-javascript/-/serialize-javascript-4.0.0.tgz#b525e1238489a5ecfc42afacc3fe99e666f4b1aa"
+  integrity sha512-GaNA54380uFefWghODBWEGisLZFj00nS5ACs6yHa9nLqlLpVLO8ChDGeKRjZnV4Nh4n0Qi7nhYZD/9fCPzEqkw==
+  dependencies:
+    randombytes "^2.1.0"
 
 set-blocking@~2.0.0:
   version "2.0.0"
   resolved "https://registry.yarnpkg.com/set-blocking/-/set-blocking-2.0.0.tgz#045f9782d011ae9a6803ddd382b24392b3d890f7"
   integrity sha1-BF+XgtARrppoA93TgrJDkrPYkPc=
 
 set-value@^2.0.0, set-value@^2.0.1:
@@ -4144,14 +5759,21 @@
   version "2.4.11"
   resolved "https://registry.yarnpkg.com/sha.js/-/sha.js-2.4.11.tgz#37a5cf0b81ecbc6943de109ba2960d1b26584ae7"
   integrity sha512-QMEp5B7cftE7APOjk5Y6xgrbWu+WkLVQwk8JNjZ8nKRciZaByEW6MubieAiToS7+dwvrjGhH8jRXz3MVd0AYqQ==
   dependencies:
     inherits "^2.0.1"
     safe-buffer "^5.0.1"
 
+shallow-clone@^3.0.0:
+  version "3.0.1"
+  resolved "https://registry.yarnpkg.com/shallow-clone/-/shallow-clone-3.0.1.tgz#8f2981ad92531f55035b01fb230769a40e02efa3"
+  integrity sha512-/6KqX+GVUdqPuPPd2LxDDxzX6CAbjJehAAOKlNpqqUpAqPM6HeL8f+o3a+JsyGjn2lv0WY8UsTgUJjU9Ok55NA==
+  dependencies:
+    kind-of "^6.0.2"
+
 shell-quote@^1.6.1:
   version "1.7.2"
   resolved "https://registry.yarnpkg.com/shell-quote/-/shell-quote-1.7.2.tgz#67a7d02c76c9da24f99d20808fcaded0e0e04be2"
   integrity sha512-mRz/m/JVscCrkMyPqHc/bczi3OQHkLTqXHEFu0zDhK/qfv3UcOA4SVmRCLmos4bhjr9ekVQubj/R7waKapmiQg==
 
 signal-exit@^3.0.0, signal-exit@^3.0.2:
   version "3.0.2"
@@ -4184,14 +5806,21 @@
     define-property "^0.2.5"
     extend-shallow "^2.0.1"
     map-cache "^0.2.2"
     source-map "^0.5.6"
     source-map-resolve "^0.5.0"
     use "^3.1.0"
 
+sort-keys@^1.0.0:
+  version "1.1.2"
+  resolved "https://registry.yarnpkg.com/sort-keys/-/sort-keys-1.1.2.tgz#441b6d4d346798f1b4e49e8920adfba0e543f9ad"
+  integrity sha1-RBttTTRnmPG05J6JIK37oOVD+a0=
+  dependencies:
+    is-plain-obj "^1.0.0"
+
 source-list-map@^2.0.0:
   version "2.0.1"
   resolved "https://registry.yarnpkg.com/source-list-map/-/source-list-map-2.0.1.tgz#3993bd873bfc48479cca9ea3a547835c7c154b34"
   integrity sha512-qnQ7gVMxGNxsiL4lEuJwe/To8UnK7fAnmbGEEH8RpLouuKbeEm0lhbQVFIrNSuB+G7tVrAlVsZgETT5nljf+Iw==
 
 source-map-resolve@^0.5.0, source-map-resolve@^0.5.2:
   version "0.5.2"
@@ -4200,15 +5829,15 @@
   dependencies:
     atob "^2.1.1"
     decode-uri-component "^0.2.0"
     resolve-url "^0.2.1"
     source-map-url "^0.4.0"
     urix "^0.1.0"
 
-source-map-support@~0.5.10, source-map-support@~0.5.12:
+source-map-support@~0.5.12:
   version "0.5.13"
   resolved "https://registry.yarnpkg.com/source-map-support/-/source-map-support-0.5.13.tgz#31b24a9c2e73c2de85066c0feb7d44767ed52932"
   integrity sha512-SHSKFHadjVA5oR4PPqhtAVdcBWwRYVd6g6cAXnIbRiIwc2EhPrTuKUBdSLvlEKyIP3GCf89fltvcZiP9MMFA1w==
   dependencies:
     buffer-from "^1.0.0"
     source-map "^0.6.0"
 
@@ -4261,18 +5890,23 @@
 split-string@^3.0.1, split-string@^3.0.2:
   version "3.1.0"
   resolved "https://registry.yarnpkg.com/split-string/-/split-string-3.1.0.tgz#7cb09dda3a86585705c64b39a6466038682e8fe2"
   integrity sha512-NzNVhJDYpwceVVii8/Hu6DKfD2G+NrQHlS/V/qgv763EYudVwEcMQNxd2lh+0VrUByXN/oJkl5grOhYWvQUYiw==
   dependencies:
     extend-shallow "^3.0.0"
 
+sprintf-js@~1.0.2:
+  version "1.0.3"
+  resolved "https://registry.yarnpkg.com/sprintf-js/-/sprintf-js-1.0.3.tgz#04e6926f662895354f3dd015203633b857297e2c"
+  integrity sha1-BOaSb2YolTVPPdAVIDYzuFcpfiw=
+
 ssri@^6.0.1:
-  version "6.0.1"
-  resolved "https://registry.yarnpkg.com/ssri/-/ssri-6.0.1.tgz#2a3c41b28dd45b62b63676ecb74001265ae9edd8"
-  integrity sha512-3Wge10hNcT1Kur4PDFwEieXSCMCJs/7WvSACcrMYrNp+b8kDL1/0wJch5Ni2WrtwEa2IO8OsVfeKIciKCDx/QA==
+  version "6.0.2"
+  resolved "https://registry.yarnpkg.com/ssri/-/ssri-6.0.2.tgz#157939134f20464e7301ddba3e90ffa8f7728ac5"
+  integrity sha512-cepbSq/neFK7xB6A50KHN0xHDotYzq58wWCa5LeWqnPrHG8GzfEjO/4O8kpmcGW+oaxkvhEJCWgbgNk4/ZV93Q==
   dependencies:
     figgy-pudding "^3.5.1"
 
 static-extend@^0.1.1:
   version "0.1.2"
   resolved "https://registry.yarnpkg.com/static-extend/-/static-extend-0.1.2.tgz#60809c39cbff55337226fd5e0b520f341f1fb5c6"
   integrity sha1-YICcOcv/VTNyJv1eC1IPNB8ftcY=
@@ -4313,14 +5947,19 @@
     xtend "^4.0.0"
 
 stream-shift@^1.0.0:
   version "1.0.0"
   resolved "https://registry.yarnpkg.com/stream-shift/-/stream-shift-1.0.0.tgz#d5c752825e5367e786f78e18e445ea223a155952"
   integrity sha1-1cdSgl5TZ+eG944Y5EXqIjoVWVI=
 
+strict-uri-encode@^1.0.0:
+  version "1.1.0"
+  resolved "https://registry.yarnpkg.com/strict-uri-encode/-/strict-uri-encode-1.1.0.tgz#279b225df1d582b1f54e65addd4352e18faa0713"
+  integrity sha1-J5siXfHVgrH1TmWt3UNS4Y+qBxM=
+
 string-hash@1.1.3:
   version "1.1.3"
   resolved "https://registry.yarnpkg.com/string-hash/-/string-hash-1.1.3.tgz#e8aafc0ac1855b4666929ed7dd1275df5d6c811b"
   integrity sha1-6Kr8CsGFW0Zmkp7X3RJ1311sgRs=
 
 string-width@^1.0.1:
   version "1.0.2"
@@ -4380,14 +6019,22 @@
   integrity sha1-IzTBjpx1n3vdVv3vfprj1YjmjtM=
 
 strip-json-comments@~2.0.1:
   version "2.0.1"
   resolved "https://registry.yarnpkg.com/strip-json-comments/-/strip-json-comments-2.0.1.tgz#3c531942e908c2697c0ec344858c286c7ca0a60a"
   integrity sha1-PFMZQukIwml8DsNEhYwobHygpgo=
 
+style-loader@1.0.0:
+  version "1.0.0"
+  resolved "https://registry.yarnpkg.com/style-loader/-/style-loader-1.0.0.tgz#1d5296f9165e8e2c85d24eee0b7caf9ec8ca1f82"
+  integrity sha512-B0dOCFwv7/eY31a5PCieNwMgMhVGFe9w+rh7s/Bx8kfFkrth9zfTZquoYvdw8URgiqxObQKcpW51Ugz1HjfdZw==
+  dependencies:
+    loader-utils "^1.2.3"
+    schema-utils "^2.0.1"
+
 styled-components@^4.4.0:
   version "4.4.0"
   resolved "https://registry.yarnpkg.com/styled-components/-/styled-components-4.4.0.tgz#4e381e2dab831d0e6ea431c2840a96323e84e21b"
   integrity sha512-xQ6vTI/0zNjZ1BBDRxyjvBddrxhQ3DxjeCdaLM1lSn5FDnkTOQgRkmWvcUiTajqc5nJqKVl+7sUioMqktD0+Zw==
   dependencies:
     "@babel/helper-module-imports" "^7.0.0"
     "@babel/traverse" "^7.0.0"
@@ -4399,22 +6046,22 @@
     merge-anything "^2.2.4"
     prop-types "^15.5.4"
     react-is "^16.6.0"
     stylis "^3.5.0"
     stylis-rule-sheet "^0.0.10"
     supports-color "^5.5.0"
 
-styled-jsx@3.2.2:
-  version "3.2.2"
-  resolved "https://registry.yarnpkg.com/styled-jsx/-/styled-jsx-3.2.2.tgz#03d02d26725195d17b6a979eb8d7c34761a16bf8"
-  integrity sha512-Xb9TPFY2REShznvHt/fw78wk+nxejTr8poepDeS5fRvkQ7lW49CDIWWGLzzALCLcKBIRFK/1Wi4PDZNetpig4w==
+styled-jsx@3.2.5:
+  version "3.2.5"
+  resolved "https://registry.yarnpkg.com/styled-jsx/-/styled-jsx-3.2.5.tgz#0172a3e13a0d6d8bf09167dcaf32cf7102d932ca"
+  integrity sha512-prEahkYwQHomUljJzXzrFnBmQrSMtWOBbXn8QeEkpfFkqMZQGshxzzp4H8ebBIsbVlHF/3+GSXMnmK/fp7qVYQ==
   dependencies:
+    "@babel/types" "7.8.3"
     babel-plugin-syntax-jsx "6.18.0"
-    babel-types "6.26.0"
-    convert-source-map "1.6.0"
+    convert-source-map "1.7.0"
     loader-utils "1.2.3"
     source-map "0.7.3"
     string-hash "1.1.3"
     stylis "3.5.4"
     stylis-rule-sheet "0.0.10"
 
 stylis-rule-sheet@0.0.10, stylis-rule-sheet@^0.0.10:
@@ -4460,47 +6107,56 @@
     fs-minipass "^1.2.5"
     minipass "^2.8.6"
     minizlib "^1.2.1"
     mkdirp "^0.5.0"
     safe-buffer "^5.1.2"
     yallist "^3.0.3"
 
-terser-webpack-plugin@^1.4.1:
-  version "1.4.1"
-  resolved "https://registry.yarnpkg.com/terser-webpack-plugin/-/terser-webpack-plugin-1.4.1.tgz#61b18e40eaee5be97e771cdbb10ed1280888c2b4"
-  integrity sha512-ZXmmfiwtCLfz8WKZyYUuuHf3dMYEjg8NrjHMb0JqHVHVOSkzp3cW2/XG1fP3tRhqEqSzMwzzRQGtAPbs4Cncxg==
+terser-webpack-plugin@^1.4.3:
+  version "1.4.5"
+  resolved "https://registry.yarnpkg.com/terser-webpack-plugin/-/terser-webpack-plugin-1.4.5.tgz#a217aefaea330e734ffacb6120ec1fa312d6040b"
+  integrity sha512-04Rfe496lN8EYruwi6oPQkG0vo8C+HT49X687FZnpPF0qMAIHONI6HEXYPKDOE8e5HjXTyKfqRd/agHtH0kOtw==
   dependencies:
     cacache "^12.0.2"
     find-cache-dir "^2.1.0"
     is-wsl "^1.1.0"
     schema-utils "^1.0.0"
-    serialize-javascript "^1.7.0"
+    serialize-javascript "^4.0.0"
     source-map "^0.6.1"
     terser "^4.1.2"
     webpack-sources "^1.4.0"
     worker-farm "^1.7.0"
 
-terser@4.0.0:
-  version "4.0.0"
-  resolved "https://registry.yarnpkg.com/terser/-/terser-4.0.0.tgz#ef356f6f359a963e2cc675517f21c1c382877374"
-  integrity sha512-dOapGTU0hETFl1tCo4t56FN+2jffoKyER9qBGoUFyZ6y7WLoKT0bF+lAYi6B6YsILcGF3q1C2FBh8QcKSCgkgA==
+terser@4.4.2:
+  version "4.4.2"
+  resolved "https://registry.yarnpkg.com/terser/-/terser-4.4.2.tgz#448fffad0245f4c8a277ce89788b458bfd7706e8"
+  integrity sha512-Uufrsvhj9O1ikwgITGsZ5EZS6qPokUOkCegS7fYOdGTv+OA90vndUbU6PEjr5ePqHfNUbGyMO7xyIZv2MhsALQ==
   dependencies:
-    commander "^2.19.0"
+    commander "^2.20.0"
     source-map "~0.6.1"
-    source-map-support "~0.5.10"
+    source-map-support "~0.5.12"
 
-terser@^4.1.2:
-  version "4.3.2"
-  resolved "https://registry.yarnpkg.com/terser/-/terser-4.3.2.tgz#ed830de484b0103652799063e605618e80f97f93"
-  integrity sha512-obxk4x19Zlzj9zY4QeXj9iPCb5W8YGn4v3pn4/fHj0Nw8+R7N02Kvwvz9VpOItCZZD8RC+vnYCDL0gP6FAJ7Xg==
+terser@4.6.7, terser@^4.1.2:
+  version "4.6.7"
+  resolved "https://registry.yarnpkg.com/terser/-/terser-4.6.7.tgz#478d7f9394ec1907f0e488c5f6a6a9a2bad55e72"
+  integrity sha512-fmr7M1f7DBly5cX2+rFDvmGBAaaZyPrHYK4mMdHEDAdNTqXSZgSOfqsfGq2HqPGT/1V0foZZuCZFx8CHKgAk3g==
   dependencies:
     commander "^2.20.0"
     source-map "~0.6.1"
     source-map-support "~0.5.12"
 
+thread-loader@2.1.3:
+  version "2.1.3"
+  resolved "https://registry.yarnpkg.com/thread-loader/-/thread-loader-2.1.3.tgz#cbd2c139fc2b2de6e9d28f62286ab770c1acbdda"
+  integrity sha512-wNrVKH2Lcf8ZrWxDF/khdlLlsTMczdcwPA9VEK4c2exlEPynYWxi9op3nPTo5lAnDIkE0rQEB3VBP+4Zncc9Hg==
+  dependencies:
+    loader-runner "^2.3.1"
+    loader-utils "^1.1.0"
+    neo-async "^2.6.0"
+
 through2@^2.0.0:
   version "2.0.5"
   resolved "https://registry.yarnpkg.com/through2/-/through2-2.0.5.tgz#01c1e39eb31d07cb7d03a96a70823260b23132cd"
   integrity sha512-/mrRod8xqpA+IHSLyGCQ2s8SPHiCDEeQJSep1jqLYeEUClOFG2Qsh+4FU6G9VeqpZnGW/Su8LQGc4YKni5rYSQ==
   dependencies:
     readable-stream "~2.3.6"
     xtend "~4.0.1"
@@ -4513,19 +6169,14 @@
     setimmediate "^1.0.4"
 
 to-arraybuffer@^1.0.0:
   version "1.0.1"
   resolved "https://registry.yarnpkg.com/to-arraybuffer/-/to-arraybuffer-1.0.1.tgz#7d229b1fcc637e466ca081180836a7aabff83f43"
   integrity sha1-fSKbH8xjfkZsoIEYCDanqr/4P0M=
 
-to-fast-properties@^1.0.3:
-  version "1.0.3"
-  resolved "https://registry.yarnpkg.com/to-fast-properties/-/to-fast-properties-1.0.3.tgz#b83571fa4d8c25b82e231b06e3a3055de4ca1a47"
-  integrity sha1-uDVx+k2MJbguIxsG46MFXeTKGkc=
-
 to-fast-properties@^2.0.0:
   version "2.0.0"
   resolved "https://registry.yarnpkg.com/to-fast-properties/-/to-fast-properties-2.0.0.tgz#dc5e698cbd079265bc73e0377681a4e4e83f616e"
   integrity sha1-3F5pjL0HkmW8c+A3doGk5Og/YW4=
 
 to-object-path@^0.3.0:
   version "0.3.0"
@@ -4538,14 +6189,21 @@
   version "2.1.1"
   resolved "https://registry.yarnpkg.com/to-regex-range/-/to-regex-range-2.1.1.tgz#7c80c17b9dfebe599e27367e0d4dd5590141db38"
   integrity sha1-fIDBe53+vlmeJzZ+DU3VWQFB2zg=
   dependencies:
     is-number "^3.0.0"
     repeat-string "^1.6.1"
 
+to-regex-range@^5.0.1:
+  version "5.0.1"
+  resolved "https://registry.yarnpkg.com/to-regex-range/-/to-regex-range-5.0.1.tgz#1648c44aae7c8d988a326018ed72f5b4dd0392e4"
+  integrity sha512-65P7iz6X5yEr1cwcgvQxbbIw7Uk3gOy5dIdtZ4rDveLqhrdJP+Li/Hx6tyK0NEb+2GCyneCMJiGqrADCSNk8sQ==
+  dependencies:
+    is-number "^7.0.0"
+
 to-regex@^3.0.1, to-regex@^3.0.2:
   version "3.0.2"
   resolved "https://registry.yarnpkg.com/to-regex/-/to-regex-3.0.2.tgz#13cfdd9b336552f30b51f33a8ae1b42a7a7599ce"
   integrity sha512-FWtleNAtZ/Ki2qtqej2CXTOayOH9bHDQF+Q48VpWyDXjbYxA4Yz8iDB31zXOBUlOHHKidDbqGVrTUvQMPmBGBw==
   dependencies:
     define-property "^2.0.2"
     extend-shallow "^3.0.2"
@@ -4573,14 +6231,24 @@
   integrity sha512-qOebF53frne81cf0S9B41ByenJ3/IuH8yJKngAX35CmiZySA0khhkovshKK+jGCaMnVomla7gVlIcc3EvKPbTQ==
 
 tty-browserify@0.0.0:
   version "0.0.0"
   resolved "https://registry.yarnpkg.com/tty-browserify/-/tty-browserify-0.0.0.tgz#a157ba402da24e9bf957f9aa69d524eed42901a6"
   integrity sha1-oVe6QC2iTpv5V/mqadUk7tQpAaY=
 
+type@^1.0.1:
+  version "1.2.0"
+  resolved "https://registry.yarnpkg.com/type/-/type-1.2.0.tgz#848dd7698dafa3e54a6c479e759c4bc3f18847a0"
+  integrity sha512-+5nt5AAniqsCnu2cEQQdpzCAh33kVx8n0VoFidKpB1dVVLAN/F+bgVOqOJqOnEnrhp222clB5p3vUlD+1QAnfg==
+
+type@^2.0.0:
+  version "2.5.0"
+  resolved "https://registry.yarnpkg.com/type/-/type-2.5.0.tgz#0a2e78c2e77907b252abe5f298c1b01c63f0db3d"
+  integrity sha512-180WMDQaIMm3+7hGXWf12GtdniDEy7nYcyFMKJn/eZz/6tSLXrUN9V0wKSbMjej0I1WHWbpREDEKHtqPQa9NNw==
+
 typedarray-to-buffer@^3.1.5:
   version "3.1.5"
   resolved "https://registry.yarnpkg.com/typedarray-to-buffer/-/typedarray-to-buffer-3.1.5.tgz#a97ee7a9ff42691b9f783ff1bc5112fe3fca9080"
   integrity sha512-zdu8XMNEDepKKR+XYOXAVPtWui0ly0NtohUscw+UmaHiAWT8hrV1rr//H6V+0DvJ3OQ19S979M0laLfX8rm82Q==
   dependencies:
     is-typedarray "^1.0.0"
 
@@ -4613,14 +6281,19 @@
     unicode-property-aliases-ecmascript "^1.0.4"
 
 unicode-match-property-value-ecmascript@^1.1.0:
   version "1.1.0"
   resolved "https://registry.yarnpkg.com/unicode-match-property-value-ecmascript/-/unicode-match-property-value-ecmascript-1.1.0.tgz#5b4b426e08d13a80365e0d657ac7a6c1ec46a277"
   integrity sha512-hDTHvaBk3RmFzvSl0UVrUmC3PuW9wKVnpoUDYH0JDkSIovzw+J5viQmeYHxVSBptubnr7PbH2e0fnpDRQnQl5g==
 
+unicode-match-property-value-ecmascript@^1.2.0:
+  version "1.2.0"
+  resolved "https://registry.yarnpkg.com/unicode-match-property-value-ecmascript/-/unicode-match-property-value-ecmascript-1.2.0.tgz#0d91f600eeeb3096aa962b1d6fc88876e64ea531"
+  integrity sha512-wjuQHGQVofmSJv1uVISKLE5zO2rNGzM/KCYZch/QQvez7C1hUhBIuZ701fYXExuufJFMPhv2SyL8CyoIfMLbIQ==
+
 unicode-property-aliases-ecmascript@^1.0.4:
   version "1.0.5"
   resolved "https://registry.yarnpkg.com/unicode-property-aliases-ecmascript/-/unicode-property-aliases-ecmascript-1.0.5.tgz#a9cc6cc7ce63a0a3023fc99e341b94431d405a57"
   integrity sha512-L5RAqCfXqAwR3RriF8pM0lU0w4Ryf/GgzONwi6KnL1taJQa7x1TCxdJnILX59WIGOwR57IVxn7Nej0fz1Ny6fw==
 
 union-value@^1.0.0:
   version "1.0.1"
@@ -4628,14 +6301,19 @@
   integrity sha512-tJfXmxMeWYnczCVs7XAEvIV7ieppALdyepWMkHkwciRpZraG/xwT+s2JN8+pr1+8jCRf80FFzvr+MpQeeoF4Xg==
   dependencies:
     arr-union "^3.1.0"
     get-value "^2.0.6"
     is-extendable "^0.1.1"
     set-value "^2.0.1"
 
+uniq@^1.0.1:
+  version "1.0.1"
+  resolved "https://registry.yarnpkg.com/uniq/-/uniq-1.0.1.tgz#b31c5ae8254844a3a8281541ce2b04b865a734ff"
+  integrity sha1-sxxa6CVIRKOoKBVBzisEuGWnNP8=
+
 unique-filename@^1.1.1:
   version "1.1.1"
   resolved "https://registry.yarnpkg.com/unique-filename/-/unique-filename-1.1.1.tgz#1d69769369ada0583103a1e6ae87681b56573230"
   integrity sha512-Vmp0jIp2ln35UTXuryvjzkjGdRyf9b2lTXuSYUiPmzRcl3FDtYqAwOnTJkAngD9SWhnoJzDbTKwaOrZ+STtxNQ==
   dependencies:
     unique-slug "^2.0.0"
 
@@ -4680,20 +6358,25 @@
   version "0.11.0"
   resolved "https://registry.yarnpkg.com/url/-/url-0.11.0.tgz#3838e97cfc60521eb73c525a8e55bfdd9e2e28f1"
   integrity sha1-ODjpfPxgUh63PFJajlW/3Z4uKPE=
   dependencies:
     punycode "1.3.2"
     querystring "0.2.0"
 
+use-subscription@1.1.1:
+  version "1.1.1"
+  resolved "https://registry.yarnpkg.com/use-subscription/-/use-subscription-1.1.1.tgz#5509363e9bb152c4fb334151d4dceb943beaa7bb"
+  integrity sha512-gk4fPTYvNhs6Ia7u8/+K7bM7sZ7O7AMfWtS+zPO8luH+zWuiGgGcrW0hL4MRWZSzXo+4ofNorf87wZwBKz2YdQ==
+
 use@^3.1.0:
   version "3.1.1"
   resolved "https://registry.yarnpkg.com/use/-/use-3.1.1.tgz#d50c8cac79a19fbc20f2911f56eb973f4e10070f"
   integrity sha512-cwESVXlO3url9YWlFW/TA9cshCEhtu7IKJ/p5soJ/gGpj7vbvFrAY/eIioQ6Dw23KjZhYgiIo8HOs1nQ2vr/oQ==
 
-util-deprecate@~1.0.1:
+util-deprecate@^1.0.2, util-deprecate@~1.0.1:
   version "1.0.2"
   resolved "https://registry.yarnpkg.com/util-deprecate/-/util-deprecate-1.0.2.tgz#450d4dc9fa70de732762fbd2d4a28981419a0ccf"
   integrity sha1-RQ1Nyfpw3nMnYvvS1KKJgUGaDM8=
 
 util@0.10.3:
   version "0.10.3"
   resolved "https://registry.yarnpkg.com/util/-/util-0.10.3.tgz#7afb1afe50805246489e3db7fe0ed379336ac0f9"
@@ -4727,22 +6410,21 @@
   integrity sha1-IpnwLG3tMNSllhsLn3RSShj2NPw=
 
 vm-browserify@^1.0.1:
   version "1.1.0"
   resolved "https://registry.yarnpkg.com/vm-browserify/-/vm-browserify-1.1.0.tgz#bd76d6a23323e2ca8ffa12028dc04559c75f9019"
   integrity sha512-iq+S7vZJE60yejDYM0ek6zg308+UZsdtPExWP9VZoCFCz1zkJoXFnAX7aZfd/ZwrkidzdUZL0C/ryW+JwAiIGw==
 
-watchpack@2.0.0-beta.5:
-  version "2.0.0-beta.5"
-  resolved "https://registry.yarnpkg.com/watchpack/-/watchpack-2.0.0-beta.5.tgz#c005db39570d81d9d34334870abc0f548901b880"
-  integrity sha512-HGqh9e9QZFhow8JYX+1+E+kIYK0uTTsk6rCOkI0ff0f9kMO0wX783yW8saQC9WDx7qHpVGPXsRnld9nY7iwzQA==
+watchpack@2.0.0-beta.13:
+  version "2.0.0-beta.13"
+  resolved "https://registry.yarnpkg.com/watchpack/-/watchpack-2.0.0-beta.13.tgz#9d9b0c094b8402139333e04eb6194643c8384f55"
+  integrity sha512-ZEFq2mx/k5qgQwgi6NOm+2ImICb8ngAkA/rZ6oyXZ7SgPn3pncf+nfhYTCrs3lmHwOxnPtGLTOuFLfpSMh1VMA==
   dependencies:
     glob-to-regexp "^0.4.1"
     graceful-fs "^4.1.2"
-    neo-async "^2.5.0"
 
 watchpack@^1.6.0:
   version "1.6.0"
   resolved "https://registry.yarnpkg.com/watchpack/-/watchpack-1.6.0.tgz#4bc12c2ebe8aa277a71f1d3f14d685c7b446cd00"
   integrity sha512-i6dHe3EyLjMmDlU1/bGQpEw25XSjkJULPuAVKCbNRefQVq48yXKUpwg538F7AZTf9kyr57zj++pQFltUa5H7yA==
   dependencies:
     chokidar "^2.0.2"
@@ -4787,34 +6469,26 @@
 webpack-merge@^4.1.0:
   version "4.2.2"
   resolved "https://registry.yarnpkg.com/webpack-merge/-/webpack-merge-4.2.2.tgz#a27c52ea783d1398afd2087f547d7b9d2f43634d"
   integrity sha512-TUE1UGoTX2Cd42j3krGYqObZbOD+xF7u28WB7tfUordytSjbWTIjK/8V0amkBfTYN4/pB/GIDlJZZ657BGG19g==
   dependencies:
     lodash "^4.17.15"
 
-webpack-sources@1.3.0:
-  version "1.3.0"
-  resolved "https://registry.yarnpkg.com/webpack-sources/-/webpack-sources-1.3.0.tgz#2a28dcb9f1f45fe960d8f1493252b5ee6530fa85"
-  integrity sha512-OiVgSrbGu7NEnEvQJJgdSFPl2qWKkWq5lHMhgiToIiN9w34EBnjYzSYs+VbL5KoYiLNtFFa7BZIKxRED3I32pA==
-  dependencies:
-    source-list-map "^2.0.0"
-    source-map "~0.6.1"
-
-webpack-sources@^1.0.1, webpack-sources@^1.4.0, webpack-sources@^1.4.1:
+webpack-sources@1.4.3, webpack-sources@^1.0.1, webpack-sources@^1.1.0, webpack-sources@^1.4.0, webpack-sources@^1.4.1:
   version "1.4.3"
   resolved "https://registry.yarnpkg.com/webpack-sources/-/webpack-sources-1.4.3.tgz#eedd8ec0b928fbf1cbfe994e22d2d890f330a933"
   integrity sha512-lgTS3Xhv1lCOKo7SA5TjKXMjpSM4sBjNV5+q2bqesbSPs5FjGmU6jjtBSkX9b4qW87vDIsCIlUPOEhbZrMdjeQ==
   dependencies:
     source-list-map "^2.0.0"
     source-map "~0.6.1"
 
-webpack@4.39.0:
-  version "4.39.0"
-  resolved "https://registry.yarnpkg.com/webpack/-/webpack-4.39.0.tgz#1d511308c3dd8f9fe3152c9447ce30f1814a620c"
-  integrity sha512-nrxFNSEKm4T1C/EsgOgN50skt//Pl4X7kgJC1MrlE47M292LSCVmMOC47iTGL0CGxbdwhKGgeThrJcw0bstEfA==
+webpack@4.42.0:
+  version "4.42.0"
+  resolved "https://registry.yarnpkg.com/webpack/-/webpack-4.42.0.tgz#b901635dd6179391d90740a63c93f76f39883eb8"
+  integrity sha512-EzJRHvwQyBiYrYqhyjW9AqM90dE4+s1/XtCfn7uWg6cS72zH+2VPFAlsnW0+W0cDi0XRjNKUMoJtpSi50+Ph6w==
   dependencies:
     "@webassemblyjs/ast" "1.8.5"
     "@webassemblyjs/helper-module-context" "1.8.5"
     "@webassemblyjs/wasm-edit" "1.8.5"
     "@webassemblyjs/wasm-parser" "1.8.5"
     acorn "^6.2.1"
     ajv "^6.10.2"
@@ -4828,15 +6502,15 @@
     memory-fs "^0.4.1"
     micromatch "^3.1.10"
     mkdirp "^0.5.1"
     neo-async "^2.6.1"
     node-libs-browser "^2.2.1"
     schema-utils "^1.0.0"
     tapable "^1.1.3"
-    terser-webpack-plugin "^1.4.1"
+    terser-webpack-plugin "^1.4.3"
     watchpack "^1.6.0"
     webpack-sources "^1.4.1"
 
 wide-align@^1.1.0:
   version "1.1.3"
   resolved "https://registry.yarnpkg.com/wide-align/-/wide-align-1.1.3.tgz#ae074e6bdc0c14a431e804e624549c633b000457"
   integrity sha512-QGkOQc8XL6Bt5PwnsExKBPuMKBxnGxWWW3fU55Xt4feHozMUhdUMaBCk290qpm/wG5u/RSKzwdAC4i51YigihA==
@@ -4874,15 +6548,20 @@
 
 xtend@^4.0.0, xtend@~4.0.1:
   version "4.0.2"
   resolved "https://registry.yarnpkg.com/xtend/-/xtend-4.0.2.tgz#bb72779f5fa465186b1f438f674fa347fdb5db54"
   integrity sha512-LKYU1iAXJXUgAXn9URjiu+MWhyUXHsvfp7mcuYm9dSUKK0/CjtrUwFAxD82/mCWbtLsGjFIad0wIsod4zrTAEQ==
 
 y18n@^4.0.0:
-  version "4.0.0"
-  resolved "https://registry.yarnpkg.com/y18n/-/y18n-4.0.0.tgz#95ef94f85ecc81d007c264e190a120f0a3c8566b"
-  integrity sha512-r9S/ZyXu/Xu9q1tYlpsLIsa3EeLXXk0VwlxqTcFRfg9EhMW+17kbt9G0NrgCmhGb5vT2hyhJZLfDGx+7+5Uj/w==
+  version "4.0.1"
+  resolved "https://registry.yarnpkg.com/y18n/-/y18n-4.0.1.tgz#8db2b83c31c5d75099bb890b23f3094891e247d4"
+  integrity sha512-wNcy4NvjMYL8gogWWYAO7ZFWFfHcbdbE57tZO8e4cbpj8tfUcwrwqSl3ad8HxpYWCdXcJUCeKKZS62Av1affwQ==
 
 yallist@^3.0.0, yallist@^3.0.2, yallist@^3.0.3:
   version "3.0.3"
   resolved "https://registry.yarnpkg.com/yallist/-/yallist-3.0.3.tgz#b4b049e314be545e3ce802236d6cd22cd91c3de9"
   integrity sha512-S+Zk8DEWE6oKpV+vI3qWkaK+jSbIK86pCwe2IF/xwIpQ8jEuxpw9NyaGjmp9+BoJv5FV2piqCDcoCtStppiq2A==
+
+yallist@^4.0.0:
+  version "4.0.0"
+  resolved "https://registry.yarnpkg.com/yallist/-/yallist-4.0.0.tgz#9bb92790d9c0effec63be73519e11a35019a3a72"
+  integrity sha512-3wdGidZyq5PB084XLES5TpOSRA3wjXAlIWMhum2kRcv/41Sn2emQ0dycQW4uZXLejwKvg6EsvbdlVL+FYEct7A==
```

### Comparing `weco-datascience-0.1.8/weco_datascience/api/catalogue.py` & `weco-datascience-0.1.9/weco_datascience/api/catalogue.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 import random
+
 from . import api_url
 
 catalogue_url = api_url / "works"
 valid_query_args = set(
     [
         "pageSize",
         "page",
```

### Comparing `weco-datascience-0.1.8/weco_datascience/api/image.py` & `weco-datascience-0.1.9/weco_datascience/api/image.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 import random
+
 from . import api_url
 
 images_url = api_url / "images"
 valid_query_args = set(
     ["query", "locations.license", "colors", "page", "pageSize"]
 )
```

### Comparing `weco-datascience-0.1.8/weco_datascience/api/snapshot.py` & `weco-datascience-0.1.9/weco_datascience/api/snapshot.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/weco_datascience/batching.py` & `weco-datascience-0.1.9/weco_datascience/batching.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/weco_datascience/credentials.py` & `weco-datascience-0.1.9/weco_datascience/credentials.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/weco_datascience/http.py` & `weco-datascience-0.1.9/weco_datascience/http.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/weco_datascience/image.py` & `weco-datascience-0.1.9/weco_datascience/image.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/weco_datascience/reporting.py` & `weco-datascience-0.1.9/weco_datascience/reporting.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,12 +1,15 @@
-from collections import MutableMapping
+from collections.abc import MutableMapping
 
-import pandas as pd
 from elasticsearch import Elasticsearch, helpers
 
+import pandas as pd
+
+from .aws import get_secret_string, get_session
+
 
 def flatten(d, parent_key="", sep="."):
     """
     flatten a nested dictionary so that it can be more neatly loaded into a
     pandas dataframe
 
     https://stackoverflow.com/questions/6027558/flatten-nested-dictionaries-compressing-keys
@@ -31,48 +34,56 @@
         if isinstance(v, MutableMapping):
             items.extend(flatten(v, new_key, sep=sep).items())
         else:
             items.append((new_key, v))
     return dict(items)
 
 
-def query_es(config, query):
+def query_es(config, index, query):
     """
     Run a query against a specified elasticsearch index
 
     Parameters
     ----------
     config: dict
-        elasticsearch username, password, index, and endpoint for the request
+        elasticsearch username, password, and endpoint for the request
+    index: str
+        the name of the index to query
     query: dict
         the query, following the elasticsearch json query structure
 
     Returns
     -------
     df: pd.DataFrame
         a pandas dataframe containing the flattened response data
     """
     client = Elasticsearch(
         config["host"], http_auth=(config["username"], config["password"])
     )
-    response = client.search(body=query, index=config["index"])
+    response = client.search(body=query, index=index)
     data = [flatten(event["_source"]) for event in response["hits"]["hits"]]
     return pd.DataFrame(data)
 
 
 def get_data_in_date_range(
-    config, start_date="now-1d", end_date="now", timestamp_field="@timestamp"
+    config,
+    index,
+    start_date="now-1d",
+    end_date="now",
+    timestamp_field="@timestamp",
 ):
     """
     Fetch data within a specified date/time range
 
     Parameters
     ----------
     config: dict
-        elasticsearch username, password, index, and endpoint for the request
+        elasticsearch username, password, and endpoint for the request
+    index: str
+        the name of the index to query
     start_date: str, datetime, optional
         defaults to now
     end_date: str, datetime, optional
         defaults to now
     timestamp_field: str, optional
         the timestamp field which should be used to sort the data by recency
 
@@ -83,39 +94,70 @@
     """
     query = {
         "query": {
             "range": {timestamp_field: {"gte": start_date, "lt": end_date}}
         },
         "size": 1_000_000,
     }
-    return query_es(config, query)
+    return query_es(config, index, query)
 
 
 def get_recent_data(config, n, index, timestamp_field="@timestamp"):
     """
     Fetch the `n` most recent documents from a specified elasticsearch index
 
     Parameters
     ----------
     config: dict
-        elasticsearch username, password, index, and endpoint for the request
+        elasticsearch username, password, and endpoint for the request
     n: int
         the number of documents to return
+    index: str
+        the name of the index to query
     timestamp_field: str, optional
         the timestamp field which should be used to sort the data by recency
 
     Returns
     -------
     df: pd.DataFrame
         a pandas dataframe containing the flattened response data
     """
     client = Elasticsearch(
         config["host"], http_auth=(config["username"], config["password"])
     )
     response = helpers.scan(
         client,
         query={"sort": [{timestamp_field: "desc"}]},
-        index=config["index"],
+        index=index,
         preserve_order=True,
     )
     data = [flatten(next(response)["_source"]) for _ in range(n)]
     return pd.DataFrame(data)
+
+
+def get_es_config():
+    session = get_session(
+        role_arn="arn:aws:iam::760097843905:role/platform-developer"
+    )
+
+    config = {
+        "host": get_secret_string(session, secret_id="reporting/es_host"),
+        "username": get_secret_string(
+            session, secret_id="reporting/read_only/es_username"
+        ),
+        "password": get_secret_string(
+            session, secret_id="reporting/read_only/es_password"
+        ),
+    }
+    return config
+
+
+def get_es_client():
+    """
+    Returns an Elasticsearch client with read-only access to the
+    reporting cluster.
+    """
+    config = get_es_config()
+    username = config["username"]
+    password = config["password"]
+    host = config["host"]
+    return Elasticsearch(f"https://{username}:{password}@{host}:9243")
```

### Comparing `weco-datascience-0.1.8/weco_datascience/test/V0002882.jpg` & `weco-datascience-0.1.9/weco_datascience/test/V0002882.jpg`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/weco_datascience/test/api/test_catalogue.py` & `weco-datascience-0.1.9/weco_datascience/test/api/test_catalogue.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/weco_datascience/test/api/test_images.py` & `weco-datascience-0.1.9/weco_datascience/test/api/test_images.py`

 * *Files identical despite different names*

### Comparing `weco-datascience-0.1.8/weco_datascience/test/test_http.py` & `weco-datascience-0.1.9/weco_datascience/test/test_http.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 import json
 
 import pytest
+
 from weco_datascience.http import (
     close_persistent_client_session,
     fetch_redirect_url,
     fetch_url_bytes,
     fetch_url_json,
     start_persistent_client_session,
 )
@@ -36,21 +37,19 @@
     with pytest.raises(ValueError):
         await fetch_url_json(image_url)
     await close_persistent_client_session()
 
 
 @pytest.mark.asyncio
 async def test_redirect():
-    expected = "https://id.loc.gov/authorities/subjects/sh85101552.html"
     start_persistent_client_session()
-    response = await fetch_redirect_url(
-        "https://id.loc.gov/authorities/label/Physical geography"
-    )
+    original_url = "https://id.loc.gov/authorities/label/Physical%20geography"
+    response = await fetch_redirect_url(original_url)
     await close_persistent_client_session()
-    assert response["url"] == expected
+    assert response["url"] != original_url
 
 
 @pytest.mark.asyncio
 async def test_non_redirect():
     start_persistent_client_session()
     response = await fetch_redirect_url(iiif_url)
     await close_persistent_client_session()
```

### Comparing `weco-datascience-0.1.8/weco_datascience/test/test_image.py` & `weco-datascience-0.1.9/weco_datascience/test/test_image.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 import pytest
 from PIL.Image import Image
+
 from weco_datascience.http import (
     close_persistent_client_session,
     fetch_url_bytes,
     start_persistent_client_session,
 )
 from weco_datascience.image import (
     get_image_from_url,
```

### Comparing `weco-datascience-0.1.8/setup.py` & `weco-datascience-0.1.9/setup.py`

 * *Files 16% similar despite different names*

```diff
@@ -15,24 +15,25 @@
 install_requires = \
 ['aiofile==3.1.0',
  'aiohttp[speedups]==3.6.2',
  'async-timeout==3.0.1',
  'boto3==1.12.14',
  'Pillow==7.0.0',
  'piffle==0.3.0',
- 'urlpath==1.1.7']
+ 'urlpath==1.1.7',
+ 'elasticsearch==7.14']
 
 extras_require = \
 {'dev': ['black==19.10b0',
          'flake8==3.8.1',
          'isort==4.3.21',
          'pytest-asyncio==0.14.0']}
 
 setup(name='weco-datascience',
-      version='0.1.8',
+      version='0.1.9',
       description='Common functionality for data science applications at Wellcome Collection',
       author='Harrison Pim',
       author_email='h.pim@wellcome.ac.uk',
       url='https://github.com/wellcomecollection/data-science',
       packages=packages,
       package_data=package_data,
       install_requires=install_requires,
```

