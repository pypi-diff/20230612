# Comparing `tmp/scikit_recommender-0.0.3-cp39-cp39-win_amd64.whl.zip` & `tmp/scikit_recommender-0.0.4-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,60 +1,63 @@
-Zip file size: 295273 bytes, number of entries: 58
--rw-rw-rw-  2.0 fat      192 b- defN 23-May-30 03:05 skrec/__init__.py
--rw-rw-rw-  2.0 fat      543 b- defN 23-May-30 03:05 skrec/io/__init__.py
--rw-rw-rw-  2.0 fat    14644 b- defN 23-May-30 03:05 skrec/io/data_iterator.py
--rw-rw-rw-  2.0 fat    13778 b- defN 23-May-30 03:05 skrec/io/dataset.py
--rw-rw-rw-  2.0 fat     2623 b- defN 23-May-30 03:05 skrec/io/logger.py
--rw-rw-rw-  2.0 fat     1483 b- defN 23-May-30 03:05 skrec/io/movielens.py
--rw-rw-rw-  2.0 fat    13177 b- defN 23-May-30 03:05 skrec/io/preprocessor.py
--rw-rw-rw-  2.0 fat     5794 b- defN 23-May-30 03:05 skrec/recommender/BPRMF.py
--rw-rw-rw-  2.0 fat     9858 b- defN 23-May-30 03:05 skrec/recommender/CDAE.py
--rw-rw-rw-  2.0 fat     8800 b- defN 23-May-30 03:05 skrec/recommender/CML.py
--rw-rw-rw-  2.0 fat     9587 b- defN 23-May-30 03:05 skrec/recommender/Caser.py
--rw-rw-rw-  2.0 fat     6660 b- defN 23-May-30 03:05 skrec/recommender/FPMC.py
--rw-rw-rw-  2.0 fat    12848 b- defN 23-May-30 03:05 skrec/recommender/GRU4Rec.py
--rw-rw-rw-  2.0 fat    14646 b- defN 23-May-30 03:05 skrec/recommender/GRU4RecPlus.py
--rw-rw-rw-  2.0 fat     9658 b- defN 23-May-30 03:05 skrec/recommender/HGN.py
--rw-rw-rw-  2.0 fat     9540 b- defN 23-May-30 03:05 skrec/recommender/LightGCN.py
--rw-rw-rw-  2.0 fat     8726 b- defN 23-May-30 03:05 skrec/recommender/MultVAE.py
--rw-rw-rw-  2.0 fat     1635 b- defN 23-May-30 03:05 skrec/recommender/Pop.py
--rw-rw-rw-  2.0 fat    21741 b- defN 23-May-30 03:05 skrec/recommender/SASRec.py
--rw-rw-rw-  2.0 fat    15764 b- defN 23-May-30 03:05 skrec/recommender/SGAT.py
--rw-rw-rw-  2.0 fat    15481 b- defN 23-May-30 03:05 skrec/recommender/SRGNN.py
--rw-rw-rw-  2.0 fat     7098 b- defN 23-May-30 03:05 skrec/recommender/TransRec.py
--rw-rw-rw-  2.0 fat       86 b- defN 23-May-30 03:05 skrec/recommender/__init__.py
--rw-rw-rw-  2.0 fat     1699 b- defN 23-May-30 03:05 skrec/recommender/base.py
--rw-rw-rw-  2.0 fat     4196 b- defN 23-May-30 03:05 skrec/recommender/AOBPR/AOBPR.py
--rw-rw-rw-  2.0 fat       73 b- defN 23-May-30 03:05 skrec/recommender/AOBPR/__init__.py
--rw-rw-rw-  2.0 fat    68608 b- defN 23-May-30 03:06 skrec/recommender/AOBPR/pyx_aobpr_func.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat     7915 b- defN 23-May-30 03:05 skrec/recommender/BERT4Rec/BERT4Rec.py
--rw-rw-rw-  2.0 fat       88 b- defN 23-May-30 03:05 skrec/recommender/BERT4Rec/__init__.py
--rw-rw-rw-  2.0 fat    18467 b- defN 23-May-30 03:05 skrec/recommender/BERT4Rec/bert4rec_gen_data.py
--rw-rw-rw-  2.0 fat    11820 b- defN 23-May-30 03:05 skrec/recommender/BERT4Rec/bert4rec_utils.py
--rw-rw-rw-  2.0 fat    41607 b- defN 23-May-30 03:05 skrec/recommender/BERT4Rec/modeling.py
--rw-rw-rw-  2.0 fat     6676 b- defN 23-May-30 03:05 skrec/recommender/BERT4Rec/optimization.py
--rw-rw-rw-  2.0 fat     2336 b- defN 23-May-30 03:05 skrec/recommender/BERT4Rec/vocab.py
--rw-rw-rw-  2.0 fat      127 b- defN 23-May-30 03:05 skrec/utils/__init__.py
--rw-rw-rw-  2.0 fat     1261 b- defN 23-May-30 03:05 skrec/utils/common.py
--rw-rw-rw-  2.0 fat     1255 b- defN 23-May-30 03:05 skrec/utils/registry.py
--rw-rw-rw-  2.0 fat     1265 b- defN 23-May-30 03:05 skrec/utils/tf1x.py
--rw-rw-rw-  2.0 fat       86 b- defN 23-May-30 03:05 skrec/utils/tf2x.py
--rw-rw-rw-  2.0 fat     4122 b- defN 23-May-30 03:05 skrec/utils/torch.py
--rw-rw-rw-  2.0 fat      538 b- defN 23-May-30 03:05 skrec/utils/py/__init__.py
--rw-rw-rw-  2.0 fat     6779 b- defN 23-May-30 03:05 skrec/utils/py/batch_iterator.py
--rw-rw-rw-  2.0 fat     2593 b- defN 23-May-30 03:05 skrec/utils/py/config.py
--rw-rw-rw-  2.0 fat     1678 b- defN 23-May-30 03:05 skrec/utils/py/decorator.py
--rw-rw-rw-  2.0 fat     9089 b- defN 23-May-30 03:05 skrec/utils/py/evaluator.py
--rw-rw-rw-  2.0 fat     4876 b- defN 23-May-30 03:05 skrec/utils/py/generic.py
--rw-rw-rw-  2.0 fat     1450 b- defN 23-May-30 03:05 skrec/utils/py/random.py
--rw-rw-rw-  2.0 fat      173 b- defN 23-May-30 03:05 skrec/utils/py/cython/__init__.py
--rw-rw-rw-  2.0 fat    84992 b- defN 23-May-30 03:06 skrec/utils/py/cython/pyx_eval_matrix.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat    26112 b- defN 23-May-30 03:06 skrec/utils/py/cython/pyx_init.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   109568 b- defN 23-May-30 03:06 skrec/utils/py/cython/pyx_random.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   115712 b- defN 23-May-30 03:06 skrec/utils/py/cython/pyx_sort.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat    30720 b- defN 23-May-30 03:06 skrec/utils/py/cython/pyx_utils.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat        0 b- defN 23-May-30 03:06 scikit_recommender-0.0.3.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     7326 b- defN 23-May-30 03:06 scikit_recommender-0.0.3.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 23-May-30 03:06 scikit_recommender-0.0.3.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        6 b- defN 23-May-30 03:06 scikit_recommender-0.0.3.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     5065 b- defN 23-May-30 03:06 scikit_recommender-0.0.3.dist-info/RECORD
-58 files, 782740 bytes uncompressed, 287215 bytes compressed:  63.3%
+Zip file size: 305529 bytes, number of entries: 61
+-rw-rw-rw-  2.0 fat      227 b- defN 23-Jun-12 14:28 skrec/__init__.py
+-rw-rw-rw-  2.0 fat     1811 b- defN 23-Jun-12 14:28 skrec/run_config.py
+-rw-rw-rw-  2.0 fat      661 b- defN 23-Jun-12 14:28 skrec/io/__init__.py
+-rw-rw-rw-  2.0 fat    18167 b- defN 23-Jun-12 14:28 skrec/io/data_iterator.py
+-rw-rw-rw-  2.0 fat    20439 b- defN 23-Jun-12 14:28 skrec/io/dataset.py
+-rw-rw-rw-  2.0 fat     2623 b- defN 23-Jun-12 14:28 skrec/io/logger.py
+-rw-rw-rw-  2.0 fat     1483 b- defN 23-Jun-12 14:28 skrec/io/movielens.py
+-rw-rw-rw-  2.0 fat    13177 b- defN 23-Jun-12 14:28 skrec/io/preprocessor.py
+-rw-rw-rw-  2.0 fat     5633 b- defN 23-Jun-12 14:28 skrec/recommender/BPRMF.py
+-rw-rw-rw-  2.0 fat     9711 b- defN 23-Jun-12 14:28 skrec/recommender/CDAE.py
+-rw-rw-rw-  2.0 fat     8645 b- defN 23-Jun-12 14:28 skrec/recommender/CML.py
+-rw-rw-rw-  2.0 fat     9431 b- defN 23-Jun-12 14:28 skrec/recommender/Caser.py
+-rw-rw-rw-  2.0 fat    21847 b- defN 23-Jun-12 14:28 skrec/recommender/DENS.py
+-rw-rw-rw-  2.0 fat     6515 b- defN 23-Jun-12 14:28 skrec/recommender/FPMC.py
+-rw-rw-rw-  2.0 fat    12713 b- defN 23-Jun-12 14:28 skrec/recommender/GRU4Rec.py
+-rw-rw-rw-  2.0 fat    14524 b- defN 23-Jun-12 14:28 skrec/recommender/GRU4RecPlus.py
+-rw-rw-rw-  2.0 fat     9535 b- defN 23-Jun-12 14:28 skrec/recommender/HGN.py
+-rw-rw-rw-  2.0 fat    10543 b- defN 23-Jun-12 14:28 skrec/recommender/LightGCL.py
+-rw-rw-rw-  2.0 fat     9426 b- defN 23-Jun-12 14:28 skrec/recommender/LightGCN.py
+-rw-rw-rw-  2.0 fat     8593 b- defN 23-Jun-12 14:28 skrec/recommender/MultVAE.py
+-rw-rw-rw-  2.0 fat     1624 b- defN 23-Jun-12 14:28 skrec/recommender/Pop.py
+-rw-rw-rw-  2.0 fat    21580 b- defN 23-Jun-12 14:28 skrec/recommender/SASRec.py
+-rw-rw-rw-  2.0 fat    15607 b- defN 23-Jun-12 14:28 skrec/recommender/SGAT.py
+-rw-rw-rw-  2.0 fat    15322 b- defN 23-Jun-12 14:28 skrec/recommender/SRGNN.py
+-rw-rw-rw-  2.0 fat     6945 b- defN 23-Jun-12 14:28 skrec/recommender/TransRec.py
+-rw-rw-rw-  2.0 fat       86 b- defN 23-Jun-12 14:28 skrec/recommender/__init__.py
+-rw-rw-rw-  2.0 fat     2201 b- defN 23-Jun-12 14:28 skrec/recommender/base.py
+-rw-rw-rw-  2.0 fat     3956 b- defN 23-Jun-12 14:28 skrec/recommender/AOBPR/AOBPR.py
+-rw-rw-rw-  2.0 fat       73 b- defN 23-Jun-12 14:28 skrec/recommender/AOBPR/__init__.py
+-rw-rw-rw-  2.0 fat    68608 b- defN 23-Jun-12 14:30 skrec/recommender/AOBPR/pyx_aobpr_func.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat     8077 b- defN 23-Jun-12 14:28 skrec/recommender/BERT4Rec/BERT4Rec.py
+-rw-rw-rw-  2.0 fat       88 b- defN 23-Jun-12 14:28 skrec/recommender/BERT4Rec/__init__.py
+-rw-rw-rw-  2.0 fat    18471 b- defN 23-Jun-12 14:28 skrec/recommender/BERT4Rec/bert4rec_gen_data.py
+-rw-rw-rw-  2.0 fat    11695 b- defN 23-Jun-12 14:28 skrec/recommender/BERT4Rec/bert4rec_utils.py
+-rw-rw-rw-  2.0 fat    41607 b- defN 23-Jun-12 14:28 skrec/recommender/BERT4Rec/modeling.py
+-rw-rw-rw-  2.0 fat     6676 b- defN 23-Jun-12 14:28 skrec/recommender/BERT4Rec/optimization.py
+-rw-rw-rw-  2.0 fat     2336 b- defN 23-Jun-12 14:28 skrec/recommender/BERT4Rec/vocab.py
+-rw-rw-rw-  2.0 fat      127 b- defN 23-Jun-12 14:28 skrec/utils/__init__.py
+-rw-rw-rw-  2.0 fat     1494 b- defN 23-Jun-12 14:28 skrec/utils/common.py
+-rw-rw-rw-  2.0 fat     1255 b- defN 23-Jun-12 14:28 skrec/utils/registry.py
+-rw-rw-rw-  2.0 fat     1265 b- defN 23-Jun-12 14:28 skrec/utils/tf1x.py
+-rw-rw-rw-  2.0 fat       86 b- defN 23-Jun-12 14:28 skrec/utils/tf2x.py
+-rw-rw-rw-  2.0 fat     4122 b- defN 23-Jun-12 14:28 skrec/utils/torch.py
+-rw-rw-rw-  2.0 fat      576 b- defN 23-Jun-12 14:28 skrec/utils/py/__init__.py
+-rw-rw-rw-  2.0 fat     6779 b- defN 23-Jun-12 14:28 skrec/utils/py/batch_iterator.py
+-rw-rw-rw-  2.0 fat     2593 b- defN 23-Jun-12 14:28 skrec/utils/py/config.py
+-rw-rw-rw-  2.0 fat     1678 b- defN 23-Jun-12 14:28 skrec/utils/py/decorator.py
+-rw-rw-rw-  2.0 fat    10186 b- defN 23-Jun-12 14:28 skrec/utils/py/evaluator.py
+-rw-rw-rw-  2.0 fat     4876 b- defN 23-Jun-12 14:28 skrec/utils/py/generic.py
+-rw-rw-rw-  2.0 fat     1450 b- defN 23-Jun-12 14:28 skrec/utils/py/random.py
+-rw-rw-rw-  2.0 fat      173 b- defN 23-Jun-12 14:28 skrec/utils/py/cython/__init__.py
+-rw-rw-rw-  2.0 fat    84992 b- defN 23-Jun-12 14:30 skrec/utils/py/cython/pyx_eval_matrix.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat    26112 b- defN 23-Jun-12 14:30 skrec/utils/py/cython/pyx_init.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat   109056 b- defN 23-Jun-12 14:30 skrec/utils/py/cython/pyx_random.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat   115712 b- defN 23-Jun-12 14:30 skrec/utils/py/cython/pyx_sort.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat    30720 b- defN 23-Jun-12 14:30 skrec/utils/py/cython/pyx_utils.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-12 14:30 scikit_recommender-0.0.4.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     7783 b- defN 23-Jun-12 14:30 scikit_recommender-0.0.4.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 23-Jun-12 14:30 scikit_recommender-0.0.4.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        6 b- defN 23-Jun-12 14:30 scikit_recommender-0.0.4.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     5312 b- defN 23-Jun-12 14:30 scikit_recommender-0.0.4.dist-info/RECORD
+61 files, 827109 bytes uncompressed, 297097 bytes compressed:  64.1%
```

## zipnote {}

```diff
@@ -1,10 +1,13 @@
 Filename: skrec/__init__.py
 Comment: 
 
+Filename: skrec/run_config.py
+Comment: 
+
 Filename: skrec/io/__init__.py
 Comment: 
 
 Filename: skrec/io/data_iterator.py
 Comment: 
 
 Filename: skrec/io/dataset.py
@@ -27,26 +30,32 @@
 
 Filename: skrec/recommender/CML.py
 Comment: 
 
 Filename: skrec/recommender/Caser.py
 Comment: 
 
+Filename: skrec/recommender/DENS.py
+Comment: 
+
 Filename: skrec/recommender/FPMC.py
 Comment: 
 
 Filename: skrec/recommender/GRU4Rec.py
 Comment: 
 
 Filename: skrec/recommender/GRU4RecPlus.py
 Comment: 
 
 Filename: skrec/recommender/HGN.py
 Comment: 
 
+Filename: skrec/recommender/LightGCL.py
+Comment: 
+
 Filename: skrec/recommender/LightGCN.py
 Comment: 
 
 Filename: skrec/recommender/MultVAE.py
 Comment: 
 
 Filename: skrec/recommender/Pop.py
@@ -153,23 +162,23 @@
 
 Filename: skrec/utils/py/cython/pyx_sort.cp39-win_amd64.pyd
 Comment: 
 
 Filename: skrec/utils/py/cython/pyx_utils.cp39-win_amd64.pyd
 Comment: 
 
-Filename: scikit_recommender-0.0.3.dist-info/LICENSE
+Filename: scikit_recommender-0.0.4.dist-info/LICENSE
 Comment: 
 
-Filename: scikit_recommender-0.0.3.dist-info/METADATA
+Filename: scikit_recommender-0.0.4.dist-info/METADATA
 Comment: 
 
-Filename: scikit_recommender-0.0.3.dist-info/WHEEL
+Filename: scikit_recommender-0.0.4.dist-info/WHEEL
 Comment: 
 
-Filename: scikit_recommender-0.0.3.dist-info/top_level.txt
+Filename: scikit_recommender-0.0.4.dist-info/top_level.txt
 Comment: 
 
-Filename: scikit_recommender-0.0.3.dist-info/RECORD
+Filename: scikit_recommender-0.0.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## skrec/__init__.py

```diff
@@ -2,10 +2,11 @@
 __email__ = "zhongchuansun@gmail.com"
 
 # __all__ = []
 
 from .io import *
 from .utils.py import *
 from .utils import *
+from .run_config import RunConfig
 
 import colorama
 colorama.init()
```

## skrec/io/__init__.py

```diff
@@ -1,17 +1,20 @@
 __author__ = "Zhongchuan Sun"
 __email__ = "zhongchuansun@gmail.com"
 
-from .dataset import Dataset
+from .dataset import CFDataset
 from .dataset import ImplicitFeedback
+from .dataset import KnowledgeGraph
+from .dataset import KGDataset
 
 from .data_iterator import PointwiseIterator
 from .data_iterator import PairwiseIterator
 from .data_iterator import SequentialPointwiseIterator
 from .data_iterator import SequentialPairwiseIterator
 from .data_iterator import UserVecIterator
 from .data_iterator import ItemVecIterator
+from .data_iterator import KGPairwiseIterator
 
 from .logger import Logger
 
 from .movielens import MovieLens100k
 from .preprocessor import Preprocessor
```

## skrec/io/data_iterator.py

```diff
@@ -1,24 +1,26 @@
 __author__ = "Zhongchuan Sun"
 __email__ = "zhongchuansun@gmail.com"
 
 __all__ = ["PointwiseIterator", "PairwiseIterator",
            "SequentialPointwiseIterator", "SequentialPairwiseIterator",
-           "UserVecIterator", "ItemVecIterator"
+           "UserVecIterator", "ItemVecIterator",
+           "KGPairwiseIterator"
            ]
 
 from typing import Dict
 from collections import Iterable
 from collections import OrderedDict
 import numpy as np
 from ..utils.py import OrderedDefaultDict
 from ..utils.py import BatchIterator
 from ..utils.py import randint_choice
 from ..utils.py import pad_sequences
-from .dataset import ImplicitFeedback
+from .dataset import ImplicitFeedback, KnowledgeGraph
+from .dataset import _HEAD, _RELATION, _TAIL
 
 
 class _Iterator(object):
     def __iter__(self):
         raise NotImplementedError
 
     def __len__(self):
@@ -79,19 +81,21 @@
 def _sampling_negative_items(user_n_pos: OrderedDict, num_neg: int, num_items: int,
                              user_pos_dict: Dict[int, np.ndarray]):
     assert num_neg > 0, "'num_neg' must be a positive integer."
 
     neg_items_list = []
     for user, n_pos in user_n_pos.items():
         neg_items = randint_choice(num_items, size=n_pos*num_neg, exclusion=user_pos_dict[user])
-        neg_items = neg_items if isinstance(neg_items, Iterable) else np.int32([neg_items])  # only one item
-        neg_items = np.reshape(neg_items, newshape=[n_pos, num_neg])
+        if num_neg == 1:
+            neg_items = neg_items if isinstance(neg_items, Iterable) else np.int32([neg_items])
+        else:
+            neg_items = np.reshape(neg_items, newshape=[n_pos, num_neg])
         neg_items_list.append(neg_items)
 
-    return np.concatenate(neg_items_list, axis=0)
+    return np.concatenate(neg_items_list)
 
 
 class PointwiseIterator(_Iterator):
     """Sample negative items and iterate dataset with pointwise training instances.
 
     The training instances consist of `batch_users`, `batch_items` and
     `batch_labels`, which are lists of users, items and labels. All lengths of
@@ -153,31 +157,34 @@
                                   shuffle=self.shuffle, drop_last=self.drop_last)
 
         for bat_users, bat_items, bat_labels in data_iter:
             yield np.asarray(bat_users), np.asarray(bat_items), np.asarray(bat_labels)
 
 
 class PairwiseIterator(_Iterator):
-    def __init__(self, dataset: ImplicitFeedback,
+    def __init__(self, dataset: ImplicitFeedback, num_neg: int=1,
                  batch_size: int=1024, shuffle: bool=True, drop_last: bool=False):
         """Initializes a new `PairwiseSampler` instance.
 
         Args:
             dataset (data.Interaction): An instance of `data.Interaction`.
             batch_size (int): How many samples per batch to load.
                 Defaults to `1`.
             shuffle (bool): Whether reshuffling the samples at every epoch.
                 Defaults to `False`.
             drop_last (bool): Whether dropping the last incomplete batch.
                 Defaults to `False`.
         """
         super(PairwiseIterator, self).__init__()
+        if num_neg <= 0:
+            raise ValueError("'num_neg' must be a positive integer.")
 
         self.batch_size = batch_size
         self.shuffle = shuffle
+        self.num_neg = num_neg
         self.drop_last = drop_last
         self.num_items = dataset.num_items
         self.user_pos_dict = dataset.to_user_dict()
 
         self.user_n_pos, self.all_users, self.pos_items = \
             _generate_positive_items(self.user_pos_dict)
 
@@ -185,16 +192,16 @@
         n_sample = len(self.all_users)
         if self.drop_last:
             return n_sample // self.batch_size
         else:
             return (n_sample + self.batch_size - 1) // self.batch_size
 
     def __iter__(self):
-        neg_items = _sampling_negative_items(self.user_n_pos, 1, self.num_items,
-                                             self.user_pos_dict).squeeze()
+        neg_items = _sampling_negative_items(self.user_n_pos, self.num_neg,
+                                             self.num_items, self.user_pos_dict)
 
         data_iter = BatchIterator(self.all_users, self.pos_items, neg_items,
                                   batch_size=self.batch_size,
                                   shuffle=self.shuffle, drop_last=self.drop_last)
         for bat_users, bat_pos_items, bat_neg_items in data_iter:
             yield np.asarray(bat_users), np.asarray(bat_pos_items), np.asarray(bat_neg_items)
 
@@ -336,7 +343,81 @@
 
     def __len__(self):
         return len(self.item_iter)
 
     def __iter__(self):
         for bat_items in self.item_iter:
             yield self.item_csr_matrix[bat_items].toarray()
+
+
+def _generate_positive_triples(head_pos_dict: Dict[int, Dict[str, np.ndarray]]):
+    # positive: (h1,r1,t1),  negative (h1,r1,t2)
+    assert head_pos_dict, "'head_pos_dict' cannot be empty."
+
+    list_heads, list_relations, list_tails = [], [], []
+    head_n_pos = OrderedDict()
+
+    for head, rel_tail_dict in head_pos_dict.items():
+        relations = rel_tail_dict[_RELATION]
+        tails = rel_tail_dict[_TAIL]
+        list_tails.append(tails)
+        list_relations.append(relations)
+        list_heads.append(np.full_like(tails, head))
+        head_n_pos[head] = len(tails)
+    heads_ary = np.concatenate(list_heads, axis=0)
+    relations_ary = np.concatenate(list_relations, axis=0)
+    tails_ary = np.concatenate(list_tails, axis=0)
+    return head_n_pos, heads_ary, relations_ary, tails_ary
+
+
+def _sampling_negative_tails(head_n_pos: OrderedDict, num_neg: int, num_entities: int,
+                             head_pos_dict: Dict[int, Dict[str, np.ndarray]]):
+    # positive: (h1,r1,t1),  negative (h1,r1,t2)
+    assert num_neg > 0, "'num_neg' must be a positive integer."
+
+    neg_tails_list = []
+    for head, n_pos in head_n_pos.items():
+        neg_tails = randint_choice(num_entities, size=n_pos*num_neg, exclusion=head_pos_dict[head][_TAIL])
+        if num_neg == 1:
+            neg_tails = neg_tails if isinstance(neg_tails, Iterable) else np.int32([neg_tails])
+        else:
+            neg_tails = np.reshape(neg_tails, newshape=[n_pos, num_neg])
+        neg_tails_list.append(neg_tails)
+
+    return np.concatenate(neg_tails_list)
+
+
+class KGPairwiseIterator(_Iterator):
+    def __init__(self, dataset: KnowledgeGraph, num_neg: int = 1, batch_size: int = 1024,
+                 shuffle: bool = True, drop_last: bool = False):
+        super(KGPairwiseIterator, self).__init__()
+        # positive: (h1,r1,t1),  negative (h1,r1,t2)
+        if num_neg <= 0:
+            raise ValueError("'num_neg' must be a positive integer.")
+
+        self.batch_size = batch_size
+        self.shuffle = shuffle
+        self.num_neg = num_neg
+        self.num_entities = dataset.num_entities
+        self.drop_last = drop_last
+        self.head_pos_dict = dataset.to_head_dict()
+
+        self.head_n_pos, self.all_heads, self.relations, self.pos_tails = \
+            _generate_positive_triples(self.head_pos_dict)
+
+    def __len__(self):
+        n_sample = len(self.all_heads)
+        if self.drop_last:
+            return n_sample // self.batch_size
+        else:
+            return (n_sample + self.batch_size - 1) // self.batch_size
+
+    def __iter__(self):
+        neg_tails = _sampling_negative_tails(self.head_n_pos, self.num_neg,
+                                             self.num_entities, self.head_pos_dict)
+
+        data_iter = BatchIterator(self.all_heads, self.relations, self.pos_tails, neg_tails,
+                                  batch_size=self.batch_size,
+                                  shuffle=self.shuffle, drop_last=self.drop_last)
+        for bat_heads, bat_relations, bat_pos_tails, bat_neg_tails in data_iter:
+            yield np.asarray(bat_heads), np.asarray(bat_relations), \
+                  np.asarray(bat_pos_tails), np.asarray(bat_neg_tails)
```

## skrec/io/dataset.py

```diff
@@ -1,90 +1,93 @@
 __author__ = "Zhongchuan Sun"
 __email__ = "zhongchuansun@gmail.com"
 
-__all__ = ["ImplicitFeedback", "Dataset"]
+__all__ = ["ImplicitFeedback", "KnowledgeGraph", "CFDataset", "KGDataset"]
 
 
 import os
 import pickle
 import warnings
-from typing import Dict, Callable
+from typing import Dict, Callable, List
 from copy import deepcopy
-from functools import wraps
+import functools
 from collections import OrderedDict
 import numpy as np
 import pandas as pd
 import scipy.sparse as sp
-import weakref
-from ..utils.py import pad_sequences, md5sum
+import atexit
+from ..utils.py import pad_sequences
+from ..utils.common import PostInitMeta
 
 _USER = "user"
 _ITEM = "item"
 _RATING = "rating"
 _TIME = "time"
 _DColumns = {"UI": [_USER, _ITEM],
              "UIR": [_USER, _ITEM, _RATING],
              "UIT": [_USER, _ITEM, _TIME],
              "UIRT": [_USER, _ITEM, _RATING, _TIME]
              }
 
+_HEAD = "head"
+_TAIL = "tail"
+_RELATION = "relation"
 
-class Interaction(object):
-    def __init__(self):
-        self._buffer = dict()
-        self._buffer_modified_flag = False
 
-    def is_empty(self) -> bool:
-        raise NotImplementedError
+class DataCacheABC(object):
+    def __init__(self):
+        self._cache_buffer = dict()
+        self._cache_modified = False
 
-    def _exist_in_buffer(self, name):
-        return name in self._buffer
+    def _is_cached(self, key) -> bool:
+        return key in self._cache_buffer
 
-    def _write_to_buffer(self, name, value):
-        self._buffer[name] = value
-        self._buffer_modified_flag = True
+    def _get_from_cache(self, key):
+        return deepcopy(self._cache_buffer[key])
 
-    def _read_from_buffer(self, name):
-        return deepcopy(self._buffer[name])
+    def _set_to_cache(self, key, value):
+        self._cache_buffer[key] = value
+        self._cache_modified = True
 
-    def _clean_buffer(self):
-        self._buffer.clear()
-        self._buffer_modified_flag = True
+    def clear_cache(self):
+        self._cache_buffer.clear()
+        self._cache_modified = True
 
-    def is_buffer_modified(self):
-        return self._buffer_modified_flag
+    def loads_cached_data(self, cached_data: Dict):
+        self._cache_buffer = deepcopy(cached_data)
 
-    def reset_buffer_flag(self):
-        self._buffer_modified_flag = False
+    def dumps_cached_data(self) -> Dict:
+        return deepcopy(self._cache_buffer)
 
-    def __setstate__(self, state):
-        self.__dict__ = state
-        self.reset_buffer_flag()  # reset flag after pickle.load()
+    def is_cache_modified(self) -> bool:
+        return self._cache_modified
 
 
-def fetch_data(data_generator):
+def data_cache(func):
     # read from buffer
-    @wraps(data_generator)
-    def wrapper(self: Interaction, *args, **kwargs):
-        _data_name = data_generator.__name__
-        if self.is_empty():
-            raise ValueError("data is empty!")
-
-        if self._exist_in_buffer(_data_name) is False:
-            _data = data_generator(self, *args, **kwargs)
-            self._write_to_buffer(_data_name, _data)
-
-        return self._read_from_buffer(_data_name)
+    @functools.wraps(func)
+    def wrapper(self: DataCacheABC, *args, **kwargs):
+        # Generate a cache key based on the function name and arguments
+        cache_key = pickle.dumps((func.__name__, args, kwargs))
+
+        if self._is_cached(cache_key):
+            # If the cached result exists, retrieve and return it
+            return self._get_from_cache(cache_key)
+        else:
+            # Otherwise, process the data and store the result in the cache
+            result = func(self, *args, **kwargs)
+            self._set_to_cache(cache_key, result)
+            return result
 
     return wrapper
 
 
-class ImplicitFeedback(Interaction):
+class ImplicitFeedback(DataCacheABC):
     def __init__(self, data: pd.DataFrame=None, num_users: int=None, num_items: int=None):
-        super(ImplicitFeedback, self).__init__()
+        super().__init__()
         assert data is None or isinstance(data, pd.DataFrame)
 
         if data is None or data.empty:
             self._data = pd.DataFrame()
             self.num_users = 0
             self.num_items = 0
             self.num_ratings = 0
@@ -93,78 +96,79 @@
             self.num_users = num_users if num_users is not None else max(data[_USER]) + 1
             self.num_items = num_items if num_items is not None else max(data[_ITEM]) + 1
             self.num_ratings = len(data)
 
     def is_empty(self) -> bool:
         return self._data is None or self._data.empty
 
-    @fetch_data
+    @data_cache
     def to_user_item_pairs(self) -> np.ndarray:
         ui_pairs = self._data[[_USER, _ITEM]].to_numpy(copy=True, dtype=np.int32)
         return ui_pairs
 
-    @fetch_data
+    @data_cache
     def to_user_item_pairs_by_time(self) -> np.ndarray:
         if _TIME not in self._data:
             raise ValueError("This dataset do not contain timestamp.")
         data_uit = self._data[[_USER, _ITEM, _TIME]]
         data_uit = data_uit.sort_values(by=["user", "time"], inplace=False)
         data_ui = data_uit[[_USER, _ITEM]].to_numpy(copy=True, dtype=np.int32)
         return data_ui
 
-    @fetch_data
+    @data_cache
     def to_csr_matrix(self) -> sp.csr_matrix:
         users, items = self._data[_USER].to_numpy(), self._data[_ITEM].to_numpy()
         ratings = np.ones(len(users), dtype=np.float32)
         csr_mat = sp.csr_matrix((ratings, (users, items)), shape=(self.num_users, self.num_items), copy=True)
         return csr_mat
 
-    @fetch_data
+    @data_cache
     def to_csc_matrix(self) -> sp.csc_matrix:
         return self.to_csr_matrix().tocsc()
 
-    @fetch_data
+    @data_cache
     def to_dok_matrix(self) -> sp.dok_matrix:
         return self.to_csr_matrix().todok()
 
-    @fetch_data
+    @data_cache
     def to_coo_matrix(self) -> sp.coo_matrix:
         return self.to_csr_matrix().tocoo()
 
-    @fetch_data
+    @data_cache
     def to_user_dict(self) -> Dict[int, np.ndarray]:
         user_dict = OrderedDict()
         user_grouped = self._data.groupby(_USER)
         for user, user_data in user_grouped:
             user_dict[user] = user_data[_ITEM].to_numpy(dtype=np.int32)
         return user_dict
 
-    @fetch_data
+    @data_cache
     def to_user_dict_by_time(self) -> Dict[int, np.ndarray]:
         # in chronological
         if _TIME not in self._data:
             raise ValueError("This dataset do not contain timestamp.")
 
         user_dict = OrderedDict()
         user_grouped = self._data.groupby(_USER)
         for user, user_data in user_grouped:
             user_data = user_data.sort_values(by=[_TIME])
             user_dict[user] = user_data[_ITEM].to_numpy(dtype=np.int32)
 
         return user_dict
 
-    @fetch_data
+    @data_cache
     def to_item_dict(self) -> Dict[int, np.ndarray]:
         item_dict = OrderedDict()
         item_grouped = self._data.groupby(_ITEM)
         for item, item_data in item_grouped:
             item_dict[item] = item_data[_USER].to_numpy(dtype=np.int32)
 
         return item_dict
 
+    @data_cache
     def to_truncated_seq_dict(self, max_len: int, pad_value: int=0,
                               padding='pre', truncating='pre') -> Dict[int, np.ndarray]:
         user_seq_dict = self.to_user_dict_by_time()
         if max_len is None:
             max_len = max([len(seqs) for seqs in user_seq_dict.values()])
         item_seq_list = [item_seq[-max_len:] for item_seq in user_seq_dict.values()]
         item_seq_arr = pad_sequences(item_seq_list, value=pad_value, max_len=max_len,
@@ -174,17 +178,95 @@
                                 zip(user_seq_dict.keys(), item_seq_arr)])
         return seq_dict
 
     def __len__(self):
         return len(self._data)
 
 
-class Dataset(object):
+class KnowledgeGraph(DataCacheABC):
+    def __init__(self, data: pd.DataFrame=None, num_entities: int=None, num_relations: int=None):
+        super().__init__()
+        assert data is None or isinstance(data, pd.DataFrame)
+
+        if data is None or data.empty:
+            self._data = pd.DataFrame()
+            self.num_entities = 0
+            self.num_relations = 0
+            self.num_triplets = 0
+        else:
+            self._data = data
+            self.num_entities = num_entities if num_entities is not None else max(max(data[_HEAD]), max(data[_TAIL])) + 1
+            self.num_relations = num_relations if num_relations is not None else max(data[_RELATION]) + 1
+            self.num_triplets = len(data)
+
+    def is_empty(self) -> bool:
+        return self._data is None or self._data.empty
+
+    @data_cache
+    def to_triplets(self) -> np.ndarray:
+        triplets = self._data[[_HEAD, _RELATION, _TAIL]].to_numpy(copy=True, dtype=np.int32)
+        return triplets
+
+    @data_cache
+    def to_head_dict(self) -> Dict[int, Dict[str, np.ndarray]]:
+        head_dict = OrderedDict()
+        head_grouped = self._data.groupby(_HEAD)
+        for head, head_data in head_grouped:
+            head_dict[head] = {_RELATION: head_data[_RELATION].to_numpy(dtype=np.int32),
+                               _TAIL: head_data[_TAIL].to_numpy(dtype=np.int32)}
+        return head_dict
+
+    @data_cache
+    def to_tail_dict(self) -> Dict[int, Dict[str, np.ndarray]]:
+        tail_dict = OrderedDict()
+        tail_grouped = self._data.groupby(_TAIL)
+        for tail, tail_data in tail_grouped:
+            tail_dict[tail] = {_RELATION: tail_data[_RELATION].to_numpy(dtype=np.int32),
+                               _HEAD: tail_data[_HEAD].to_numpy(dtype=np.int32)}
+        return tail_dict
+
+    @data_cache
+    def to_relation_dict(self) -> Dict[int, Dict[str, np.ndarray]]:
+        rel_dict = OrderedDict()
+        rel_grouped = self._data.groupby(_RELATION)
+        for rel, rel_data in rel_grouped:
+            rel_dict[rel] = {_HEAD: rel_data[_HEAD].to_numpy(dtype=np.int32),
+                             _TAIL: rel_data[_TAIL].to_numpy(dtype=np.int32)}
+        return rel_dict
+
+    @data_cache
+    def to_csr_matrix_dict(self) -> Dict[int, sp.csr_matrix]:
+        rel_csr_dict = OrderedDict()
+        rel_dict = self.to_relation_dict()
+        for rel, data in rel_dict.items():
+            heads, tails = data[_HEAD], data[_TAIL]
+            pass
+            # users, items = self._data[_USER].to_numpy(), self._data[_ITEM].to_numpy()
+            ones = np.ones(len(heads), dtype=np.float32)
+            csr_mat = sp.csr_matrix((ones, (heads, tails)), shape=(self.num_entities, self.num_entities), copy=True)
+            rel_csr_dict[rel] = csr_mat
+        return rel_csr_dict
+
+    @data_cache
+    def to_coo_matrix_dict(self) -> Dict[int, sp.coo_matrix]:
+        rel_coo_dict = OrderedDict()
+        rel_csr_dict = self.to_csr_matrix_dict()
+        for rel, data in rel_csr_dict.items():
+            rel_coo_dict[rel] = data.tocsc()
+        return rel_coo_dict
+
+
+class SocialNetwork(DataCacheABC):
+    # TODO
+    pass
+
+
+class CFDataset(metaclass=PostInitMeta):
     def __init__(self, data_dir, sep, columns):
-        """Dataset
+        """CFDataset
 
         Notes:
             The prefix name of data files is same as the data_dir, and the
             suffix/extension names are 'train', 'test', 'user2id', 'item2id'.
             Directory structure:
                 data_dir
                     ├── data_dir.train      // training data
@@ -197,77 +279,33 @@
             data_dir: The directory of dataset.
             sep: The separator/delimiter of file columns.
             columns: The format of columns, must be one of 'UI',
                 'UIR', 'UIT' and 'UIRT'
         """
 
         self._data_dir = data_dir
+        self._cache_file = os.path.join(self.data_dir, "_cache_" + self.data_name + ".bin")
+        self._load_cf_data(sep, columns)
 
-        # metadata
-        self.train_data = ImplicitFeedback()
-        self.valid_data = ImplicitFeedback()
-        self.test_data = ImplicitFeedback()
-        self.user2id = None
-        self.item2id = None
-        self.id2user = None
-        self.id2item = None
-
-        # statistic
-        self.num_users = 0
-        self.num_items = 0
-        self.num_ratings = 0
-        self._my_md5 = ""
-        self._cache_file = os.path.join(self.data_dir, "_cache_" + self.data_name + ".pkl")
-        self._load_data(sep, columns)
-        weakref.finalize(self, self._destructor)
+    def __post_init__(self):
+        self._restore_cached_data()
+        atexit.register(self._save_cached_data)
 
     @property
     def data_name(self):
         return os.path.split(self.data_dir)[-1]
 
     @property
     def data_dir(self):
         return self._data_dir
 
     @property
     def _file_prefix(self):
         return os.path.join(self.data_dir, self.data_name)
 
-    def _load_data(self, sep, columns):
-        if os.path.exists(self._cache_file):
-            try:
-                with open(self._cache_file, 'rb') as fin:
-                    _t_data: Dataset = pickle.load(fin)
-                if _t_data._my_md5 == self._raw_md5:
-                    _t_data._data_dir = self._data_dir  # keep data path up-to-date
-                    self.__dict__ = _t_data.__dict__
-                    return
-            except EOFError as e:
-                pass
-
-        self._load_from_raw(sep, columns)
-
-    def _dump_data(self):
-        with open(self._cache_file, 'wb') as fout:
-            pickle.dump(self, fout)
-
-    def _destructor(self):
-        if self.train_data.is_buffer_modified() or \
-                self.valid_data.is_buffer_modified() or \
-                self.test_data.is_buffer_modified():
-            self._dump_data()
-
-    @property
-    def _raw_md5(self):
-        files = [self._file_prefix+postfix for postfix in (".train", ".valid", ".test")
-                 if os.path.isfile(self._file_prefix+postfix)]
-        md5summary = md5sum(*files)
-        md5summary = "_".join([md5 for md5 in md5summary if md5 is not None])
-        return md5summary
-
     @staticmethod
     def _read_csv(csv_file, sep, header, names, handle: Callable=lambda x: x):
         if os.path.isfile(csv_file):
             csv_data = pd.read_csv(csv_file, sep=sep, header=header, names=names)
         else:
             handle(f"'{csv_file}' does not exist.")
             csv_data = pd.DataFrame()
@@ -281,56 +319,53 @@
             reverses = OrderedDict([(second, first) for first, second in maps.items()])
         else:
             maps = None
             reverses = None
             warnings.warn(f"'{map_file}' does not exist.")
         return maps, reverses
 
-    def _load_from_raw(self, sep, columns):
+    def _load_cf_data(self, sep, columns):
+        # Load collaborative filtering model data
         if columns not in _DColumns:
             key_str = ", ".join(_DColumns.keys())
             raise ValueError("'columns' must be one of '%s'." % key_str)
 
         columns = _DColumns[columns]
 
         # load data
         def raise_error(err: str): raise FileNotFoundError(err)
-        _train_data = self._read_csv(self._file_prefix + ".train",
-                                     sep=sep, header=None, names=columns,
-                                     handle=raise_error)
-        _valid_data = self._read_csv(self._file_prefix + ".valid",
-                                     sep=sep, header=None, names=columns,
-                                     handle=warnings.warn)
-        _test_data = self._read_csv(self._file_prefix + ".test",
-                                    sep=sep, header=None, names=columns,
-                                    handle=raise_error)
+        _train_data = self._read_csv(self._file_prefix + ".train", sep=sep, names=columns,
+                                     header=None, handle=raise_error)
+        _valid_data = self._read_csv(self._file_prefix + ".valid", sep=sep, names=columns,
+                                     header=None, handle=warnings.warn)
+        _test_data = self._read_csv(self._file_prefix + ".test", sep=sep, names=columns,
+                                    header=None, handle=raise_error)
+
         if _train_data.isnull().values.any():
             warnings.warn(f"'Training data has None value, please check the file or the separator.")
         if _valid_data.isnull().values.any():
             warnings.warn(f"'Validation data has None value, please check the file or the separator.")
         if _test_data.isnull().values.any():
             warnings.warn(f"'Test data has None value, please check the file or the separator.")
+
         self.user2id, self.id2user = self._read_map_file(self._file_prefix + ".user2id", sep)
         self.item2id, self.id2item = self._read_map_file(self._file_prefix + ".item2id", sep)
 
         # statistical information
         data_info = [(max(data[_USER]), max(data[_ITEM]), len(data))
                      for data in [_train_data, _valid_data, _test_data] if not data.empty]
         self.num_users = max([d[0] for d in data_info]) + 1
         self.num_items = max([d[1] for d in data_info]) + 1
         self.num_ratings = sum([d[2] for d in data_info])
 
-        # convert to to the object of Interaction
+        # convert to to the object of ImplicitFeedback
         self.train_data = ImplicitFeedback(_train_data, num_users=self.num_users, num_items=self.num_items)
         self.valid_data = ImplicitFeedback(_valid_data, num_users=self.num_users, num_items=self.num_items)
         self.test_data = ImplicitFeedback(_test_data, num_users=self.num_users, num_items=self.num_items)
 
-        self._my_md5 = self._raw_md5
-        self._dump_data()
-
     @property
     def statistic_info(self):
         """The statistic of dataset.
 
         Returns:
             str: The summary of statistic information
         """
@@ -350,9 +385,144 @@
                          f"Average actions of items: {(1.0 * num_ratings / num_items):.2f}",
                          f"The sparsity of the dataset: {(sparsity * 100):.6f}%%",
                          "",
                          f"The number of training: {len(self.train_data)}",
                          f"The number of validation: {len(self.valid_data)}",
                          f"The number of testing: {len(self.test_data)}"
                          ]
+
             statistic = "\n".join(statistic)
             return statistic
+
+    def _read_from_cache_file(self):
+        cache_data = dict()
+        try:
+            with open(self._cache_file, 'rb') as fin:
+                cache_data = pickle.load(fin)
+        except Exception as e:
+            warnings.warn(f"_read_cache_file error: {e}")
+        return cache_data
+
+    def _write_to_cache_file(self, cache_data):
+        try:
+            with open(self._cache_file, 'wb') as fout:
+                pickle.dump(cache_data, fout)
+        except Exception as e:
+            warnings.warn(f"_write_to_cache_file error: {e}")
+
+    def _update_cache_file(self, new_caches):
+        cache_data = dict()
+        if os.path.exists(self._cache_file):
+            cache_data = self._read_from_cache_file()
+
+        cache_data.update(new_caches)
+        self._write_to_cache_file(cache_data)
+
+    def _save_cached_data(self):
+        if not self._is_cache_modified():
+            return
+
+        _t_data = self._dumps_cached_data()
+        # save cached data
+        self._update_cache_file(_t_data)
+
+    def _restore_cached_data(self):
+        if self._is_data_updated():
+            return
+        # restore cached data
+        _t_data = self._read_from_cache_file()
+        try:
+            self._loads_cached_data(_t_data)
+        except Exception as e:
+            warnings.warn(f"_restore_cached_data error: {e}")
+
+    def _is_cache_modified(self) -> bool:
+        return self.train_data.is_cache_modified() or \
+               self.valid_data.is_cache_modified() or \
+               self.test_data.is_cache_modified()
+
+    def _is_data_updated(self) -> bool:
+        if not os.path.exists(self._cache_file):
+            return True
+        cached_time = os.path.getmtime(self._cache_file)
+
+        for file_suffix in [".train", ".test", ".valid"]:
+            filename = self._file_prefix + file_suffix
+            if os.path.exists(filename) and os.path.getmtime(filename) > cached_time:
+                return True
+        return False
+
+    def _dumps_cached_data(self):
+        _t_data = dict()
+        _t_data["train_data"] = self.train_data.dumps_cached_data()
+        _t_data["test_data"] = self.test_data.dumps_cached_data()
+        _t_data["valid_data"] = self.valid_data.dumps_cached_data()
+        return _t_data
+
+    def _loads_cached_data(self, _t_data):
+        # load cached data
+        try:
+            self.train_data.loads_cached_data(_t_data["train_data"])
+            self.test_data.loads_cached_data(_t_data["test_data"])
+            self.valid_data.loads_cached_data(_t_data["valid_data"])
+        except Exception as e:
+            warnings.warn(f"_loads_cached_data error: {e}")
+
+
+class KGDataset(CFDataset):
+    def __init__(self, data_dir, sep, columns):
+        super().__init__(data_dir, sep, columns)
+        self._load_kg_data(sep)
+
+    def _load_kg_data(self, sep):
+        # Load knowledge graph data
+        def raise_error(err: str): raise FileNotFoundError(err)
+        _kg_data = self._read_csv(self._file_prefix + ".kg", sep=sep, names=[_HEAD, _RELATION, _TAIL],
+                                  header=None, handle=raise_error)
+        if _kg_data.isnull().values.any():
+            warnings.warn(f"'Knowledge graph data has None value, please check the file or the separator.")
+        _kg_data = _kg_data.drop_duplicates()
+
+        self.kg_data = KnowledgeGraph(_kg_data)
+        self.num_entities = self.kg_data.num_entities
+        self.num_relations = self.kg_data.num_relations
+        self.num_triplets = self.kg_data.num_triplets
+
+    @property
+    def statistic_info(self):
+        cf_info = super().statistic_info
+        statistic = ["",
+                     f"The number of entities: {self.num_entities}",
+                     f"The number of relations: {self.num_relations}",
+                     f"The number of triplets: {self.num_triplets}"
+                     ]
+        kg_info = "\n".join(statistic)
+
+        return cf_info + kg_info
+
+    def _is_cache_modified(self) -> bool:
+        return super()._is_cache_modified() or self.kg_data.is_cache_modified()
+
+    def _is_data_updated(self) -> bool:
+        if super()._is_data_updated():
+            return True
+        cached_time = os.path.getmtime(self._cache_file)
+        kg_time = os.path.getmtime(self._file_prefix + ".kg")
+        return kg_time > cached_time
+
+    def _dumps_cached_data(self):
+        _t_data = super()._dumps_cached_data()
+        _t_data["kg_data"] = self.kg_data.dumps_cached_data()
+        return _t_data
+
+    def _loads_cached_data(self, _t_data):
+        # load cached data
+        super()._loads_cached_data(_t_data)
+        try:
+            self.kg_data.loads_cached_data(_t_data["kg_data"])
+        except Exception as e:
+            warnings.warn(f"_loads_cached_data error: {e}")
+
+
+class SocialDataset(CFDataset):
+    # TODO
+    pass
```

## skrec/recommender/BPRMF.py

```diff
@@ -9,31 +9,32 @@
 __all__ = ["BPRMF"]
 
 import torch
 import torch.nn as nn
 import numpy as np
 from typing import Dict
 from .base import AbstractRecommender
+from ..io.dataset import CFDataset
+from ..run_config import RunConfig
 from ..utils.py import Config
-from ..io import Dataset
-from ..utils.py import RankingEvaluator, MetricReport
+from ..utils.py import EarlyStopping
 from ..utils.torch import inner_product, bpr_loss, l2_loss, get_initializer
 from ..io import PairwiseIterator
 
 
 class BPRMFConfig(Config):
     def __init__(self,
                  lr=1e-3,
                  reg=1e-3,
                  n_dim=64,
                  batch_size=1024,
                  epochs=1000,
                  early_stop=200,
                  **kwargs):
-        super(BPRMFConfig, self).__init__()
+        super().__init__()
         self.lr: float = lr
         self.reg: float = reg
         self.n_dim: int = n_dim
         self.batch_size: int = batch_size
         self.epochs: int = epochs
         self.early_stop: int = early_stop
         self._validate()
@@ -79,34 +80,31 @@
         user_embeds = self.user_embeddings(user_ids)
         ratings = torch.matmul(user_embeds, self.item_embeddings.weight.T)
         ratings += self.item_biases.weight.squeeze()
         return ratings
 
 
 class BPRMF(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = BPRMFConfig(**cfg_dict)
-        super(BPRMF, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
+    def __init__(self, run_config: RunConfig, model_config: Dict):
+        self.dataset = CFDataset(run_config.data_dir, run_config.sep, run_config.file_column)
+        self.config = BPRMFConfig(**model_config)
+        super().__init__(run_config, self.config, self.dataset)
         self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
         self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
 
-        self.mf = _MF(self.num_users, self.num_items, config.n_dim).to(self.device)
-        self.optimizer = torch.optim.Adam(self.mf.parameters(), lr=config.lr)
+        self.mf = _MF(self.num_users, self.num_items, self.config.n_dim).to(self.device)
+        self.optimizer = torch.optim.Adam(self.mf.parameters(), lr=self.config.lr)
 
     def fit(self):
         data_iter = PairwiseIterator(self.dataset.train_data,
                                      batch_size=self.config.batch_size,
                                      shuffle=True, drop_last=False)
 
         self.logger.info("metrics:".ljust(12)+f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
+        early_stopping = EarlyStopping(metric="NDCG@10", patience=self.config.early_stop)
         for epoch in range(self.config.epochs):
             self.mf.train()
             for bat_users, bat_pos_items, bat_neg_items in data_iter:
                 bat_users = torch.from_numpy(bat_users).long().to(self.device)
                 bat_pos_items = torch.from_numpy(bat_pos_items).long().to(self.device)
                 bat_neg_items = torch.from_numpy(bat_neg_items).long().to(self.device)
                 yui = self.mf(bat_users, bat_pos_items)
@@ -122,23 +120,20 @@
                 loss += self.config.reg * reg_loss
                 self.optimizer.zero_grad()
                 loss.backward()
                 self.optimizer.step()
 
             cur_result = self.evaluate()
             self.logger.info(f"epoch {epoch}:".ljust(12)+f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
+            if early_stopping(cur_result):
                 self.logger.info("early stop")
                 break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-        self.logger.info("best:".ljust(12)+f"\t{best_result.values_str}")
 
-    def evaluate(self) -> MetricReport:
+        self.logger.info("best:".ljust(12) + f"\t{early_stopping.best_result.values_str}")
+
+    def evaluate(self):
         self.mf.eval()
         return self.evaluator.evaluate(self)
 
     def predict(self, users) -> np.ndarray:
         users = torch.from_numpy(np.asarray(users)).long().to(self.device)
         return self.mf.predict(users).cpu().detach().numpy()
```

## skrec/recommender/CDAE.py

```diff
@@ -12,21 +12,22 @@
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 import torch.sparse as torch_sp
 from typing import Dict
 from .base import AbstractRecommender
 from ..utils.py import Config
-from ..io import Dataset
-from ..utils.py import RankingEvaluator, MetricReport
+from ..utils.py import EarlyStopping
 from ..utils.py import BatchIterator
 from ..utils.torch import l2_loss, get_initializer, inner_product
 from ..utils.torch import square_loss, sigmoid_cross_entropy
 from ..utils.torch import sp_mat_to_sp_tensor, dropout_sparse
 from ..utils.py import randint_choice
+from ..io.dataset import CFDataset
+from ..run_config import RunConfig
 
 
 class CDAEConfig(Config):
     def __init__(self,
                  lr=0.001,
                  reg=0.001,
                  hidden_dim=64,
@@ -34,15 +35,15 @@
                  num_neg=5,
                  hidden_act="sigmoid",
                  loss_func="sigmoid_cross_entropy",
                  batch_size=256,
                  epochs=1000,
                  early_stop=200,
                  **kwargs):
-        super(CDAEConfig, self).__init__()
+        super().__init__()
         self.lr: float = lr
         self.reg: float = reg
         self.hidden_dim: int = hidden_dim
         self.dropout: float = dropout
         self.num_neg: int = num_neg
         self.hidden_act: str = hidden_act  # hidden_act = identity, sigmoid
         self.loss_func: str = loss_func  # loss_func = sigmoid_cross_entropy, square
@@ -128,50 +129,47 @@
         user_emb = self._encoding(user_ids, sp_item_mat)  # (b,d)
         ratings = user_emb.matmul(self.de_embeddings.weight.T)  # (b,d)x(d,n)->(b,n)
         ratings += self.de_bias.weight.view([1, -1])
         return ratings
 
 
 class CDAE(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = CDAEConfig(**cfg_dict)
-        super(CDAE, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
+    def __init__(self, run_config: RunConfig, model_config: Dict):
+        self.dataset = CFDataset(run_config.data_dir, run_config.sep, run_config.file_column)
+        self.config = CDAEConfig(**model_config)
+        super().__init__(run_config, self.config, self.dataset)
 
         self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
         self.train_csr_mat = self.dataset.train_data.to_csr_matrix()
         self.train_csr_mat.data[:] = 1.0
         self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
 
         if self.config.hidden_act == "identity":
             hidden_act = nn.Identity()
         elif self.config.hidden_act == "sigmoid":
             hidden_act = nn.Sigmoid()
         else:
             raise ValueError(f"hidden activate function '{self.config.hidden_act}' is invalid.")
 
-        if config.loss_func == "sigmoid_cross_entropy":
+        if self.config.loss_func == "sigmoid_cross_entropy":
             self.loss_func = sigmoid_cross_entropy
-        elif config.loss_func == "square_loss":
+        elif self.config.loss_func == "square_loss":
             self.loss_func = square_loss
         else:
             raise ValueError(f"loss function '{self.config.loss_func}' is invalid.")
 
         self.cdae = _CDAE(self.num_users, self.num_items, self.config.hidden_dim,
                           self.config.dropout, hidden_act).to(self.device)
         self.optimizer = torch.optim.Adam(self.cdae.parameters(), lr=self.config.lr)
 
     def fit(self):
         train_users = [user for user in range(self.num_users) if self.train_csr_mat[user].nnz]
         user_iter = BatchIterator(train_users, batch_size=self.config.batch_size, shuffle=True, drop_last=False)
         self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
+        early_stopping = EarlyStopping(metric="NDCG@10", patience=self.config.early_stop)
         for epoch in range(self.config.epochs):
             self.cdae.train()
             for bat_users in user_iter:
                 bat_sp_mat = self.train_csr_mat[bat_users]
                 bat_items = []
                 bat_labels = []
                 bat_idx = []  # used to decoder
@@ -207,23 +205,19 @@
 
                 loss += self.config.reg * reg_loss
                 self.optimizer.zero_grad()
                 loss.backward()
                 self.optimizer.step()
             cur_result = self.evaluate()
             self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
+            if early_stopping(cur_result):
                 self.logger.info("early stop")
                 break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
 
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+        self.logger.info("best:".ljust(12) + f"\t{early_stopping.best_result.values_str}")
 
     def evaluate(self):
         self.cdae.eval()
         return self.evaluator.evaluate(self)
 
     def predict(self, users):
         user_ids = torch.from_numpy(np.asarray(users)).long().to(self.device)
```

## skrec/recommender/CML.py

```diff
@@ -10,34 +10,35 @@
 __all__ = ["CML"]
 
 import tensorflow as tf
 from tensorflow import keras
 from typing import Dict
 from .base import AbstractRecommender
 from ..utils.py import Config
-from ..io import Dataset
-from ..utils.py import RankingEvaluator, MetricReport
+from ..utils.py import EarlyStopping
 from ..utils.py import BatchIterator
 from ..utils.tf1x import euclidean_distance, hinge_loss
 from ..io.data_iterator import _generate_positive_items, _sampling_negative_items
+from ..io.dataset import CFDataset
+from ..run_config import RunConfig
 
 
 class CMLConfig(Config):
     def __init__(self,
                  lr=0.05,
                  reg=10.0,
                  embed_size=64,
                  margin=0.5,
                  clip_norm=1.0,
                  dns=10,
                  batch_size=256,
                  epochs=500,
                  early_stop=100,
                  **kwargs):
-        super(CMLConfig, self).__init__()
+        super().__init__()
         self.lr: float = lr
         self.reg: float = reg
         self.embed_size: int = embed_size
         self.margin: float = margin
         self.clip_norm: float = clip_norm
         self.dns: int = dns  # dns > 1
         self.batch_size: int = batch_size
@@ -54,20 +55,18 @@
         assert isinstance(self.dns, int) and self.dns > 0
         assert isinstance(self.batch_size, int) and self.batch_size > 0
         assert isinstance(self.epochs, int) and self.epochs >= 0
         assert isinstance(self.early_stop, int)
 
 
 class CML(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = CMLConfig(**cfg_dict)
-        super(CML, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
+    def __init__(self, run_config: RunConfig, model_config: Dict):
+        self.dataset = CFDataset(run_config.data_dir, run_config.sep, run_config.file_column)
+        self.config = CMLConfig(**model_config)
+        super().__init__(run_config, self.config, self.dataset)
 
         self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
         self.user_pos_train = self.dataset.train_data.to_user_dict()
 
         self._build_model()
         tf_config = tf.ConfigProto()  # allow_soft_placement=False, log_device_placement=True
         tf_config.gpu_options.allow_growth = True
@@ -154,38 +153,33 @@
         user_embedding = tf.reshape(self.user_elayer(self.user_h), [-1, 1, self.config.embed_size])
         item_embedding = tf.reshape(self.item_elayer.weights[0], [1, self.num_items, self.config.embed_size])
         self.pre_logits = -euclidean_distance(user_embedding, item_embedding)
 
     def fit(self):
         user_n_pos, all_users, pos_items = _generate_positive_items(self.user_pos_train)
         self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
-
+        early_stopping = EarlyStopping(metric="NDCG@10", patience=self.config.early_stop)
         for epoch in range(self.config.epochs):
             neg_items = _sampling_negative_items(user_n_pos, self.config.dns, self.num_items,
                                                  self.user_pos_train).squeeze()
             data_iter = BatchIterator(all_users, pos_items, neg_items,
                                       batch_size=self.config.batch_size,
                                       shuffle=True, drop_last=False)
 
             for user, pos_item, neg_item in data_iter:
                 feed = {self.user_h: user, self.pos_item_h: pos_item, self.neg_item_h: neg_item}
                 self.sess.run(self.update, feed_dict=feed)
 
             cur_result = self.evaluate()
             self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
+            if early_stopping(cur_result):
                 self.logger.info("early stop")
                 break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+        self.logger.info("best:".ljust(12) + f"\t{early_stopping.best_result.values_str}")
 
     def evaluate(self):
         return self.evaluator.evaluate(self)
 
     def predict(self, users):
         ratings = self.sess.run(self.pre_logits, feed_dict={self.user_h: users})
         return ratings
```

## skrec/recommender/Caser.py

```diff
@@ -11,20 +11,21 @@
 
 import torch
 import torch.nn as nn
 import numpy as np
 import torch.nn.functional as F
 from typing import Dict
 from .base import AbstractRecommender
-from ..io import Dataset
 from ..utils.py import Config
-from ..utils.py import RankingEvaluator, MetricReport
+from ..utils.py import EarlyStopping
 from ..utils.torch import get_initializer
 from ..utils.torch import sigmoid_cross_entropy
 from ..io import SequentialPairwiseIterator
+from ..io.dataset import CFDataset
+from ..run_config import RunConfig
 
 
 class CaserConfig(Config):
     def __init__(self,
                  lr=1e-3,
                  l2_reg=1e-6,
                  embed_size=64,
@@ -33,15 +34,15 @@
                  nv=4,
                  nh=16,
                  dropout=0.5,
                  batch_size=1024,
                  epochs=500,
                  early_stop=100,
                  **kwargs):
-        super(CaserConfig, self).__init__()
+        super().__init__()
         self.lr: float = lr
         self.l2_reg: float = l2_reg
         self.embed_size: int = embed_size
         self.seq_L: int = seq_L
         self.seq_T: int = seq_T
         self.nv: int = nv
         self.nh: int = nh
@@ -160,42 +161,38 @@
     def predict(self, user_var, seq_var):
         x = self._forward_user(user_var, seq_var)
         res = torch.matmul(x, self.W2.weight.T) + self.b2.weight.squeeze()
         return res
 
 
 class Caser(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = CaserConfig(**cfg_dict)
-        super(Caser, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
+    def __init__(self, run_config: RunConfig, model_config: Dict):
+        self.dataset = CFDataset(run_config.data_dir, run_config.sep, run_config.file_column)
+        self.config = CaserConfig(**model_config)
+        super().__init__(run_config, self.config, self.dataset)
 
         self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
         self.pad_idx = self.num_items
         self.num_items += 1
 
         self.user_truncated_seq = self.dataset.train_data.to_truncated_seq_dict(self.config.seq_L,
                                                                                 pad_value=self.pad_idx,
                                                                                 padding='pre', truncating='pre')
 
         self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
-        self.caser = _Caser(self.num_users, self.num_items, self.config.embed_size, config).to(self.device)
+        self.caser = _Caser(self.num_users, self.num_items, self.config.embed_size, self.config).to(self.device)
         self.optimizer = torch.optim.Adam(self.caser.parameters(), weight_decay=self.config.l2_reg, lr=self.config.lr)
 
     def fit(self):
         data_iter = SequentialPairwiseIterator(self.dataset.train_data,
                                                num_previous=self.config.seq_L, num_next=self.config.seq_T,
                                                pad=self.pad_idx, batch_size=self.config.batch_size,
                                                shuffle=True, drop_last=False)
         self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
-
+        early_stopping = EarlyStopping(metric="NDCG@10", patience=self.config.early_stop)
         for epoch in range(self.config.epochs):
             self.caser.train()
             for bat_users, bat_item_seqs, bat_pos_items, bat_neg_items in data_iter:
                 bat_users = torch.from_numpy(bat_users).long().to(self.device)
                 bat_item_seqs = torch.from_numpy(bat_item_seqs).long().to(self.device)
                 bat_pos_items = torch.from_numpy(bat_pos_items).long().to(self.device)
                 bat_neg_items = torch.from_numpy(bat_neg_items).long().to(self.device)
@@ -210,23 +207,19 @@
 
                 self.optimizer.zero_grad()
                 loss.backward()
                 self.optimizer.step()
 
             cur_result = self.evaluate()
             self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
+            if early_stopping(cur_result):
                 self.logger.info("early stop")
                 break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
 
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+        self.logger.info("best:".ljust(12) + f"\t{early_stopping.best_result.values_str}")
 
     def evaluate(self):
         self.caser.eval()
         return self.evaluator.evaluate(self)
 
     def predict(self, users):
         bat_seq = [self.user_truncated_seq[u] for u in users]
```

## skrec/recommender/FPMC.py

```diff
@@ -10,32 +10,33 @@
 
 
 import torch
 import torch.nn as nn
 import numpy as np
 from typing import Dict
 from .base import AbstractRecommender
-from ..io import Dataset
 from ..utils.py import Config
-from ..utils.py import RankingEvaluator, MetricReport
+from ..utils.py import EarlyStopping
 from ..utils.torch import get_initializer
 from ..utils.torch import bpr_loss, l2_loss, inner_product
 from ..io import SequentialPairwiseIterator
+from ..io.dataset import CFDataset
+from ..run_config import RunConfig
 
 
 class FPMCConfig(Config):
     def __init__(self,
                  lr=0.001,
                  reg=0.001,
                  embed_size=64,
                  batch_size=1024,
                  epochs=500,
                  early_stop=100,
                  **kwargs):
-        super(FPMCConfig, self).__init__()
+        super().__init__()
         self.lr: float = lr
         self.reg: float = reg
         self.embed_size: int = embed_size
         self.batch_size: int = batch_size
         self.epochs: int = epochs
         self.early_stop: int = early_stop
         self._validate()
@@ -85,37 +86,34 @@
         ratings = torch.matmul(ui_emb, self.IU_embeddings.weight.T) + \
                   torch.matmul(last_emb, self.IL_embeddings.weight.T)
 
         return ratings
 
 
 class FPMC(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = FPMCConfig(**cfg_dict)
-        super(FPMC, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
+    def __init__(self, run_config: RunConfig, model_config: Dict):
+        self.dataset = CFDataset(run_config.data_dir, run_config.sep, run_config.file_column)
+        self.config = FPMCConfig(**model_config)
+        super().__init__(run_config, self.config, self.dataset)
 
         self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
         self.user_pos_dict = self.dataset.train_data.to_user_dict_by_time()
 
         self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
-        self.fpmc = _FPMC(self.num_users, self.num_items, config.embed_size).to(self.device)
-        self.optimizer = torch.optim.Adam(self.fpmc.parameters(), lr=config.lr)
+        self.fpmc = _FPMC(self.num_users, self.num_items, self.config.embed_size).to(self.device)
+        self.optimizer = torch.optim.Adam(self.fpmc.parameters(), lr=self.config.lr)
 
     def fit(self):
         data_iter = SequentialPairwiseIterator(self.dataset.train_data,
                                                num_previous=1, num_next=1,
                                                batch_size=self.config.batch_size,
                                                shuffle=True, drop_last=False)
 
         self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
+        early_stopping = EarlyStopping(metric="NDCG@10", patience=self.config.early_stop)
         for epoch in range(self.config.epochs):
             self.fpmc.train()
             for bat_users, bat_last_items, bat_pos_items, bat_neg_items in data_iter:
                 bat_users = torch.from_numpy(bat_users).long().to(self.device)
                 bat_last_items = torch.from_numpy(bat_last_items).long().to(self.device)
                 bat_pos_items = torch.from_numpy(bat_pos_items).long().to(self.device)
                 bat_neg_items = torch.from_numpy(bat_neg_items).long().to(self.device)
@@ -133,22 +131,19 @@
                 loss += self.config.reg * reg_loss
                 self.optimizer.zero_grad()
                 loss.backward()
                 self.optimizer.step()
 
             cur_result = self.evaluate()
             self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
+            if early_stopping(cur_result):
                 self.logger.info("early stop")
                 break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+        self.logger.info("best:".ljust(12) + f"\t{early_stopping.best_result.values_str}")
 
     def evaluate(self):
         self.fpmc.eval()
         return self.evaluator.evaluate(self)
 
     def predict(self, users):
         last_items = [self.user_pos_dict[u][-1] for u in users]
```

## skrec/recommender/GRU4Rec.py

```diff
@@ -10,33 +10,34 @@
 
 __all__ = ["GRU4Rec"]
 
 import numpy as np
 import tensorflow as tf
 from typing import List, Dict
 from .base import AbstractRecommender
-from ..utils.py import RankingEvaluator, MetricReport
-from ..io import Dataset
+from ..utils.py import EarlyStopping
 from ..utils.py import Config
 from ..utils.tf1x import bpr_loss, l2_loss
+from ..io.dataset import CFDataset
+from ..run_config import RunConfig
 
 
 class GRU4RecConfig(Config):
     def __init__(self,
                  lr=0.001,
                  reg=0.0,
                  layers=[64],
                  batch_size=128,
                  loss="top1",
                  hidden_act="tanh",
                  final_act="linear",
                  epochs=500,
                  early_stop=100,
                  **kwargs):
-        super(GRU4RecConfig, self).__init__()
+        super().__init__()
         self.lr: float = lr
         self.reg: float = reg
         self.layers: List[int] = layers
         self.batch_size: int = batch_size
         self.loss: str = loss  # loss = top1, bpr
         self.hidden_act: str = hidden_act  # hidden_act = relu, tanh
         self.final_act: str = final_act  # final_act = linear, relu, leaky_relu
@@ -53,20 +54,19 @@
         assert isinstance(self.hidden_act, str) and self.hidden_act in {"relu", "tanh"}
         assert isinstance(self.final_act, str) and self.final_act in {"linear", "relu", "leaky_relu"}
         assert isinstance(self.epochs, int) and self.epochs >= 0
         assert isinstance(self.early_stop, int)
 
 
 class GRU4Rec(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = GRU4RecConfig(**cfg_dict)
-        super(GRU4Rec, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
+    def __init__(self, run_config: RunConfig, model_config: Dict):
+        self.dataset = CFDataset(run_config.data_dir, run_config.sep, run_config.file_column)
+        self.config = GRU4RecConfig(**model_config)
+        super().__init__(run_config, self.config, self.dataset)
+        config = self.config
 
         if config.hidden_act == "relu":
             self.hidden_act = tf.nn.relu
         elif config.hidden_act == "tanh":
             self.hidden_act = tf.nn.tanh
         else:
             raise ValueError("There is not hidden_act named '%s'." % config.hidden_act)
@@ -166,20 +166,17 @@
 
         reg_loss = l2_loss(inputs, items_embed, items_bias)
         final_loss = loss + self.config.reg*reg_loss
         self.update_opt = tf.train.AdamOptimizer(self.config.lr).minimize(final_loss)
 
     def fit(self):
         self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
-
         data_ui, offset_idx = self.data_ui, self.offset_idx
         data_items = data_ui[:, 1]
-
+        early_stopping = EarlyStopping(metric="NDCG@10", patience=self.config.early_stop)
         for epoch in range(self.config.epochs):
             state = [np.zeros([self.config.batch_size, n_unit], dtype=np.float32) for n_unit in self.config.layers]
             user_idx = np.random.permutation(len(offset_idx) - 1)
             iters = np.arange(self.config.batch_size, dtype=np.int32)
             maxiter = iters.max()
             start = offset_idx[user_idx[iters]]
             end = offset_idx[user_idx[iters]+1]
@@ -210,22 +207,19 @@
                     end[idx] = offset_idx[user_idx[maxiter]+1]
                 if len(mask):
                     for i in range(len(self.config.layers)):
                         state[i][mask] = 0
 
             cur_result = self.evaluate()
             self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
+            if early_stopping(cur_result):
                 self.logger.info("early stop")
                 break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+        self.logger.info("best:".ljust(12) + f"\t{early_stopping.best_result.values_str}")
 
     def _get_user_embeddings(self):
         users = np.array(list(self.user_pos_train.keys()), dtype=np.int32)
         u_nnz = np.array([len(self.user_pos_train[u]) for u in users], dtype=np.int32)
         users = users[np.argsort(-u_nnz)]
         data_ui, offset_idx = self.data_ui, self.offset_idx
         data_items = data_ui[:, 1]
```

## skrec/recommender/GRU4RecPlus.py

```diff
@@ -11,18 +11,19 @@
 __all__ = ["GRU4RecPlus"]
 
 
 import numpy as np
 import tensorflow as tf
 from typing import List, Dict
 from .base import AbstractRecommender
-from ..utils.py import RankingEvaluator, MetricReport
-from ..io import Dataset
+from ..utils.py import EarlyStopping
 from ..utils.py import Config
 from ..utils.tf1x import l2_loss
+from ..io.dataset import CFDataset
+from ..run_config import RunConfig
 
 
 class GRU4RecPlusConfig(Config):
     def __init__(self,
                  lr=0.001,
                  reg=0.0,
                  bpr_reg=1.0,
@@ -32,15 +33,15 @@
                  hidden_act="tanh",
                  final_act="linear",
                  n_sample=2048,
                  sample_alpha=0.75,
                  epochs=500,
                  early_stop=100,
                  **kwargs):
-        super(GRU4RecPlusConfig, self).__init__()
+        super().__init__()
         self.lr: float = lr
         self.reg: float = reg
         self.bpr_reg: float = bpr_reg
         self.layers: List[int] = layers
         self.batch_size: int = batch_size
         self.loss: str = loss  # loss = top1_max, bpr_max
         self.hidden_act: str = hidden_act  # hidden_act = relu, tanh
@@ -63,20 +64,19 @@
         assert isinstance(self.n_sample, int) and self.n_sample >= 0
         assert isinstance(self.sample_alpha, float) and 0 < self.sample_alpha <= 1
         assert isinstance(self.epochs, int) and self.epochs >= 0
         assert isinstance(self.early_stop, int)
 
 
 class GRU4RecPlus(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = GRU4RecPlusConfig(**cfg_dict)
-        super(GRU4RecPlus, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
+    def __init__(self, run_config: RunConfig, model_config: Dict):
+        self.dataset = CFDataset(run_config.data_dir, run_config.sep, run_config.file_column)
+        self.config = GRU4RecPlusConfig(**model_config)
+        super().__init__(run_config, self.config, self.dataset)
+        config: GRU4RecPlusConfig = self.config
 
         if config.hidden_act == "relu":
             self.hidden_act = tf.nn.relu
         elif config.hidden_act == "tanh":
             self.hidden_act = tf.nn.tanh
         else:
             raise ValueError("There is not hidden_act named '%s'." % config.hidden_act)
@@ -200,19 +200,18 @@
 
     def _sample_neg_items(self, size):
         samples = np.searchsorted(self.pop_cumsum, np.random.rand(size))
         return samples
 
     def fit(self):
         self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
 
         data_ui, offset_idx = self.data_ui, self.offset_idx
         data_items = data_ui[:, 1]
+        early_stopping = EarlyStopping(metric="NDCG@10", patience=self.config.early_stop)
         for epoch in range(self.config.epochs):
             state = [np.zeros([self.config.batch_size, n_unit], dtype=np.float32) for n_unit in self.config.layers]
             user_idx = np.random.permutation(len(offset_idx) - 1)
             iters = np.arange(self.config.batch_size, dtype=np.int32)
             maxiter = iters.max()
             start = offset_idx[user_idx[iters]]
             end = offset_idx[user_idx[iters]+1]
@@ -246,23 +245,19 @@
                     end[idx] = offset_idx[user_idx[maxiter]+1]
                 if len(mask):
                     for i in range(len(self.config.layers)):
                         state[i][mask] = 0
 
             cur_result = self.evaluate()
             self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
+            if early_stopping(cur_result):
                 self.logger.info("early stop")
                 break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
 
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+        self.logger.info("best:".ljust(12) + f"\t{early_stopping.best_result.values_str}")
 
     def _get_user_embeddings(self):
         users = np.array(list(self.user_pos_train.keys()), dtype=np.int32)
         u_nnz = np.array([len(self.user_pos_train[u]) for u in users], dtype=np.int32)
         users = users[np.argsort(-u_nnz)]
         user_embeddings = np.zeros([self.users_num, self.config.layers[-1]], dtype=np.float32)  # saving user embedding
```

## skrec/recommender/HGN.py

```diff
@@ -11,33 +11,34 @@
 
 import numpy as np
 import torch
 import torch.nn as nn
 from torch.nn import Parameter
 from typing import Dict
 from .base import AbstractRecommender
-from ..utils.py import RankingEvaluator, MetricReport
-from ..io import Dataset
+from ..utils.py import EarlyStopping
 from ..utils.py import Config
 from ..utils.torch import bpr_loss, get_initializer
 from ..io import SequentialPairwiseIterator
+from ..io.dataset import CFDataset
+from ..run_config import RunConfig
 
 
 class HGNConfig(Config):
     def __init__(self,
                  lr=1e-3,
                  reg=1e-3,
                  seq_L=5,
                  seq_T=3,
                  embed_size=64,
                  batch_size=1024,
                  epochs=1000,
                  early_stop=100,
                  **kwargs):
-        super(HGNConfig, self).__init__()
+        super().__init__()
         self.lr: float = lr
         self.reg: float = reg
         self.seq_L: int = seq_L
         self.seq_T: int = seq_T
         self.embed_size: int = embed_size
         self.batch_size: int = batch_size
         self.epochs: int = epochs
@@ -161,20 +162,19 @@
         # item-item product
         res += torch.matmul(item_embs, w2.T.unsqueeze(dim=0)).sum(dim=1)  # (b,l,d)x(1,d,n)->(b,l,n)->(b,n)
 
         return res
 
 
 class HGN(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = HGNConfig(**cfg_dict)
-        super(HGN, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
+    def __init__(self, run_config: RunConfig, model_config: Dict):
+        self.dataset = CFDataset(run_config.data_dir, run_config.sep, run_config.file_column)
+        self.config = HGNConfig(**model_config)
+        super().__init__(run_config, self.config, self.dataset)
+        config = self.config
 
         self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
         self.pad_idx = self.num_items
         self.num_items += 1
 
         self.user_truncated_seq = self.dataset.train_data.to_truncated_seq_dict(config.seq_L,
                                                                                 pad_value=self.pad_idx,
@@ -187,16 +187,15 @@
     def fit(self):
         data_iter = SequentialPairwiseIterator(self.dataset.train_data,
                                                num_previous=self.config.seq_L, num_next=self.config.seq_T,
                                                pad=self.pad_idx, batch_size=self.config.batch_size,
                                                shuffle=True, drop_last=False)
 
         self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
+        early_stopping = EarlyStopping(metric="NDCG@10", patience=self.config.early_stop)
         for epoch in range(self.config.epochs):
             self.hgn.train()
             for bat_users, bat_item_seqs, bat_pos_items, bat_neg_items in data_iter:
                 bat_users = torch.from_numpy(bat_users).long().to(self.device)
                 bat_item_seqs = torch.from_numpy(bat_item_seqs).long().to(self.device)
                 bat_pos_items = torch.from_numpy(bat_pos_items).long().to(self.device)
                 bat_neg_items = torch.from_numpy(bat_neg_items).long().to(self.device)
@@ -208,22 +207,19 @@
 
                 self.optimizer.zero_grad()
                 loss.backward()
                 self.optimizer.step()
 
             cur_result = self.evaluate()
             self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
+            if early_stopping(cur_result):
                 self.logger.info("early stop")
                 break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+        self.logger.info("best:".ljust(12) + f"\t{early_stopping.best_result.values_str}")
 
     def evaluate(self):
         self.hgn.eval()
         return self.evaluator.evaluate(self)
 
     def predict(self, users):
         bat_seq = [self.user_truncated_seq[u] for u in users]
```

## skrec/recommender/LightGCN.py

```diff
@@ -11,37 +11,39 @@
 
 import os
 import torch
 import torch.sparse as torch_sp
 import torch.nn as nn
 import torch.nn.functional as F
 from typing import Dict
-from .base import AbstractRecommender
-from ..utils.torch import inner_product, bpr_loss, l2_loss, get_initializer
-from ..utils.py import RankingEvaluator, MetricReport
-from ..io import PairwiseIterator, Dataset
 import numpy as np
 import scipy.sparse as sp
+from .base import AbstractRecommender
+from ..utils.torch import inner_product, bpr_loss, l2_loss, get_initializer
+from ..utils.py import EarlyStopping
+from ..io import PairwiseIterator
 from ..utils.common import normalize_adj_matrix
 from ..utils.torch import sp_mat_to_sp_tensor
 from ..utils.py import Config
+from ..io.dataset import CFDataset
+from ..run_config import RunConfig
 
 
 class LightGCNConfig(Config):
     def __init__(self,
                  lr=1e-3,
                  reg=1e-3,
                  embed_size=64,
                  n_layers=3,
                  adj_type="pre",
                  batch_size=1024,
                  epochs=1000,
                  early_stop=100,
                  **kwargs):
-        super(LightGCNConfig, self).__init__()
+        super().__init__()
         self.lr: float = lr
         self.reg: float = reg
         self.embed_size: int = embed_size
         self.n_layers: int = n_layers
         self.adj_type: str = adj_type  # plain, norm, gcmc, pre
         self.batch_size: int = batch_size
         self.epochs: int = epochs
@@ -108,20 +110,20 @@
 
     def eval(self):
         super(_LightGCN, self).eval()
         self._user_embeddings_final, self._item_embeddings_final = self._forward_gcn()
 
 
 class LightGCN(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = LightGCNConfig(**cfg_dict)
-        super(LightGCN, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
+    def __init__(self, run_config: RunConfig, model_config: Dict):
+        self.dataset = CFDataset(run_config.data_dir, run_config.sep, run_config.file_column)
+        self.config = LightGCNConfig(**model_config)
+        super().__init__(run_config, self.config, self.dataset)
+        config = self.config
+
         self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
         self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
 
         adj_matrix = self._load_adj_mat(config.adj_type)
         adj_matrix = sp_mat_to_sp_tensor(adj_matrix).to(self.device)
 
         self.lightgcn = _LightGCN(self.num_users, self.num_items, config.embed_size,
@@ -172,16 +174,15 @@
 
     def fit(self):
         data_iter = PairwiseIterator(self.dataset.train_data,
                                      batch_size=self.config.batch_size,
                                      shuffle=True, drop_last=False)
 
         self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
+        early_stopping = EarlyStopping(metric="NDCG@10", patience=self.config.early_stop)
         for epoch in range(self.config.epochs):
             self.lightgcn.train()
             for bat_users, bat_pos_items, bat_neg_items in data_iter:
                 bat_users = torch.from_numpy(bat_users).long().to(self.device)
                 bat_pos_items = torch.from_numpy(bat_pos_items).long().to(self.device)
                 bat_neg_items = torch.from_numpy(bat_neg_items).long().to(self.device)
 
@@ -199,22 +200,19 @@
                 loss += self.config.reg * reg_loss / self.config.batch_size
                 self.optimizer.zero_grad()
                 loss.backward()
                 self.optimizer.step()
 
             cur_result = self.evaluate()
             self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
+            if early_stopping(cur_result):
                 self.logger.info("early stop")
                 break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+        self.logger.info("best:".ljust(12) + f"\t{early_stopping.best_result.values_str}")
 
     def evaluate(self):
         self.lightgcn.eval()
         return self.evaluator.evaluate(self)
 
     def predict(self, users):
         users = torch.from_numpy(np.asarray(users)).long().to(self.device)
```

## skrec/recommender/MultVAE.py

```diff
@@ -11,18 +11,19 @@
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from typing import List, Dict
 from .base import AbstractRecommender
 from ..utils.py import Config
-from ..io import Dataset
-from ..utils.py import RankingEvaluator, MetricReport
+from ..utils.py import EarlyStopping
 from ..utils.py import BatchIterator
 from ..utils.torch import l2_loss, get_initializer
+from ..io.dataset import CFDataset
+from ..run_config import RunConfig
 
 
 class MultVAEConfig(Config):
     def __init__(self,
                  lr=1e-3,
                  reg=0.0,
                  p_dims=[64],
@@ -30,15 +31,15 @@
                  keep_prob=0.5,
                  anneal_steps=200000,
                  anneal_cap=0.2,
                  batch_size=256,
                  epochs=1000,
                  early_stop=200,
                  **kwargs):
-        super(MultVAEConfig, self).__init__()
+        super().__init__()
         self.lr: float = lr
         self.reg: float = reg
         # p_dims is decoder's dimensions and q_dims is encoder's dimensions
         # if q_dims is None, it will be symmetrical with p_dims
         self.p_dims: List[int] = p_dims
         self.q_dims: List[int] = q_dims
         self.keep_prob: float = keep_prob
@@ -139,20 +140,19 @@
     def predict(self, input_x):
         ratings, _ = self.forward(input_x)
 
         return ratings
 
 
 class MultVAE(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = MultVAEConfig(**cfg_dict)
-        super(MultVAE, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
+    def __init__(self, run_config: RunConfig, model_config: Dict):
+        self.dataset = CFDataset(run_config.data_dir, run_config.sep, run_config.file_column)
+        self.config = MultVAEConfig(**model_config)
+        super().__init__(run_config, self.config, self.dataset)
+        config = self.config
 
         self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
         self.train_csr_mat = self.dataset.train_data.to_csr_matrix()
         self.train_csr_mat.data[:] = 1.0
 
         p_dims = config.p_dims
         self.p_dims = p_dims + [self.num_items]
@@ -170,17 +170,16 @@
         self.optimizer = torch.optim.Adam(self.multvae.parameters(), lr=self.config.lr)
 
     def fit(self):
         train_users = [user for user in range(self.num_users) if self.train_csr_mat[user].nnz]
         user_iter = BatchIterator(train_users, batch_size=self.config.batch_size, shuffle=True, drop_last=False)
 
         self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
+        early_stopping = EarlyStopping(metric="NDCG@10", patience=self.config.early_stop)
         update_count = 0.0
-        stop_counter = 0
-        best_result: MetricReport = None
         for epoch in range(self.config.epochs):
             self.multvae.train()
             for bat_users in user_iter:
                 bat_input = self.train_csr_mat[bat_users].toarray()
                 if self.config.anneal_steps > 0:
                     anneal = min(self.config.anneal_cap, 1.*update_count/self.config.anneal_steps)
                 else:
@@ -202,23 +201,19 @@
 
                 self.optimizer.zero_grad()
                 neg_elbo.backward()
                 self.optimizer.step()
                 update_count += 1
             cur_result = self.evaluate()
             self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
+            if early_stopping(cur_result):
                 self.logger.info("early stop")
                 break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
 
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+        self.logger.info("best:".ljust(12) + f"\t{early_stopping.best_result.values_str}")
 
     def evaluate(self):
         self.multvae.eval()
         return self.evaluator.evaluate(self)
 
     def predict(self, users):
         bat_input = self.train_csr_mat[users].toarray()
```

## skrec/recommender/Pop.py

```diff
@@ -2,32 +2,30 @@
 __email__ = "zhongchuansun@gmail.com"
 
 __all__ = ["Pop"]
 
 import numpy as np
 import pandas as pd
 from typing import Dict
-from ..io import Dataset
 from ..utils.py import Config
-from ..utils.py import RankingEvaluator
 from .base import AbstractRecommender
+from ..io.dataset import CFDataset
+from ..run_config import RunConfig
 
 
 class PopConfig(Config):
     def __init__(self, **kwargs):
-        super(PopConfig, self).__init__()
+        super().__init__()
 
 
 class Pop(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = PopConfig(**cfg_dict)
-        super(Pop, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
+    def __init__(self, run_config: RunConfig, model_config: Dict):
+        self.dataset = CFDataset(run_config.data_dir, run_config.sep, run_config.file_column)
+        self.config = PopConfig(**model_config)
+        super().__init__(run_config, self.config, self.dataset)
         self.users_num, self.items_num = self.dataset.num_users, self.dataset.num_items
         self.ranking_score = np.zeros([self.items_num], dtype=np.float32)
 
     def fit(self):
         self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
         items = self.dataset.train_data.to_user_item_pairs()[:, 1]
         items_count = pd.value_counts(items, sort=False)
```

## skrec/recommender/SASRec.py

```diff
@@ -6,21 +6,22 @@
 __author__ = "Zhongchuan Sun"
 __email__ = "zhongchuansun@gmail.com"
 
 __all__ = ["SASRec"]
 
 import numpy as np
 from typing import Dict
+import tensorflow as tf
 from .base import AbstractRecommender
-from ..utils.py import RankingEvaluator, MetricReport
-from ..io import Dataset
+from ..utils.py import EarlyStopping
 from ..utils.tf1x import inner_product
 from ..utils.py import pad_sequences, batch_randint_choice, BatchIterator
 from ..utils.py import Config
-import tensorflow as tf
+from ..io.dataset import CFDataset
+from ..run_config import RunConfig
 
 
 def normalize(inputs,
               epsilon=1e-8,
               scope="ln",
               reuse=None):
     '''Applies layer normalization.
@@ -279,15 +280,15 @@
                  max_len=50,
                  num_blocks=2,
                  num_heads=1,
                  batch_size=128,
                  epochs=1000,
                  early_stop=100,
                  **kwargs):
-        super(SASRecConfig, self).__init__()
+        super().__init__()
         self.lr: float = lr
         self.l2_emb: float = l2_emb
         self.hidden_units: int = hidden_units
         self.dropout_rate: float = dropout_rate
         self.max_len: int = max_len
         self.num_blocks: int = num_blocks
         self.num_heads: int = num_heads
@@ -306,20 +307,18 @@
         assert isinstance(self.num_heads, int) and self.num_heads > 0
         assert isinstance(self.batch_size, int) and self.batch_size > 0
         assert isinstance(self.epochs, int) and self.epochs >= 0
         assert isinstance(self.early_stop, int)
 
 
 class SASRec(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = SASRecConfig(**cfg_dict)
-        super(SASRec, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
+    def __init__(self, run_config: RunConfig, model_config: Dict):
+        self.dataset = CFDataset(run_config.data_dir, run_config.sep, run_config.file_column)
+        self.config = SASRecConfig(**model_config)
+        super().__init__(run_config, self.config, self.dataset)
 
         self.users_num, self.items_num = self.dataset.num_users, self.dataset.num_items
         self.user_pos_train = self.dataset.train_data.to_user_dict_by_time()
         self.all_users = list(self.user_pos_train.keys())
 
         self._build_model()
         tf_config = tf.ConfigProto()  # allow_soft_placement=False, log_device_placement=True
@@ -465,39 +464,34 @@
         # for predication/test
         items_embeddings = item_emb_table[:-1]  # remove the padding item
         self.all_logits = tf.matmul(last_emb, items_embeddings, transpose_b=True)
 
     def fit(self):
         item_seq_list, item_pos_list = self._generate_train_data()
         self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-
-        stop_counter = 0
-        best_result: MetricReport = None
+        early_stopping = EarlyStopping(metric="NDCG@10", patience=self.config.early_stop)
         for epoch in range(self.config.epochs):
             item_neg_list = self._sample_negative()
             data = BatchIterator(item_seq_list, item_pos_list, item_neg_list,
                                  batch_size=self.config.batch_size, shuffle=True, drop_last=False)
             for bat_item_seq, bat_item_pos, bat_item_neg in data:
                 feed = {self.item_seq_ph: bat_item_seq,
                         self.item_pos_ph: bat_item_pos,
                         self.item_neg_ph: bat_item_neg,
                         self.is_training: True}
 
                 self.sess.run(self.train_opt, feed_dict=feed)
 
             cur_result = self.evaluate()
             self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
+            if early_stopping(cur_result):
                 self.logger.info("early stop")
                 break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+        self.logger.info("best:".ljust(12) + f"\t{early_stopping.best_result.values_str}")
 
     def evaluate(self):
         return self.evaluator.evaluate(self)
 
     def predict(self, users):
         bat_seq = [self.test_item_seqs[u] for u in users]
         feed = {self.item_seq_ph: bat_seq,
```

## skrec/recommender/SGAT.py

```diff
@@ -10,38 +10,39 @@
 __all__ = ["SGAT"]
 
 import os
 import numpy as np
 import tensorflow as tf
 import scipy.sparse as sp
 from typing import Dict
+from collections import defaultdict
 from .base import AbstractRecommender
 from ..utils.py import Config
-from ..io import Dataset
-from ..utils.py import RankingEvaluator, MetricReport
+from ..utils.py import EarlyStopping
 from ..utils.py import pad_sequences
 from ..utils.tf1x import bpr_loss, l2_loss, l2_distance
-from collections import defaultdict
 from ..io import SequentialPairwiseIterator
 from ..utils.common import normalize_adj_matrix
+from ..io.dataset import CFDataset
+from ..run_config import RunConfig
 
 
 class SGATConfig(Config):
     def __init__(self,
                  lr=0.001,
                  reg=1e-4,
                  n_layers=5,
                  n_seqs=5,
                  n_next=3,
                  embed_size=64,
                  batch_size=1024,
                  epochs=500,
                  early_stop=100,
                  **kwargs):
-        super(SGATConfig, self).__init__()
+        super().__init__()
         self.lr: float = lr
         self.reg: float = reg
         self.n_layers: int = n_layers
         self.n_seqs: int = n_seqs
         self.n_next: int = n_next
         self.embed_size: int = embed_size
         self.batch_size: int = batch_size
@@ -70,20 +71,18 @@
     # calculate attention for each pair of items
     # used for calculating softmax
     exp_x = tf.exp(norm_x/tau)
     return exp_x
 
 
 class SGAT(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = SGATConfig(**cfg_dict)
-        super(SGAT, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
+    def __init__(self, run_config: RunConfig, model_config: Dict):
+        self.dataset = CFDataset(run_config.data_dir, run_config.sep, run_config.file_column)
+        self.config = SGATConfig(**model_config)
+        super().__init__(run_config, self.config, self.dataset)
 
         self.users_num, self.items_num = self.dataset.num_users, self.dataset.num_items
         self.user_pos_train = self.dataset.train_data.to_user_dict_by_time()
         self._process_test()
 
         self._build_model()
         tf_config = tf.ConfigProto()  # allow_soft_placement=False, log_device_placement=True
@@ -307,36 +306,31 @@
     def fit(self):
         data_iter = SequentialPairwiseIterator(self.dataset.train_data,
                                                num_previous=self.config.n_seqs, num_next=self.config.n_next,
                                                pad=self.items_num, batch_size=self.config.batch_size,
                                                shuffle=True, drop_last=False)
 
         self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
+        early_stopping = EarlyStopping(metric="NDCG@10", patience=self.config.early_stop)
         for epoch in range(self.config.epochs):
             for bat_users, bat_head, bat_pos_tail, bat_neg_tail in data_iter:
                 feed = {self.user_ph: bat_users,
                         self.head_ph: bat_head.reshape([-1, self.config.n_seqs]),
                         self.pos_tail_ph: bat_pos_tail.reshape([-1, self.config.n_next]),
                         self.neg_tail_ph: bat_neg_tail.reshape([-1, self.config.n_next])
                         }
                 self.sess.run(self.update_opt, feed_dict=feed)
 
             cur_result = self.evaluate()
             self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
+            if early_stopping(cur_result):
                 self.logger.info("early stop")
                 break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
 
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+        self.logger.info("best:".ljust(12) + f"\t{early_stopping.best_result.values_str}")
 
     def evaluate(self):
         self.sess.run(self.assign_opt)
         return self.evaluator.evaluate(self)
 
     def predict(self, users):
         last_items = [self.test_item_seqs[u] for u in users]
```

## skrec/recommender/SRGNN.py

```diff
@@ -10,19 +10,20 @@
 __all__ = ["SRGNN"]
 
 import math
 import numpy as np
 import tensorflow as tf
 from typing import Dict
 from .base import AbstractRecommender
-from ..io import Dataset
 from ..utils.py import Config
-from ..utils.py import RankingEvaluator, MetricReport
+from ..utils.py import EarlyStopping
 from ..utils.py import pad_sequences
 from ..utils.py import BatchIterator
+from ..io.dataset import CFDataset
+from ..run_config import RunConfig
 
 
 class SRGNNConfig(Config):
     def __init__(self,
                  lr=1e-3,
                  l2_reg=1e-5,
                  hidden_size=64,
@@ -31,15 +32,15 @@
                  step=1,
                  nonhybrid=False,
                  max_seq_len=200,
                  batch_size=256,
                  epochs=500,
                  early_stop=50,
                  **kwargs):
-        super(SRGNNConfig, self).__init__()
+        super().__init__()
         self.lr: float = lr
         self.l2_reg: float = l2_reg
         self.hidden_size: int = hidden_size
         self.lr_dc: float = lr_dc
         self.lr_dc_step: int = lr_dc_step
         self.step: int = step
         self.nonhybrid: bool = nonhybrid
@@ -61,20 +62,18 @@
         assert isinstance(self.max_seq_len, int) and self.max_seq_len > 0
         assert isinstance(self.batch_size, int) and self.batch_size > 0
         assert isinstance(self.epochs, int) and self.epochs >= 0
         assert isinstance(self.early_stop, int)
 
 
 class SRGNN(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = SRGNNConfig(**cfg_dict)
-        super(SRGNN, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
+    def __init__(self, run_config: RunConfig, model_config: Dict):
+        self.dataset = CFDataset(run_config.data_dir, run_config.sep, run_config.file_column)
+        self.config = SRGNNConfig(**model_config)
+        super().__init__(run_config, self.config, self.dataset)
 
         self.num_users, self.num_item = self.dataset.num_users, self.dataset.num_items
         self.user_pos_train = self.dataset.train_data.to_user_dict_by_time()
 
         self.train_seq = []
         self.train_tar = []
         for user, seqs in self.user_pos_train.items():
@@ -193,16 +192,15 @@
 
     def fit(self):
         train_seq_len = [(idx, len(seq)) for idx, seq in enumerate(self.train_seq)]
         train_seq_len = sorted(train_seq_len, key=lambda x: x[1], reverse=True)
         train_seq_index, _ = list(zip(*train_seq_len))
 
         self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
+        early_stopping = EarlyStopping(metric="NDCG@10", patience=self.config.early_stop)
         for epoch in range(self.config.epochs):
             for bat_index in self._shuffle_index(train_seq_index):
                 item_seqs = [self.train_seq[idx] for idx in bat_index]
                 bat_tars = [self.train_tar[idx] for idx in bat_index]
                 bat_adj_in, bat_adj_out, bat_alias, bat_items, bat_mask = self._build_session_graph(item_seqs)
                 feed = {self.target_ph: bat_tars,
                         self.item_ph: bat_items,
@@ -211,23 +209,19 @@
                         self.alias_ph: bat_alias,
                         self.mask_ph: bat_mask}
 
                 self.sess.run(self.train_opt, feed_dict=feed)
 
             cur_result = self.evaluate()
             self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
+            if early_stopping(cur_result):
                 self.logger.info("early stop")
                 break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
 
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+        self.logger.info("best:".ljust(12) + f"\t{early_stopping.best_result.values_str}")
 
     def _shuffle_index(self, seq_index):
         """NOTE: two-step shuffle for saving memory"""
         index_chunks = BatchIterator(seq_index, batch_size=self.config.batch_size*32,
                                      shuffle=False, drop_last=False)  # chunking
         index_chunks = list(index_chunks)
         index_chunks_iter = BatchIterator(index_chunks, batch_size=1,
```

## skrec/recommender/TransRec.py

```diff
@@ -11,32 +11,33 @@
 
 import numpy as np
 import torch
 import torch.nn as nn
 from torch.nn.parameter import Parameter
 from typing import Dict
 from .base import AbstractRecommender
-from ..io import Dataset
 from ..utils.py import Config
-from ..utils.py import RankingEvaluator, MetricReport
+from ..utils.py import EarlyStopping
 from ..utils.torch import bpr_loss, l2_loss, l2_distance
 from ..utils.torch import get_initializer
 from ..io import SequentialPairwiseIterator
+from ..io.dataset import CFDataset
+from ..run_config import RunConfig
 
 
 class TransRecConfig(Config):
     def __init__(self,
                  lr=1e-3,
                  reg=0.0,
                  embed_size=64,
                  batch_size=1024,
                  epochs=500,
                  early_stop=100,
                  **kwargs):
-        super(TransRecConfig, self).__init__()
+        super().__init__()
         self.lr: float = lr
         self.reg: float = reg
         self.embed_size: int = embed_size
         self.batch_size: int = batch_size
         self.epochs: int = epochs
         self.early_stop: int = early_stop
         self._validate()
@@ -91,37 +92,34 @@
         ratings = -l2_distance(transed_emb.unsqueeze(dim=1), self.item_embeddings.weight)
 
         ratings += torch.squeeze(self.item_biases.weight)
         return ratings
 
 
 class TransRec(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = TransRecConfig(**cfg_dict)
-        super(TransRec, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
+    def __init__(self, run_config: RunConfig, model_config: Dict):
+        self.dataset = CFDataset(run_config.data_dir, run_config.sep, run_config.file_column)
+        self.config = TransRecConfig(**model_config)
+        super().__init__(run_config, self.config, self.dataset)
 
         self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
         self.user_pos_dict = self.dataset.train_data.to_user_dict_by_time()
 
         self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
-        self.transrec = _TransRec(self.num_users, self.num_items, config.embed_size).to(self.device)
-        self.optimizer = torch.optim.Adam(self.transrec.parameters(), lr=config.lr)
+        self.transrec = _TransRec(self.num_users, self.num_items, self.config.embed_size).to(self.device)
+        self.optimizer = torch.optim.Adam(self.transrec.parameters(), lr=self.config.lr)
 
     def fit(self):
         data_iter = SequentialPairwiseIterator(self.dataset.train_data,
                                                num_previous=1, num_next=1,
                                                batch_size=self.config.batch_size,
                                                shuffle=True, drop_last=False)
 
         self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
+        early_stopping = EarlyStopping(metric="NDCG@10", patience=self.config.early_stop)
         for epoch in range(self.config.epochs):
             self.transrec.train()
             for bat_users, bat_last_items, bat_pos_items, bat_neg_items in data_iter:
                 bat_users = torch.from_numpy(bat_users).long().to(self.device)
                 bat_last_items = torch.from_numpy(bat_last_items).long().to(self.device)
                 bat_pos_items = torch.from_numpy(bat_pos_items).long().to(self.device)
                 bat_neg_items = torch.from_numpy(bat_neg_items).long().to(self.device)
@@ -140,22 +138,19 @@
                 loss += self.config.reg * reg_loss
                 self.optimizer.zero_grad()
                 loss.backward()
                 self.optimizer.step()
 
             cur_result = self.evaluate()
             self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
+            if early_stopping(cur_result):
                 self.logger.info("early stop")
                 break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+        self.logger.info("best:".ljust(12) + f"\t{early_stopping.best_result.values_str}")
 
     def evaluate(self):
         self.transrec.eval()
         return self.evaluator.evaluate(self)
 
     def predict(self, users):
         last_items = [self.user_pos_dict[u][-1] for u in users]
```

## skrec/recommender/base.py

```diff
@@ -2,26 +2,33 @@
 __email__ = "zhongchuansun@gmail.com"
 
 __all__ = ["AbstractRecommender"]
 
 
 import os
 import time
-from typing import Union, List
+from typing import Union, List, Dict, Callable
 import numpy as np
-from ..io import Logger, Dataset
+from ..io import Logger, CFDataset
 from ..utils.py import Config, slugify
 from ..utils.py import MetricReport
+from ..run_config import RunConfig
+from ..utils.py import RankingEvaluator
 
 
 class AbstractRecommender(object):
-    def __init__(self, dataset: Dataset, config: Config):
-        self.logger: Logger = self._create_logger(dataset, config)
+    def __init__(self, run_config: RunConfig, model_config: Config, dataset: CFDataset):
+        self.evaluator = RankingEvaluator(dataset.train_data.to_user_dict(),
+                                          dataset.test_data.to_user_dict(),
+                                          metric=run_config.metric, top_k=run_config.top_k,
+                                          batch_size=run_config.test_batch_size,
+                                          num_thread=run_config.test_thread)
+        self.logger: Logger = self._create_logger(dataset, model_config)
 
-    def _create_logger(self, dataset: Dataset, config: Config) -> Logger:
+    def _create_logger(self, dataset: CFDataset, config: Config) -> Logger:
         timestamp = time.time()
         model_name = self.__class__.__name__
         data_name = dataset.data_name
 
         param_str = f"{data_name}_{model_name}_{config.to_string('_')}"
         param_str = slugify(param_str, max_length=255 - 100)
         # run_id: data_name, model_name, hyper-parameters, timestamp
@@ -33,20 +40,19 @@
 
         # show basic information
         logger.info(f"PID: {os.getpid()}")
         logger.info(f"Model: {self.__class__.__module__}")
 
         logger.info(f"\n{dataset.statistic_info}")
         cfg_str = config.to_string('\n')
-        logger.info(f"\nHyper-parameters:\n{cfg_str}")
+        logger.info(f"\nHyper-parameters:\n{cfg_str}\n")
 
         return logger
 
     def fit(self):
-        # TODO how to early stop fitting
         raise NotImplementedError
 
     def evaluate(self) -> MetricReport:
         raise NotImplementedError
 
     def predict(self, users: Union[List[int], np.ndarray]) -> np.ndarray:
         raise NotImplementedError
```

## skrec/recommender/AOBPR/AOBPR.py

```diff
@@ -10,29 +10,30 @@
 
 import numpy as np
 from typing import Dict
 from ..base import AbstractRecommender
 from ...io import PairwiseIterator
 from ...utils.py import randint_choice
 from ...utils.py import Config
-from ...utils.py import RankingEvaluator, MetricReport
-from ...io import Dataset
+from ...utils.py import EarlyStopping
+from ...io.dataset import CFDataset
+from ...run_config import RunConfig
 from .pyx_aobpr_func import aobpr_update
 
 
 class AOBPRConfig(Config):
     def __init__(self,
                  lr=1e-2,
                  reg=5e-2,
                  embed_size=64,
                  alpha=6682,
                  epochs=500,
                  early_stop=100,
                  **kwargs):
-        super(AOBPRConfig, self).__init__()
+        super().__init__()
         self.lr: float = lr
         self.reg: float = reg
         self.embed_size: int = embed_size
         self.alpha: int = alpha
         self.epochs: int = epochs
         self.early_stop: int = early_stop
         self._validate()
@@ -43,20 +44,18 @@
         assert isinstance(self.embed_size, int) and self.embed_size > 0
         assert isinstance(self.alpha, int) and self.alpha > 0
         assert isinstance(self.epochs, int) and self.epochs >= 0
         assert isinstance(self.early_stop, int)
 
 
 class AOBPR(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = AOBPRConfig(**cfg_dict)
-        super(AOBPR, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
+    def __init__(self, run_config: RunConfig, model_config: Dict):
+        self.dataset = CFDataset(run_config.data_dir, run_config.sep, run_config.file_column)
+        self.config = AOBPRConfig(**model_config)
+        super().__init__(run_config, self.config, self.dataset)
         self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
 
         low, high = 0.0, 1.0
         self.user_embeds = np.random.uniform(low=low, high=high, size=[self.num_users, self.config.embed_size]).astype(np.float32)
         self.item_embeds = np.random.uniform(low=low, high=high, size=[self.num_items, self.config.embed_size]).astype(np.float32)
 
         rank = np.arange(1, self.num_items+1)
@@ -67,37 +66,31 @@
         data_iter = PairwiseIterator(self.dataset.train_data,
                                      batch_size=len(self.dataset.train_data),
                                      shuffle=False, drop_last=False)
 
         user1d, pos_item1d, _ = list(data_iter)[0]
         len_data = len(user1d)
         self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
+        early_stopping = EarlyStopping(metric="NDCG@10", patience=self.config.early_stop)
         shuffle_idx = np.arange(len_data)
         for epoch in range(self.config.epochs):
             rank_idx = randint_choice(self.num_items, size=len_data,
                                       replace=True, p=self.rank_prob)
             np.random.shuffle(shuffle_idx)
             aobpr_update(user1d[shuffle_idx], pos_item1d[shuffle_idx],
                          rank_idx, self.config.lr, self.config.reg,
                          self.user_embeds, self.item_embeds)
 
-            # self.logger.info("epoch %d:\t%s" % (epoch, self.evaluate_model()))
             cur_result = self.evaluate()
             self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
+            if early_stopping(cur_result):
                 self.logger.info("early stop")
                 break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
 
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+        self.logger.info("best:".ljust(12) + f"\t{early_stopping.best_result.values_str}")
 
     def evaluate(self):
         return self.evaluator.evaluate(self)
 
     def predict(self, users):
         user_embedding = self.user_embeds[users]
         all_ratings = np.matmul(user_embedding, self.item_embeds.T)
```

## skrec/recommender/BERT4Rec/BERT4Rec.py

```diff
@@ -10,21 +10,20 @@
 
 import os
 import numpy as np
 import time
 import pickle
 from typing import Dict
 from skrec.utils.py import Config
-from skrec.io import Dataset
 from skrec.recommender.base import AbstractRecommender
-from skrec.utils.py import RankingEvaluator
 from . import modeling
 import tensorflow as tf
 from .bert4rec_utils import model_fn_builder, input_fn_builder, EvalHooks
-
+from ...io.dataset import CFDataset
+from ...run_config import RunConfig
 
 class BERT4RecConfig(Config):
     def __init__(self,
                  # prepare data
                  max_seq_len=5,
                  masked_lm_prob=0.4,
                  sliding_step=1,
@@ -43,15 +42,15 @@
                  save_ckpt_epoch=10,
                  init_ckpt=None,
                  epochs=3000,
                  early_stop=80,
                  verbose=10,
                  pool_size=10,
                  **kwargs):
-        super(BERT4RecConfig, self).__init__()
+        super().__init__()
         # prepare data
         self.max_seq_len: int = max_seq_len
         self.masked_lm_prob: float = masked_lm_prob
         self.sliding_step: int = sliding_step
         self.dupe_factor: int = dupe_factor
         # bert model
         self.att_drop: float = att_drop
@@ -70,24 +69,23 @@
         self.epochs: int = epochs
         self.early_stop: int = early_stop
         self.verbose: int = verbose
         self.pool_size: int = pool_size
 
 
 class BERT4Rec(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = BERT4RecConfig(**cfg_dict)
-        super(BERT4Rec, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
+    def __init__(self, run_config: RunConfig, model_config: Dict):
+        self.dataset = CFDataset(run_config.data_dir, run_config.sep, run_config.file_column)
+        self.config = BERT4RecConfig(**model_config)
+        super().__init__(run_config, self.config, self.dataset)
         self._prepare_data()
         self._build_model()
 
     def _prepare_data(self):
+        self.logger.info("prepare data...")
         output_dir = self.dataset.data_dir
         tf_record_name = [f"max_seq_len={self.config.max_seq_len}",
                           f"masked_lm_prob={self.config.masked_lm_prob}",
                           f"sliding_step={self.config.sliding_step}",
                           f"dupe_factor={self.config.dupe_factor}"]
         tf_record_name = "_".join(tf_record_name)
 
@@ -108,14 +106,15 @@
 
         with open(vocab_filename, 'rb') as input_file:
             vocab = pickle.load(input_file)
         self.item_size = len(vocab.token_to_ids)
         self.num_instances = np.load(num_trains_file)
 
     def _build_model(self):
+        self.logger.info("build model...")
         config = self.config
         bert_config = modeling.BertConfig(self.item_size,
                                           hidden_size=config.h_size,
                                           num_hidden_layers=config.n_layers,
                                           num_attention_heads=config.att_heads,
                                           intermediate_size=config.h_size*4,
                                           hidden_act=config.h_act,
@@ -180,7 +179,8 @@
         eval_hook = EvalHooks(config, self.user_history_filename, self.evaluator, self.logger)
         cur_steps = 0
         for epoch in range(0, config.epochs, config.verbose):
             self.estimator.train(input_fn=self.train_input_fn,
                                  max_steps=cur_steps+self.steps_per_epoch*config.verbose)
             cur_steps += self.steps_per_epoch*config.verbose
             self.estimator.evaluate(input_fn=self.eval_input_fn, steps=None, hooks=[eval_hook])
+        self.logger.info("best:".ljust(12) + f"\t{eval_hook.early_stopping.best_result.values_str}")
```

## skrec/recommender/BERT4Rec/bert4rec_gen_data.py

```diff
@@ -5,15 +5,15 @@
 
 import tensorflow as tf
 
 from .vocab import FreqVocab
 import pickle
 import multiprocessing
 import time
-from ...io import Dataset
+from ...io import CFDataset
 
 random_seed = 12345
 short_seq_prob = 0  # Probability of creating sequences which are shorter than the maximum length。
 
 
 def printable_text(text):
     """Returns text encoded in a way suitable for print or `tf.logging`."""
@@ -413,15 +413,15 @@
 
     total_written = write_instance_to_example_files(instances, max_seq_length,
                                                     max_predictions_per_seq, vocab,
                                                     [output_filename])
     return total_written
 
 
-def main(config, dataset: Dataset, output_dir: str, tf_record_name: str):
+def main(config, dataset: CFDataset, output_dir: str, tf_record_name: str):
     tf.logging.set_verbosity(tf.logging.DEBUG)
 
     max_seq_length = config.max_seq_len
     max_predictions_per_seq = int(round(config.max_seq_len*config.masked_lm_prob))
     masked_lm_prob = config.masked_lm_prob
     mask_prob = 1.0
     dupe_factor = config.dupe_factor
```

## skrec/recommender/BERT4Rec/bert4rec_utils.py

```diff
@@ -1,13 +1,13 @@
 import pickle
 from collections import defaultdict
 import numpy as np
 import tensorflow as tf
 from . import modeling, optimization
-from ...utils.py import MetricReport
+from ...utils.py import MetricReport, EarlyStopping
 from ...utils.py.cython import eval_score_matrix
 
 
 class EvalHooks(tf.train.SessionRunHook):
     def __init__(self, config, user_history_filename, evaluator, logger):
         if user_history_filename is not None:
             # print('load user history from :' + user_history_filename)
@@ -29,39 +29,35 @@
             self.seq_len_num[len(item_seq)] += 1
 
         self._evaluator = evaluator
         self._logger = logger
 
         self._logger.info("metrics:".ljust(12)+f"\t{self._evaluator.metrics_str}")
         # self._logger.info(self._evaluator.metrics_info())
-        self.counter = 0
-        self._best_result: MetricReport = None
         self._epoch = 0
+        self.early_stopping = EarlyStopping(metric="NDCG@10", patience=self.config.early_stop)
 
     def begin(self):
         self._eval_results = []
 
     def end(self, session):
         all_user_result = np.concatenate(self._eval_results, axis=0)  # (num_users, metrics_num*max_top)
         final_result = np.mean(all_user_result, axis=0)  # (1, metrics_num*max_top)
         final_result = np.reshape(final_result, newshape=[self._evaluator.metrics_num, self._evaluator.max_top])  # (metrics_num, max_top)
         final_result = final_result[:, self._evaluator.top_show - 1]
         final_result = np.reshape(final_result, newshape=[-1])
         cur_result = MetricReport(self._evaluator.metrics_list, final_result)
         self._logger.info(f"test:".ljust(12)+f"\t{cur_result.values_str}")
 
         self.counter += 1
-        if self._epoch >= 80 and self.counter > self.config.early_stop:
+        if self._epoch >= 80 and self.early_stopping(cur_result):
             self._logger.info("early stop")
-            self._logger.info("best:".ljust(12)+f"\t{self._best_result.values_str}")
+            self._logger.info("best:".ljust(12)+f"\t{self.early_stopping.best_result.values_str}")
             exit(0)
 
-        if self._best_result is None or cur_result["NDCG@10"] >= self._best_result["NDCG@10"]:
-            self._best_result = cur_result
-            self.counter = 0
         self._epoch += 1
 
     def before_run(self, run_context):
         variables = tf.get_collection('eval_sp')
         return tf.train.SessionRunArgs(variables)
 
     def after_run(self, run_context, run_values):
```

## skrec/utils/common.py

```diff
@@ -1,14 +1,14 @@
 import numpy as np
 import scipy.sparse as sp
 
 __author__ = "Zhongchuan Sun"
 __email__ = "zhongchuansun@gmail.com"
 
-__all__ = ["normalize_adj_matrix"]
+__all__ = ["normalize_adj_matrix", "PostInitMeta"]
 
 
 def normalize_adj_matrix(sp_mat, norm_method="left"):
     """Normalize adjacent matrix
 
     Args:
         sp_mat: A sparse adjacent matrix
@@ -33,7 +33,15 @@
 
         mid_sp_mat = rec_sqrt_d_in.dot(sp_mat)  # left matmul
         norm_sp_mat = mid_sp_mat.dot(rec_sqrt_d_in)  # right matmul
     else:
         raise ValueError(f"'{norm_method}' is an invalid normalization method.")
 
     return norm_sp_mat
+
+
+class PostInitMeta(type):
+    def __call__(cls, *args, **kwargs):
+        obj = super().__call__(*args, **kwargs)
+        if hasattr(obj, '__post_init__'):
+            obj.__post_init__()
+        return obj
```

## skrec/utils/py/__init__.py

```diff
@@ -12,9 +12,10 @@
 from .decorator import typeassert
 
 from .random import randint_choice
 from .random import batch_randint_choice
 
 from .evaluator import RankingEvaluator
 from .evaluator import MetricReport
+from .evaluator import EarlyStopping
 
 from .config import Config, merge_config_with_cmd_args
```

## skrec/utils/py/evaluator.py

```diff
@@ -1,11 +1,11 @@
 __author__ = "Zhongchuan Sun"
 __email__ = "zhongchuansun@gmail.com"
 
-__all__ = ["MetricReport", "RankingEvaluator", "MetricReport"]
+__all__ = ["MetricReport", "RankingEvaluator", "EarlyStopping"]
 
 from typing import Sequence, Dict, Union, Optional, Tuple, List, Iterable
 from collections import OrderedDict
 import numpy as np
 import itertools
 from colorama import Fore, Style
 from .batch_iterator import BatchIterator
@@ -202,7 +202,35 @@
         final_results = np.mean(all_results, axis=0)  # (1, metrics_num*max_top)
 
         final_results = np.reshape(final_results, newshape=[self.metrics_num, self.max_top])  # (metrics_num, max_top)
         final_results = final_results[:, self.top_show - 1]
 
         final_results = np.reshape(final_results, newshape=[-1])
         return MetricReport(self.metrics_list, final_results)
+
+
+class EarlyStopping:
+    def __init__(self, metric: str="NDCG@10", patience: int=100):
+        self._metric: str = metric
+        self._patience: int = patience  # Number of epochs to wait for improvement
+        self._best_score: MetricReport = None  # Current best score of the monitored metric
+        self._counter: int = 0  # Counter for the number of epochs without improvement
+
+    def __call__(self, val_result: MetricReport):
+        if self._best_score is None:
+            self._best_score = val_result
+        elif val_result[self._metric] <= self._best_score[self._metric]:
+            self._counter += 1
+            if self._counter >= self._patience > 0:
+                return True  # Trigger the stop condition for training
+        else:
+            self._best_score = val_result
+            self._counter = 0
+
+        return False  # Continue training
+
+    @property
+    def best_result(self) -> MetricReport:
+        if self._best_score is not None:
+            return self._best_score
+        else:
+            return MetricReport(["None"], [0])
```

## Comparing `scikit_recommender-0.0.3.dist-info/METADATA` & `scikit_recommender-0.0.4.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: scikit-recommender
-Version: 0.0.3
+Version: 0.0.4
 Summary: A science toolkit for recommender systems
 Home-page: https://github.com/ZhongchuanSun/scikit-recommender
 Author: ZhongchuanSun
 Author-email: zhongchuansun@gmail.com
 License: UNKNOWN
 Platform: Windows
 Platform: Linux
@@ -126,9 +126,11 @@
 | [GRU4Rec](skrec/recommender/GRU4Rec.py)               | TensorFlow (1.14) | [BalÃ¡zs Hidasi et al., Session-based Recommendations with Recurrent Neural Networks.](https://arxiv.org/abs/1511.06939) | ICLR 2016 |
 | [GRU4RecPlus](skrec/recommender/GRU4RecPlus.py)       | TensorFlow (1.14) | [BalÃ¡zs Hidasi et al., Recurrent Neural Networks with Top-k Gains for Session-based Recommendations.](https://dl.acm.org/doi/10.1145/3269206.3271761) | CIKM 2018 |
 | [Caser](skrec/recommender/Caser.py)                   | PyTorch           |[Jiaxi Tang et al., Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding.](https://dl.acm.org/doi/10.1145/3159652.3159656) | WSDM 2018 |
 | [CML](skrec/recommender/CML.py)                       | TensorFlow (1.14) | [Cheng-Kang Hsieh et al., Collaborative Metric Learning.](https://dl.acm.org/doi/10.1145/3038912.3052639) | WWW 2017 |
 | [MultiVAE](skrec/recommender/MultVAE.py)              | PyTorch           | [Dawen Liang, et al., Variational Autoencoders for Collaborative Filtering.](https://dl.acm.org/doi/10.1145/3178876.3186150) | WWW 2018 |
 | [CDAE](skrec/recommender/CDAE.py)                     | PyTorch           | [Yao Wu et al., Collaborative Denoising Auto-Encoders for Top-n Recommender Systems.](https://dl.acm.org/doi/10.1145/2835776.2835837) | WSDM 2016 |
 | [SGAT](skrec/recommender/SGAT.py)                     | TensorFlow (1.14) | [Zhongchuan Sun, et al., Sequential Graph Collaborative Filtering](https://www.sciencedirect.com/science/article/pii/S0020025522001049) | Information Sciences 2022 |
+| [LightGCL](skrec/recommender/LightGCL.py)             | PyTorch           | [Xuheng Cai, et al., LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation](https://openreview.net/forum?id=FKXVK9dyMM) | ICLR 2023 |
+| [DENS](skrec/recommender/DENS.py)                     | PyTorch           | [Riwei Lai, et al., Disentangled Negative Sampling for Collaborative Filtering](https://dl.acm.org/doi/10.1145/3539597.3570419) | WSDM 2023 |
```

## Comparing `scikit_recommender-0.0.3.dist-info/RECORD` & `scikit_recommender-0.0.4.dist-info/RECORD`

 * *Files 23% similar despite different names*

```diff
@@ -1,58 +1,61 @@
-skrec/__init__.py,sha256=QMjNhls-vvfgRbVu_HOdm1fik_MKzdM-n-4hg5JgtGk,192
-skrec/io/__init__.py,sha256=iymFKBKqejfEmLxGPywIznJUtit2ZmZf9uXWQI_AEN0,543
-skrec/io/data_iterator.py,sha256=2K4_XgiKia6-dQpwaBIG6i0SC7HXI9DW1upjGKXqX0s,14644
-skrec/io/dataset.py,sha256=-B6PjhSHw-3tv0NGA9xyB5mfSFAecDT7sQqzk-CxywQ,13778
+skrec/__init__.py,sha256=iIfMsxSmJ2C85Y1ANhHx7BCFzGTLOqgF-oOSz6YBKlc,227
+skrec/run_config.py,sha256=ca7GcwH6c6YcebqS99dKu6CeMjZ_5wUoMe1DZZBDfGQ,1811
+skrec/io/__init__.py,sha256=acdgZUfbxN8t9Cf3Nc9yGrpeSCwYPxUB2eHJqOCdnxk,661
+skrec/io/data_iterator.py,sha256=_tyVDhcyyDnnkXCh-q9E11CqAsRdNpHofo3FHXOS6Ok,18167
+skrec/io/dataset.py,sha256=-LwikFiDeULKBTTdK-tMs5Dz8LrL5eR9AqwHP22_6S4,20439
 skrec/io/logger.py,sha256=WVG64DERs62qrY0XttaXhrxfCz5mjOTYSvr_QYMncWU,2623
 skrec/io/movielens.py,sha256=jh22Tmjo4oucep7CUFzNrIcu4QSe8dSHKLEOnSCzPjE,1483
 skrec/io/preprocessor.py,sha256=rQblJ7FnmqOwrwrP7WW8DI8buKdRCUR8K3E5wYMqZnw,13177
-skrec/recommender/BPRMF.py,sha256=5lKpdJfFc52cy23oP7eRme4b59mltF1-DgfRp644MVo,5794
-skrec/recommender/CDAE.py,sha256=9HEyutITGC5S2NwjboKfeVSTlgO4skc6IHSq0X-C2CY,9858
-skrec/recommender/CML.py,sha256=iy9FKt75Zkaje9d2Es7hNdStKhYEM9QMlCqizovxJUA,8800
-skrec/recommender/Caser.py,sha256=QSBvfnLkVBOQnDD3TkYpwzNuYFPNPIfzL5D8cfS8vus,9587
-skrec/recommender/FPMC.py,sha256=M4BDCD1MBYpbd6CEzXVHffIK3KAuaJl0V5Gt04ILmCo,6660
-skrec/recommender/GRU4Rec.py,sha256=xg10XqZ54KmtLRvl_kPmv8l9GqgV2xD-QHJzNXXC1hg,12848
-skrec/recommender/GRU4RecPlus.py,sha256=TNXkMqQhZgyUlnbHxPhxNm-ZOMWGZdhELsId8HyyseM,14646
-skrec/recommender/HGN.py,sha256=lYHYaEdZETXzZaTpylhWoAq0lnEY6fRmKZgzpC6y-hg,9658
-skrec/recommender/LightGCN.py,sha256=PaoMwA5fNGQTqE51WS1nscrKVCelFjgZHjaFijgMkG8,9540
-skrec/recommender/MultVAE.py,sha256=VR7pMvNyfBTamCPWx8-15wyDLFdqjPkXDu4mSEkHaLw,8726
-skrec/recommender/Pop.py,sha256=C-RSsw5Ohia09XJJ9k_HloWQI3qRIJp62RvUd17KJQM,1635
-skrec/recommender/SASRec.py,sha256=k7W_5LamYv2L_dAgiy0hV7mF-eX0ImcmtVhb5-Jxn-U,21741
-skrec/recommender/SGAT.py,sha256=XAjV-b7HTzb4IJtsFZPdHfPEFjdJvnoNYbowpjQ64BU,15764
-skrec/recommender/SRGNN.py,sha256=f5a0HJAB8JwC0pB9-vyvv0Vq9iszn2moKE-TaHDoPiA,15481
-skrec/recommender/TransRec.py,sha256=ydNdtSEONG8938ZAY-MjAUeC9toxBztzFunEK9HMp8w,7098
+skrec/recommender/BPRMF.py,sha256=2eYEVEjCXeD54XL2l1aA6NqGHKnsGTQss4G6xLOobkE,5633
+skrec/recommender/CDAE.py,sha256=96Ksoio-vxbwmNKvaewp4mU2vcKu6qm9YlxWoqqw2K8,9711
+skrec/recommender/CML.py,sha256=PRyje6P12e9KF3NUpuy4MMdC2MBXbZYRcsP1y8jf2Cw,8645
+skrec/recommender/Caser.py,sha256=InCHUZUnPJM37BDC2aaRMbLYBUg5K_37YkFVchsJ924,9431
+skrec/recommender/DENS.py,sha256=vRTmpkShsOGi8kOgpeBLt7PvaQIq4GCj3uTL1dvYmFg,21847
+skrec/recommender/FPMC.py,sha256=CNJg94VUTlBFyAEEZHRMihLTe4YHPMXjrYO7uPUQw5E,6515
+skrec/recommender/GRU4Rec.py,sha256=MTcmjDZ2gncZD_p4dfSd2JUZj4RyMINfctpMAO-ZJQI,12713
+skrec/recommender/GRU4RecPlus.py,sha256=g5b6ssif7mgoHKXJl35yYQSSvLqDr5k9C5mCIvH-gvo,14524
+skrec/recommender/HGN.py,sha256=yrgql5KBxKLKZy3VNSo2M9NwQbv_ePezVkQur1HIAm0,9535
+skrec/recommender/LightGCL.py,sha256=h6RNP2m_rXy8MVm2VbtPyc5Ko3z18-OAt9Ik8uQH-Rw,10543
+skrec/recommender/LightGCN.py,sha256=RkBqSD7oRrrR8sHbTmpO2vty8_2iCxgu4IxmZqlIpvU,9426
+skrec/recommender/MultVAE.py,sha256=f2G3mFqLkXsXd5G4HPb1M6MPZ-mAvYet1ibX-C4mRe4,8593
+skrec/recommender/Pop.py,sha256=O7QGzNQPu66oi_8WCCeJfNtE-CsJfSn6B92BIfoCIvQ,1624
+skrec/recommender/SASRec.py,sha256=rYjZCYjrFVM9H5N4JnYKOVmqBSHpLn7JPa-6tEEpaAM,21580
+skrec/recommender/SGAT.py,sha256=SjLycaRqiOSCIk9PxpVJ4xPz9OAZ5J2uMZxA71AAEfE,15607
+skrec/recommender/SRGNN.py,sha256=hHzXL-oFmD1wfWf-mwiZpMP3BhR5QWipX9GCzFUpOfE,15322
+skrec/recommender/TransRec.py,sha256=sP1P9CgaOAHvOrRBX3oh0PRDAqrgxCEnMfmB5U55wow,6945
 skrec/recommender/__init__.py,sha256=t6yJ0YYonIqB3iPvW-kxMDxVaCEKUk9-kCn7Reng2YY,86
-skrec/recommender/base.py,sha256=-XE4xSNPlIOITB6Mt4Xh6vnMGLnIbj5YdEqPGehOsJQ,1699
-skrec/recommender/AOBPR/AOBPR.py,sha256=LxFT_2e-3Nn-bpgnUbtJhWkvyB70DB9oATFRWaG6hPk,4196
+skrec/recommender/base.py,sha256=JeNtitV6kgAT8kmESGaF0vMeDo-XA9Gg-YcMZNJ6zHE,2201
+skrec/recommender/AOBPR/AOBPR.py,sha256=QY4gDJnKrVXR8M2FiuItxCbt_glb_hqEq3Hz4nKkTRI,3956
 skrec/recommender/AOBPR/__init__.py,sha256=JLZkU35sL9Bh0FYDKu5ywbZpDoG-cRDmXTLUTu-8CcA,73
-skrec/recommender/AOBPR/pyx_aobpr_func.cp39-win_amd64.pyd,sha256=suNnRZMlg8vz7tZcAcuy4EQGIYDuaPPLmZn5rFFRlTY,68608
-skrec/recommender/BERT4Rec/BERT4Rec.py,sha256=8QbyqZbh_r-AAELZh9iODcOi5jYsNlsNP4C_GkRuCjk,7915
+skrec/recommender/AOBPR/pyx_aobpr_func.cp39-win_amd64.pyd,sha256=pGYrCkeON6qftECzQM73hGlE0GvEZlSzyk6D4ycbP4A,68608
+skrec/recommender/BERT4Rec/BERT4Rec.py,sha256=B8mU0nAI9E74s0rVmui7qniKDbSVRDGC3mRbjtmCvL4,8077
 skrec/recommender/BERT4Rec/__init__.py,sha256=hG_eS0hjzDhS0GpM5O79HvKapayU8ONaXtwK3Qi1B4g,88
-skrec/recommender/BERT4Rec/bert4rec_gen_data.py,sha256=7FtsnMo8zpgR1k61sCQXYEnXO50L4zMq_ehGbagW1-s,18467
-skrec/recommender/BERT4Rec/bert4rec_utils.py,sha256=nHw48GNalHP4fc6p96xW3Tcl8OeJnVMz_F4LLR36Lcg,11820
+skrec/recommender/BERT4Rec/bert4rec_gen_data.py,sha256=77e009jarVjp76W-fy6QKpikD80cttkc3LJ_ME9VdxI,18471
+skrec/recommender/BERT4Rec/bert4rec_utils.py,sha256=8LA82wjPBwtGxTyqnooeyYxUVrPlj5VjddTxcLR1e8g,11695
 skrec/recommender/BERT4Rec/modeling.py,sha256=lweMl86-bomccYtPk5nDnhAJycDxuKnZ8YNTzYKX6hA,41607
 skrec/recommender/BERT4Rec/optimization.py,sha256=6AOX2WnSfYEv6QUpLovjnMkgikiNAHTifiI20TuSjzw,6676
 skrec/recommender/BERT4Rec/vocab.py,sha256=94NBNOeZ9zRWaN7eviJViKpBET9JUFdvcboU7oRy3hc,2336
 skrec/utils/__init__.py,sha256=gYGwsGAhJkZ3pBjjOV5cgQgtbEA-XdkKiUmmQp-KluM,127
-skrec/utils/common.py,sha256=0hFHkSFOHh62Ce31OqBP6hPOtmH92CcXHSBIcnJdtrM,1261
+skrec/utils/common.py,sha256=rP6smz7FdsdAd79t7il6-lHk0Rd4nJw5w9qd8oXQaNk,1494
 skrec/utils/registry.py,sha256=5puR2tKyal24PE0qOiVdX5mYyYqvHd6b2w-AbtypaF4,1255
 skrec/utils/tf1x.py,sha256=4bhZA5RnibIEJyybCgbECKQm2N0v2CIVm-dRRaVfyzA,1265
 skrec/utils/tf2x.py,sha256=t6yJ0YYonIqB3iPvW-kxMDxVaCEKUk9-kCn7Reng2YY,86
 skrec/utils/torch.py,sha256=-plQMx_nBHuN9iWy7ucIAQnMj-iFsR3sSWaZZk1GepM,4122
-skrec/utils/py/__init__.py,sha256=dkxRcGGV5IcJ2Jho3n4UXcKZmM6FP37bbw9etpC36oA,538
+skrec/utils/py/__init__.py,sha256=5-TRUgTmu0A2yRxhYcwbHaJjljPkXjVsn0gHZy_tU-c,576
 skrec/utils/py/batch_iterator.py,sha256=RbT77WstQDz0z6nwbacxp4GnLM8HSfSO2Jpy8z9vM6w,6779
 skrec/utils/py/config.py,sha256=QXNUNgAewsXb53f9h2NSLSWz7WSNsRYFKj24cfBGDuA,2593
 skrec/utils/py/decorator.py,sha256=7Kjf_8lSKOU044cWWC4PIdphixllQ1YO9kms3iOwzDg,1678
-skrec/utils/py/evaluator.py,sha256=brqzeg_sqGx9hTmljSP6i3N546vSm_azuFWkYNvxfd0,9089
+skrec/utils/py/evaluator.py,sha256=46kTe6wu3_1UIyTkCZXffPi0TpA3rXSpoF10SAaf6UM,10186
 skrec/utils/py/generic.py,sha256=3lZCn6_wOCmjMKSUHdog12jLhXT9lqIggTevZpwQ_aE,4876
 skrec/utils/py/random.py,sha256=kPcQKKPh6qmX82nasFKOAu6qZ-WxrgnXkiU2m-LDwaY,1450
 skrec/utils/py/cython/__init__.py,sha256=3yiwEYylUE5R6JL2RiIPvvgnuIjRsk8V6OKvyGJND54,173
-skrec/utils/py/cython/pyx_eval_matrix.cp39-win_amd64.pyd,sha256=G4r6YWeItLbmBzYHy_ccFu62_IPX_GCxuRgh2U-WiQ0,84992
-skrec/utils/py/cython/pyx_init.cp39-win_amd64.pyd,sha256=vjp-5La-9jl17RlJWpnZ1H0TyRyyK_J-0Z5gFJ3A-sM,26112
-skrec/utils/py/cython/pyx_random.cp39-win_amd64.pyd,sha256=TYZ766WPZRdKDs9DaevF0hJvPvchocYS0u9RTwWXdqI,109568
-skrec/utils/py/cython/pyx_sort.cp39-win_amd64.pyd,sha256=KN2uuKndIOAo0i89tLFvyuVto9yKfgYUmZ-0FmEyxMM,115712
-skrec/utils/py/cython/pyx_utils.cp39-win_amd64.pyd,sha256=OTyiRdCGKerSGkD8s00tzdvLW_SmDJ5IK66gqMuVDS8,30720
-scikit_recommender-0.0.3.dist-info/LICENSE,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scikit_recommender-0.0.3.dist-info/METADATA,sha256=K7eocNlSsfFd13AgYiUOCQpRFD8W7D0RURe7dCZnX_Y,7326
-scikit_recommender-0.0.3.dist-info/WHEEL,sha256=eep6QWEFiQfg2wcclssb_WY-D33AnLYLnEKGA9Rn-VU,100
-scikit_recommender-0.0.3.dist-info/top_level.txt,sha256=Qv7je5rGC74taVHO3JQGhfBjISiGCzqi9vWXz2DDmYY,6
-scikit_recommender-0.0.3.dist-info/RECORD,,
+skrec/utils/py/cython/pyx_eval_matrix.cp39-win_amd64.pyd,sha256=Rg1oT6nxFSVYUVPSPBBAeTmDYJc6iQ02N5fv_KZyrBA,84992
+skrec/utils/py/cython/pyx_init.cp39-win_amd64.pyd,sha256=HSp8c0oI4QfB6zJwOrDKMaiPQc9Bw7xkNFGYm_cWGwg,26112
+skrec/utils/py/cython/pyx_random.cp39-win_amd64.pyd,sha256=IxWwFWz3v3HLiyLBMWR5EpWp9kuWyYPLrlkvpfySolM,109056
+skrec/utils/py/cython/pyx_sort.cp39-win_amd64.pyd,sha256=cyxTIY4bVvU8NXJcVwwAVpKfg_dvkTRDpcd16pp2fUw,115712
+skrec/utils/py/cython/pyx_utils.cp39-win_amd64.pyd,sha256=7OliqUrmyN22NffY2M9i_8sK2WU9Dz2-1iU4LujIlPo,30720
+scikit_recommender-0.0.4.dist-info/LICENSE,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+scikit_recommender-0.0.4.dist-info/METADATA,sha256=Co-HDJV7goNpFRKNWLBjkD9QeBXi9k6WK43OuS_m3zQ,7783
+scikit_recommender-0.0.4.dist-info/WHEEL,sha256=eep6QWEFiQfg2wcclssb_WY-D33AnLYLnEKGA9Rn-VU,100
+scikit_recommender-0.0.4.dist-info/top_level.txt,sha256=Qv7je5rGC74taVHO3JQGhfBjISiGCzqi9vWXz2DDmYY,6
+scikit_recommender-0.0.4.dist-info/RECORD,,
```

