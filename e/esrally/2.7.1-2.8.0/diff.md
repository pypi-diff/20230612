# Comparing `tmp/esrally-2.7.1-py3-none-any.whl.zip` & `tmp/esrally-2.8.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,74 +1,75 @@
-Zip file size: 296092 bytes, number of entries: 72
+Zip file size: 303803 bytes, number of entries: 73
 -rw-r--r--  2.0 unx     2665 b- defN 20-Feb-02 00:00 esrally/__init__.py
 -rw-r--r--  2.0 unx       22 b- defN 20-Feb-02 00:00 esrally/_version.py
 -rw-r--r--  2.0 unx    11356 b- defN 20-Feb-02 00:00 esrally/actor.py
 -rw-r--r--  2.0 unx    12552 b- defN 20-Feb-02 00:00 esrally/config.py
 -rw-r--r--  2.0 unx     2951 b- defN 20-Feb-02 00:00 esrally/exceptions.py
--rw-r--r--  2.0 unx     5106 b- defN 20-Feb-02 00:00 esrally/log.py
--rw-r--r--  2.0 unx    97601 b- defN 20-Feb-02 00:00 esrally/metrics.py
+-rw-r--r--  2.0 unx     6626 b- defN 20-Feb-02 00:00 esrally/log.py
+-rw-r--r--  2.0 unx    99337 b- defN 20-Feb-02 00:00 esrally/metrics.py
 -rw-r--r--  2.0 unx        6 b- defN 20-Feb-02 00:00 esrally/min-es-version.txt
 -rw-r--r--  2.0 unx     1593 b- defN 20-Feb-02 00:00 esrally/paths.py
 -rw-r--r--  2.0 unx    17239 b- defN 20-Feb-02 00:00 esrally/racecontrol.py
--rw-r--r--  2.0 unx    57132 b- defN 20-Feb-02 00:00 esrally/rally.py
+-rw-r--r--  2.0 unx    57243 b- defN 20-Feb-02 00:00 esrally/rally.py
 -rw-r--r--  2.0 unx     4495 b- defN 20-Feb-02 00:00 esrally/rallyd.py
 -rw-r--r--  2.0 unx    44396 b- defN 20-Feb-02 00:00 esrally/reporter.py
--rw-r--r--  2.0 unx   102963 b- defN 20-Feb-02 00:00 esrally/telemetry.py
+-rw-r--r--  2.0 unx   103668 b- defN 20-Feb-02 00:00 esrally/telemetry.py
 -rw-r--r--  2.0 unx     4110 b- defN 20-Feb-02 00:00 esrally/time.py
 -rw-r--r--  2.0 unx     2552 b- defN 20-Feb-02 00:00 esrally/version.py
 -rw-r--r--  2.0 unx      952 b- defN 20-Feb-02 00:00 esrally/client/__init__.py
--rw-r--r--  2.0 unx     8982 b- defN 20-Feb-02 00:00 esrally/client/asynchronous.py
--rw-r--r--  2.0 unx     3562 b- defN 20-Feb-02 00:00 esrally/client/context.py
--rw-r--r--  2.0 unx    17328 b- defN 20-Feb-02 00:00 esrally/client/factory.py
--rw-r--r--  2.0 unx     1011 b- defN 20-Feb-02 00:00 esrally/client/synchronous.py
+-rw-r--r--  2.0 unx    14597 b- defN 20-Feb-02 00:00 esrally/client/asynchronous.py
+-rw-r--r--  2.0 unx     1851 b- defN 20-Feb-02 00:00 esrally/client/common.py
+-rw-r--r--  2.0 unx     3488 b- defN 20-Feb-02 00:00 esrally/client/context.py
+-rw-r--r--  2.0 unx    20056 b- defN 20-Feb-02 00:00 esrally/client/factory.py
+-rw-r--r--  2.0 unx    10287 b- defN 20-Feb-02 00:00 esrally/client/synchronous.py
 -rw-r--r--  2.0 unx      954 b- defN 20-Feb-02 00:00 esrally/driver/__init__.py
--rw-r--r--  2.0 unx   106918 b- defN 20-Feb-02 00:00 esrally/driver/driver.py
--rw-r--r--  2.0 unx   120168 b- defN 20-Feb-02 00:00 esrally/driver/runner.py
+-rw-r--r--  2.0 unx   108166 b- defN 20-Feb-02 00:00 esrally/driver/driver.py
+-rw-r--r--  2.0 unx   120807 b- defN 20-Feb-02 00:00 esrally/driver/runner.py
 -rw-r--r--  2.0 unx    11927 b- defN 20-Feb-02 00:00 esrally/driver/scheduler.py
 -rw-r--r--  2.0 unx     1037 b- defN 20-Feb-02 00:00 esrally/mechanic/__init__.py
 -rw-r--r--  2.0 unx     1488 b- defN 20-Feb-02 00:00 esrally/mechanic/cluster.py
 -rw-r--r--  2.0 unx     2157 b- defN 20-Feb-02 00:00 esrally/mechanic/java_resolver.py
 -rw-r--r--  2.0 unx    11339 b- defN 20-Feb-02 00:00 esrally/mechanic/launcher.py
 -rw-r--r--  2.0 unx    29731 b- defN 20-Feb-02 00:00 esrally/mechanic/mechanic.py
--rw-r--r--  2.0 unx    22438 b- defN 20-Feb-02 00:00 esrally/mechanic/provisioner.py
--rw-r--r--  2.0 unx    44351 b- defN 20-Feb-02 00:00 esrally/mechanic/supplier.py
+-rw-r--r--  2.0 unx    22380 b- defN 20-Feb-02 00:00 esrally/mechanic/provisioner.py
+-rw-r--r--  2.0 unx    45447 b- defN 20-Feb-02 00:00 esrally/mechanic/supplier.py
 -rw-r--r--  2.0 unx    22353 b- defN 20-Feb-02 00:00 esrally/mechanic/team.py
 -rw-r--r--  2.0 unx      832 b- defN 20-Feb-02 00:00 esrally/resources/annotation-template.json
 -rw-r--r--  2.0 unx     1021 b- defN 20-Feb-02 00:00 esrally/resources/docker-compose.yml.j2
--rw-r--r--  2.0 unx     1340 b- defN 20-Feb-02 00:00 esrally/resources/logging.json
--rw-r--r--  2.0 unx     1656 b- defN 20-Feb-02 00:00 esrally/resources/metrics-template.json
+-rw-r--r--  2.0 unx     1466 b- defN 20-Feb-02 00:00 esrally/resources/logging.json
+-rw-r--r--  2.0 unx     1697 b- defN 20-Feb-02 00:00 esrally/resources/metrics-template.json
 -rw-r--r--  2.0 unx     1298 b- defN 20-Feb-02 00:00 esrally/resources/races-template.json
 -rw-r--r--  2.0 unx      674 b- defN 20-Feb-02 00:00 esrally/resources/rally.ini
 -rw-r--r--  2.0 unx     1978 b- defN 20-Feb-02 00:00 esrally/resources/results-template.json
 -rw-r--r--  2.0 unx    24238 b- defN 20-Feb-02 00:00 esrally/resources/track-schema.json
 -rw-r--r--  2.0 unx     1665 b- defN 20-Feb-02 00:00 esrally/resources/track.json.j2
 -rw-r--r--  2.0 unx     1024 b- defN 20-Feb-02 00:00 esrally/track/__init__.py
 -rw-r--r--  2.0 unx    80995 b- defN 20-Feb-02 00:00 esrally/track/loader.py
 -rw-r--r--  2.0 unx    57742 b- defN 20-Feb-02 00:00 esrally/track/params.py
 -rw-r--r--  2.0 unx    39369 b- defN 20-Feb-02 00:00 esrally/track/track.py
 -rw-r--r--  2.0 unx      774 b- defN 20-Feb-02 00:00 esrally/tracker/__init__.py
 -rw-r--r--  2.0 unx     3845 b- defN 20-Feb-02 00:00 esrally/tracker/corpus.py
 -rw-r--r--  2.0 unx     4930 b- defN 20-Feb-02 00:00 esrally/tracker/index.py
--rw-r--r--  2.0 unx     4503 b- defN 20-Feb-02 00:00 esrally/tracker/tracker.py
+-rw-r--r--  2.0 unx     4517 b- defN 20-Feb-02 00:00 esrally/tracker/tracker.py
 -rw-r--r--  2.0 unx      774 b- defN 20-Feb-02 00:00 esrally/utils/__init__.py
 -rw-r--r--  2.0 unx     1657 b- defN 20-Feb-02 00:00 esrally/utils/collections.py
 -rw-r--r--  2.0 unx     7448 b- defN 20-Feb-02 00:00 esrally/utils/console.py
 -rw-r--r--  2.0 unx     2835 b- defN 20-Feb-02 00:00 esrally/utils/convert.py
--rw-r--r--  2.0 unx     5948 b- defN 20-Feb-02 00:00 esrally/utils/git.py
+-rw-r--r--  2.0 unx     6139 b- defN 20-Feb-02 00:00 esrally/utils/git.py
 -rw-r--r--  2.0 unx    19710 b- defN 20-Feb-02 00:00 esrally/utils/io.py
 -rw-r--r--  2.0 unx     7112 b- defN 20-Feb-02 00:00 esrally/utils/jvm.py
 -rw-r--r--  2.0 unx     5026 b- defN 20-Feb-02 00:00 esrally/utils/modules.py
--rw-r--r--  2.0 unx    12434 b- defN 20-Feb-02 00:00 esrally/utils/net.py
--rw-r--r--  2.0 unx     7734 b- defN 20-Feb-02 00:00 esrally/utils/opts.py
+-rw-r--r--  2.0 unx    12648 b- defN 20-Feb-02 00:00 esrally/utils/net.py
+-rw-r--r--  2.0 unx     9245 b- defN 20-Feb-02 00:00 esrally/utils/opts.py
 -rw-r--r--  2.0 unx     5562 b- defN 20-Feb-02 00:00 esrally/utils/process.py
 -rw-r--r--  2.0 unx     6398 b- defN 20-Feb-02 00:00 esrally/utils/repo.py
 -rw-r--r--  2.0 unx     2711 b- defN 20-Feb-02 00:00 esrally/utils/sysstats.py
 -rw-r--r--  2.0 unx     8594 b- defN 20-Feb-02 00:00 esrally/utils/versions.py
-?rw-r--r--  2.0 unx    10303 b- defN 20-Feb-02 00:00 esrally-2.7.1.dist-info/METADATA
-?rw-r--r--  2.0 unx       87 b- defN 20-Feb-02 00:00 esrally-2.7.1.dist-info/WHEEL
-?rw-r--r--  2.0 unx       78 b- defN 20-Feb-02 00:00 esrally-2.7.1.dist-info/entry_points.txt
-?rw-r--r--  2.0 unx     1001 b- defN 20-Feb-02 00:00 esrally-2.7.1.dist-info/licenses/AUTHORS
-?rw-r--r--  2.0 unx    10143 b- defN 20-Feb-02 00:00 esrally-2.7.1.dist-info/licenses/LICENSE
-?rw-r--r--  2.0 unx       45 b- defN 20-Feb-02 00:00 esrally-2.7.1.dist-info/licenses/NOTICE
-?rw-r--r--  2.0 unx   125575 b- defN 20-Feb-02 00:00 esrally-2.7.1.dist-info/licenses/NOTICE.txt
-?rw-r--r--  2.0 unx     5945 b- defN 20-Feb-02 00:00 esrally-2.7.1.dist-info/RECORD
-72 files, 1248439 bytes uncompressed, 286832 bytes compressed:  77.0%
+?rw-r--r--  2.0 unx    10393 b- defN 20-Feb-02 00:00 esrally-2.8.0.dist-info/METADATA
+?rw-r--r--  2.0 unx       87 b- defN 20-Feb-02 00:00 esrally-2.8.0.dist-info/WHEEL
+?rw-r--r--  2.0 unx       78 b- defN 20-Feb-02 00:00 esrally-2.8.0.dist-info/entry_points.txt
+?rw-r--r--  2.0 unx     1019 b- defN 20-Feb-02 00:00 esrally-2.8.0.dist-info/licenses/AUTHORS
+?rw-r--r--  2.0 unx    10143 b- defN 20-Feb-02 00:00 esrally-2.8.0.dist-info/licenses/LICENSE
+?rw-r--r--  2.0 unx       45 b- defN 20-Feb-02 00:00 esrally-2.8.0.dist-info/licenses/NOTICE
+?rw-r--r--  2.0 unx   136348 b- defN 20-Feb-02 00:00 esrally-2.8.0.dist-info/licenses/NOTICE.txt
+?rw-r--r--  2.0 unx     6028 b- defN 20-Feb-02 00:00 esrally-2.8.0.dist-info/RECORD
+73 files, 1287893 bytes uncompressed, 294419 bytes compressed:  77.1%
```

## zipnote {}

```diff
@@ -48,14 +48,17 @@
 
 Filename: esrally/client/__init__.py
 Comment: 
 
 Filename: esrally/client/asynchronous.py
 Comment: 
 
+Filename: esrally/client/common.py
+Comment: 
+
 Filename: esrally/client/context.py
 Comment: 
 
 Filename: esrally/client/factory.py
 Comment: 
 
 Filename: esrally/client/synchronous.py
@@ -186,32 +189,32 @@
 
 Filename: esrally/utils/sysstats.py
 Comment: 
 
 Filename: esrally/utils/versions.py
 Comment: 
 
-Filename: esrally-2.7.1.dist-info/METADATA
+Filename: esrally-2.8.0.dist-info/METADATA
 Comment: 
 
-Filename: esrally-2.7.1.dist-info/WHEEL
+Filename: esrally-2.8.0.dist-info/WHEEL
 Comment: 
 
-Filename: esrally-2.7.1.dist-info/entry_points.txt
+Filename: esrally-2.8.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: esrally-2.7.1.dist-info/licenses/AUTHORS
+Filename: esrally-2.8.0.dist-info/licenses/AUTHORS
 Comment: 
 
-Filename: esrally-2.7.1.dist-info/licenses/LICENSE
+Filename: esrally-2.8.0.dist-info/licenses/LICENSE
 Comment: 
 
-Filename: esrally-2.7.1.dist-info/licenses/NOTICE
+Filename: esrally-2.8.0.dist-info/licenses/NOTICE
 Comment: 
 
-Filename: esrally-2.7.1.dist-info/licenses/NOTICE.txt
+Filename: esrally-2.8.0.dist-info/licenses/NOTICE.txt
 Comment: 
 
-Filename: esrally-2.7.1.dist-info/RECORD
+Filename: esrally-2.8.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## esrally/_version.py

```diff
@@ -1 +1 @@
-__version__ = "2.7.1"
+__version__ = "2.8.0"
```

## esrally/log.py

```diff
@@ -24,29 +24,67 @@
 from esrally import paths
 from esrally.utils import io
 
 
 # pylint: disable=unused-argument
 def configure_utc_formatter(*args, **kwargs):
     """
-    Logging formatter that renders timestamps in UTC to ensure consistent
-    timestamps across all deployments regardless of machine settings.
+    Logging formatter that renders timestamps UTC, or in the local system time zone when the user requests it.
     """
     formatter = logging.Formatter(fmt=kwargs["format"], datefmt=kwargs["datefmt"])
-    formatter.converter = time.gmtime
+    user_tz = kwargs.get("timezone", None)
+    if user_tz == "localtime":
+        formatter.converter = time.localtime
+    else:
+        formatter.converter = time.gmtime
+
     return formatter
 
 
 def log_config_path():
     """
     :return: The absolute path to Rally's log configuration file.
     """
     return os.path.join(paths.rally_confdir(), "logging.json")
 
 
+def add_missing_loggers_to_config():
+    """
+    Ensures that any missing top level loggers in resources/logging.json are
+    appended to an existing log configuration
+    """
+
+    def _missing_loggers(source, target):
+        """
+        Returns any top-level loggers present in 'source', but not in 'target'
+        :return: A dict of all loggers present in 'source', but not in 'target'
+        """
+        missing_loggers = {}
+        for logger in source:
+            if logger in source and logger in target:
+                continue
+            else:
+                missing_loggers[logger] = source[logger]
+        return missing_loggers
+
+    source_path = io.normalize_path(os.path.join(os.path.dirname(__file__), "resources", "logging.json"))
+
+    with open(log_config_path(), encoding="UTF-8") as target:
+        with open(source_path, encoding="UTF-8") as src:
+            template = json.load(src)
+            existing_logging_config = json.load(target)
+            if missing_loggers := _missing_loggers(source=template["loggers"], target=existing_logging_config["loggers"]):
+                existing_logging_config["loggers"].update(missing_loggers)
+                updated_config = json.dumps(existing_logging_config, indent=2)
+
+    if missing_loggers:
+        with open(log_config_path(), "w", encoding="UTF-8") as target:
+            target.write(updated_config)
+
+
 def install_default_log_config():
     """
     Ensures a log configuration file is present on this machine. The default
     log configuration is based on the template in resources/logging.json.
 
     It also ensures that the default log path has been created so log files
     can be successfully opened in that directory.
@@ -59,14 +97,15 @@
             with open(source_path, encoding="UTF-8") as src:
                 # Ensure we have a trailing path separator as after LOG_PATH there will only be the file name
                 log_path = os.path.join(paths.logs(), "")
                 # the logging path might contain backslashes that we need to escape
                 log_path = io.escape_path(log_path)
                 contents = src.read().replace("${LOG_PATH}", log_path)
                 target.write(contents)
+    add_missing_loggers_to_config()
     io.ensure_dir(paths.logs())
 
 
 def load_configuration():
     """
     Loads the logging configuration. This is a low-level method and usually
     `configure_logging()` should be used instead.
```

## esrally/metrics.py

```diff
@@ -26,15 +26,14 @@
 import random
 import statistics
 import sys
 import time
 import uuid
 import zlib
 from enum import Enum, IntEnum
-from http.client import responses
 
 import tabulate
 
 from esrally import client, config, exceptions, paths, time, version
 from esrally.utils import console, convert, io, versions
 
 
@@ -43,27 +42,29 @@
     Provides a stripped-down client interface that is easier to exchange for testing
     """
 
     def __init__(self, client, cluster_version=None):
         self._client = client
         self.logger = logging.getLogger(__name__)
         self._cluster_version = cluster_version
+        self.retryable_status_codes = [502, 503, 504, 429]
 
     # TODO #1335: Use version-specific support for metrics stores after 7.8.0.
     def probe_version(self):
         info = self.guarded(self._client.info)
         try:
             self._cluster_version = versions.components(info["version"]["number"])
         except BaseException:
             msg = "Could not determine version of metrics cluster"
             self.logger.exception(msg)
             raise exceptions.RallyError(msg)
 
     def put_template(self, name, template):
-        return self.guarded(self._client.indices.put_template, name=name, body=template)
+        tmpl = json.loads(template)
+        return self.guarded(self._client.indices.put_template, name=name, **tmpl)
 
     def template_exists(self, name):
         return self.guarded(self._client.indices.exists_template, name=name)
 
     def delete_template(self, name):
         self.guarded(self._client.indices.delete_template, name=name)
 
@@ -99,98 +100,133 @@
 
     def search(self, index, body):
         return self.guarded(self._client.search, index=index, body=body)
 
     def guarded(self, target, *args, **kwargs):
         # pylint: disable=import-outside-toplevel
         import elasticsearch
+        from elastic_transport import ApiError, TransportError
 
-        max_execution_count = 11
+        max_execution_count = 10
         execution_count = 0
 
-        while execution_count < max_execution_count:
+        while execution_count <= max_execution_count:
             time_to_sleep = 2**execution_count + random.random()
             execution_count += 1
 
             try:
                 return target(*args, **kwargs)
+            except elasticsearch.exceptions.ConnectionTimeout as e:
+                if execution_count <= max_execution_count:
+                    self.logger.debug(
+                        "Connection timeout [%s] in attempt [%d/%d]. Sleeping for [%f] seconds.",
+                        e.message,
+                        execution_count,
+                        max_execution_count,
+                        time_to_sleep,
+                    )
+                    time.sleep(time_to_sleep)
+                else:
+                    operation = target.__name__
+                    self.logger.exception("Connection timeout while running [%s] (retried %d times).", operation, max_execution_count)
+                    node = self._client.transport.node_pool.get()
+                    msg = (
+                        "A connection timeout occurred while running the operation [%s] against your Elasticsearch metrics store on "
+                        "host [%s] at port [%s]." % (operation, node.host, node.port)
+                    )
+                    raise exceptions.RallyError(msg)
+            except elasticsearch.exceptions.ConnectionError as e:
+                if execution_count <= max_execution_count:
+                    self.logger.debug(
+                        "Connection error [%s] in attempt [%d/%d]. Sleeping for [%f] seconds.",
+                        e.message,
+                        execution_count,
+                        max_execution_count,
+                        time_to_sleep,
+                    )
+                    time.sleep(time_to_sleep)
+                else:
+                    node = self._client.transport.node_pool.get()
+                    msg = (
+                        "Could not connect to your Elasticsearch metrics store. Please check that it is running on host [%s] at port [%s]"
+                        " or fix the configuration in [%s]." % (node.host, node.port, config.ConfigFile().location)
+                    )
+                    self.logger.exception(msg)
+                    # connection errors doesn't neccessarily mean it's during setup
+                    raise exceptions.RallyError(msg)
             except elasticsearch.exceptions.AuthenticationException:
                 # we know that it is just one host (see EsClientFactory)
-                node = self._client.transport.hosts[0]
+                node = self._client.transport.node_pool.get()
                 msg = (
                     "The configured user could not authenticate against your Elasticsearch metrics store running on host [%s] at "
                     "port [%s] (wrong password?). Please fix the configuration in [%s]."
-                    % (node["host"], node["port"], config.ConfigFile().location)
+                    % (node.host, node.port, config.ConfigFile().location)
                 )
                 self.logger.exception(msg)
                 raise exceptions.SystemSetupError(msg)
             except elasticsearch.exceptions.AuthorizationException:
-                node = self._client.transport.hosts[0]
+                node = self._client.transport.node_pool.get()
                 msg = (
                     "The configured user does not have enough privileges to run the operation [%s] against your Elasticsearch metrics "
                     "store running on host [%s] at port [%s]. Please adjust your x-pack configuration or specify a user with enough "
-                    "privileges in the configuration in [%s]." % (target.__name__, node["host"], node["port"], config.ConfigFile().location)
+                    "privileges in the configuration in [%s]." % (target.__name__, node.host, node.port, config.ConfigFile().location)
                 )
                 self.logger.exception(msg)
                 raise exceptions.SystemSetupError(msg)
-            except elasticsearch.exceptions.ConnectionTimeout:
-                if execution_count < max_execution_count:
-                    self.logger.debug("Connection timeout in attempt [%d/%d].", execution_count, max_execution_count)
+            except elasticsearch.helpers.BulkIndexError as e:
+                for err in e.errors:
+                    err_type = err.get("index", {}).get("error", {}).get("type", None)
+                    if err.get("index", {}).get("status", None) not in self.retryable_status_codes:
+                        msg = f"Unretryable error encountered when sending metrics to remote metrics store: [{err_type}]"
+                        self.logger.exception("%s - Full error(s) [%s]", msg, str(e.errors))
+                        raise exceptions.RallyError(msg)
+
+                if execution_count <= max_execution_count:
+                    self.logger.debug(
+                        "Error in sending metrics to remote metrics store [%s] in attempt [%d/%d]. Sleeping for [%f] seconds.",
+                        e,
+                        execution_count,
+                        max_execution_count,
+                        time_to_sleep,
+                    )
                     time.sleep(time_to_sleep)
                 else:
-                    operation = target.__name__
-                    self.logger.exception("Connection timeout while running [%s] (retried %d times).", operation, max_execution_count)
-                    node = self._client.transport.hosts[0]
-                    msg = (
-                        "A connection timeout occurred while running the operation [%s] against your Elasticsearch metrics store on "
-                        "host [%s] at port [%s]." % (operation, node["host"], node["port"])
-                    )
+                    msg = f"Failed to send metrics to remote metrics store: [{e.errors}]"
+                    self.logger.exception("%s - Full error(s) [%s]", msg, str(e.errors))
                     raise exceptions.RallyError(msg)
-            except elasticsearch.exceptions.ConnectionError:
-                node = self._client.transport.hosts[0]
-                msg = (
-                    "Could not connect to your Elasticsearch metrics store. Please check that it is running on host [%s] at port [%s]"
-                    " or fix the configuration in [%s]." % (node["host"], node["port"], config.ConfigFile().location)
-                )
-                self.logger.exception(msg)
-                raise exceptions.SystemSetupError(msg)
-            except elasticsearch.TransportError as e:
-                if e.status_code == 404 and e.error == "index_not_found_exception":
-                    node = self._client.transport.hosts[0]
-                    msg = (
-                        "The operation [%s] against your Elasticsearch metrics store on "
-                        "host [%s] at port [%s] failed because index [%s] does not exist."
-                        % (target.__name__, node["host"], node["port"], kwargs.get("index"))
-                    )
-                    self.logger.exception(msg)
-                    raise exceptions.RallyError(msg)
-                if e.status_code in (502, 503, 504, 429) and execution_count < max_execution_count:
+            except ApiError as e:
+                if e.status_code in self.retryable_status_codes and execution_count <= max_execution_count:
                     self.logger.debug(
                         "%s (code: %d) in attempt [%d/%d]. Sleeping for [%f] seconds.",
-                        responses[e.status_code],
+                        e.error,
                         e.status_code,
                         execution_count,
                         max_execution_count,
                         time_to_sleep,
                     )
                     time.sleep(time_to_sleep)
                 else:
-                    node = self._client.transport.hosts[0]
+                    node = self._client.transport.node_pool.get()
                     msg = (
-                        "A transport error occurred while running the operation [%s] against your Elasticsearch metrics store on "
-                        "host [%s] at port [%s]." % (target.__name__, node["host"], node["port"])
+                        "An error [%s] occurred while running the operation [%s] against your Elasticsearch metrics store on host [%s] "
+                        "at port [%s]." % (e.error, target.__name__, node.host, node.port)
                     )
                     self.logger.exception(msg)
+                    # this does not necessarily mean it's a system setup problem...
                     raise exceptions.RallyError(msg)
+            except TransportError as e:
+                node = self._client.transport.node_pool.get()
 
-            except elasticsearch.exceptions.ElasticsearchException:
-                node = self._client.transport.hosts[0]
+                if e.errors:
+                    err = e.errors
+                else:
+                    err = e
                 msg = (
-                    "An unknown error occurred while running the operation [%s] against your Elasticsearch metrics store on host [%s] "
-                    "at port [%s]." % (target.__name__, node["host"], node["port"])
+                    "Transport error(s) [%s] occurred while running the operation [%s] against your Elasticsearch metrics store on "
+                    "host [%s] at port [%s]." % (err, target.__name__, node.host, node.port)
                 )
                 self.logger.exception(msg)
                 # this does not necessarily mean it's a system setup problem...
                 raise exceptions.RallyError(msg)
 
 
 class EsClientFactory:
@@ -433,17 +469,17 @@
 
     def close(self):
         """
         Closes the metric store. Note that it is mandatory to close the metrics store when it is no longer needed as it only persists
         metrics on close (in order to avoid additional latency during the benchmark).
         """
         self.logger.info("Closing metrics store.")
+        self.opened = False
         self.flush()
         self._clear_meta_info()
-        self.opened = False
 
     def add_meta_info(self, scope, scope_key, key, value):
         """
         Adds new meta information to the metrics store. All metrics entries that are created after calling this method are guaranteed to
         contain the added meta info (provided is on the same level or a level below, e.g. a cluster level metric will not contain node
         level meta information but all cluster level meta information will be contained in a node level metrics record).
```

## esrally/rally.py

```diff
@@ -288,15 +288,16 @@
 
     build_parser = subparsers.add_parser("build", help="Builds an artifact")
     build_parser.add_argument(
         "--revision",
         help="Define the source code revision for building the benchmark candidate. 'current' uses the source tree as is,"
         " 'latest' fetches the latest version on the main branch. It is also possible to specify a commit id or a timestamp."
         ' The timestamp must be specified as: "@ts" where "ts" must be a valid ISO 8601 timestamp, '
-        'e.g. "@2013-07-27T10:37:00Z" (default: current).',
+        'e.g. "@2013-07-27T10:37:00Z" (default: current). A combination of branch and timestamp is also possible,'
+        'e.g. "@feature-branch@2023-04-06T14:52:31Z".',
         default="current",
     )  # optimized for local usage, don't fetch sources
     build_parser.add_argument(
         "--target-os",
         help="The name of the target operating system for which an artifact should be downloaded (default: current OS)",
     )
     build_parser.add_argument(
```

## esrally/telemetry.py

```diff
@@ -773,14 +773,16 @@
         if self.samplers:
             for sampler in self.samplers:
                 sampler.finish()
 
 
 class NodeStatsRecorder:
     def __init__(self, telemetry_params, cluster_name, client, metrics_store):
+        self.logger = logging.getLogger(__name__)
+        self.logger.info("node stats recorder")
         self.sample_interval = telemetry_params.get("node-stats-sample-interval", 1)
         if self.sample_interval <= 0:
             raise exceptions.SystemSetupError(
                 f"The telemetry parameter 'node-stats-sample-interval' must be greater than zero but was {self.sample_interval}."
             )
 
         self.include_indices = telemetry_params.get("node-stats-include-indices", False)
@@ -799,27 +801,30 @@
         else:
             self.include_indices_metrics_list = [
                 "docs",
                 "store",
                 "indexing",
                 "search",
                 "merges",
+                "refresh",
+                "flush",
                 "query_cache",
                 "fielddata",
                 "segments",
                 "translog",
                 "request_cache",
             ]
 
         self.include_thread_pools = telemetry_params.get("node-stats-include-thread-pools", True)
         self.include_buffer_pools = telemetry_params.get("node-stats-include-buffer-pools", True)
         self.include_breakers = telemetry_params.get("node-stats-include-breakers", True)
         self.include_network = telemetry_params.get("node-stats-include-network", True)
         self.include_process = telemetry_params.get("node-stats-include-process", True)
         self.include_mem_stats = telemetry_params.get("node-stats-include-mem", True)
+        self.include_cgroup_stats = telemetry_params.get("node-stats-include-cgroup", True)
         self.include_gc_stats = telemetry_params.get("node-stats-include-gc", True)
         self.include_indexing_pressure = telemetry_params.get("node-stats-include-indexing-pressure", True)
         self.client = client
         self.metrics_store = metrics_store
         self.cluster_name = cluster_name
 
     def __str__(self):
@@ -841,14 +846,16 @@
             if self.include_breakers:
                 collected_node_stats.update(self.circuit_breaker_stats(node_name, node_stats))
             if self.include_buffer_pools:
                 collected_node_stats.update(self.jvm_buffer_pool_stats(node_name, node_stats))
             if self.include_mem_stats:
                 collected_node_stats.update(self.jvm_mem_stats(node_name, node_stats))
                 collected_node_stats.update(self.os_mem_stats(node_name, node_stats))
+            if self.include_cgroup_stats:
+                collected_node_stats.update(self.os_cgroup_stats(node_name, node_stats))
             if self.include_gc_stats:
                 collected_node_stats.update(self.jvm_gc_stats(node_name, node_stats))
             if self.include_network:
                 collected_node_stats.update(self.network_stats(node_name, node_stats))
             if self.include_process:
                 collected_node_stats.update(self.process_stats(node_name, node_stats))
             if self.include_indexing_pressure:
@@ -903,14 +910,22 @@
 
     def jvm_mem_stats(self, node_name, node_stats):
         return self.flatten_stats_fields(prefix="jvm_mem", stats=node_stats["jvm"]["mem"])
 
     def os_mem_stats(self, node_name, node_stats):
         return self.flatten_stats_fields(prefix="os_mem", stats=node_stats["os"]["mem"])
 
+    def os_cgroup_stats(self, node_name, node_stats):
+        cgroup_stats = {}
+        try:
+            cgroup_stats = self.flatten_stats_fields(prefix="os_cgroup", stats=node_stats["os"]["cgroup"])
+        except KeyError:
+            self.logger.debug("Node cgroup stats requested with none present.")
+        return cgroup_stats
+
     def jvm_gc_stats(self, node_name, node_stats):
         return self.flatten_stats_fields(prefix="jvm_gc", stats=node_stats["jvm"]["gc"])
 
     def network_stats(self, node_name, node_stats):
         return self.flatten_stats_fields(prefix="transport", stats=node_stats.get("transport"))
 
     def process_stats(self, node_name, node_stats):
@@ -1037,15 +1052,15 @@
     def _record(self, prefix=""):
         # ES returns all stats values in bytes or ms via "human: false"
 
         # pylint: disable=import-outside-toplevel
         import elasticsearch
 
         try:
-            stats = self.client.transform.get_transform_stats("_all")
+            stats = self.client.transform.get_transform_stats(transform_id="_all")
 
         except elasticsearch.TransportError:
             msg = f"A transport error occurred while collecting transform stats on cluster [{self.cluster_name}]"
             self.logger.exception(msg)
             raise exceptions.RallyError(msg)
 
         for transform in stats["transforms"]:
@@ -1539,15 +1554,14 @@
                 self.metrics_store.put_value_node_level(
                     node_name, "ingest_pipeline_processor_failed", ingest_pipeline_processor_failed, meta_data=metadata
                 )
 
             # We have a top level pipeline stats obj, which contains the total time spent preprocessing documents in
             # the ingest pipeline.
             elif processor_name == "total":
-
                 metadata = {"pipeline_name": pipeline_name, "cluster_name": cluster_name}
 
                 start_count = start_stats_processors[processor_name].get("count", 0)
                 end_count = processor_stats.get("count", 0)
                 ingest_pipeline_pipeline_count = end_count - start_count
 
                 self.metrics_store.put_value_node_level(
@@ -2280,15 +2294,15 @@
         # pylint: disable=import-outside-toplevel
         import elasticsearch
 
         found = False
         for index in self.indices.split(","):
             self.logger.debug("Gathering disk usage for [%s]", index)
             try:
-                response = self.client.perform_request(method="POST", path=f"/{index}/_disk_usage", params={"run_expensive_tasks": "true"})
+                response = self.client.indices.disk_usage(index=index, run_expensive_tasks=True)
             except elasticsearch.RequestError:
                 msg = f"A transport error occurred while collecting disk usage for {index}"
                 self.logger.exception(msg)
                 raise exceptions.RallyError(msg)
             except elasticsearch.NotFoundError:
                 msg = f"Requested disk usage for missing index {index}"
                 self.logger.warning(msg)
@@ -2303,16 +2317,18 @@
     def handle_telemetry_usage(self, response):
         if response["_shards"]["failed"] > 0:
             failures = str(response["_shards"]["failures"])
             msg = f"Shards failed when fetching disk usage: {failures}"
             self.logger.exception(msg)
             raise exceptions.RallyError(msg)
 
-        del response["_shards"]
         for index, idx_fields in response.items():
+            if index == "_shards":
+                continue
+
             for field, field_info in idx_fields["fields"].items():
                 meta = {"index": index, "field": field}
                 self.metrics_store.put_value_cluster_level("disk_usage_total", field_info["total_in_bytes"], meta_data=meta, unit="byte")
 
                 inverted_index = field_info.get("inverted_index", {"total_in_bytes": 0})["total_in_bytes"]
                 if inverted_index > 0:
                     self.metrics_store.put_value_cluster_level("disk_usage_inverted_index", inverted_index, meta_data=meta, unit="byte")
```

## esrally/client/asynchronous.py

```diff
@@ -14,38 +14,57 @@
 # KIND, either express or implied.  See the License for the
 # specific language governing permissions and limitations
 # under the License.
 
 import asyncio
 import json
 import logging
-from typing import List, Optional
+import warnings
+from typing import Any, Iterable, List, Mapping, Optional
 
 import aiohttp
-import elasticsearch
 from aiohttp import BaseConnector, RequestInfo
 from aiohttp.client_proto import ResponseHandler
 from aiohttp.helpers import BaseTimerContext
+from elastic_transport import (
+    AiohttpHttpNode,
+    ApiResponse,
+    AsyncTransport,
+    BinaryApiResponse,
+    HeadApiResponse,
+    ListApiResponse,
+    ObjectApiResponse,
+    TextApiResponse,
+)
+from elastic_transport.client_utils import DEFAULT
+from elasticsearch import AsyncElasticsearch
+from elasticsearch._async.client import IlmClient
+from elasticsearch.compat import warn_stacklevel
+from elasticsearch.exceptions import HTTP_EXCEPTIONS, ApiError, ElasticsearchWarning
 from multidict import CIMultiDict, CIMultiDictProxy
 from yarl import URL
 
+from esrally.client.common import _WARNING_RE, _mimetype_header_to_compat, _quote_query
 from esrally.client.context import RequestContextHolder
-from esrally.utils import io
+from esrally.utils import io, versions
 
 
 class StaticTransport:
     def __init__(self):
         self.closed = False
 
     def is_closing(self):
         return False
 
     def close(self):
         self.closed = True
 
+    def abort(self):
+        self.close()
+
 
 class StaticConnector(BaseConnector):
     async def _create_connection(self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout") -> ResponseHandler:
         handler = ResponseHandler(self._loop)
         handler.transport = StaticTransport()
         handler.protocol = ""
         return handler
@@ -102,29 +121,16 @@
         self._closed = False
         self._protocol = connection.protocol
         self._connection = connection
         self._headers = CIMultiDictProxy(CIMultiDict())
         self.status = 200
         return self
 
-    async def text(self, encoding=None, errors="strict"):
-        return self.static_body
-
-
-class RawClientResponse(aiohttp.ClientResponse):
-    """
-    Returns the body as bytes object (instead of a str) to avoid decoding overhead.
-    """
-
-    async def text(self, encoding=None, errors="strict"):
-        """Read response payload and decode."""
-        if self._body is None:
-            await self.read()
-
-        return self._body
+    async def read(self):
+        return self.static_body.encode("utf-8")
 
 
 class ResponseMatcher:
     def __init__(self, responses):
         self.logger = logging.getLogger(__name__)
         self.responses = []
 
@@ -135,22 +141,15 @@
             elif path.startswith("*"):
                 matcher = ResponseMatcher.endswith(path[1:])
             elif path.endswith("*"):
                 matcher = ResponseMatcher.startswith(path[:-1])
             else:
                 matcher = ResponseMatcher.equals(path)
 
-            body = response["body"]
-            body_encoding = response.get("body-encoding", "json")
-            if body_encoding == "raw":
-                body = json.dumps(body).encode("utf-8")
-            elif body_encoding == "json":
-                body = json.dumps(body)
-            else:
-                raise ValueError(f"Unknown body encoding [{body_encoding}] for path [{path}]")
+            body = json.dumps(response["body"])
 
             self.responses.append((path, matcher, body))
 
     @staticmethod
     def always():
         def f(p):
             return True
@@ -181,98 +180,222 @@
     def response(self, path):
         for path_pattern, matcher, body in self.responses:
             if matcher(path):
                 self.logger.debug("Path pattern [%s] matches path [%s].", path_pattern, path)
                 return body
 
 
-class AIOHttpConnection(elasticsearch.AIOHttpConnection):
-    def __init__(
-        self,
-        host="localhost",
-        port=None,
-        http_auth=None,
-        use_ssl=False,
-        ssl_assert_fingerprint=None,
-        headers=None,
-        ssl_context=None,
-        http_compress=None,
-        cloud_id=None,
-        api_key=None,
-        opaque_id=None,
-        loop=None,
-        trace_config=None,
-        **kwargs,
-    ):
-        super().__init__(
-            host=host,
-            port=port,
-            http_auth=http_auth,
-            use_ssl=use_ssl,
-            ssl_assert_fingerprint=ssl_assert_fingerprint,
-            # provided to the base class via `maxsize` to keep base class state consistent despite Rally
-            # calling the attribute differently.
-            maxsize=kwargs.get("max_connections", 0),
-            headers=headers,
-            ssl_context=ssl_context,
-            http_compress=http_compress,
-            cloud_id=cloud_id,
-            api_key=api_key,
-            opaque_id=opaque_id,
-            loop=loop,
-            **kwargs,
-        )
-
-        self._trace_configs = [trace_config] if trace_config else None
-        self._enable_cleanup_closed = kwargs.get("enable_cleanup_closed", True)
-
-        static_responses = kwargs.get("static_responses")
-        self.use_static_responses = static_responses is not None
+class RallyTCPConnector(aiohttp.TCPConnector):
+    def __init__(self, *args, **kwargs):
+        self.client_id = kwargs.pop("client_id", None)
+        self.logger = logging.getLogger(__name__)
+        super().__init__(*args, **kwargs)
 
-        if self.use_static_responses:
+    async def _resolve_host(self, *args, **kwargs):
+        hosts = await super()._resolve_host(*args, **kwargs)
+        self.logger.debug("client id [%s] resolved hosts [{%s}]", self.client_id, hosts)
+        # super()._resolve_host() does actually return all the IPs a given name resolves to, but the underlying
+        # super()._create_direct_connection() logic only ever selects the first succesful host from this list from which
+        # to establish a connection
+        #
+        # here we use the factory assigned client_id to deterministically return a IP from this list, which we then swap
+        # to the beginning of the list to evenly distribute connections across _all_ clients
+        # see https://github.com/elastic/rally/issues/1598
+        idx = self.client_id % len(hosts)
+        host = hosts[idx]
+        self.logger.debug("client id [%s] selected host [{%s}]", self.client_id, host)
+        # swap order of hosts
+        hosts[0], hosts[idx] = hosts[idx], hosts[0]
+        return hosts
+
+
+class RallyAiohttpHttpNode(AiohttpHttpNode):
+    def __init__(self, config):
+        super().__init__(config)
+        self._loop = None
+        self.client_id = None
+        self.trace_configs = None
+        self.enable_cleanup_closed = None
+        self._static_responses = None
+        self._request_class = aiohttp.ClientRequest
+        self._response_class = aiohttp.ClientResponse
+
+    @property
+    def static_responses(self):
+        return self._static_responses
+
+    @static_responses.setter
+    def static_responses(self, static_responses):
+        self._static_responses = static_responses
+        if self._static_responses:
             # read static responses once and reuse them
             if not StaticRequest.RESPONSES:
-                with open(io.normalize_path(static_responses)) as f:
+                with open(io.normalize_path(self._static_responses)) as f:
                     StaticRequest.RESPONSES = ResponseMatcher(json.load(f))
 
             self._request_class = StaticRequest
             self._response_class = StaticResponse
-        else:
-            self._request_class = aiohttp.ClientRequest
-            self._response_class = RawClientResponse
 
-    async def _create_aiohttp_session(self):
-        if self.loop is None:
-            self.loop = asyncio.get_running_loop()
+    def _create_aiohttp_session(self):
+        if self._loop is None:
+            self._loop = asyncio.get_running_loop()
 
-        if self.use_static_responses:
-            connector = StaticConnector(limit=self._limit, enable_cleanup_closed=self._enable_cleanup_closed)
+        if self._static_responses:
+            connector = StaticConnector(limit_per_host=self._connections_per_node, enable_cleanup_closed=self.enable_cleanup_closed)
         else:
-            connector = aiohttp.TCPConnector(
-                limit=self._limit, use_dns_cache=True, ssl=self._ssl_context, enable_cleanup_closed=self._enable_cleanup_closed
+            connector = RallyTCPConnector(
+                limit_per_host=self._connections_per_node,
+                use_dns_cache=True,
+                ssl=self._ssl_context,
+                enable_cleanup_closed=self.enable_cleanup_closed,
+                client_id=self.client_id,
             )
 
         self.session = aiohttp.ClientSession(
             headers=self.headers,
             auto_decompress=True,
-            loop=self.loop,
+            loop=self._loop,
             cookie_jar=aiohttp.DummyCookieJar(),
             request_class=self._request_class,
             response_class=self._response_class,
             connector=connector,
-            trace_configs=self._trace_configs,
+            trace_configs=self.trace_configs,
         )
 
 
-class VerifiedAsyncTransport(elasticsearch.AsyncTransport):
+class RallyAsyncTransport(AsyncTransport):
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, node_class=RallyAiohttpHttpNode, **kwargs)
+
+
+class RallyIlmClient(IlmClient):
+    async def put_lifecycle(self, *args, **kwargs):
+        """
+        The 'elasticsearch-py' 8.x method signature renames the 'policy' param to 'name', and the previously so-called
+        'body' param becomes 'policy'
+        """
+        if args:
+            kwargs["name"] = args[0]
+
+        if body := kwargs.pop("body", None):
+            kwargs["policy"] = body.get("policy", {})
+        # pylint: disable=missing-kwoa
+        return await IlmClient.put_lifecycle(self, **kwargs)
+
+
+class RallyAsyncElasticsearch(AsyncElasticsearch, RequestContextHolder):
     def __init__(self, *args, **kwargs):
+        distribution_version = kwargs.pop("distribution_version", None)
         super().__init__(*args, **kwargs)
         # skip verification at this point; we've already verified this earlier with the synchronous client.
         # The async client is used in the hot code path and we use customized overrides (such as that we don't
         # parse response bodies in some cases for performance reasons, e.g. when using the bulk API).
         self._verified_elasticsearch = True
+        if distribution_version:
+            self.distribution_version = versions.Version.from_string(distribution_version)
+        else:
+            self.distribution_version = None
+
+        # some ILM method signatures changed in 'elasticsearch-py' 8.x,
+        # so we override method(s) here to provide BWC for any custom
+        # runners that aren't using the new kwargs
+        self.ilm = RallyIlmClient(self)
+
+    async def perform_request(
+        self,
+        method: str,
+        path: str,
+        *,
+        params: Optional[Mapping[str, Any]] = None,
+        headers: Optional[Mapping[str, str]] = None,
+        body: Optional[Any] = None,
+    ) -> ApiResponse[Any]:
+        # We need to ensure that we provide content-type and accept headers
+        if body is not None:
+            if headers is None:
+                headers = {"content-type": "application/json", "accept": "application/json"}
+            else:
+                if headers.get("content-type") is None:
+                    headers["content-type"] = "application/json"
+                if headers.get("accept") is None:
+                    headers["accept"] = "application/json"
+
+        if headers:
+            request_headers = self._headers.copy()
+            request_headers.update(headers)
+        else:
+            request_headers = self._headers
+
+        # Converts all parts of a Accept/Content-Type headers
+        # from application/X -> application/vnd.elasticsearch+X
+        # see https://github.com/elastic/elasticsearch/issues/51816
+        if self.distribution_version is not None and self.distribution_version >= versions.Version.from_string("8.0.0"):
+            _mimetype_header_to_compat("Accept", request_headers)
+            _mimetype_header_to_compat("Content-Type", request_headers)
+
+        if params:
+            target = f"{path}?{_quote_query(params)}"
+        else:
+            target = path
 
+        meta, resp_body = await self.transport.perform_request(
+            method,
+            target,
+            headers=request_headers,
+            body=body,
+            request_timeout=self._request_timeout,
+            max_retries=self._max_retries,
+            retry_on_status=self._retry_on_status,
+            retry_on_timeout=self._retry_on_timeout,
+            client_meta=self._client_meta,
+        )
+
+        # HEAD with a 404 is returned as a normal response
+        # since this is used as an 'exists' functionality.
+        if not (method == "HEAD" and meta.status == 404) and (
+            not 200 <= meta.status < 299
+            and (self._ignore_status is DEFAULT or self._ignore_status is None or meta.status not in self._ignore_status)
+        ):
+            message = str(resp_body)
+
+            # If the response is an error response try parsing
+            # the raw Elasticsearch error before raising.
+            if isinstance(resp_body, dict):
+                try:
+                    error = resp_body.get("error", message)
+                    if isinstance(error, dict) and "type" in error:
+                        error = error["type"]
+                    message = error
+                except (ValueError, KeyError, TypeError):
+                    pass
+
+            raise HTTP_EXCEPTIONS.get(meta.status, ApiError)(message=message, meta=meta, body=resp_body)
+
+        # 'Warning' headers should be reraised as 'ElasticsearchWarning'
+        if "warning" in meta.headers:
+            warning_header = (meta.headers.get("warning") or "").strip()
+            warning_messages: Iterable[str] = _WARNING_RE.findall(warning_header) or (warning_header,)
+            stacklevel = warn_stacklevel()
+            for warning_message in warning_messages:
+                warnings.warn(
+                    warning_message,
+                    category=ElasticsearchWarning,
+                    stacklevel=stacklevel,
+                )
+
+        if method == "HEAD":
+            response = HeadApiResponse(meta=meta)
+        elif isinstance(resp_body, dict):
+            response = ObjectApiResponse(body=resp_body, meta=meta)  # type: ignore[assignment]
+        elif isinstance(resp_body, list):
+            response = ListApiResponse(body=resp_body, meta=meta)  # type: ignore[assignment]
+        elif isinstance(resp_body, str):
+            response = TextApiResponse(  # type: ignore[assignment]
+                body=resp_body,
+                meta=meta,
+            )
+        elif isinstance(resp_body, bytes):
+            response = BinaryApiResponse(body=resp_body, meta=meta)  # type: ignore[assignment]
+        else:
+            response = ApiResponse(body=resp_body, meta=meta)  # type: ignore[assignment]
 
-class RallyAsyncElasticsearch(elasticsearch.AsyncElasticsearch, RequestContextHolder):
-    def perform_request(self, *args, **kwargs):
-        kwargs["url"] = kwargs.pop("path")
-        return self.transport.perform_request(*args, **kwargs)
+        return response
```

## esrally/client/context.py

```diff
@@ -27,35 +27,33 @@
     """
 
     def __init__(self, request_context_holder):
         self.ctx_holder = request_context_holder
         self.ctx = None
         self.token = None
 
-    async def __aenter__(self):
+    def __enter__(self):
         self.ctx, self.token = self.ctx_holder.init_request_context()
         return self
 
     @property
     def request_start(self):
-        return self.ctx["request_start"]
+        return self.ctx.get("request_start")
 
     @property
     def request_end(self):
-        return self.ctx["request_end"]
+        return self.ctx.get("request_end")
 
-    async def __aexit__(self, exc_type, exc_val, exc_tb):
-        # propagate earliest request start and most recent request end to parent
-        request_start = self.request_start
-        request_end = self.request_end
+    def __exit__(self, exc_type, exc_val, exc_tb):
         self.ctx_holder.restore_context(self.token)
         # don't attempt to restore these values on the top-level context as they don't exist
         if self.token.old_value != contextvars.Token.MISSING:
-            self.ctx_holder.update_request_start(request_start)
-            self.ctx_holder.update_request_end(request_end)
+            # propagate earliest request start and most recent request end to parent
+            self.ctx_holder.update_request_start(self.request_start)
+            self.ctx_holder.update_request_end(self.request_end)
         self.token = None
         return False
 
 
 class RequestContextHolder:
     """
     Holds request context variables. This class is only meant to be used together with RequestContextManager.
```

## esrally/client/factory.py

```diff
@@ -15,30 +15,38 @@
 # specific language governing permissions and limitations
 # under the License.
 
 import logging
 import time
 
 import certifi
-import urllib3
 from urllib3.connection import is_ipaddress
 
 from esrally import doc_link, exceptions
 from esrally.utils import console, convert, versions
 
 
 class EsClientFactory:
     """
-    Abstracts how the Elasticsearch client is created. Intended for testing.
+    Abstracts how the Elasticsearch client is created and customizes the client for backwards
+    compatibility guarantees that are broader than the library's defaults.
     """
 
-    def __init__(self, hosts, client_options):
-        self.hosts = hosts
+    def __init__(self, hosts, client_options, distribution_version=None):
+        def host_string(host):
+            # protocol can be set at either host or client opts level
+            protocol = "https" if client_options.get("use_ssl") or host.get("use_ssl") else "http"
+            return f"{protocol}://{host['host']}:{host['port']}"
+
+        self.hosts = [host_string(h) for h in hosts]
         self.client_options = dict(client_options)
         self.ssl_context = None
+        # This attribute is necessary for the backwards-compatibility logic contained in
+        # RallySyncElasticsearch.perform_request() and RallyAsyncElasticsearch.perform_request().
+        self.distribution_version = distribution_version
         self.logger = logging.getLogger(__name__)
 
         masked_client_options = dict(client_options)
         if "basic_auth_password" in masked_client_options:
             masked_client_options["basic_auth_password"] = "*****"
         if "http_auth" in masked_client_options:
             masked_client_options["http_auth"] = (masked_client_options["http_auth"][0], "*****")
@@ -46,34 +54,32 @@
 
         # we're using an SSL context now and it is not allowed to have use_ssl present in client options anymore
         if self.client_options.pop("use_ssl", False):
             # pylint: disable=import-outside-toplevel
             import ssl
 
             self.logger.debug("SSL support: on")
-            self.client_options["scheme"] = "https"
 
             self.ssl_context = ssl.create_default_context(
                 ssl.Purpose.SERVER_AUTH, cafile=self.client_options.pop("ca_certs", certifi.where())
             )
 
-            if not self.client_options.pop("verify_certs", True):
+            # We call get() here instead of pop() in order to pass verify_certs through as a kwarg
+            # to the elasticsearch.Elasticsearch constructor. Setting the ssl_context's verify_mode to
+            # ssl.CERT_NONE is insufficient with version 8.0+ of elasticsearch-py.
+            if not self.client_options.get("verify_certs", True):
                 self.logger.debug("SSL certificate verification: off")
                 # order matters to avoid ValueError: check_hostname needs a SSL context with either CERT_OPTIONAL or CERT_REQUIRED
                 self.ssl_context.check_hostname = False
                 self.ssl_context.verify_mode = ssl.CERT_NONE
+                self.client_options["ssl_show_warn"] = False
 
                 self.logger.warning(
-                    "User has enabled SSL but disabled certificate verification. This is dangerous but may be ok for a "
-                    "benchmark. Disabling urllib warnings now to avoid a logging storm. "
-                    "See https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings for details."
+                    "User has enabled SSL but disabled certificate verification. This is dangerous but may be ok for a benchmark."
                 )
-                # disable:  "InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly \
-                # advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings"
-                urllib3.disable_warnings()
             else:
                 # check_hostname should not be set when host is an IP address
                 self.ssl_context.check_hostname = self._only_hostnames(hosts)
                 self.ssl_context.verify_mode = ssl.CERT_REQUIRED
                 self.logger.debug("SSL certificate verification: on")
 
             # When using SSL_context, all SSL related kwargs in client options get ignored
@@ -101,17 +107,17 @@
                     )
                 )
             elif client_cert and client_key:
                 self.logger.debug("SSL client authentication: on")
                 self.ssl_context.load_cert_chain(certfile=client_cert, keyfile=client_key)
         else:
             self.logger.debug("SSL support: off")
-            self.client_options["scheme"] = "http"
 
         if self._is_set(self.client_options, "create_api_key_per_client"):
+            self.client_options.pop("create_api_key_per_client")
             basic_auth_user = self.client_options.get("basic_auth_user", False)
             basic_auth_password = self.client_options.get("basic_auth_password", False)
             provided_auth = {"basic_auth_user": basic_auth_user, "basic_auth_password": basic_auth_password}
             missing_auth = [k for k, v in provided_auth.items() if not v]
             if missing_auth:
                 console.println(
                     "Basic auth credentials are required in order to create API keys.\n"
@@ -123,30 +129,34 @@
                     "to 'create_api_key_per_client' in order to create client API keys."
                 )
             self.logger.debug("Automatic creation of client API keys: on")
         else:
             self.logger.debug("Automatic creation of client API keys: off")
 
         if self._is_set(self.client_options, "basic_auth_user") and self._is_set(self.client_options, "basic_auth_password"):
+            self.client_options["basic_auth"] = (self.client_options.pop("basic_auth_user"), self.client_options.pop("basic_auth_password"))
             self.logger.debug("HTTP basic authentication: on")
-            self.client_options["http_auth"] = (self.client_options.pop("basic_auth_user"), self.client_options.pop("basic_auth_password"))
         else:
             self.logger.debug("HTTP basic authentication: off")
 
         if self._is_set(self.client_options, "compressed"):
             console.warn("You set the deprecated client option 'compressed. Please use 'http_compress' instead.", logger=self.logger)
             self.client_options["http_compress"] = self.client_options.pop("compressed")
 
         if self._is_set(self.client_options, "http_compress"):
             self.logger.debug("HTTP compression: on")
         else:
             self.logger.debug("HTTP compression: off")
 
-        if self._is_set(self.client_options, "enable_cleanup_closed"):
-            self.client_options["enable_cleanup_closed"] = convert.to_bool(self.client_options.pop("enable_cleanup_closed"))
+        self.enable_cleanup_closed = convert.to_bool(self.client_options.pop("enable_cleanup_closed", True))
+        self.max_connections = max(256, self.client_options.pop("max_connections", 0))
+        self.static_responses = self.client_options.pop("static_responses", None)
+
+        if self._is_set(self.client_options, "timeout"):
+            self.client_options["request_timeout"] = self.client_options.pop("timeout")
 
     @staticmethod
     def _only_hostnames(hosts):
         has_ip = False
         has_hostname = False
         for host in hosts:
             is_ip = is_ipaddress(host["host"])
@@ -166,36 +176,37 @@
         except KeyError:
             return False
 
     def create(self):
         # pylint: disable=import-outside-toplevel
         from esrally.client.synchronous import RallySyncElasticsearch
 
-        return RallySyncElasticsearch(hosts=self.hosts, ssl_context=self.ssl_context, **self.client_options)
+        return RallySyncElasticsearch(
+            distribution_version=self.distribution_version, hosts=self.hosts, ssl_context=self.ssl_context, **self.client_options
+        )
 
-    def create_async(self, api_key=None):
+    def create_async(self, api_key=None, client_id=None):
         # pylint: disable=import-outside-toplevel
         import io
 
         import aiohttp
         from elasticsearch.serializer import JSONSerializer
 
         from esrally.client.asynchronous import (
-            AIOHttpConnection,
             RallyAsyncElasticsearch,
-            VerifiedAsyncTransport,
+            RallyAsyncTransport,
         )
 
         class LazyJSONSerializer(JSONSerializer):
-            def loads(self, s):
+            def loads(self, data):
                 meta = RallyAsyncElasticsearch.request_context.get()
                 if "raw_response" in meta:
-                    return io.BytesIO(s)
+                    return io.BytesIO(data)
                 else:
-                    return super().loads(s)
+                    return super().loads(data)
 
         async def on_request_start(session, trace_config_ctx, params):
             RallyAsyncElasticsearch.on_request_start()
 
         async def on_request_end(session, trace_config_ctx, params):
             RallyAsyncElasticsearch.on_request_end()
 
@@ -203,65 +214,109 @@
         trace_config.on_request_start.append(on_request_start)
         trace_config.on_request_end.append(on_request_end)
         # ensure that we also stop the timer when a request "ends" with an exception (e.g. a timeout)
         trace_config.on_request_exception.append(on_request_end)
 
         # override the builtin JSON serializer
         self.client_options["serializer"] = LazyJSONSerializer()
-        self.client_options["trace_config"] = trace_config
 
         if api_key is not None:
-            self.client_options.pop("http_auth")
+            self.client_options.pop("http_auth", None)
+            self.client_options.pop("basic_auth", None)
             self.client_options["api_key"] = api_key
 
-        return RallyAsyncElasticsearch(
+        async_client = RallyAsyncElasticsearch(
+            distribution_version=self.distribution_version,
             hosts=self.hosts,
-            transport_class=VerifiedAsyncTransport,
-            connection_class=AIOHttpConnection,
+            transport_class=RallyAsyncTransport,
             ssl_context=self.ssl_context,
+            maxsize=self.max_connections,
             **self.client_options,
         )
 
+        # the AsyncElasticsearch constructor automatically creates the corresponding NodeConfig objects, so we set
+        # their instance attributes after they've been instantiated
+        for node_connection in async_client.transport.node_pool.all():
+            node_connection.trace_configs = [trace_config]
+            node_connection.enable_cleanup_closed = self.enable_cleanup_closed
+            node_connection.static_responses = self.static_responses
+            node_connection.client_id = client_id
+
+        return async_client
+
 
 def wait_for_rest_layer(es, max_attempts=40):
     """
     Waits for ``max_attempts`` until Elasticsearch's REST API is available.
 
     :param es: Elasticsearch client to use for connecting.
     :param max_attempts: The maximum number of attempts to check whether the REST API is available.
     :return: True iff Elasticsearch's REST API is available.
     """
     # assume that at least the hosts that we expect to contact should be available. Note that this is not 100%
     # bullet-proof as a cluster could have e.g. dedicated masters which are not contained in our list of target hosts
     # but this is still better than just checking for any random node's REST API being reachable.
-    expected_node_count = len(es.transport.hosts)
+    expected_node_count = len(es.transport.node_pool)
     logger = logging.getLogger(__name__)
-    for attempt in range(max_attempts):
+    attempt = 0
+    while attempt <= max_attempts:
+        attempt += 1
         # pylint: disable=import-outside-toplevel
-        import elasticsearch
+        from elastic_transport import (
+            ApiError,
+            ConnectionError,
+            SerializationError,
+            TlsError,
+            TransportError,
+        )
 
         try:
             # see also WaitForHttpResource in Elasticsearch tests. Contrary to the ES tests we consider the API also
             # available when the cluster status is RED (as long as all required nodes are present)
             es.cluster.health(wait_for_nodes=f">={expected_node_count}")
             logger.debug("REST API is available for >= [%s] nodes after [%s] attempts.", expected_node_count, attempt)
             return True
-        except elasticsearch.ConnectionError as e:
-            if "SSL: UNKNOWN_PROTOCOL" in str(e):
-                raise exceptions.SystemSetupError("Could not connect to cluster via https. Is this an https endpoint?", e)
-            logger.debug("Got connection error on attempt [%s]. Sleeping...", attempt)
-            time.sleep(3)
-        except elasticsearch.TransportError as e:
+        except SerializationError as e:
+            if "Client sent an HTTP request to an HTTPS server" in str(e):
+                raise exceptions.SystemSetupError(
+                    "Rally sent an HTTP request to an HTTPS server. Are you sure this is an HTTP endpoint?", e
+                )
+
+            if attempt <= max_attempts:
+                logger.debug("Got serialization error [%s] on attempt [%s]. Sleeping...", e, attempt)
+                time.sleep(3)
+            else:
+                raise
+        except TlsError as e:
+            raise exceptions.SystemSetupError("Could not connect to cluster via HTTPS. Are you sure this is an HTTPS endpoint?", e)
+        except ConnectionError as e:
+            if "ProtocolError" in str(e):
+                raise exceptions.SystemSetupError(
+                    "Received a protocol error. Are you sure you're using the correct scheme (HTTP or HTTPS)?", e
+                )
+
+            if attempt <= max_attempts:
+                logger.debug("Got connection error on attempt [%s]. Sleeping...", attempt)
+                time.sleep(3)
+            else:
+                raise
+        except TransportError as e:
+            if attempt <= max_attempts:
+                logger.debug("Got transport error on attempt [%s]. Sleeping...", attempt)
+                time.sleep(3)
+            else:
+                raise
+        except ApiError as e:
             # cluster block, x-pack not initialized yet, our wait condition is not reached
-            if e.status_code in (503, 401, 408):
-                logger.debug("Got status code [%s] on attempt [%s]. Sleeping...", e.status_code, attempt)
+            if e.status_code in (503, 401, 408) and attempt <= max_attempts:
+                logger.debug("Got status code [%s] on attempt [%s]. Sleeping...", e.message, attempt)
                 time.sleep(3)
             else:
-                logger.warning("Got unexpected status code [%s] on attempt [%s].", e.status_code, attempt)
-                raise e
+                logger.warning("Got unexpected status code [%s] on attempt [%s].", e.message, attempt)
+                raise
     return False
 
 
 def create_api_key(es, client_id, max_attempts=5):
     """
     Creates an API key for the provided ``client_id``.
 
@@ -274,17 +329,20 @@
 
     for attempt in range(1, max_attempts + 1):
         # pylint: disable=import-outside-toplevel
         import elasticsearch
 
         try:
             logger.debug("Creating ES API key for client ID [%s]", client_id)
-            return es.security.create_api_key({"name": f"rally-client-{client_id}"})
+            return es.security.create_api_key(name=f"rally-client-{client_id}")
         except elasticsearch.TransportError as e:
-            if e.status_code == 405:
+            logger.debug("Got transport error [%s] on attempt [%s]. Sleeping...", str(e), attempt)
+            time.sleep(1)
+        except elasticsearch.ApiError as e:
+            if e.meta.status == 405:
                 # We don't retry on 405 since it indicates a misconfigured benchmark candidate and isn't recoverable
                 raise exceptions.SystemSetupError(
                     "Got status code 405 when attempting to create API keys. Is Elasticsearch Security enabled?", e
                 )
             logger.debug("Got status code [%s] on attempt [%s] of [%s]. Sleeping...", e.status_code, attempt, max_attempts)
             time.sleep(1)
 
@@ -316,15 +374,15 @@
 
     for attempt in range(1, max_attempts + 1):
         # pylint: disable=import-outside-toplevel
         import elasticsearch
 
         try:
             if current_version >= minimum_version:
-                resp = es.security.invalidate_api_key({"ids": remaining})
+                resp = es.security.invalidate_api_key(ids=remaining)
                 deleted += resp["invalidated_api_keys"]
                 remaining = [i for i in ids if i not in deleted]
                 # Like bulk indexing requests, we can get an HTTP 200, but the
                 # response body could still contain an array of individual errors.
                 # So, we have to handle the case were some keys weren't deleted, but
                 # the request overall succeeded (i.e. we didn't encounter an exception)
                 if attempt < max_attempts:
@@ -342,24 +400,24 @@
                             resp["error_details"],
                         )
                         raise_exception(remaining)
             else:
                 remaining = [i for i in ids if i not in deleted]
                 if attempt < max_attempts:
                     for i in remaining:
-                        es.security.invalidate_api_key({"id": i})
+                        es.security.invalidate_api_key(id=i)
                         deleted.append(i)
                 else:
                     if remaining:
                         raise_exception(remaining)
             return True
 
-        except elasticsearch.TransportError as e:
+        except elasticsearch.ApiError as e:
             if attempt < max_attempts:
-                logger.debug("Got status code [%s] on attempt [%s] of [%s]. Sleeping...", e.status_code, attempt, max_attempts)
+                logger.debug("Got status code [%s] on attempt [%s] of [%s]. Sleeping...", e.meta.status, attempt, max_attempts)
                 time.sleep(1)
             else:
                 raise_exception(remaining, cause=e)
         except Exception as e:
             if attempt < max_attempts:
                 logger.debug("Got error on attempt [%s] of [%s]. Sleeping...", attempt, max_attempts)
                 time.sleep(1)
```

## esrally/client/synchronous.py

```diff
@@ -11,14 +11,235 @@
 # Unless required by applicable law or agreed to in writing,
 # software distributed under the License is distributed on an
 # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 # KIND, either express or implied.  See the License for the
 # specific language governing permissions and limitations
 # under the License.
 
-import elasticsearch
+import re
+import warnings
+from typing import Any, Iterable, Mapping, Optional
 
+from elastic_transport import (
+    ApiResponse,
+    BinaryApiResponse,
+    HeadApiResponse,
+    ListApiResponse,
+    ObjectApiResponse,
+    TextApiResponse,
+)
+from elastic_transport.client_utils import DEFAULT
+from elasticsearch import Elasticsearch
+from elasticsearch.compat import warn_stacklevel
+from elasticsearch.exceptions import (
+    HTTP_EXCEPTIONS,
+    ApiError,
+    ElasticsearchWarning,
+    UnsupportedProductError,
+)
 
-class RallySyncElasticsearch(elasticsearch.Elasticsearch):
-    def perform_request(self, *args, **kwargs):
-        kwargs["url"] = kwargs.pop("path")
-        return self.transport.perform_request(*args, **kwargs)
+from esrally.client.common import _WARNING_RE, _mimetype_header_to_compat, _quote_query
+from esrally.utils import versions
+
+
+# This reproduces the product verification behavior of v7.14.0 of the client:
+# https://github.com/elastic/elasticsearch-py/blob/v7.14.0/elasticsearch/transport.py#L606
+#
+# As of v8.0.0, the client determines whether the server is Elasticsearch by checking
+# whether HTTP responses contain the `X-elastic-product` header. If they do not, it raises
+# an `UnsupportedProductError`. This header was only introduced in Elasticsearch 7.14.0,
+# however, so the client will consider any version of ES prior to 7.14.0 unsupported due to
+# responses not including it.
+#
+# Because Rally needs to support versions of ES >= 6.8.0, we resurrect the previous
+# logic for determining the authenticity of the server, which does not rely exclusively
+# on this header.
+class _ProductChecker:
+    """Class which verifies we're connected to a supported product"""
+
+    # States that can be returned from 'check_product'
+    SUCCESS = True
+    UNSUPPORTED_PRODUCT = 2
+    UNSUPPORTED_DISTRIBUTION = 3
+
+    @classmethod
+    def raise_error(cls, state, meta, body):
+        # These states mean the product_check() didn't fail so do nothing.
+        if state in (None, True):
+            return
+
+        if state == cls.UNSUPPORTED_DISTRIBUTION:
+            message = "The client noticed that the server is not a supported distribution of Elasticsearch"
+        else:  # UNSUPPORTED_PRODUCT
+            message = "The client noticed that the server is not Elasticsearch and we do not support this unknown product"
+        raise UnsupportedProductError(message, meta=meta, body=body)
+
+    @classmethod
+    def check_product(cls, headers, response):
+        # type: (dict[str, str], dict[str, str]) -> int
+        """Verifies that the server we're talking to is Elasticsearch.
+        Does this by checking HTTP headers and the deserialized
+        response to the 'info' API. Returns one of the states above.
+        """
+        try:
+            version = response.get("version", {})
+            version_number = tuple(
+                int(x) if x is not None else 999 for x in re.search(r"^([0-9]+)\.([0-9]+)(?:\.([0-9]+))?", version["number"]).groups()
+            )
+        except (KeyError, TypeError, ValueError, AttributeError):
+            # No valid 'version.number' field, effectively 0.0.0
+            version = {}
+            version_number = (0, 0, 0)
+
+        # Check all of the fields and headers for missing/valid values.
+        try:
+            bad_tagline = response.get("tagline", None) != "You Know, for Search"
+            bad_build_flavor = version.get("build_flavor", None) != "default"
+            bad_product_header = headers.get("x-elastic-product", None) != "Elasticsearch"
+        except (AttributeError, TypeError):
+            bad_tagline = True
+            bad_build_flavor = True
+            bad_product_header = True
+
+        # 7.0-7.13 and there's a bad 'tagline' or unsupported 'build_flavor'
+        if (7, 0, 0) <= version_number < (7, 14, 0):
+            if bad_tagline:
+                return cls.UNSUPPORTED_PRODUCT
+            elif bad_build_flavor:
+                return cls.UNSUPPORTED_DISTRIBUTION
+
+        elif (
+            # No version or version less than 6.x
+            version_number < (6, 0, 0)
+            # 6.x and there's a bad 'tagline'
+            or ((6, 0, 0) <= version_number < (7, 0, 0) and bad_tagline)
+            # 7.14+ and there's a bad 'X-Elastic-Product' HTTP header
+            or ((7, 14, 0) <= version_number and bad_product_header)
+        ):
+            return cls.UNSUPPORTED_PRODUCT
+
+        return True
+
+
+class RallySyncElasticsearch(Elasticsearch):
+    def __init__(self, *args, **kwargs):
+        distribution_version = kwargs.pop("distribution_version", None)
+        super().__init__(*args, **kwargs)
+        self._verified_elasticsearch = None
+
+        if distribution_version:
+            self.distribution_version = versions.Version.from_string(distribution_version)
+        else:
+            self.distribution_version = None
+
+    def perform_request(
+        self,
+        method: str,
+        path: str,
+        *,
+        params: Optional[Mapping[str, Any]] = None,
+        headers: Optional[Mapping[str, str]] = None,
+        body: Optional[Any] = None,
+    ) -> ApiResponse[Any]:
+        # We need to ensure that we provide content-type and accept headers
+        if body is not None:
+            if headers is None:
+                headers = {"content-type": "application/json", "accept": "application/json"}
+            else:
+                if headers.get("content-type") is None:
+                    headers["content-type"] = "application/json"
+                if headers.get("accept") is None:
+                    headers["accept"] = "application/json"
+
+        if headers:
+            request_headers = self._headers.copy()
+            request_headers.update(headers)
+        else:
+            request_headers = self._headers
+
+        if self._verified_elasticsearch is None:
+            info = self.transport.perform_request(method="GET", target="/", headers=request_headers)
+            info_meta = info.meta
+            info_body = info.body
+
+            if not 200 <= info_meta.status < 299:
+                raise HTTP_EXCEPTIONS.get(info_meta.status, ApiError)(message=str(info_body), meta=info_meta, body=info_body)
+
+            self._verified_elasticsearch = _ProductChecker.check_product(info_meta.headers, info_body)
+
+            if self._verified_elasticsearch is not True:
+                _ProductChecker.raise_error(self._verified_elasticsearch, info_meta, info_body)
+
+        # Converts all parts of a Accept/Content-Type headers
+        # from application/X -> application/vnd.elasticsearch+X
+        # see https://github.com/elastic/elasticsearch/issues/51816
+        if self.distribution_version is not None and self.distribution_version >= versions.Version.from_string("8.0.0"):
+            _mimetype_header_to_compat("Accept", request_headers)
+            _mimetype_header_to_compat("Content-Type", request_headers)
+
+        if params:
+            target = f"{path}?{_quote_query(params)}"
+        else:
+            target = path
+
+        meta, resp_body = self.transport.perform_request(
+            method,
+            target,
+            headers=request_headers,
+            body=body,
+            request_timeout=self._request_timeout,
+            max_retries=self._max_retries,
+            retry_on_status=self._retry_on_status,
+            retry_on_timeout=self._retry_on_timeout,
+            client_meta=self._client_meta,
+        )
+
+        # HEAD with a 404 is returned as a normal response
+        # since this is used as an 'exists' functionality.
+        if not (method == "HEAD" and meta.status == 404) and (
+            not 200 <= meta.status < 299
+            and (self._ignore_status is DEFAULT or self._ignore_status is None or meta.status not in self._ignore_status)
+        ):
+            message = str(resp_body)
+
+            # If the response is an error response try parsing
+            # the raw Elasticsearch error before raising.
+            if isinstance(resp_body, dict):
+                try:
+                    error = resp_body.get("error", message)
+                    if isinstance(error, dict) and "type" in error:
+                        error = error["type"]
+                    message = error
+                except (ValueError, KeyError, TypeError):
+                    pass
+
+            raise HTTP_EXCEPTIONS.get(meta.status, ApiError)(message=message, meta=meta, body=resp_body)
+
+        # 'Warning' headers should be reraised as 'ElasticsearchWarning'
+        if "warning" in meta.headers:
+            warning_header = (meta.headers.get("warning") or "").strip()
+            warning_messages: Iterable[str] = _WARNING_RE.findall(warning_header) or (warning_header,)
+            stacklevel = warn_stacklevel()
+            for warning_message in warning_messages:
+                warnings.warn(
+                    warning_message,
+                    category=ElasticsearchWarning,
+                    stacklevel=stacklevel,
+                )
+
+        if method == "HEAD":
+            response = HeadApiResponse(meta=meta)
+        elif isinstance(resp_body, dict):
+            response = ObjectApiResponse(body=resp_body, meta=meta)  # type: ignore[assignment]
+        elif isinstance(resp_body, list):
+            response = ListApiResponse(body=resp_body, meta=meta)  # type: ignore[assignment]
+        elif isinstance(resp_body, str):
+            response = TextApiResponse(  # type: ignore[assignment]
+                body=resp_body,
+                meta=meta,
+            )
+        elif isinstance(resp_body, bytes):
+            response = BinaryApiResponse(body=resp_body, meta=meta)  # type: ignore[assignment]
+        else:
+            response = ApiResponse(body=resp_body, meta=meta)  # type: ignore[assignment]
+
+        return response
```

## esrally/driver/driver.py

```diff
@@ -24,14 +24,15 @@
 import math
 import multiprocessing
 import queue
 import threading
 import time
 from dataclasses import dataclass
 from enum import Enum
+from io import BytesIO
 from typing import Callable
 
 import thespian.actors
 
 from esrally import (
     PROGRAM_NAME,
     actor,
@@ -593,21 +594,24 @@
         self.tasks_per_join_point = None
         self.complete_current_task_sent = False
 
         self.telemetry = None
 
     def create_es_clients(self):
         all_hosts = self.config.opts("client", "hosts").all_hosts
+        distribution_version = self.config.opts("mechanic", "distribution.version", mandatory=False)
         es = {}
         for cluster_name, cluster_hosts in all_hosts.items():
             all_client_options = self.config.opts("client", "options").all_client_options
             cluster_client_options = dict(all_client_options[cluster_name])
             # Use retries to avoid aborts on long living connections for telemetry devices
-            cluster_client_options["retry-on-timeout"] = True
-            es[cluster_name] = self.es_client_factory(cluster_hosts, cluster_client_options).create()
+            cluster_client_options["retry_on_timeout"] = True
+            es[cluster_name] = self.es_client_factory(
+                cluster_hosts, cluster_client_options, distribution_version=distribution_version
+            ).create()
         return es
 
     def prepare_telemetry(self, es, enable, index_names, data_stream_names):
         enabled_devices = self.config.opts("telemetry", "devices")
         telemetry_params = self.config.opts("telemetry", "params")
         log_root = paths.race_root(self.config)
 
@@ -1705,39 +1709,38 @@
         self.parent_worker_id = worker_id
         self.profiling_enabled = self.cfg.opts("driver", "profiling")
         self.assertions_enabled = self.cfg.opts("driver", "assertions")
         self.debug_event_loop = self.cfg.opts("system", "async.debug", mandatory=False, default_value=False)
         self.logger = logging.getLogger(__name__)
 
     def __call__(self, *args, **kwargs):
-        # only possible in Python 3.7+ (has introduced get_running_loop)
-        # try:
-        #     loop = asyncio.get_running_loop()
-        # except RuntimeError:
-        #     loop = asyncio.new_event_loop()
-        #     asyncio.set_event_loop(loop)
-        loop = asyncio.new_event_loop()
-        loop.set_debug(self.debug_event_loop)
-        loop.set_exception_handler(self._logging_exception_handler)
-        asyncio.set_event_loop(loop)
+        try:
+            loop = asyncio.get_running_loop()
+        except RuntimeError:
+            loop = asyncio.new_event_loop()
+            loop.set_debug(self.debug_event_loop)
+            loop.set_exception_handler(self._logging_exception_handler)
+            asyncio.set_event_loop(loop)
         try:
             loop.run_until_complete(self.run())
         finally:
             loop.close()
 
     def _logging_exception_handler(self, loop, context):
         self.logger.error("Uncaught exception in event loop: %s", context)
 
     async def run(self):
-        def es_clients(client_id, all_hosts, all_client_options):
+        def es_clients(client_id, all_hosts, all_client_options, distribution_version):
             es = {}
             context = self.client_contexts.get(client_id)
             api_key = context.api_key
             for cluster_name, cluster_hosts in all_hosts.items():
-                es[cluster_name] = client.EsClientFactory(cluster_hosts, all_client_options[cluster_name]).create_async(api_key=api_key)
+                es[cluster_name] = client.EsClientFactory(
+                    cluster_hosts, all_client_options[cluster_name], distribution_version=distribution_version
+                ).create_async(api_key=api_key, client_id=client_id)
             return es
 
         if self.assertions_enabled:
             self.logger.info("Task assertions enabled")
         runner.enable_assertions(self.assertions_enabled)
 
         clients = []
@@ -1746,15 +1749,20 @@
         params_per_task = {}
         for client_id, task_allocation in self.task_allocations:
             task = task_allocation.task
             if task not in params_per_task:
                 param_source = track.operation_parameters(self.track, task)
                 params_per_task[task] = param_source
             schedule = schedule_for(task_allocation, params_per_task[task])
-            es = es_clients(client_id, self.cfg.opts("client", "hosts").all_hosts, self.cfg.opts("client", "options"))
+            es = es_clients(
+                client_id,
+                self.cfg.opts("client", "hosts").all_hosts,
+                self.cfg.opts("client", "options"),
+                self.cfg.opts("mechanic", "distribution.version", mandatory=False),
+            )
             clients.append(es)
             async_executor = AsyncExecutor(
                 client_id, task, schedule, es, self.sampler, self.cancel, self.complete, task.error_behavior(self.abort_on_error)
             )
             final_executor = AsyncProfiler(async_executor) if self.profiling_enabled else async_executor
             aws.append(final_executor())
         task_names = [t.task.task.name for t in self.task_allocations]
@@ -1859,15 +1867,15 @@
                     rest = absolute_expected_schedule_time - time.perf_counter()
                     if rest > 0:
                         await asyncio.sleep(rest)
 
                 absolute_processing_start = time.time()
                 processing_start = time.perf_counter()
                 self.schedule_handle.before_request(processing_start)
-                async with self.es["default"].new_request_context() as request_context:
+                with self.es["default"].new_request_context() as request_context:
                     total_ops, total_ops_unit, request_meta_data = await execute_single(runner, self.es, params, self.on_error)
                     request_start = request_context.request_start
                     request_end = request_context.request_end
 
                 processing_end = time.perf_counter()
                 service_time = request_end - request_start
                 processing_time = processing_end - processing_start
@@ -1977,28 +1985,55 @@
         # pylint: disable=unidiomatic-typecheck
         if type(e) is elasticsearch.ConnectionError:
             fatal_error = True
 
         total_ops = 0
         total_ops_unit = "ops"
         request_meta_data = {"success": False, "error-type": "transport"}
-        # The ES client will sometimes return string like "N/A" or "TIMEOUT" for connection errors.
-        if isinstance(e.status_code, int):
-            request_meta_data["http-status"] = e.status_code
+        # For the 'errors' attribute, errors are ordered from
+        # most recently raised (index=0) to least recently raised (index=N)
+        #
+        # If an HTTP status code is available with the error it
+        # will be stored under 'status'. If HTTP headers are available
+        # they are stored under 'headers'.
+        if e.errors:
+            if hasattr(e.errors[0], "status"):
+                request_meta_data["http-status"] = e.errors[0].status
         # connection timeout errors don't provide a helpful description
         if isinstance(e, elasticsearch.ConnectionTimeout):
             request_meta_data["error-description"] = "network connection timed out"
-        elif e.info:
-            request_meta_data["error-description"] = f"{e.error} ({e.info})"
         else:
-            if isinstance(e.error, bytes):
-                error_description = e.error.decode("utf-8")
-            else:
-                error_description = str(e.error)
+            error_description = e.message
             request_meta_data["error-description"] = error_description
+    except elasticsearch.ApiError as e:
+        total_ops = 0
+        total_ops_unit = "ops"
+        request_meta_data = {"success": False, "error-type": "api"}
+
+        if isinstance(e.error, bytes):
+            error_message = e.error.decode("utf-8")
+        elif isinstance(e.error, BytesIO):
+            error_message = e.error.read().decode("utf-8")
+        else:
+            error_message = e.error
+
+        if isinstance(e.info, bytes):
+            error_body = e.info.decode("utf-8")
+        elif isinstance(e.info, BytesIO):
+            error_body = e.info.read().decode("utf-8")
+        else:
+            error_body = e.info
+
+        if error_body:
+            error_message += f" ({error_body})"
+        error_description = error_message
+
+        request_meta_data["error-description"] = error_description
+        if e.status_code:
+            request_meta_data["http-status"] = e.status_code
     except KeyError as e:
         logging.getLogger(__name__).exception("Cannot execute runner [%s]; most likely due to missing parameters.", str(runner))
         msg = "Cannot execute [%s]. Provided parameters are: %s. Error: [%s]." % (str(runner), list(params.keys()), str(e))
         raise exceptions.SystemSetupError(msg)
 
     if not request_meta_data["success"]:
         if on_error == "abort" or fatal_error:
```

## esrally/driver/runner.py

```diff
@@ -200,24 +200,37 @@
             "params": "request-params",
             "request_timeout": "request-timeout",
         }
         full_result = {k: params.get(v) for (k, v) in kw_dict.items()}
         # filter Nones
         return dict(filter(lambda kv: kv[1] is not None, full_result.items()))
 
-    def _transport_request_params(self, params):
+    @staticmethod
+    def _transport_request_params(params):
+        """
+        Takes all of a runner's params and splits out request parameters, transport
+        level parameters, and headers into their own respective dicts.
+
+        :param params: A hash with all the respective runner's parameters.
+        :return: A tuple of the specific runner's params, request level parameters, transport level parameters, and headers, respectively.
+        """
+        transport_params = {}
         request_params = params.get("request-params", {})
-        request_timeout = params.get("request-timeout")
-        if request_timeout is not None:
-            request_params["request_timeout"] = request_timeout
-        headers = params.get("headers") or {}
-        opaque_id = params.get("opaque-id")
-        if opaque_id is not None:
+
+        if request_timeout := params.pop("request-timeout", None):
+            transport_params["request_timeout"] = request_timeout
+
+        if (ignore_status := request_params.pop("ignore", None)) or (ignore_status := params.pop("ignore", None)):
+            transport_params["ignore_status"] = ignore_status
+
+        headers = params.pop("headers", None) or {}
+        if opaque_id := params.pop("opaque-id", None):
             headers.update({"x-opaque-id": opaque_id})
-        return request_params, headers
+
+        return params, request_params, transport_params, headers
 
 
 class Delegator:
     """
     Mixin to unify delegate handling
     """
 
@@ -726,15 +739,15 @@
 class NodeStats(Runner):
     """
     Gather node stats for all nodes.
     """
 
     async def __call__(self, es, params):
         request_timeout = params.get("request-timeout")
-        await es.nodes.stats(metric="_all", request_timeout=request_timeout)
+        await es.options(request_timeout=request_timeout).nodes.stats(metric="_all")
 
     def __repr__(self, *args, **kwargs):
         return "node-stats"
 
 
 def parse(text: BytesIO, props: List[str], lists: List[str] = None, objects: List[str] = None) -> dict:
     """
@@ -843,15 +856,17 @@
 
     def __init__(self):
         super().__init__()
         self._search_after_extractor = SearchAfterExtractor()
         self._composite_agg_extractor = CompositeAggExtractor()
 
     async def __call__(self, es, params):
-        request_params, headers = self._transport_request_params(params)
+        params, request_params, transport_params, headers = self._transport_request_params(params)
+        # we don't set headers at the options level because the Query runner sets them via the client's '_perform_request' method
+        es = es.options(**transport_params)
         # Mandatory to ensure it is always provided. This is especially important when this runner is used in a
         # composite context where there is no actual parameter source and the entire request structure must be provided
         # by the composite's parameter source.
         index = mandatory(params, "index", self)
         body = mandatory(params, "body", self)
         operation_type = params.get("operation-type")
         size = params.get("results-per-page")
@@ -1389,25 +1404,20 @@
         indices = mandatory(params, "indices", self)
         only_if_exists = params.get("only-if-exists", False)
         request_params = params.get("request-params", {})
         prior_destructive_setting = await set_destructive_requires_name(es, False)
         try:
             for index_name in indices:
                 if not only_if_exists:
+                    await es.indices.delete(index=index_name, ignore=[404], params=request_params)
+                    ops += 1
+                elif only_if_exists and await es.indices.exists(index=index_name):
+                    self.logger.info("Index [%s] already exists. Deleting it.", index_name)
                     await es.indices.delete(index=index_name, params=request_params)
                     ops += 1
-                elif only_if_exists:
-                    # here we use .get() and check for 404 instead of exists due to a bug in some versions
-                    # of elasticsearch-py/elastic-transport with HEAD calls.
-                    # can change back once using elasticsearch-py >= 8.0.0 and elastic-transport >= 8.1.0
-                    get_response = await es.indices.get(index=index_name, ignore=[404])
-                    if not get_response.get("status") == 404:
-                        self.logger.info("Index [%s] already exists. Deleting it.", index_name)
-                        await es.indices.delete(index=index_name, params=request_params)
-                        ops += 1
         finally:
             await set_destructive_requires_name(es, prior_destructive_setting)
         return {
             "weight": ops,
             "unit": "ops",
             "success": True,
         }
@@ -1428,23 +1438,18 @@
         only_if_exists = mandatory(params, "only-if-exists", self)
         request_params = mandatory(params, "request-params", self)
 
         for data_stream in data_streams:
             if not only_if_exists:
                 await es.indices.delete_data_stream(name=data_stream, ignore=[404], params=request_params)
                 ops += 1
-            elif only_if_exists:
-                # here we use .get() and check for 404 instead of exists due to a bug in some versions
-                # of elasticsearch-py/elastic-transport with HEAD calls.
-                # can change back once using elasticsearch-py >= 8.0.0 and elastic-transport >= 8.1.0
-                get_response = await es.indices.get(index=data_stream, ignore=[404])
-                if not get_response.get("status") == 404:
-                    self.logger.info("Data stream [%s] already exists. Deleting it.", data_stream)
-                    await es.indices.delete_data_stream(name=data_stream, params=request_params)
-                    ops += 1
+            elif only_if_exists and await es.indices.exists(index=data_stream):
+                self.logger.info("Data stream [%s] already exists. Deleting it.", data_stream)
+                await es.indices.delete_data_stream(name=data_stream, params=request_params)
+                ops += 1
 
         return {
             "weight": ops,
             "unit": "ops",
             "success": True,
         }
 
@@ -1457,16 +1462,16 @@
     Execute the `PUT component template API
     <https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-component-template.html>`_.
     """
 
     async def __call__(self, es, params):
         templates = mandatory(params, "templates", self)
         request_params = mandatory(params, "request-params", self)
-        for template, body in templates:
-            await es.cluster.put_component_template(name=template, body=body, params=request_params)
+        for name, body in templates:
+            await es.cluster.put_component_template(name=name, template=body["template"], params=request_params)
         return {
             "weight": len(templates),
             "unit": "ops",
             "success": True,
         }
 
     def __repr__(self, *args, **kwargs):
@@ -1485,23 +1490,18 @@
         request_params = mandatory(params, "request-params", self)
 
         ops_count = 0
         for template_name in template_names:
             if not only_if_exists:
                 await es.cluster.delete_component_template(name=template_name, params=request_params, ignore=[404])
                 ops_count += 1
-            elif only_if_exists:
-                # here we use .get() and check for 404 instead of exists_component_template due to a bug in some versions
-                # of elasticsearch-py/elastic-transport with HEAD calls.
-                # can change back once using elasticsearch-py >= 8.0.0 and elastic-transport >= 8.1.0
-                component_template_exists = await es.cluster.get_component_template(name=template_name, ignore=[404])
-                if not component_template_exists.get("status") == 404:
-                    self.logger.info("Component Index template [%s] already exists. Deleting it.", template_name)
-                    await es.cluster.delete_component_template(name=template_name, params=request_params)
-                    ops_count += 1
+            elif only_if_exists and await es.cluster.exists_component_template(name=template_name):
+                self.logger.info("Component Index template [%s] already exists. Deleting it.", template_name)
+                await es.cluster.delete_component_template(name=template_name, params=request_params)
+                ops_count += 1
         return {
             "weight": ops_count,
             "unit": "ops",
             "success": True,
         }
 
     def __repr__(self, *args, **kwargs):
@@ -1536,30 +1536,38 @@
 
     async def __call__(self, es, params):
         templates = mandatory(params, "templates", self)
         only_if_exists = mandatory(params, "only-if-exists", self)
         request_params = mandatory(params, "request-params", self)
         ops_count = 0
 
-        for template_name, delete_matching_indices, index_pattern in templates:
-            if not only_if_exists:
-                await es.indices.delete_index_template(name=template_name, params=request_params, ignore=[404])
-                ops_count += 1
-            elif only_if_exists:
-                # here we use .get() and check for 404 instead of exists_index_template due to a bug in some versions
-                # of elasticsearch-py/elastic-transport with HEAD calls.
-                # can change back once using elasticsearch-py >= 8.0.0 and elastic-transport >= 8.1.0
-                index_template_exists = await es.indices.get_index_template(name=template_name, ignore=[404])
-                if not index_template_exists.get("status") == 404:
+        prior_destructive_setting = None
+        current_destructive_setting = None
+        try:
+            for template_name, delete_matching_indices, index_pattern in templates:
+                if not only_if_exists:
+                    await es.indices.delete_index_template(name=template_name, params=request_params, ignore=[404])
+                    ops_count += 1
+                elif only_if_exists and await es.indices.exists_index_template(name=template_name):
                     self.logger.info("Composable Index template [%s] already exists. Deleting it.", template_name)
                     await es.indices.delete_index_template(name=template_name, params=request_params)
                     ops_count += 1
-            # ensure that we do not provide an empty index pattern by accident
-            if delete_matching_indices and index_pattern:
-                await es.indices.delete(index=index_pattern)
+                # ensure that we do not provide an empty index pattern by accident
+                if delete_matching_indices and index_pattern:
+                    # only set if really required
+                    if current_destructive_setting is None:
+                        current_destructive_setting = False
+                        prior_destructive_setting = await set_destructive_requires_name(es, current_destructive_setting)
+                        ops_count += 1
+
+                    await es.indices.delete(index=index_pattern)
+                    ops_count += 1
+        finally:
+            if current_destructive_setting is not None:
+                await set_destructive_requires_name(es, prior_destructive_setting)
                 ops_count += 1
 
         return {
             "weight": ops_count,
             "unit": "ops",
             "success": True,
         }
@@ -1596,28 +1604,39 @@
 
     async def __call__(self, es, params):
         template_names = mandatory(params, "templates", self)
         only_if_exists = params.get("only-if-exists", False)
         request_params = params.get("request-params", {})
         ops_count = 0
 
-        for template_name, delete_matching_indices, index_pattern in template_names:
-            if not only_if_exists:
-                await es.indices.delete_template(name=template_name, params=request_params)
-                ops_count += 1
-            # here we use .get_template() and check for empty instead of exists_template due to a bug in some versions
-            # of elasticsearch-py/elastic-transport with HEAD calls.
-            # can change back once using elasticsearch-py >= 8.0.0 and elastic-transport >= 8.1.0
-            elif only_if_exists and await es.indices.get_template(name=template_name, ignore=[404]):
-                self.logger.info("Index template [%s] already exists. Deleting it.", template_name)
-                await es.indices.delete_template(name=template_name, params=request_params)
-                ops_count += 1
-            # ensure that we do not provide an empty index pattern by accident
-            if delete_matching_indices and index_pattern:
-                await es.indices.delete(index=index_pattern)
+        prior_destructive_setting = None
+        current_destructive_setting = None
+
+        try:
+            for template_name, delete_matching_indices, index_pattern in template_names:
+                if not only_if_exists:
+                    await es.indices.delete_template(name=template_name, ignore=[404], params=request_params)
+                    ops_count += 1
+                elif only_if_exists and await es.indices.exists_template(name=template_name):
+                    self.logger.info("Index template [%s] already exists. Deleting it.", template_name)
+                    await es.indices.delete_template(name=template_name, params=request_params)
+                    ops_count += 1
+                # ensure that we do not provide an empty index pattern by accident
+                if delete_matching_indices and index_pattern:
+                    # only set if really required
+                    if current_destructive_setting is None:
+                        current_destructive_setting = False
+                        prior_destructive_setting = await set_destructive_requires_name(es, current_destructive_setting)
+                        ops_count += 1
+
+                    await es.indices.delete(index=index_pattern)
+                    ops_count += 1
+        finally:
+            if current_destructive_setting is not None:
+                await set_destructive_requires_name(es, prior_destructive_setting)
                 ops_count += 1
 
         return {
             "weight": ops_count,
             "unit": "ops",
             "success": True,
         }
@@ -1717,24 +1736,21 @@
         # pylint: disable=import-outside-toplevel
         import elasticsearch
 
         datafeed_id = mandatory(params, "datafeed-id", self)
         body = mandatory(params, "body", self)
         try:
             await es.ml.put_datafeed(datafeed_id=datafeed_id, body=body)
-        except elasticsearch.TransportError as e:
-            # fallback to old path
-            if e.status_code == 400:
-                await es.perform_request(
-                    method="PUT",
-                    path=f"/_xpack/ml/datafeeds/{datafeed_id}",
-                    body=body,
-                )
-            else:
-                raise e
+        except elasticsearch.BadRequestError:
+            # TODO: remove the fallback to '_xpack' path when we drop support for Elasticsearch 6.8
+            await es.perform_request(
+                method="PUT",
+                path=f"/_xpack/ml/datafeeds/{datafeed_id}",
+                body=body,
+            )
 
     def __repr__(self, *args, **kwargs):
         return "create-ml-datafeed"
 
 
 class DeleteMlDatafeed(Runner):
     """
@@ -1746,24 +1762,21 @@
         import elasticsearch
 
         datafeed_id = mandatory(params, "datafeed-id", self)
         force = params.get("force", False)
         try:
             # we don't want to fail if a datafeed does not exist, thus we ignore 404s.
             await es.ml.delete_datafeed(datafeed_id=datafeed_id, force=force, ignore=[404])
-        except elasticsearch.TransportError as e:
-            # fallback to old path (ES < 7)
-            if e.status_code == 400:
-                await es.perform_request(
-                    method="DELETE",
-                    path=f"/_xpack/ml/datafeeds/{datafeed_id}",
-                    params={"force": escape(force), "ignore": 404},
-                )
-            else:
-                raise e
+        except elasticsearch.BadRequestError:
+            # TODO: remove the fallback to '_xpack' path when we drop support for Elasticsearch 6.8
+            await es.perform_request(
+                method="DELETE",
+                path=f"/_xpack/ml/datafeeds/{datafeed_id}",
+                params={"force": escape(force), "ignore": 404},
+            )
 
     def __repr__(self, *args, **kwargs):
         return "delete-ml-datafeed"
 
 
 class StartMlDatafeed(Runner):
     """
@@ -1777,24 +1790,21 @@
         datafeed_id = mandatory(params, "datafeed-id", self)
         body = params.get("body")
         start = params.get("start")
         end = params.get("end")
         timeout = params.get("timeout")
         try:
             await es.ml.start_datafeed(datafeed_id=datafeed_id, body=body, start=start, end=end, timeout=timeout)
-        except elasticsearch.TransportError as e:
-            # fallback to old path (ES < 7)
-            if e.status_code == 400:
-                await es.perform_request(
-                    method="POST",
-                    path=f"/_xpack/ml/datafeeds/{datafeed_id}/_start",
-                    body=body,
-                )
-            else:
-                raise e
+        except elasticsearch.BadRequestError:
+            # TODO: remove the fallback to '_xpack' path when we drop support for Elasticsearch 6.8
+            await es.perform_request(
+                method="POST",
+                path=f"/_xpack/ml/datafeeds/{datafeed_id}/_start",
+                body=body,
+            )
 
     def __repr__(self, *args, **kwargs):
         return "start-ml-datafeed"
 
 
 class StopMlDatafeed(Runner):
     """
@@ -1806,29 +1816,26 @@
         import elasticsearch
 
         datafeed_id = mandatory(params, "datafeed-id", self)
         force = params.get("force", False)
         timeout = params.get("timeout")
         try:
             await es.ml.stop_datafeed(datafeed_id=datafeed_id, force=force, timeout=timeout)
-        except elasticsearch.TransportError as e:
-            # fallback to old path (ES < 7)
-            if e.status_code == 400:
-                request_params = {
-                    "force": escape(force),
-                }
-                if timeout:
-                    request_params["timeout"] = escape(timeout)
-                await es.perform_request(
-                    method="POST",
-                    path=f"/_xpack/ml/datafeeds/{datafeed_id}/_stop",
-                    params=request_params,
-                )
-            else:
-                raise e
+        except elasticsearch.BadRequestError:
+            # TODO: remove the fallback to '_xpack' path when we drop support for Elasticsearch 6.8
+            request_params = {
+                "force": escape(force),
+            }
+            if timeout:
+                request_params["timeout"] = escape(timeout)
+            await es.perform_request(
+                method="POST",
+                path=f"/_xpack/ml/datafeeds/{datafeed_id}/_stop",
+                params=request_params,
+            )
 
     def __repr__(self, *args, **kwargs):
         return "stop-ml-datafeed"
 
 
 class CreateMlJob(Runner):
     """
@@ -1839,24 +1846,21 @@
         # pylint: disable=import-outside-toplevel
         import elasticsearch
 
         job_id = mandatory(params, "job-id", self)
         body = mandatory(params, "body", self)
         try:
             await es.ml.put_job(job_id=job_id, body=body)
-        except elasticsearch.TransportError as e:
-            # fallback to old path (ES < 7)
-            if e.status_code == 400:
-                await es.perform_request(
-                    method="PUT",
-                    path=f"/_xpack/ml/anomaly_detectors/{job_id}",
-                    body=body,
-                )
-            else:
-                raise e
+        except elasticsearch.BadRequestError:
+            # TODO: remove the fallback to '_xpack' path when we drop support for Elasticsearch 6.8
+            await es.perform_request(
+                method="PUT",
+                path=f"/_xpack/ml/anomaly_detectors/{job_id}",
+                body=body,
+            )
 
     def __repr__(self, *args, **kwargs):
         return "create-ml-job"
 
 
 class DeleteMlJob(Runner):
     """
@@ -1868,24 +1872,21 @@
         import elasticsearch
 
         job_id = mandatory(params, "job-id", self)
         force = params.get("force", False)
         # we don't want to fail if a job does not exist, thus we ignore 404s.
         try:
             await es.ml.delete_job(job_id=job_id, force=force, ignore=[404])
-        except elasticsearch.TransportError as e:
-            # fallback to old path (ES < 7)
-            if e.status_code == 400:
-                await es.perform_request(
-                    method="DELETE",
-                    path=f"/_xpack/ml/anomaly_detectors/{job_id}",
-                    params={"force": escape(force), "ignore": 404},
-                )
-            else:
-                raise e
+        except elasticsearch.BadRequestError:
+            # TODO: remove the fallback to '_xpack' path when we drop support for Elasticsearch 6.8
+            await es.perform_request(
+                method="DELETE",
+                path=f"/_xpack/ml/anomaly_detectors/{job_id}",
+                params={"force": escape(force), "ignore": 404},
+            )
 
     def __repr__(self, *args, **kwargs):
         return "delete-ml-job"
 
 
 class OpenMlJob(Runner):
     """
@@ -1895,23 +1896,20 @@
     async def __call__(self, es, params):
         # pylint: disable=import-outside-toplevel
         import elasticsearch
 
         job_id = mandatory(params, "job-id", self)
         try:
             await es.ml.open_job(job_id=job_id)
-        except elasticsearch.TransportError as e:
-            # fallback to old path (ES < 7)
-            if e.status_code == 400:
-                await es.perform_request(
-                    method="POST",
-                    path=f"/_xpack/ml/anomaly_detectors/{job_id}/_open",
-                )
-            else:
-                raise e
+        except elasticsearch.BadRequestError:
+            # TODO: remove the fallback to '_xpack' path when we drop support for Elasticsearch 6.8
+            await es.perform_request(
+                method="POST",
+                path=f"/_xpack/ml/anomaly_detectors/{job_id}/_open",
+            )
 
     def __repr__(self, *args, **kwargs):
         return "open-ml-job"
 
 
 class CloseMlJob(Runner):
     """
@@ -1923,47 +1921,47 @@
         import elasticsearch
 
         job_id = mandatory(params, "job-id", self)
         force = params.get("force", False)
         timeout = params.get("timeout")
         try:
             await es.ml.close_job(job_id=job_id, force=force, timeout=timeout)
-        except elasticsearch.TransportError as e:
-            # fallback to old path (ES < 7)
-            if e.status_code == 400:
-                request_params = {
-                    "force": escape(force),
-                }
-                if timeout:
-                    request_params["timeout"] = escape(timeout)
+        except elasticsearch.BadRequestError:
+            # TODO: remove the fallback to '_xpack' path when we drop support for Elasticsearch 6.8
+            request_params = {
+                "force": escape(force),
+            }
+            if timeout:
+                request_params["timeout"] = escape(timeout)
 
-                await es.perform_request(
-                    method="POST",
-                    path=f"/_xpack/ml/anomaly_detectors/{job_id}/_close",
-                    params=request_params,
-                )
-            else:
-                raise e
+            await es.perform_request(
+                method="POST",
+                path=f"/_xpack/ml/anomaly_detectors/{job_id}/_close",
+                params=request_params,
+            )
 
     def __repr__(self, *args, **kwargs):
         return "close-ml-job"
 
 
 class RawRequest(Runner):
     async def __call__(self, es, params):
-        request_params, headers = self._transport_request_params(params)
-        if "ignore" in params:
-            request_params["ignore"] = params["ignore"]
+        params, request_params, transport_params, headers = self._transport_request_params(params)
+        es = es.options(**transport_params)
+
         path = mandatory(params, "path", self)
+
         if not path.startswith("/"):
             self.logger.error("RawRequest failed. Path parameter: [%s] must begin with a '/'.", path)
             raise exceptions.RallyAssertionError(f"RawRequest [{path}] failed. Path parameter must begin with a '/'.")
+
         if not bool(headers):
             # counter-intuitive, but preserves prior behavior
             headers = None
+
         # disable eager response parsing - responses might be huge thus skewing results
         es.return_raw_response()
 
         await es.perform_request(
             method=params.get("method", "GET"), path=path, headers=headers, body=params.get("body"), params=request_params
         )
 
@@ -1989,29 +1987,29 @@
 
 class DeleteSnapshotRepository(Runner):
     """
     Deletes a snapshot repository
     """
 
     async def __call__(self, es, params):
-        await es.snapshot.delete_repository(repository=mandatory(params, "repository", repr(self)))
+        await es.snapshot.delete_repository(repository=mandatory(params, "repository", repr(self)), ignore=[404])
 
     def __repr__(self, *args, **kwargs):
         return "delete-snapshot-repository"
 
 
 class CreateSnapshotRepository(Runner):
     """
     Creates a new snapshot repository
     """
 
     async def __call__(self, es, params):
         request_params = params.get("request-params", {})
         await es.snapshot.create_repository(
-            repository=mandatory(params, "repository", repr(self)), body=mandatory(params, "body", repr(self)), params=request_params
+            name=mandatory(params, "repository", repr(self)), body=mandatory(params, "body", repr(self)), params=request_params
         )
 
     def __repr__(self, *args, **kwargs):
         return "create-snapshot-repository"
 
 
 class CreateSnapshot(Runner):
@@ -2093,40 +2091,23 @@
     """
 
     async def __call__(self, es, params):
         repository = mandatory(params, "repository", repr(self))
         wait_period = params.get("completion-recheck-wait-period", 1)
         es_info = await es.info()
         es_version = Version.from_string(es_info["version"]["number"])
-        api = es.snapshot.get
         request_args = {"repository": repository, "snapshot": "_current", "verbose": False}
 
         # significantly reduce response size when lots of snapshots have been taken
         # only available since ES 8.3.0 (https://github.com/elastic/elasticsearch/pull/86269)
         if (es_version.major, es_version.minor) >= (8, 3):
-            request_params, headers = self._transport_request_params(params)
-            headers["Content-Type"] = "application/json"
-
-            request_params["index_names"] = "false"
-            request_params["verbose"] = "false"
-
-            request_args = {
-                "method": "GET",
-                "path": f"_snapshot/{repository}/_current",
-                "headers": headers,
-                "params": request_params,
-            }
-
-            # TODO: Switch to native es.snapshot.get once `index_names` becomes supported in
-            #       `es.snapshot.get` of the elasticsearch-py client and we've upgraded the client in Rally, see:
-            #       https://elasticsearch-py.readthedocs.io/en/latest/api.html#elasticsearch.client.SnapshotClient.get
-            api = es.perform_request
+            request_args["index_names"] = False
 
         while True:
-            response = await api(**request_args)
+            response = await es.snapshot.get(**request_args)
 
             if int(response.get("total")) == 0:
                 break
 
             await asyncio.sleep(wait_period)
 
         # getting detailed stats per snapshot using the snapshot status api can be very expensive.
@@ -2138,21 +2119,23 @@
 
 class RestoreSnapshot(Runner):
     """
     Restores a snapshot from an already registered repository
     """
 
     async def __call__(self, es, params):
+        wait_for_completion = params.get("wait-for-completion", False)
+        params.get("request-params", {}).update({"wait_for_completion": wait_for_completion})
         api_kwargs = self._default_kw_params(params)
-        await es.snapshot.restore(
-            repository=mandatory(params, "repository", repr(self)),
-            snapshot=mandatory(params, "snapshot", repr(self)),
-            wait_for_completion=params.get("wait-for-completion", False),
-            **api_kwargs,
-        )
+        repo = mandatory(params, "repository", repr(self))
+        snapshot = mandatory(params, "snapshot", repr(self))
+
+        # TODO: Replace 'perform_request' with 'SnapshotClient.restore()' when https://github.com/elastic/elasticsearch-py/issues/2168
+        # is fixed
+        await es.perform_request(method="POST", path=f"/_snapshot/{repo}/{snapshot}/_restore", **api_kwargs)
 
     def __repr__(self, *args, **kwargs):
         return "restore-snapshot"
 
 
 class IndicesRecovery(Runner):
     async def __call__(self, es, params):
@@ -2665,16 +2648,29 @@
     Execute the `PUT index lifecycle policy API
     <https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-put-lifecycle.html>`_.
     """
 
     async def __call__(self, es, params):
         policy_name = mandatory(params, "policy-name", self)
         body = mandatory(params, "body", self)
+        policy = mandatory(body, "policy", self)
         request_params = params.get("request-params", {})
-        await es.ilm.put_lifecycle(policy=policy_name, body=body, params=request_params)
+        error_trace = request_params.get("error_trace", None)
+        filter_path = request_params.get("filter_path", None)
+        master_timeout = request_params.get("master_timeout", None)
+        timeout = request_params.get("timeout", None)
+
+        await es.ilm.put_lifecycle(
+            name=policy_name,
+            policy=policy,
+            error_trace=error_trace,
+            filter_path=filter_path,
+            master_timeout=master_timeout,
+            timeout=timeout,
+        )
         return {
             "weight": 1,
             "unit": "ops",
             "success": True,
         }
 
     def __repr__(self, *args, **kwargs):
@@ -2686,15 +2682,22 @@
     Execute the `DELETE index lifecycle policy API
     <https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-delete-lifecycle.html>`_.
     """
 
     async def __call__(self, es, params):
         policy_name = mandatory(params, "policy-name", self)
         request_params = params.get("request-params", {})
-        await es.ilm.delete_lifecycle(policy=policy_name, params=request_params)
+        error_trace = request_params.get("error_trace", None)
+        filter_path = request_params.get("filter_path", None)
+        master_timeout = request_params.get("master_timeout", None)
+        timeout = request_params.get("timeout", None)
+
+        await es.ilm.delete_lifecycle(
+            name=policy_name, error_trace=error_trace, filter_path=filter_path, master_timeout=master_timeout, timeout=timeout, ignore=[404]
+        )
         return {
             "weight": 1,
             "unit": "ops",
             "success": True,
         }
 
     def __repr__(self, *args, **kwargs):
@@ -2739,16 +2742,16 @@
 
 class Downsample(Runner):
     """
     Executes a downsampling operation creating the target index and aggregating data in the source index on the @timestamp field.
     """
 
     async def __call__(self, es, params):
-
-        request_params, request_headers = self._transport_request_params(params)
+        params, request_params, transport_params, request_headers = self._transport_request_params(params)
+        es = es.options(**transport_params)
 
         fixed_interval = mandatory(params, "fixed-interval", self)
         if fixed_interval is None:
             raise exceptions.DataError(
                 "Parameter source for operation 'downsample' did not provide the mandatory parameter 'fixed-interval'. "
                 "Add it to your parameter source and try again."
             )
@@ -2807,15 +2810,15 @@
 
     async def __aenter__(self):
         await self.delegate.__aenter__()
         return self
 
     async def __call__(self, es, params):
         absolute_time = time.time()
-        async with es["default"].new_request_context() as request_context:
+        with es["default"].new_request_context() as request_context:
             return_value = await self.delegate(es, params)
             if isinstance(return_value, tuple) and len(return_value) == 2:
                 total_ops, total_ops_unit = return_value
                 result = {
                     "weight": total_ops,
                     "unit": total_ops_unit,
                     "success": True,
@@ -2907,22 +2910,32 @@
                         await asyncio.sleep(sleep_time)
                 else:
                     return return_value
             except (socket.timeout, elasticsearch.exceptions.ConnectionError):
                 if last_attempt or not retry_on_timeout:
                     raise
                 await asyncio.sleep(sleep_time)
-            except elasticsearch.exceptions.TransportError as e:
+            except elasticsearch.ApiError as e:
                 if last_attempt or not retry_on_timeout:
                     raise e
 
                 if e.status_code == 408:
                     self.logger.info("[%s] has timed out. Retrying in [%.2f] seconds.", repr(self.delegate), sleep_time)
                     await asyncio.sleep(sleep_time)
                 else:
                     raise e
 
+            except elasticsearch.exceptions.ConnectionTimeout as e:
+                if last_attempt or not retry_on_timeout:
+                    raise e
+
+                self.logger.info("[%s] has timed out. Retrying in [%.2f] seconds.", repr(self.delegate), sleep_time)
+                await asyncio.sleep(sleep_time)
+            except elasticsearch.exceptions.TransportError as e:
+                if last_attempt or not retry_on_timeout:
+                    raise e
+
     async def __aexit__(self, exc_type, exc_val, exc_tb):
         return await self.delegate.__aexit__(exc_type, exc_val, exc_tb)
 
     def __repr__(self, *args, **kwargs):
         return "retryable %s" % repr(self.delegate)
```

## esrally/mechanic/provisioner.py

```diff
@@ -315,15 +315,14 @@
             # this is the IP address that the node will be bound to. Rally will bind to the node's IP address (but not to 0.0.0.0). The
             # reason is that we use the node's IP address as subject alternative name in x-pack.
             "network_host": network_host,
             "http_port": str(self.http_port),
             "transport_port": str(self.http_port + 100),
             "all_node_ips": '["%s"]' % '","'.join(self.all_node_ips),
             "all_node_names": '["%s"]' % '","'.join(self.all_node_names),
-            "all_node_ips_count": len(self.all_node_ips),
             # at the moment we are strict and enforce that all nodes are master eligible nodes
             "minimum_master_nodes": len(self.all_node_ips),
             "install_root_path": self.es_home_path,
         }
         variables = {}
         variables.update(self.car.variables)
         variables.update(defaults)
```

## esrally/mechanic/supplier.py

```diff
@@ -17,25 +17,24 @@
 
 import datetime
 import getpass
 import glob
 import grp
 import logging
 import os
-import re
 import shutil
 import urllib.error
 
 import docker
 from esrally import PROGRAM_NAME, exceptions, paths
 from esrally.exceptions import BuildError, SystemSetupError
 from esrally.utils import console, convert, git, io, jvm, net, process, sysstats
 
-# e.g. my-plugin:current - we cannot simply use String#split(":") as this would not work for timestamp-based revisions
-REVISION_PATTERN = r"(\w.*?):(.*)"
+DEFAULT_ELASTICSEARCH_BRANCH = "main"
+DEFAULT_PLUGIN_BRANCH = "main"
 
 
 def create(cfg, sources, distribution, car, plugins=None):
     logger = logging.getLogger(__name__)
     if plugins is None:
         plugins = []
     caching_enabled = cfg.opts("source", "cache", mandatory=False, default_value=True)
@@ -396,15 +395,15 @@
         self.src_dir = es_src_dir
         self.remote_url = remote_url
         self.car = car
         self.builder = builder
         self.template_renderer = template_renderer
 
     def fetch(self):
-        return SourceRepository("Elasticsearch", self.remote_url, self.src_dir, branch="main").fetch(self.revision)
+        return SourceRepository("Elasticsearch", self.remote_url, self.src_dir, branch=DEFAULT_ELASTICSEARCH_BRANCH).fetch(self.revision)
 
     def prepare(self):
         if self.builder:
             self.builder.build_jdk = self.resolve_build_jdk_major(self.src_dir)
 
             # There are no 'x86_64' specific gradle build commands
             if self.template_renderer.arch != "x86_64":
@@ -555,15 +554,15 @@
 
     @staticmethod
     def can_handle(plugin):
         return plugin.core_plugin
 
     def fetch(self):
         # Just retrieve the current revision *number* and assume that Elasticsearch has prepared the source tree.
-        return SourceRepository("Elasticsearch", None, self.es_src_dir, branch="main").fetch(revision="current")
+        return SourceRepository("Elasticsearch", None, self.es_src_dir, branch=DEFAULT_PLUGIN_BRANCH).fetch(revision="current")
 
     def prepare(self):
         if self.builder:
             self.builder.build_jdk = ElasticsearchSourceSupplier.resolve_build_jdk_major(self.es_src_dir)
             self.builder.build([f"./gradlew :plugins:{self.plugin.name}:assemble"])
 
     def add(self, binaries):
@@ -645,38 +644,69 @@
             "Mandatory config key [%s] is undefined. Please add it in the [source] section of the config file." % key
         )
 
 
 def _extract_revisions(revision):
     revisions = revision.split(",") if revision else []
     if len(revisions) == 1:
-        r = revisions[0]
-        if r.startswith("elasticsearch:"):
+        c, r = _component_from_revision(revisions[0])
+        if c.startswith("elasticsearch:"):
             r = r[len("elasticsearch:") :]
-        # may as well be just a single plugin
-        m = re.match(REVISION_PATTERN, r)
-        if m:
-            return {m.group(1): m.group(2)}
+        # elasticsearch or single plugin
+        if c:
+            return {c: r}
         else:
             return {
                 "elasticsearch": r,
                 # use a catch-all value
                 "all": r,
             }
     else:
         results = {}
-        for r in revisions:
-            m = re.match(REVISION_PATTERN, r)
-            if m:
-                results[m.group(1)] = m.group(2)
+        for rev in revisions:
+            c, r = _component_from_revision(rev)
+            if c:
+                results[c] = r
             else:
                 raise exceptions.SystemSetupError("Revision [%s] does not match expected format [name:revision]." % r)
         return results
 
 
+def _branch_from_revision_with_ts(revision, default_branch):
+    """
+    Extracts the branch and revision from a `revision` that uses @timestamp.
+    If a branch can't be found in `revision`, default_branch is used.
+    """
+
+    # ":" separator maybe used in both the timestamp and the component
+    # e.g. elasticsearch:<branch>@TS
+    _, r = _component_from_revision(revision)
+    branch, git_ts_revision = r.split("@")
+    if not branch:
+        branch = default_branch
+    return branch, git_ts_revision
+
+
+def _component_from_revision(revision):
+    """Extracts the (optional) component and remaining data from `revision`"""
+
+    component = ""
+    r = revision
+    if "@" not in revision and ":" in revision:
+        # e.g. @2023-04-20T01:00:00Z
+        component, r = revision.split(":")
+    elif "@" in revision and ":" in revision:
+        # e.g. "elasticsearch:<optional_branch>@2023-04-20T01:00:00Z"
+        revision_without_ts = revision[: revision.find("@")]
+        if ":" in revision_without_ts:
+            component = revision_without_ts.split(":")[0]
+            r = revision[revision.find(":", 1) + 1 :]
+    return component, r
+
+
 class SourceRepository:
     """
     Supplier fetches the benchmark candidate source tree from the remote repository.
     """
 
     def __init__(self, name, remote_url, src_dir, *, branch):
         self.name = name
@@ -697,27 +727,32 @@
         if not git.is_working_copy(self.src_dir):
             if self.has_remote():
                 self.logger.info("Downloading sources for %s from %s to %s.", self.name, self.remote_url, self.src_dir)
                 git.clone(self.src_dir, remote=self.remote_url)
             elif os.path.isdir(self.src_dir) and may_skip_init:
                 self.logger.info("Skipping repository initialization for %s.", self.name)
             else:
-                exceptions.SystemSetupError("A remote repository URL is mandatory for %s" % self.name)
+                raise exceptions.SystemSetupError("A remote repository URL is mandatory for %s" % self.name)
 
     def _update(self, revision):
         if self.has_remote() and revision == "latest":
             self.logger.info("Fetching latest sources for %s from origin.", self.name)
             git.pull(self.src_dir, remote="origin", branch=self.branch)
         elif revision == "current":
             self.logger.info("Skip fetching sources for %s.", self.name)
-        elif self.has_remote() and revision.startswith("@"):
-            # convert timestamp annotated for Rally to something git understands -> we strip leading and trailing " and the @.
-            git_ts_revision = revision[1:]
-            self.logger.info("Fetching from remote and checking out revision with timestamp [%s] for %s.", git_ts_revision, self.name)
-            git.pull_ts(self.src_dir, git_ts_revision, remote="origin", branch=self.branch)
+        # revision contains a timestamp
+        elif self.has_remote() and "@" in revision:
+            branch, git_ts_revision = _branch_from_revision_with_ts(revision, self.branch)
+            self.logger.info(
+                "Fetching from remote and checking out revision with timestamp [%s] from branch %s for %s.",
+                git_ts_revision,
+                branch,
+                self.name,
+            )
+            git.pull_ts(self.src_dir, git_ts_revision, remote="origin", branch=branch)
         elif self.has_remote():  # we can have either a commit hash, branch name, or tag
             git.fetch(self.src_dir, remote="origin")
             if git.is_branch(self.src_dir, identifier=revision):
                 self.logger.info("Fetching from remote and checking out branch [%s] for %s.", revision, self.name)
                 git.checkout_branch(self.src_dir, remote="origin", branch=revision)
             else:  # tag or commit hash
                 self.logger.info("Fetching from remote and checking out revision [%s] for %s.", revision, self.name)
```

## esrally/resources/logging.json

### Pretty-printed

 * *Similarity: 0.9722222222222222%*

 * *Differences: {"'loggers'": "{'elastic_transport': OrderedDict([('handlers', ['rally_log_handler']), ('level', "*

 * *              "'WARNING'), ('propagate', False)])}"}*

```diff
@@ -31,14 +31,21 @@
             "delay": true,
             "encoding": "UTF-8",
             "filename": "${LOG_PATH}profile.log",
             "formatter": "profile"
         }
     },
     "loggers": {
+        "elastic_transport": {
+            "handlers": [
+                "rally_log_handler"
+            ],
+            "level": "WARNING",
+            "propagate": false
+        },
         "elasticsearch": {
             "handlers": [
                 "rally_log_handler"
             ],
             "level": "WARNING",
             "propagate": false
         },
```

## esrally/resources/metrics-template.json

### Pretty-printed

 * *Similarity: 0.9166666666666666%*

 * *Differences: {"'settings'": "{'index': {replace: OrderedDict([('mapping.total_fields.limit', 2000)])}}"}*

```diff
@@ -85,10 +85,12 @@
             },
             "value": {
                 "type": "float"
             }
         }
     },
     "settings": {
-        "index": {}
+        "index": {
+            "mapping.total_fields.limit": 2000
+        }
     }
 }
```

## esrally/tracker/tracker.py

```diff
@@ -14,15 +14,15 @@
 # KIND, either express or implied.  See the License for the
 # specific language governing permissions and limitations
 # under the License.
 
 import logging
 import os
 
-from elasticsearch import ElasticsearchException
+from elastic_transport import ApiError, TransportError
 from jinja2 import Environment, FileSystemLoader
 
 from esrally import PROGRAM_NAME
 from esrally.client import EsClientFactory
 from esrally.tracker import corpus, index
 from esrally.utils import console, io, opts
 
@@ -38,29 +38,29 @@
 def extract_indices_from_data_streams(client, data_streams_to_extract):
     indices = []
     # first extract index metadata (which is cheap) and defer extracting data to reduce the potential for
     # errors due to invalid index names late in the process.
     for data_stream_name in data_streams_to_extract:
         try:
             indices += index.extract_indices_from_data_stream(client, data_stream_name)
-        except ElasticsearchException:
+        except (ApiError, TransportError):
             logging.getLogger(__name__).exception("Failed to extract indices from data stream [%s]", data_stream_name)
 
     return indices
 
 
 def extract_mappings_and_corpora(client, output_path, indices_to_extract):
     indices = []
     corpora = []
     # first extract index metadata (which is cheap) and defer extracting data to reduce the potential for
     # errors due to invalid index names late in the process.
     for index_name in indices_to_extract:
         try:
             indices += index.extract(client, output_path, index_name)
-        except ElasticsearchException:
+        except (ApiError, TransportError):
             logging.getLogger(__name__).exception("Failed to extract index [%s]", index_name)
 
     # That list only contains valid indices (with index patterns already resolved)
     for i in indices:
         c = corpus.extract(client, output_path, i["name"])
         if c:
             corpora.append(c)
```

## esrally/utils/git.py

```diff
@@ -143,17 +143,23 @@
 
 
 @probed
 def tags(src_dir):
     return _cleanup_tag_names(process.run_subprocess_with_output(f"git -C {io.escape_path(src_dir)} tag"))
 
 
-def _cleanup_remote_branch_names(branch_names):
-    return [(b[b.index("/") + 1 :]).strip() for b in branch_names if not b.endswith("/HEAD")]
+def _cleanup_remote_branch_names(refs):
+    branches = []
+    for ref in refs:
+        # git >= 2.40.0 reports an `origin` ref without a slash while previous versions
+        # reported a `origin/HEAD` ref.
+        if "/" in ref and not ref.endswith("/HEAD"):
+            branches.append(ref[ref.index("/") + 1 :].strip())
+    return branches
 
 
-def _cleanup_local_branch_names(branch_names):
-    return [b.strip() for b in branch_names if not b.endswith("HEAD")]
+def _cleanup_local_branch_names(refs):
+    return [ref.strip() for ref in refs if not ref.endswith("HEAD")]
 
 
 def _cleanup_tag_names(tag_names):
     return [t.strip() for t in tag_names]
```

## esrally/utils/net.py

```diff
@@ -161,14 +161,17 @@
         download.consume_next_chunk(transport)
         if not expected_size_in_bytes:
             expected_size_in_bytes = download.total_bytes
         while not download.finished:
             if progress_indicator and download.bytes_downloaded and download.total_bytes:
                 progress_indicator(download.bytes_downloaded, expected_size_in_bytes)
             download.consume_next_chunk(transport)
+        # show final progress (for large files) or any progress (for files < chunk_size)
+        if progress_indicator and download.bytes_downloaded and expected_size_in_bytes:
+            progress_indicator(download.bytes_downloaded, expected_size_in_bytes)
 
 
 def download_from_bucket(blobstore, url, local_path, expected_size_in_bytes=None, progress_indicator=None):
     blob_downloader = {"s3": _download_from_s3_bucket, "gs": _download_from_gcs_bucket}
     logger = logging.getLogger(__name__)
 
     bucket_and_path = url[5:]  # s3:// or gs:// prefix for now
@@ -258,24 +261,24 @@
             expected_size_in_bytes = download_from_bucket(scheme, url, tmp_data_set_path, expected_size_in_bytes, progress_indicator)
         else:
             expected_size_in_bytes = download_http(url, tmp_data_set_path, expected_size_in_bytes, progress_indicator)
     except BaseException:
         if os.path.isfile(tmp_data_set_path):
             os.remove(tmp_data_set_path)
         raise
-    else:
-        download_size = os.path.getsize(tmp_data_set_path)
-        if expected_size_in_bytes is not None and download_size != expected_size_in_bytes:
-            if os.path.isfile(tmp_data_set_path):
-                os.remove(tmp_data_set_path)
-            raise exceptions.DataError(
-                "Download of [%s] is corrupt. Downloaded [%d] bytes but [%d] bytes are expected. Please retry."
-                % (local_path, download_size, expected_size_in_bytes)
-            )
-        os.rename(tmp_data_set_path, local_path)
+
+    download_size = os.path.getsize(tmp_data_set_path)
+    if expected_size_in_bytes is not None and download_size != expected_size_in_bytes:
+        if os.path.isfile(tmp_data_set_path):
+            os.remove(tmp_data_set_path)
+        raise exceptions.DataError(
+            "Download of [%s] is corrupt. Downloaded [%d] bytes but [%d] bytes are expected. Please retry."
+            % (local_path, download_size, expected_size_in_bytes)
+        )
+    os.rename(tmp_data_set_path, local_path)
 
 
 def retrieve_content_as_string(url):
     with _request("GET", url, timeout=urllib3.Timeout(connect=45, read=240)) as response:
         return response.read().decode("utf-8")
```

## esrally/utils/opts.py

```diff
@@ -167,27 +167,74 @@
     def __init__(self, argvalue):
         self.argname = "--target-hosts"
         self.argvalue = argvalue
         self.parsed_options = []
 
         self.parse_options()
 
+    @classmethod
+    def _normalize_hosts(cls, hosts):
+        # pylint: disable=import-outside-toplevel
+        from urllib.parse import unquote, urlparse
+
+        string_types = str, bytes
+        # if hosts are empty, just defer to defaults down the line
+        if hosts is None:
+            return [{}]
+
+        # passed in just one string
+        if isinstance(hosts, string_types):
+            hosts = [hosts]
+
+        out = []
+        # normalize hosts to dicts
+        for host in hosts:
+            if isinstance(host, string_types):
+                if "://" not in host:
+                    host = "//%s" % host
+
+                parsed_url = urlparse(host)
+                h = {"host": parsed_url.hostname}
+
+                if parsed_url.port:
+                    h["port"] = parsed_url.port
+
+                if parsed_url.scheme == "https":
+                    h["port"] = parsed_url.port or 443
+                    h["use_ssl"] = True
+
+                if parsed_url.username or parsed_url.password:
+                    h["http_auth"] = "%s:%s" % (
+                        unquote(parsed_url.username),
+                        unquote(parsed_url.password),
+                    )
+
+                if parsed_url.path and parsed_url.path != "/":
+                    h["url_prefix"] = parsed_url.path
+
+                out.append(h)
+            else:
+                out.append(host)
+        return out
+
     def parse_options(self):
         def normalize_to_dict(arg):
             """
             Return parsed comma separated host string as dict with "default" key.
             This is needed to support backwards compatible --target-hosts for single clusters that are not
             defined as a json string or file.
             """
-            # pylint: disable=import-outside-toplevel
-            from elasticsearch.client import _normalize_hosts
+            return {TargetHosts.DEFAULT: self._normalize_hosts(arg)}
 
-            return {TargetHosts.DEFAULT: _normalize_hosts(arg)}
+        parsed_options = to_dict(self.argvalue, default_parser=normalize_to_dict)
+        p_opts_copy = parsed_options.copy()
+        for cluster_name, nodes in p_opts_copy.items():
+            parsed_options[cluster_name] = self._normalize_hosts(nodes)
 
-        self.parsed_options = to_dict(self.argvalue, default_parser=normalize_to_dict)
+        self.parsed_options = parsed_options
 
     @property
     def all_hosts(self):
         """Return a dict with all parsed options"""
         return self.all_options
 
 
@@ -214,15 +261,14 @@
             # --client-options unset but multi-clusters used in --target-hosts? apply options defaults for all cluster names.
             self.parsed_options = {cluster_name: default_client_map for cluster_name in self.target_hosts.all_hosts.keys()}
         else:
             self.parsed_options = to_dict(self.argvalue, default_parser=ClientOptions.normalize_to_dict)
 
     @staticmethod
     def normalize_to_dict(arg):
-
         """
         When --client-options is a non-json csv string (single cluster mode),
         return parsed client options as dict with "default" key
         This is needed to support single cluster use of --client-options when not
         defined as a json string or file.
         """
         default_client_map = kv_to_map([ClientOptions.DEFAULT_CLIENT_OPTIONS])
```

## Comparing `esrally-2.7.1.dist-info/METADATA` & `esrally-2.8.0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: esrally
-Version: 2.7.1
+Version: 2.8.0
 Summary: Macrobenchmarking framework for Elasticsearch
 Project-URL: Changelog, https://github.com/elastic/rally/blob/master/CHANGELOG.md
 Project-URL: Documentation, https://esrally.readthedocs.io/
 Project-URL: Code, https://github.com/elastic/rally
 Project-URL: Issue tracker, https://github.com/elastic/rally/issues
 Author-email: Daniel Mitterdorfer <daniel.mitterdorfer@gmail.com>
 License: Apache License 2.0
@@ -19,39 +19,41 @@
 Classifier: Operating System :: POSIX
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3 :: Only
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
 Classifier: Topic :: System :: Benchmark
 Requires-Python: >=3.8
 Requires-Dist: certifi
 Requires-Dist: docker==6.0.0
-Requires-Dist: elasticsearch[async]==7.14.0
+Requires-Dist: elastic-transport==8.4.0
+Requires-Dist: elasticsearch[async]==8.6.1
 Requires-Dist: google-auth==1.22.1
 Requires-Dist: google-resumable-media[requests]==1.1.0
 Requires-Dist: ijson==2.6.1
 Requires-Dist: jinja2==2.11.3
 Requires-Dist: jsonschema==3.1.1
 Requires-Dist: markupsafe==2.0.1
-Requires-Dist: psutil==5.8.0
+Requires-Dist: psutil==5.9.4
 Requires-Dist: py-cpuinfo==7.0.0
 Requires-Dist: tabulate==0.8.9
 Requires-Dist: thespian==3.10.1
 Requires-Dist: urllib3==1.26.9
-Requires-Dist: yappi==1.3.3
+Requires-Dist: yappi==1.4.0
 Provides-Extra: develop
 Requires-Dist: boto3==1.18.46; extra == 'develop'
 Requires-Dist: furo==2022.06.21; extra == 'develop'
 Requires-Dist: github3-py==3.2.0; extra == 'develop'
 Requires-Dist: gitpython==3.1.30; extra == 'develop'
 Requires-Dist: nox==2022.11.21; extra == 'develop'
 Requires-Dist: pre-commit==2.20.0; extra == 'develop'
-Requires-Dist: pylint==2.14.5; extra == 'develop'
+Requires-Dist: pylint==2.17.0; extra == 'develop'
 Requires-Dist: pytest-asyncio==0.19.0; extra == 'develop'
 Requires-Dist: pytest-benchmark==3.4.1; extra == 'develop'
 Requires-Dist: pytest-httpserver==1.0.5; extra == 'develop'
 Requires-Dist: pytest==7.1.2; extra == 'develop'
 Requires-Dist: sphinx==5.1.1; extra == 'develop'
 Requires-Dist: tox==3.25.0; extra == 'develop'
 Requires-Dist: trustme==0.9.0; extra == 'develop'
```

## Comparing `esrally-2.7.1.dist-info/licenses/AUTHORS` & `esrally-2.8.0.dist-info/licenses/AUTHORS`

 * *Files 6% similar despite different names*

```diff
@@ -14,14 +14,15 @@
 Debanjan Choudhury
 Dennis Lawler
 Dimitrios Liappis
 Domenico Andreoli
 Evgenia Badiyanova
 Evgenia Badyanova
 Gavin Fowler
+Grzegorz Banasiak
 Guido Lena Cota
 Hendrik Muhs
 Honza Krl
 Irina Truong
 Itamar Syn-Hershko
 Jason Bryan
 Jason Tedor
```

## Comparing `esrally-2.7.1.dist-info/licenses/LICENSE` & `esrally-2.8.0.dist-info/licenses/LICENSE`

 * *Files identical despite different names*

## Comparing `esrally-2.7.1.dist-info/licenses/NOTICE.txt` & `esrally-2.8.0.dist-info/licenses/NOTICE.txt`

 * *Files 1% similar despite different names*

```diff
@@ -323,15 +323,15 @@
 COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
 IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
 CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 
 ======================================
 tabulate LICENSE
 ======================================
-Copyright (c) 2011-2017 Sergey Astanin
+Copyright (c) 2011-2020 Sergey Astanin and contributors
 
 Permission is hereby granted, free of charge, to any person obtaining
 a copy of this software and associated documentation files (the
 "Software"), to deal in the Software without restriction, including
 without limitation the rights to use, copy, modify, merge, publish,
 distribute, sublicense, and/or sell copies of the Software, and to
 permit persons to whom the Software is furnished to do so, subject to
@@ -1072,15 +1072,15 @@
 SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 ======================================
 urllib3 LICENSE
 ======================================
 MIT License
 
-Copyright (c) 2008-2020 Andrey Petrov and contributors (see CONTRIBUTORS.txt)
+Copyright (c) 2008-2020 Andrey Petrov and contributors.
 
 Permission is hereby granted, free of charge, to any person obtaining a copy
 of this software and associated documentation files (the "Software"), to deal
 in the Software without restriction, including without limitation the rights
 to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 copies of the Software, and to permit persons to whom the Software is
 furnished to do so, subject to the following conditions:
@@ -1677,15 +1677,204 @@
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
 
 ======================================
 yarl LICENSE
 ======================================
-   Copyright 2016-2021, Andrew Svetlov and aio-libs team
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
 
    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at
 
        http://www.apache.org/licenses/LICENSE-2.0
 
@@ -1935,34 +2124,35 @@
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
 
 ======================================
 jmespath LICENSE
 ======================================
+MIT License
+
 Copyright (c) 2013 Amazon.com, Inc. or its affiliates.  All Rights Reserved
 
-Permission is hereby granted, free of charge, to any person obtaining a
-copy of this software and associated documentation files (the
-"Software"), to deal in the Software without restriction, including
-without limitation the rights to use, copy, modify, merge, publish, dis-
-tribute, sublicense, and/or sell copies of the Software, and to permit
-persons to whom the Software is furnished to do so, subject to the fol-
-lowing conditions:
-
-The above copyright notice and this permission notice shall be included
-in all copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
-OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-
-ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT
-SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
-WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
-IN THE SOFTWARE.
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
 
 ======================================
 botocore LICENSE
 ======================================
 
                                  Apache License
                            Version 2.0, January 2004
```

## Comparing `esrally-2.7.1.dist-info/RECORD` & `esrally-2.8.0.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,72 +1,73 @@
 esrally/__init__.py,sha256=BcTCpTEgPGdWmbprdVXtxTRuO6D19VD9RqcMDealIM4,2665
-esrally/_version.py,sha256=yRpSH6mBb4BJgbFlT7rt8MSjCUW17Ycx0RziLf-lQLA,22
+esrally/_version.py,sha256=z6im3C9Qb6qiQIpaJdE4f9WQiCnFGSUQnQXDPw_dvDg,22
 esrally/actor.py,sha256=wsN6_eQL7t7gZY_DDL3tVOWleRaytsSRJw7FzPDEOSA,11356
 esrally/config.py,sha256=cW_5CtDMrQUHpv0WlA5gZ9yfXqYqG5rAKOjIHNwDK_o,12552
 esrally/exceptions.py,sha256=cNQYPNvAOf7RClUIRTmP0cM3EfsMLYuYrK45ju5sTto,2951
-esrally/log.py,sha256=39Gl70DhoFNkv6lcbF-LDix2Kr7O1B-ErTKwZvDQXR4,5106
-esrally/metrics.py,sha256=SlA4C1Z9cW9LFeTPdf9fz2yGG085ujETm8vISARnqB4,97601
+esrally/log.py,sha256=DRb8Ekj1LmK2Mgtpf7kAY_LjuLepa9vAzB2qc95Sffw,6626
+esrally/metrics.py,sha256=enL-6Gjc5e4Yv4LhG_i42bhLo3EIalwCuaf1GlzNrVk,99337
 esrally/min-es-version.txt,sha256=TH0kXR4cLHDibBWnkGcfWqyqz_-vvL6Ohs6d8LvYNb8,6
 esrally/paths.py,sha256=1P0XpLEj4gFL4F4fmwqs3A3PMd7jcSlKSRvIcr5fC6c,1593
 esrally/racecontrol.py,sha256=RD13i4nFKP70ofm7L3YdvHEAWW8Xw83sMyCkcZ17Idg,17239
-esrally/rally.py,sha256=nHSdxPOd5jBnw-VxbkMLwFI9dR0IgFvXnSdb-6XoZgU,57132
+esrally/rally.py,sha256=w3ioSLBHfzeLY9cZYoVGHymWGeh_1l8eR3k5A8C95Wk,57243
 esrally/rallyd.py,sha256=1dRyuPgF6LMfDIqW7lxLvG3BLSqz3lPWJ_HdaiGNs7c,4495
 esrally/reporter.py,sha256=MNIMoZc5lvV-978ylmB9U6yKKl5Z1xaAygGWnFnVyvQ,44396
-esrally/telemetry.py,sha256=AKD3Rej_gKqW14L4Q7EIQ6Vxp34daMcpo_YNstVfhs0,102963
+esrally/telemetry.py,sha256=atRQ5r1OaOfcIUP84q6eRdlViQe8815-wADBSmSwuN4,103668
 esrally/time.py,sha256=7Ebg8w_GPFUoEhQyTpAEH5hJ0IMFPNG-evuNNSdxof0,4110
 esrally/version.py,sha256=jOtMvKRS_n6iwGF16oTkqHaHc99CzjbkXExcRYuNdxU,2552
 esrally/client/__init__.py,sha256=HHMBC6pYMrByzO-6I2cJMM7jw9LrYHfDAKBuBLuKe_c,952
-esrally/client/asynchronous.py,sha256=-YL1TzK9F07tUmzhmDkJ8JD9l1EX5vCorNDvKZw8qZU,8982
-esrally/client/context.py,sha256=6hxbQcNT_p3WseNTxAsTHSg3WqOJZrt_plhXl7Im94g,3562
-esrally/client/factory.py,sha256=nj_9iITaShEQsToYXWHcWzwbuYWRxUU3QCqta_b5W4M,17328
-esrally/client/synchronous.py,sha256=moolXaIID1eMBLh9HVuRITtOYMgdrM9JNglHOgGUSCA,1011
+esrally/client/asynchronous.py,sha256=T6mwnfLhtWDH1RIcghsNHTmNtOkHNmmff_gWhwOA1Cg,14597
+esrally/client/common.py,sha256=YlNo8oB6kkPBE6oRALFkbqZTMszy_VxayQbJXGmVtuU,1851
+esrally/client/context.py,sha256=VerREiSU4mzBNrerVdLrQmv7QCzv4FQ64PK_Q89AU3U,3488
+esrally/client/factory.py,sha256=VnsXdi0VcJ2O1XS_PTdKfxKiES0mC2ssPFRc3u0wsro,20056
+esrally/client/synchronous.py,sha256=PhXYSOogBGaQ0qdcn4IqD0eZ_dOllShl1gHWq_xpNJ0,10287
 esrally/driver/__init__.py,sha256=HCWHt1sa5yoovkHv8M-QAA5-oLZm2f8NXXUnSmDzN90,954
-esrally/driver/driver.py,sha256=R-lxLYFo0tNJBA223dlmZX-VXC3wUoJxxWxIRpGFn7o,106918
-esrally/driver/runner.py,sha256=x9BrV2-JzvHSBuv5rUK5BvaBbGwz9WMDRTLMJnVnnGU,120168
+esrally/driver/driver.py,sha256=GDGnVZiQayxzF-uCwtpfVh5tdL8NW9va8jz1qUdCgM0,108166
+esrally/driver/runner.py,sha256=RafU-U0dtFAV2YDXuRTDpoqieRImLy5wWGKovucAkcY,120807
 esrally/driver/scheduler.py,sha256=Iz4LyMWB5DlbJzvWbQouAoz3NaSIwULSHOkGzVG5aMc,11927
 esrally/mechanic/__init__.py,sha256=LZGCoNbECzbaDt-WWzpNQVc24go80YO1OWOmH7bUi4E,1037
 esrally/mechanic/cluster.py,sha256=DTciT5hnfQRs0bZFJoBUBl4FP9MAH520mwMk8u3sGxk,1488
 esrally/mechanic/java_resolver.py,sha256=XgdBjuOLChptdLBzN8cNXvkBBYotyXSo9H-MMj66VTc,2157
 esrally/mechanic/launcher.py,sha256=4p7maXGI5O0r1VHGqNVNhhMe7y6UI7ufYoZGV2cMWXs,11339
 esrally/mechanic/mechanic.py,sha256=rT0kRCjbuLlMQMHrpQa9J_q1WEhXh8oHu2Q9mc-7QEM,29731
-esrally/mechanic/provisioner.py,sha256=1mcSOdEymVJbbuuLiUay6n9NTSPV3evss0V8oWpzS_s,22438
-esrally/mechanic/supplier.py,sha256=UORiJQb9v29uGX-eKINGaTQnf8MB1h7NGyHe_pNxNRA,44351
+esrally/mechanic/provisioner.py,sha256=B7GKLJMIDQOZx3pV-N-7QDiCv3XMpt6FJ0EjAYgnRNk,22380
+esrally/mechanic/supplier.py,sha256=3Ww39yQ60Ov3eNacqgKm-DBINEF2K53kvwwUxobtG4A,45447
 esrally/mechanic/team.py,sha256=L6fQQEXhbC97WmUIKERjQL0ryYO8hYwWyTf8JhQtaBg,22353
 esrally/resources/annotation-template.json,sha256=ctM0jpOkijJsKDWXkYG3AHagDfiU753tJ0vbkSEG9xI,832
 esrally/resources/docker-compose.yml.j2,sha256=SvOEeJfl_aA_-PnwHR43ya7IrB9ZOFslZ13VeG1kUX4,1021
-esrally/resources/logging.json,sha256=_z0L7JgQ-uYiA1V0sfaKVAGL8HHrUu6EZR_vL3du7R4,1340
-esrally/resources/metrics-template.json,sha256=ehOLP-ly4Msn93sWz1pMWLzCloyzI8f-e1ITeGi6iy4,1656
+esrally/resources/logging.json,sha256=lX5XyjssfcDhYFJHbFAt0fJHew8xMhgdinecSv1UO-4,1466
+esrally/resources/metrics-template.json,sha256=QzcAQCu4LexN6lzFdnw9x2br7b6L_NLCY7F_dq-h-8M,1697
 esrally/resources/races-template.json,sha256=T1PS8iBkVbmDnVzRgTzuq0EGaGMlv4tGa8ss3pAcaXA,1298
 esrally/resources/rally.ini,sha256=aCkLc9-zQwX3k7rvCwr_ajUt_F5ImRLw2wrRmj_cCto,674
 esrally/resources/results-template.json,sha256=h9ZCrJlLJ7_6YOV9c2-uJD2EIpyIDRO_rqbMolnUXuI,1978
 esrally/resources/track-schema.json,sha256=UI-L3cUYRqp-mXY6pLne7HgdsaHQdkYJgnvpQXx5y-Y,24238
 esrally/resources/track.json.j2,sha256=1ZFfR6TLXG0lGW0bO1X2pMsfCjY4-jgH0LH_mVH-5_o,1665
 esrally/track/__init__.py,sha256=uirAng0EsUgOoKBLrmURjmqTjuz1vb6cBA772Udl_cw,1024
 esrally/track/loader.py,sha256=mEfshQuzVfcY_PY-skwtWMOS5jiwU8S5VLdroTRRmvs,80995
 esrally/track/params.py,sha256=UBqX3RX5cjysklSS3IU6sMnRUJ1Qd9p9kdcB60q6Jw8,57742
 esrally/track/track.py,sha256=sYQoWMEWB38Y9-r_sKQK2GmncFHp8DqogcZG4jeSIvQ,39369
 esrally/tracker/__init__.py,sha256=tyGFS-PfSC7QtTHliJzbvzW_rMTEWOqxmV4Mpw3_F4k,774
 esrally/tracker/corpus.py,sha256=u542r36-vPgnzDl1E7AChaaoa_o2vg8GACoVArC8_yo,3845
 esrally/tracker/index.py,sha256=Zrrlu4DstZMMSCeQCukLiM_6T1JW1tvCw-PLr1KU9eM,4930
-esrally/tracker/tracker.py,sha256=lIWur296I6sCKURZldpcAiFj4I7thlymSS9_oIX_RJ4,4503
+esrally/tracker/tracker.py,sha256=QQvkcNqvENyodotopgJ0GhWv-9I4ITxgaov6gSxh5Go,4517
 esrally/utils/__init__.py,sha256=tyGFS-PfSC7QtTHliJzbvzW_rMTEWOqxmV4Mpw3_F4k,774
 esrally/utils/collections.py,sha256=i4UktbOLRxpwrQVaGGiTdaLWFox3h434hoJgf6C5N2s,1657
 esrally/utils/console.py,sha256=RXubmIpxDHhVZgy6gBl2k8XQCiMnw9nR33ZXIKktIic,7448
 esrally/utils/convert.py,sha256=tWqN5ytbriMWIb2ogA1ctif_fGS88HuH-yA0IEJkrN4,2835
-esrally/utils/git.py,sha256=6Mw1dauJxpMQGPIB0t5YVpcFypw_yxYR1IFwrR0Kx4I,5948
+esrally/utils/git.py,sha256=KqnYRcALfoIOUmed69orTHX6Ye-lUYTVu1j57Knc4VY,6139
 esrally/utils/io.py,sha256=ktZqWkpTRYhIweXe4EkrpNLhawYrNVPrkpSrjKpZAUg,19710
 esrally/utils/jvm.py,sha256=rZq0vE-gthoPtmNcIgLiPqzWTjgD4n5bxUat-cWp8zY,7112
 esrally/utils/modules.py,sha256=ZNsUCmFhKd6PSfe_u-4TQSqfdhwdARp0uUAEmFoqGKI,5026
-esrally/utils/net.py,sha256=H04MGY2S6iHEjWdr7YBmzAWqvPAExAIOAgH9VpMEe4s,12434
-esrally/utils/opts.py,sha256=KrYys6gHbiu29bY3HkhfaK6IeZD6GtTU3CM8ELfEZI0,7734
+esrally/utils/net.py,sha256=s9dzrUG0wXb-10R9V5AnqBLb8lUH5R6XqrdloKDSiz0,12648
+esrally/utils/opts.py,sha256=t-9on_dEXFyCT65TExbULfnb1mYBJ_DZK5QpPH0YcJE,9245
 esrally/utils/process.py,sha256=JRd2pWZ8RV4tws80Zo0ytCgjK8fGMY0gBiSAWGptReQ,5562
 esrally/utils/repo.py,sha256=pKdlumBDwEqeBPONQj8bhsNrt7Y-xkMsL05K7aIblSw,6398
 esrally/utils/sysstats.py,sha256=B5iS_Jtq8hDL9ikaduru7jSmQKIeLhkDD-NuoZbaVfk,2711
 esrally/utils/versions.py,sha256=MZQT20H2wjhPXpOiGiLvOWSy6DqtvydtP4rUmFLnduM,8594
-esrally-2.7.1.dist-info/METADATA,sha256=0TNq68emLjcRRGJFpLG01r89jvVuqKS5xts0p9-NHpM,10303
-esrally-2.7.1.dist-info/WHEEL,sha256=Fd6mP6ydyRguakwUJ05oBE7fh2IPxgtDN9IwHJ9OqJQ,87
-esrally-2.7.1.dist-info/entry_points.txt,sha256=gfr1vE_2rGilGggKFXyaVxuEdhC0kQLeZ0yVbNKYyG0,78
-esrally-2.7.1.dist-info/licenses/AUTHORS,sha256=74TtEa3X74rddMElCkuUMxybDaCPfZb_ZnpxDj8jWvI,1001
-esrally-2.7.1.dist-info/licenses/LICENSE,sha256=XfKg2H1sVi8OoRxoisUlMqoo10TKvHmU_wU39ks7MyA,10143
-esrally-2.7.1.dist-info/licenses/NOTICE,sha256=C4b9iQVByQXC8NhRbG4Kyun29czOPkcK_xJ6IF9NTzs,45
-esrally-2.7.1.dist-info/licenses/NOTICE.txt,sha256=f7LbtJ8qOX5QATWozcR7hDJYxtZKgaEMVlN2QVH-mhw,125575
-esrally-2.7.1.dist-info/RECORD,,
+esrally-2.8.0.dist-info/METADATA,sha256=GpSMJO9QjNZuMei8eDguwIRrQ6fSlNSEK5UO8QoAdR8,10393
+esrally-2.8.0.dist-info/WHEEL,sha256=KGYbc1zXlYddvwxnNty23BeaKzh7YuoSIvIMO4jEhvw,87
+esrally-2.8.0.dist-info/entry_points.txt,sha256=gfr1vE_2rGilGggKFXyaVxuEdhC0kQLeZ0yVbNKYyG0,78
+esrally-2.8.0.dist-info/licenses/AUTHORS,sha256=Qv6QnRlN9mwL62MgN_oQ5NPQXhHWemfea3XRHDeLk8g,1019
+esrally-2.8.0.dist-info/licenses/LICENSE,sha256=XfKg2H1sVi8OoRxoisUlMqoo10TKvHmU_wU39ks7MyA,10143
+esrally-2.8.0.dist-info/licenses/NOTICE,sha256=C4b9iQVByQXC8NhRbG4Kyun29czOPkcK_xJ6IF9NTzs,45
+esrally-2.8.0.dist-info/licenses/NOTICE.txt,sha256=wXFC4aI8jjWoAXUvZJuwdZ3miTI50lOEB0R03-V74SM,136348
+esrally-2.8.0.dist-info/RECORD,,
```

